id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:27722,Safety,detect,detection,27722,"nforce; billing limits, we have the worker send us the job attempts it has; running as well as the current time approximately every 1 minute. We; then update the rollup_time for each job which is guaranteed to be; greater than or equal to the start time and less than or equal to the; end time. The rollup time is then used in billing calculations to; figure out the duration the job has been running thus far. Quota Exhaustion; ^^^^^^^^^^^^^^^^. There is a mechanism in GCP by which we monitor our current quotas and; assign jobs that can be run in any region to a different region if; we've exceeded our quota. Cloud Price Monitoring; ^^^^^^^^^^^^^^^^^^^^^^. We periodically call the corresponding cloud APIs to get up to date; billing information and update the current rates of each product used; accordingly. Case Studies; ^^^^^^^^^^^^. This section describes some of the emergent behaviors that occur with; multiple components acting together. Preemption; """""""""""""""""""". Preemption is a special case of ""dead node"" detection. 1. The ``batch-driver`` service has a background loop in ``instance_collection/base.py``; that checks the status of all instances. - If a running instance is not reachable then a ``failed_request_count`` is incremented.; - If the ``failed_request_count`` exceeds a threshold, the instance is deleted.; - As part of deletion, the database's ``deactivate_instance`` stored procedure is called.; - During the stored procedure, any jobs that are running on the instance are marked as ``Ready``. 2. The newly ``Ready`` jobs will be picked up by the next iteration of the schedule; loop (eg in``instance_collection/pool.py``). - The scheduler is also what creates a new attempt record in the database for the new attempt. Database; --------. The batch database has a series of tables, triggers, and stored; procedures that are used to keep track of the state of billing; projects, batches, jobs, attempts, resources, and instances. We; previously discussed how the database oper",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:29497,Safety,avoid,avoid,29497,"at are used to keep track of the state of billing; projects, batches, jobs, attempts, resources, and instances. We; previously discussed how the database operations SJ, MJS, and MJC; work. There are three key principles in how the database is structured. 1. Any values that are dynamic should be separated from tables that; have static state. For example, to represent that a batch is; cancelled, we have a separate ``batches_cancelled`` table rather; than adding a cancelled field to the ``batches`` table. 2. Any tables with state that is updated in parallel should be; ""tokenized"" in order to reduce contention for updating rows. For; example, when keeping track of the number of running jobs per user; per instance collection, we'll need to update this count for every; schedule job operation. If there is only one row representing this; value, we'll end up serializing the schedule operations as each one; waits for the exclusive write lock. To avoid this, we have up to; 200 rows per value we want to represent where each row has a unique; ""token"". This way concurrent transactions can update rows; simultaneously and the probability of serialized writes is; equivalent to the birthday problem in mathematics. Note that there; is a drawback to this approach in that queries to obtain the actual; value are more complicated to write as they include an aggregation; and the number of rows to store this in the database can make; queries slower and data more expensive to store. Key tables have triggers on them to support billing, job state counts,; and fast cancellation which will be described in more detail below. Billing; ^^^^^^^. Billing is implemented by keeping track of the resources each attempt; uses as well as the duration of time each attempt runs for. It is; trivial to write a query to compute the cost per attempt or even per; job. However, the query speed is linear in the number of total; attempts when computing the cost for a batch by scanning over the; entire table which is",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:33062,Safety,avoid,avoiding,33062," The updates to the ``user_inst_coll_resources``; table happen in a trigger on the ``jobs`` table. Cancellation; ^^^^^^^^^^^^. A user can trigger a cancellation of a batch via the cancel button in; the UI or a REST request. The batch system also monitors how much has; been spent in a billing project. Once that limit has been exceeded,; all running batches in the billing project are cancelled. Cancellation is the most complicated part of the Batch system. The; goal is to make cancellation as fast as possible such that we don't; waste resources spinning up worker VMs and running user jobs that are; ultimately going to get cancelled. Therefore, we need a way of quickly; notifying the autoscaler and scheduler to not spin up resources or; schedule jobs for batches that have been cancelled. We set a ""flag"" in; the database indicating the batch has been cancelled via the; ``batches_cancelled`` table. This allows the query the scheduler; executes to find Ready jobs to run to not read rows for jobs in batches that; have been cancelled thereby avoiding scheduling them in the first; place. We also execute a similar query for the autoscaler. The only; place where we need to quickly know how many cores we have that are; ready and have not been cancelled is in the fair share calculation via; the ``user_inst_coll_resources`` table. To accomplish a fast update of; this table, we currently keep track of the number of **cancellable**; resources per batch in a tokenized table; ``batch_inst_coll_cancellable_resources`` such as the number of; cancellable ready cores. When we execute a cancellation operation, we; quickly count the number of cancellable ready cores or other similar; values from the ``batch_inst_coll_cancellable_resources`` table and; subtract those numbers from the ``user_inst_coll_resources`` table to; have an O(1) update such that the fair share computation can quickly; adjust to the change in demand for resources. The background canceller loops iterate through the cance",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:3529,Security,access,accessible,3529,"ices and cloud infrastructure components:. - Kubernetes Services; - Gateway (gateway); - Internal Gateway (internal-gateway); - Auth (auth); - Auth Driver (auth-driver); - Batch Front End (batch); - Batch Driver (batch-driver); - Worker VMs; - MySQL Database; - Cloud Storage; - Container Registry. Kubernetes Services; -------------------. Gateway; ^^^^^^^. Gateway is a Kubernetes service and associated cloud-provider-managed; external load balancer. It is associated with a statically; known external IP Address. This is the entry point in which external; users send requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes names",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:3874,Security,authenticat,authenticating,3874,"d cloud-provider-managed; external load balancer. It is associated with a statically; known external IP Address. This is the entry point in which external; users send requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:3952,Security,access,accessing,3952,"d cloud-provider-managed; external load balancer. It is associated with a statically; known external IP Address. This is the entry point in which external; users send requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4062,Security,authenticat,authentication,4062,"d requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4079,Security,authoriz,authorization,4079,"d requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4163,Security,authoriz,authorization,4163,"obs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4576,Security,authoriz,authorization,4576,"int is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except for /healthcheck), the; Batch service sends a request to the Auth service to see if the token; submitted in the request is valid and in exchange get information; about the user. The Batch Front End can also send requests to the; Batch Driver notifying the driver that a batch has been created or; needs to be cancelled (""push notification""",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4657,Security,certificate,certificates,4657,"int is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except for /healthcheck), the; Batch service sends a request to the Auth service to see if the token; submitted in the request is valid and in exchange get information; about the user. The Batch Front End can also send requests to the; Batch Driver notifying the driver that a batch has been created or; needs to be cancelled (""push notification""",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4694,Security,access,access,4694,"int is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except for /healthcheck), the; Batch service sends a request to the Auth service to see if the token; submitted in the request is valid and in exchange get information; about the user. The Batch Front End can also send requests to the; Batch Driver notifying the driver that a batch has been created or; needs to be cancelled (""push notification""",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:5219,Security,authenticat,authenticated,5219," requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except for /healthcheck), the; Batch service sends a request to the Auth service to see if the token; submitted in the request is valid and in exchange get information; about the user. The Batch Front End can also send requests to the; Batch Driver notifying the driver that a batch has been created or; needs to be cancelled (""push notification""). The application is stateless; and 3 copies are running simultaneously. The Front End; extensively updates and queries the MySQL database to obtain the; information necessary to fulfill user requests. It also writes job; specs to cloud storage for use downstream by the worker VMs. Batch Driver; ^^^^^^^^^^^^. The Batch Driver is a Kubernetes service responsible for provisioning; worker VMs in response to demand, scheduling jobs on free worker VMs,; and cancelling jobs that no longer should be run. The Driver is; stateless, but only 1 copy can be running at a single time. This is; because our current strategy for knowing how many free cores per VM; are available requires a single process to accurately u",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:7726,Security,certificate,certificates,7726,"ing TLS handshakes so as to reduce the CPU; load on the actual Python web server. Worker VMs; ----------. Worker VMs are virtual machines that are created outside of the; Kubernetes cluster. They share a network with the Kubernetes VMs, but; not with the Kubernetes pods. They are created with a default service; account that has permissions to read and write files to cloud storage; such as job specs and job logs as well as delete VMs (so it can delete; itself). Virtual machines are created with a preconfigured boot disk; image that has Docker preinstalled. Startup scripts then initialize; the worker VM, download the worker server application image from a; container registry, and then create the worker Docker container. Once; the worker container is running, it notifies the Batch Driver that it; is active and starts executing jobs. MySQL Database; --------------. All Batch and Auth state is stored in a cloud-provider managed MySQL; database. We use SSL certificates to secure communication between; Kubernetes services and the database. Worker VMs cannot talk directly; to the database. Cloud Storage; -------------. Users store the data they want to compute on in Cloud Storage (Google; Cloud Storage or Azure Blob Storage). All Batch created files such as; user job specs, job log files, job status files, and job resource; usage monitoring files are stored in cloud storage. Container Registry; ------------------. Container images used to execute user jobs as well as the images used; in our Kubernetes services are stored in a cloud provider managed; Container Registry (Google Artifact Registry and Azure Container; Registry). Terraform; ---------. TBD. Bootstrapping; -------------. TBD. Application Details; ===================. Batch Lifecycle; ---------------. 1. A user submits a request to the Batch front end service to create a; batch along with job specifications.; 2. The Batch front end service records the batch and job information; into a MySQL database and writes the j",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:7742,Security,secur,secure,7742,"ing TLS handshakes so as to reduce the CPU; load on the actual Python web server. Worker VMs; ----------. Worker VMs are virtual machines that are created outside of the; Kubernetes cluster. They share a network with the Kubernetes VMs, but; not with the Kubernetes pods. They are created with a default service; account that has permissions to read and write files to cloud storage; such as job specs and job logs as well as delete VMs (so it can delete; itself). Virtual machines are created with a preconfigured boot disk; image that has Docker preinstalled. Startup scripts then initialize; the worker VM, download the worker server application image from a; container registry, and then create the worker Docker container. Once; the worker container is running, it notifies the Batch Driver that it; is active and starts executing jobs. MySQL Database; --------------. All Batch and Auth state is stored in a cloud-provider managed MySQL; database. We use SSL certificates to secure communication between; Kubernetes services and the database. Worker VMs cannot talk directly; to the database. Cloud Storage; -------------. Users store the data they want to compute on in Cloud Storage (Google; Cloud Storage or Azure Blob Storage). All Batch created files such as; user job specs, job log files, job status files, and job resource; usage monitoring files are stored in cloud storage. Container Registry; ------------------. Container images used to execute user jobs as well as the images used; in our Kubernetes services are stored in a cloud provider managed; Container Registry (Google Artifact Registry and Azure Container; Registry). Terraform; ---------. TBD. Bootstrapping; -------------. TBD. Application Details; ===================. Batch Lifecycle; ---------------. 1. A user submits a request to the Batch front end service to create a; batch along with job specifications.; 2. The Batch front end service records the batch and job information; into a MySQL database and writes the j",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:10367,Security,authoriz,authorized,10367,"ainer, uploads any logs and output files that have been; generated, and then notifies the Batch driver that the job has; completed.; 7. Once all jobs have completed, the batch is set to complete in the; database. Any callbacks that have been specified on batch; completion are called.; 8. Meanwhile, the user can find the status of their batch through the; UI or using a Python client library to get the batch status, cancel; the batch, list the jobs in the batch and their statuses, and wait; for the batch or an individual job to complete. The implementation; of the wait operation is by continuously polling the Batch Front; End until the batch state is ""complete"". Data Model; ----------. The core concepts in the Batch data model are billing projects,; batches, jobs, updates, attempts, and resources. A **billing project** is a mechanism for cost accounting, cost control, and; enabling the ability to share information about batches and jobs; across users. Each billing project has a list of authorized users and; a billing limit. Any users in the billing project can view information; about batches created in that billing project. Developers can; add/delete users in a billing project and modify billing limits. Right; now, these operations are manually done after a Batch user submits a; formal request to the Hail team. Note that the Hail billing project is; different than a GCP billing project. A **batch** is a set of **jobs**. Each batch is associated with a; single billing project. A batch also consists of a set of; **updates**. Each update contains a distinct set of jobs. Updates are; distinct submissions of jobs to an existing batch in the system. They; are used as a way to add jobs to a batch. A batch is always created; with 0 updates and 0 total jobs. To add jobs to a batch, an update; must be created with an additional API call and the number of jobs in; the update must be known at the time of the API call. The reason for; this is because an update reserves a block of ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:14452,Security,expose,exposes,14452," creating the container, an out; of memory error, or a Batch bug (ex: user tries to use a nonexistent; image). The allowed state transitions are: Pending -> Ready Ready ->; {Creating, Running, Cancelled} Creating -> {Running, Cancelled}; Running -> {Success, Failed, Error, Cancelled}. A job's initial state depends on the states of its parent jobs. If it; has no parent jobs, its initial state is Ready. A batch can be in one of the following states:. - completed: All jobs are in a completed state {Success, Failed,; Error, Cancelled}; - running: At least one job is in a non-completed state {Pending,; Ready, Running}. The batch and job states are critical for database performance and; must be indexed appropriately. Batch Front End; ---------------. The Batch Front End service (batch) is a stateless web service that; handles requests from the user. The front end exposes a REST API; interface for handling user requests such as creating a batch,; updating a batch, creating jobs in a batch, getting the status of a; batch, getting the status of a job, listing all the batches in a; billing project, and listing all of the jobs in a batch. There are; usually 3 copies of the batch front end service running at a given; time to be able to handle requests to create jobs in a batch with a; high degree of parallelism. This is necessary for batches with more; than a million jobs. Flow for Creating and Updating Batches; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The following flow is used to create a new batch or update an existing; batch with a set of job specifications:. 1. The client library submits a POST request to create a new batch at; ``/api/v1alpha/batches/create``. A new entry for the batch is; inserted into the database along with any associated tables. For; example, if a user provides attributes (labels) on the batch, that; information is populated into the ``batch_attributes`` table. A new; update is also created for that batch if the request contains a; reservation with more",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:3847,Testability,log,logging,3847,"d cloud-provider-managed; external load balancer. It is associated with a statically; known external IP Address. This is the entry point in which external; users send requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:3903,Testability,log,logged,3903,"d cloud-provider-managed; external load balancer. It is associated with a statically; known external IP Address. This is the entry point in which external; users send requests to the Batch system such as submitting batches and; getting information on their jobs. There is a an Envoy server behind; the load balancer that forwards requests to the appropriate service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:4253,Testability,log,login,4253," service. Internal Gateway; ^^^^^^^^^^^^^^^^. Internal Gateway is a Kubernetes service and associated cloud-provider-managed; internal load balancer. Unlike the Gateway, the Internal; Gateway is associated with a statically known **internal** IP address; that is only accessible from virtual machines within our private; network. This endpoint is how Batch worker VMs are able to talk to the; Batch Driver Kubernetes Service directly without going through the public; internet. Auth / Auth-Driver; ^^^^^^^^^^^^^^^^^^. The Auth Kubernetes service is responsible for creating new users,; logging in existing users, authenticating requests from logged in; users, verifying developer status for accessing protected services; like a batch deployment in a developer namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:5084,Testability,log,logs,5084,"veloper namespace. We will soon be; changing how authentication / authorization is implemented. Currently,; for REST API requests, a user provides an authorization bearer header; with a Hail-issued token. This token is generated when users login and; has a default expiration date for 30 days. UI web requests have an; associated cookie that includes the token. The Auth Driver service is; responsible for creating new user resources such as service accounts,; secondary Kubernetes namespaces for developers, Kubernetes secrets; that store the user's active Hail authorization token and their Google; service account or Azure service principal certificates, which allows; users to access their resources required to execute jobs such as; Docker images and data stored in Google Cloud Storage or Azure Blob; Storage. When a user is deleted, their corresponding resources are; deleted as well. Batch Front End; ^^^^^^^^^^^^^^^. The Batch Front End is a Kubernetes service responsible for handling; user requests such as creating batches, updating batches, and viewing; job logs. How the Batch Front End Python service works is described in; more detail later in this document. When users submit requests to; authenticated endpoints (everything except for /healthcheck), the; Batch service sends a request to the Auth service to see if the token; submitted in the request is valid and in exchange get information; about the user. The Batch Front End can also send requests to the; Batch Driver notifying the driver that a batch has been created or; needs to be cancelled (""push notification""). The application is stateless; and 3 copies are running simultaneously. The Front End; extensively updates and queries the MySQL database to obtain the; information necessary to fulfill user requests. It also writes job; specs to cloud storage for use downstream by the worker VMs. Batch Driver; ^^^^^^^^^^^^. The Batch Driver is a Kubernetes service responsible for provisioning; worker VMs in response to dema",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:7171,Testability,log,logs,7171,"s, but only 1 copy can be running at a single time. This is; because our current strategy for knowing how many free cores per VM; are available requires a single process to accurately update the; number of free cores when we schedule a job on a VM. The Driver; communicates with worker VMs when it schedules or unschedules; jobs. The worker VMs then communicate back to the Driver when a worker; is ready to activate itself and start receiving work, notifying a job; has been completed, and deactivating itself when it is idle. The Batch; Driver has a second container inside the pod that is an Envoy server; responsible for maintaining TLS handshakes so as to reduce the CPU; load on the actual Python web server. Worker VMs; ----------. Worker VMs are virtual machines that are created outside of the; Kubernetes cluster. They share a network with the Kubernetes VMs, but; not with the Kubernetes pods. They are created with a default service; account that has permissions to read and write files to cloud storage; such as job specs and job logs as well as delete VMs (so it can delete; itself). Virtual machines are created with a preconfigured boot disk; image that has Docker preinstalled. Startup scripts then initialize; the worker VM, download the worker server application image from a; container registry, and then create the worker Docker container. Once; the worker container is running, it notifies the Batch Driver that it; is active and starts executing jobs. MySQL Database; --------------. All Batch and Auth state is stored in a cloud-provider managed MySQL; database. We use SSL certificates to secure communication between; Kubernetes services and the database. Worker VMs cannot talk directly; to the database. Cloud Storage; -------------. Users store the data they want to compute on in Cloud Storage (Google; Cloud Storage or Azure Blob Storage). All Batch created files such as; user job specs, job log files, job status files, and job resource; usage monitoring files are sto",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:8052,Testability,log,log,8052,"account that has permissions to read and write files to cloud storage; such as job specs and job logs as well as delete VMs (so it can delete; itself). Virtual machines are created with a preconfigured boot disk; image that has Docker preinstalled. Startup scripts then initialize; the worker VM, download the worker server application image from a; container registry, and then create the worker Docker container. Once; the worker container is running, it notifies the Batch Driver that it; is active and starts executing jobs. MySQL Database; --------------. All Batch and Auth state is stored in a cloud-provider managed MySQL; database. We use SSL certificates to secure communication between; Kubernetes services and the database. Worker VMs cannot talk directly; to the database. Cloud Storage; -------------. Users store the data they want to compute on in Cloud Storage (Google; Cloud Storage or Azure Blob Storage). All Batch created files such as; user job specs, job log files, job status files, and job resource; usage monitoring files are stored in cloud storage. Container Registry; ------------------. Container images used to execute user jobs as well as the images used; in our Kubernetes services are stored in a cloud provider managed; Container Registry (Google Artifact Registry and Azure Container; Registry). Terraform; ---------. TBD. Bootstrapping; -------------. TBD. Application Details; ===================. Batch Lifecycle; ---------------. 1. A user submits a request to the Batch front end service to create a; batch along with job specifications.; 2. The Batch front end service records the batch and job information; into a MySQL database and writes the job specifications to cloud; storage.; 3. The Batch driver notices that there is work available either; through a push request from the Batch front end or by polling the; state in the MySQL database and spins up worker VMs.; 4. The worker VMs startup and notify the Batch driver they are active; and have resource",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:9387,Testability,log,logs,9387," Container Registry (Google Artifact Registry and Azure Container; Registry). Terraform; ---------. TBD. Bootstrapping; -------------. TBD. Application Details; ===================. Batch Lifecycle; ---------------. 1. A user submits a request to the Batch front end service to create a; batch along with job specifications.; 2. The Batch front end service records the batch and job information; into a MySQL database and writes the job specifications to cloud; storage.; 3. The Batch driver notices that there is work available either; through a push request from the Batch front end or by polling the; state in the MySQL database and spins up worker VMs.; 4. The worker VMs startup and notify the Batch driver they are active; and have resources to run jobs.; 5. The Batch driver schedules jobs to run on the active workers.; 6. The worker VM downloads the job specification from cloud storage,; downloads any input files the job needs from cloud storage, creates; a container for the job to execute in, executes the code inside the; container, uploads any logs and output files that have been; generated, and then notifies the Batch driver that the job has; completed.; 7. Once all jobs have completed, the batch is set to complete in the; database. Any callbacks that have been specified on batch; completion are called.; 8. Meanwhile, the user can find the status of their batch through the; UI or using a Python client library to get the batch status, cancel; the batch, list the jobs in the batch and their statuses, and wait; for the batch or an individual job to complete. The implementation; of the wait operation is by continuously polling the Batch Front; End until the batch state is ""complete"". Data Model; ----------. The core concepts in the Batch data model are billing projects,; batches, jobs, updates, attempts, and resources. A **billing project** is a mechanism for cost accounting, cost control, and; enabling the ability to share information about batches and jobs; across user",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:21551,Testability,log,logic,21551,"ere are many; jobs with a short execution time. Due to differences in resource prices across regions and extra fees; for inter-region data transfer, the autoscaler needs to be aware of; the regions a job can run in when scaling up the cluster in order to; avoid suboptimal cluster utilization or jobs not being able to be; scheduled due to a lack of resources. The current autoscaler works by running every 15 seconds and executing; the following operations to determine the optimal number of instances; to spin up per region:. 1. Get the fair share resource allocations for each user across all; regions and figure out the share for each user out of 300 (this; represents number of scheduling opportunities this user gets; relative to other users).; 2. For every user, sort the ""Ready"" jobs by regions the job can run in; and take the first N jobs where N is equal to the user share; computed in (1) multiplied by the autoscaler window, which is; currently set to 2.5 minutes. The logic behind this number is it; takes ~2.5 minutes to spin up a new instance so we only want to; look at a small window at a time to avoid spinning up too many; instances. It also makes this query feasible to set a limit on it; and only look at the head of the job queue.; 3. Take the union of the result sets for all of the users in (2) in; fair share order. Do another pass over the result set where we; assign each job a scheduling iteration which represents an estimate; of which iteration of the scheduler that job will be scheduled in; assuming the user's fair share.; 4. Sort the result set by user fair share and the scheduling iteration; and the regions that job can run in. Aggregate the free cores by; regions in order in the result set. This becomes the number of free; cores to use when computing the number of required instances and; the possible regions the instance can be spun up in. Scheduler; ^^^^^^^^^. The scheduler finds the set of jobs to schedule by iterating through; each user in fair share o",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:22808,Testability,log,logic,22808,"fair share order. Do another pass over the result set where we; assign each job a scheduling iteration which represents an estimate; of which iteration of the scheduler that job will be scheduled in; assuming the user's fair share.; 4. Sort the result set by user fair share and the scheduling iteration; and the regions that job can run in. Aggregate the free cores by; regions in order in the result set. This becomes the number of free; cores to use when computing the number of required instances and; the possible regions the instance can be spun up in. Scheduler; ^^^^^^^^^. The scheduler finds the set of jobs to schedule by iterating through; each user in fair share order and then scheduling jobs with a ""Ready""; state until the user's fair share allocation has been met. The result; set for each user is sorted by regions so that the scheduler matches; what the autoscaler is trying to provision for. The logic behind; scheduling is not very sophisticated so it is possible to have a job; get stuck if for example it requires 8 cores, but two instances are; live with 4 cores each. Once the scheduler has assigned jobs to their respective instances,; the scheduler performs the work necessary to grab any secrets from; Kubernetes, update the job state and add an attempt in the database,; and then communicate with the worker VM to start running the; job. There must be a timeout on this scheduling attempt that is short; (1 second) in order to ensure that a delay in one job doesn't cause; the scheduler to get stuck waiting for that one job to be finished; scheduling. We wait at the end of the scheduling iteration for all; jobs to finish scheduling. If we didn't wait, then we might try and; reschedule the same job multiple times before the original operation; to schedule the job in the database completes. Job State Updates; ^^^^^^^^^^^^^^^^^. There are three main job state update operations:; - SJ: Schedule Job; - MJS: Mark job started; - MJC: Mark job completed. SJ is a database ",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:577,Usability,user-friendly,user-friendly,577,"============; Batch Design; ============. .. sectnum::; .. contents::. ********; Overview; ********. Hail Batch is a multi-tenant batch job processing system. The Hail; team maintains deployments in GCP and Azure. There are also a few; deployments outside of the control of the Hail team as well as alpha; support in Terra. Hail Batch has two main use cases: (1) a batch job; processing system that executes arbitrary bash or Python code in; containerized environments that are generated using a Python client; library that handles file localization and job dependencies in a; user-friendly manner (hailtop.batch) and (2) as the backend for; running Hail Query on Batch (QoB) inside containers running Hail team; approved JVM byte code. Typical users of hailtop.batch are looking to execute code for a; stand-alone scientific tool that can be run massively in parallel such; as across samples in a dataset and regions in a genome. Their; workloads usually consist of a single scatter layer with no; dependencies between jobs with sizes on the order of 100s to 100Ks of; jobs. The largest batch that has been processed by the Hail Batch; system is ~16 million jobs. Likewise, QoB consists of a single,; nonpreemptible driver job and subsequent sets of updates of jobs to; the directed acyclic graph (DAG) for subsequent stages of worker; jobs. There is a single job per partition within a stage. The number; of jobs within a stage can be on the order of 100K jobs. ****************************; How the Current System Works; ****************************. The Batch system is a set of services and infrastructure components; that work in concert to allow users to submit requests describing; workloads or sets of jobs to run and then executes the jobs on a set; of worker VMs. There is both a UI and a REST API for interacting with; Batch. The infrastructure required for a working Hail Batch system; consists of a Kubernetes cluster, a container registry, blob storage,; a MySQL database, and virtual m",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst:20068,Usability,simpl,simplest,20068,"h the user who has the fewest cores running. We then allocate as; many cores as possible that are live in the cluster until we reach the; number of cores the next user has currently running. We then divide up; the remaining cores equally amongst the two users until we reach the; number of cores the next user has running. We repeat until we have; either exhausted all free cores in the cluster or have satisfied all; user resource requests. The query to get the number of ready cores in the fair; share algorithm is fast because we aggregate across a global table; ``user_inst_coll_resources`` that has a limited number of rows; maintaining counts of the number of ready cores per instance; collection and user. Autoscaler; ^^^^^^^^^^. At a high level, the autoscaler is in charge of figuring out how many; worker VMs are required to run all of the jobs that are ready to run; without wasting resources. The simplest autoscaler takes the number of; ready cores total across all users and divides up that amount by the; number of cores per worker to get the number of instances that are; required. It then spins up a configurable number of instances each; time the autoscaler runs to avoid cloud provider API rate limits. This; approach works well for large workloads that have long running; jobs. However, the autoscaler can produce more cores than the; scheduler can keep busy with work. This happens when there are many; jobs with a short execution time. Due to differences in resource prices across regions and extra fees; for inter-region data transfer, the autoscaler needs to be aware of; the regions a job can run in when scaling up the cluster in order to; avoid suboptimal cluster utilization or jobs not being able to be; scheduled due to a lack of resources. The current autoscaler works by running every 15 seconds and executing; the following operations to determine the optimal number of instances; to spin up per region:. 1. Get the fair share resource allocations for each user across",MatchSource.DOCS,dev-docs/services/batch/design.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/design.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst:486,Availability,down,downsample,486,".. _sec-aggregators:. Aggregators; ===========. The ``aggregators`` module is exposed as ``hl.agg``, e.g. ``hl.agg.sum``. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.expr.aggregators. .. autosummary::. collect; collect_as_set; count; count_where; counter; any; all; take; min; max; sum; array_sum; mean; approx_quantiles; approx_median; stats; product; fraction; hardy_weinberg_test; explode; filter; inbreeding; call_stats; info_score; hist; linreg; corr; group_by; array_agg; downsample; approx_cdf. .. autofunction:: collect; .. autofunction:: collect_as_set; .. autofunction:: count; .. autofunction:: count_where; .. autofunction:: counter; .. autofunction:: any; .. autofunction:: all; .. autofunction:: take; .. autofunction:: min; .. autofunction:: max; .. autofunction:: sum; .. autofunction:: array_sum; .. autofunction:: mean; .. autofunction:: approx_quantiles; .. autofunction:: approx_median; .. autofunction:: stats; .. autofunction:: product; .. autofunction:: fraction; .. autofunction:: hardy_weinberg_test; .. autofunction:: explode; .. autofunction:: filter; .. autofunction:: inbreeding; .. autofunction:: call_stats; .. autofunction:: info_score; .. autofunction:: hist; .. autofunction:: linreg; .. autofunction:: corr; .. autofunction:: group_by; .. autofunction:: array_agg; .. autofunction:: downsample; .. autofunction:: approx_cdf; ",MatchSource.DOCS,hail/python/hail/docs/aggregators.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst:1326,Availability,down,downsample,1326,".. _sec-aggregators:. Aggregators; ===========. The ``aggregators`` module is exposed as ``hl.agg``, e.g. ``hl.agg.sum``. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.expr.aggregators. .. autosummary::. collect; collect_as_set; count; count_where; counter; any; all; take; min; max; sum; array_sum; mean; approx_quantiles; approx_median; stats; product; fraction; hardy_weinberg_test; explode; filter; inbreeding; call_stats; info_score; hist; linreg; corr; group_by; array_agg; downsample; approx_cdf. .. autofunction:: collect; .. autofunction:: collect_as_set; .. autofunction:: count; .. autofunction:: count_where; .. autofunction:: counter; .. autofunction:: any; .. autofunction:: all; .. autofunction:: take; .. autofunction:: min; .. autofunction:: max; .. autofunction:: sum; .. autofunction:: array_sum; .. autofunction:: mean; .. autofunction:: approx_quantiles; .. autofunction:: approx_median; .. autofunction:: stats; .. autofunction:: product; .. autofunction:: fraction; .. autofunction:: hardy_weinberg_test; .. autofunction:: explode; .. autofunction:: filter; .. autofunction:: inbreeding; .. autofunction:: call_stats; .. autofunction:: info_score; .. autofunction:: hist; .. autofunction:: linreg; .. autofunction:: corr; .. autofunction:: group_by; .. autofunction:: array_agg; .. autofunction:: downsample; .. autofunction:: approx_cdf; ",MatchSource.DOCS,hail/python/hail/docs/aggregators.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst:78,Security,expose,exposed,78,".. _sec-aggregators:. Aggregators; ===========. The ``aggregators`` module is exposed as ``hl.agg``, e.g. ``hl.agg.sum``. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.expr.aggregators. .. autosummary::. collect; collect_as_set; count; count_where; counter; any; all; take; min; max; sum; array_sum; mean; approx_quantiles; approx_median; stats; product; fraction; hardy_weinberg_test; explode; filter; inbreeding; call_stats; info_score; hist; linreg; corr; group_by; array_agg; downsample; approx_cdf. .. autofunction:: collect; .. autofunction:: collect_as_set; .. autofunction:: count; .. autofunction:: count_where; .. autofunction:: counter; .. autofunction:: any; .. autofunction:: all; .. autofunction:: take; .. autofunction:: min; .. autofunction:: max; .. autofunction:: sum; .. autofunction:: array_sum; .. autofunction:: mean; .. autofunction:: approx_quantiles; .. autofunction:: approx_median; .. autofunction:: stats; .. autofunction:: product; .. autofunction:: fraction; .. autofunction:: hardy_weinberg_test; .. autofunction:: explode; .. autofunction:: filter; .. autofunction:: inbreeding; .. autofunction:: call_stats; .. autofunction:: info_score; .. autofunction:: hist; .. autofunction:: linreg; .. autofunction:: corr; .. autofunction:: group_by; .. autofunction:: array_agg; .. autofunction:: downsample; .. autofunction:: approx_cdf; ",MatchSource.DOCS,hail/python/hail/docs/aggregators.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/aggregators.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:889,Availability,avail,available,889,".. _Annotation Database:. ===================; Annotation Database; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script.",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:1569,Availability,avail,available,1569,"se; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script. .. raw:: html; :file: _static/annotationdb/annotationdb.html; ",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:320,Deployability,pipeline,pipelines,320,".. _Annotation Database:. ===================; Annotation Database; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script.",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:390,Deployability,pipeline,pipeline,390,".. _Annotation Database:. ===================; Annotation Database; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script.",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:960,Energy Efficiency,charge,charges,960,".. _Annotation Database:. ===================; Annotation Database; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script.",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:258,Security,access,accessible,258,".. _Annotation Database:. ===================; Annotation Database; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script.",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:1081,Security,access,access,1081,"se; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script. .. raw:: html; :file: _static/annotationdb/annotationdb.html; ",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst:1469,Security,access,accessed,1469,"se; ===================. .. warning::; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines. To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script. Check out the :class:`.DB` class documentation for more detail on creating an; annotation database instance and annotating a :class:`.MatrixTable` or a; :class:`.Table`. .. rubric:: Google Cloud Storage. Note that these annotations are stored in :ref:`Requester Pays<GCP Requester; Pays>` buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance. To access these buckets on a cluster started with ``hailctl dataproc``, you; can use the additional argument ``--requester-pays-annotation-db`` as follows:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. .. rubric:: Amazon S3. Annotation datasets are now shared via `Open Data on AWS <https://aws.amazon; .com/opendata/>`__ as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; --------------. Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below. In addition, a search bar is provided if looking for a specific annotation; within our curated collection. Use the ""Copy to Clipboard"" button to copy the generated Hail code, and paste; the command into your own Hail script. .. raw:: html; :file: _static/annotationdb/annotationdb.html; ",MatchSource.DOCS,hail/python/hail/docs/annotation_database_ui.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/annotation_database_ui.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/api.rst:200,Integrability,interface,interface,200,".. _sec-query-api:. =====================; Hail Query Python API; =====================. This is the API documentation for ``Hail Query``, and provides detailed information; on the Python programming interface. Use ``import hail as hl`` to access this functionality. Classes; ~~~~~~~. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.Table; hail.GroupedTable; hail.MatrixTable; hail.GroupedMatrixTable. Modules; ~~~~~~~. .. toctree::; :maxdepth: 1. expressions <expressions>; types <types>; functions <functions/index>; aggregators <aggregators>; scans <scans>; methods <methods/index>; nd <nd/index>; utils <utils/index>; linalg <linalg/index>; stats <stats/index>; genetics <genetics/index>; plot <plot>; ggplot <ggplot/index>; vds <vds/index>; experimental <experimental/index>. Top-Level Functions; ~~~~~~~~~~~~~~~~~~~. .. autofunction:: hail.init; .. autofunction:: hail.asc; .. autofunction:: hail.desc; .. autofunction:: hail.stop; .. autofunction:: hail.spark_context; .. autofunction:: hail.tmp_dir; .. autofunction:: hail.default_reference; .. autofunction:: hail.get_reference; .. autofunction:: hail.set_global_seed; .. autofunction:: hail.reset_global_randomness; .. autofunction:: hail.citation; .. autofunction:: hail.version; ",MatchSource.DOCS,hail/python/hail/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/api.rst:240,Security,access,access,240,".. _sec-query-api:. =====================; Hail Query Python API; =====================. This is the API documentation for ``Hail Query``, and provides detailed information; on the Python programming interface. Use ``import hail as hl`` to access this functionality. Classes; ~~~~~~~. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.Table; hail.GroupedTable; hail.MatrixTable; hail.GroupedMatrixTable. Modules; ~~~~~~~. .. toctree::; :maxdepth: 1. expressions <expressions>; types <types>; functions <functions/index>; aggregators <aggregators>; scans <scans>; methods <methods/index>; nd <nd/index>; utils <utils/index>; linalg <linalg/index>; stats <stats/index>; genetics <genetics/index>; plot <plot>; ggplot <ggplot/index>; vds <vds/index>; experimental <experimental/index>. Top-Level Functions; ~~~~~~~~~~~~~~~~~~~. .. autofunction:: hail.init; .. autofunction:: hail.asc; .. autofunction:: hail.desc; .. autofunction:: hail.stop; .. autofunction:: hail.spark_context; .. autofunction:: hail.tmp_dir; .. autofunction:: hail.default_reference; .. autofunction:: hail.get_reference; .. autofunction:: hail.set_global_seed; .. autofunction:: hail.reset_global_randomness; .. autofunction:: hail.citation; .. autofunction:: hail.version; ",MatchSource.DOCS,hail/python/hail/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:207,Availability,avail,available,207,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:325,Availability,avail,available,325,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:414,Availability,avail,available,414,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:1020,Availability,avail,available,1020,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:435,Deployability,deploy,deploy,435,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:554,Usability,learn,learn,554,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst:798,Usability,learn,learn,798,".. _sec-batch-api:. ========================; hailtop.batch Python API; ========================. The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our `GitHub repository <https://github.com/hail-is/hail>`__.; To learn more about the Hail Batch Service, take a look at our `documentation <https://hail.is/docs/batch/service.html>`__. The Python library ``hailtop.batch`` is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a `tutorial <https://hail.is/docs/batch/tutorial.html>`__ and; `cookbooks <https://hail.is/docs/batch/cookbook.html>`__ with detailed examples. The API documentation is available `here <https://hail.is/docs/batch/api.html>`__.; ",MatchSource.DOCS,hail/python/hail/docs/batch_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/batch_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cheatsheets.rst:180,Usability,feedback,feedback,180,".. _sec-cheatsheets:. =============; Cheat Sheets; =============. .. note::; Hail's cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you'd like to add a cheatsheet to the documentation, make a pull request!. `Hail Tables Cheat Sheet <_static/cheatsheets/hail_tables_cheat_sheet.pdf>`_. `Hail MatrixTables Cheat Sheet <_static/cheatsheets/hail_matrix_tables_cheat_sheet.pdf>`_. ",MatchSource.DOCS,hail/python/hail/docs/cheatsheets.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cheatsheets.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:2090,Availability,error,erroring,2090," listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket.; * - Shared between Query and Batch; - Yes; ",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:128,Deployability,configurat,configuration-reference,128,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:666,Deployability,configurat,configuration,666,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:784,Deployability,configurat,configuration,784,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:1233,Deployability,configurat,configuration,1233,"bles can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:128,Modifiability,config,configuration-reference,128,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:218,Modifiability,variab,variables,218,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:363,Modifiability,config,config,363,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:472,Modifiability,variab,variables,472,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:598,Modifiability,variab,variable,598,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:666,Modifiability,config,configuration,666,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:680,Modifiability,variab,variables,680,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:713,Modifiability,variab,variables,713,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:784,Modifiability,config,configuration,784,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:798,Modifiability,variab,variable,798,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:876,Modifiability,variab,variable,876,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:935,Modifiability,variab,variables,935,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:977,Modifiability,variab,variables,977,".. role:: python(code); :language: python; :class: highlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Enviro",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:1054,Modifiability,variab,variables,1054,"hlight. .. role:: bash(code); :language: bash; :class: highlight. .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - ",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:1116,Modifiability,variab,variables,1116,". .. _sec-configuration-reference:. Configuration Reference; =======================. Configuration variables can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default stor",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:1233,Modifiability,config,configuration,1233,"bles can be set for Hail Query by:. #. passing them as keyword arguments to :func:`.init`,; #. running a command of the form :bash:`hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE>` from the command line, or; #. setting them as shell environment variables by running a command of the form; :bash:`export <VARIABLE_NAME>=<VARIABLE_VALUE>` in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:2285,Modifiability,config,configured,2285," listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket.; * - Shared between Query and Batch; - Yes; ",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst:2366,Security,access,access,2366," listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to :func:`.init` will override any values set for the; variable using either :bash:`hailctl` or shell environment variables. .. warning::; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; :func:`.init`, :bash:`hailctl`, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for :python:`gcs_bucket_allow_list` is passed to :func:`.init`, a different value; may be passed to the constructor for Batch's :python:`ServiceBackend`, which will only affect that instance of the; class (which can only be used within Batch), and won't affect Query. Supported Configuration Variables; ---------------------------------. .. list-table:: GCS Bucket Allowlist; :widths: 50 50. * - Keyword Argument Name; - :python:`gcs_bucket_allow_list`; * - Keyword Argument Format; - :python:`[""bucket1"", ""bucket2""]`; * - :bash:`hailctl` Variable Name; - :bash:`gcs/bucket_allow_list`; * - Environment Variable Name; - :bash:`HAIL_GCS_BUCKET_ALLOW_LIST`; * - :bash:`hailctl` and Environment Variable Format; - :bash:`bucket1,bucket2`; * - Effect; - Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket.; * - Shared between Query and Batch; - Yes; ",MatchSource.DOCS,hail/python/hail/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst:366,Availability,avail,available,366,".. _sec-datasets:. ========; Datasets; ========. .. warning::; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; :ref:`Requester Pays<GCP Requester Pays>` buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via `Open Data on AWS <https://aws.amazon.com/opendata/>`__ and are in buckets; in the US region. Check out the :func:`.load_dataset` function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified. .. rubric:: Schemas for Available Datasets. .. toctree::; :maxdepth: 1. datasets/schemas.rst. .. raw:: html; :file: _static/datasets/datasets.html; ",MatchSource.DOCS,hail/python/hail/docs/datasets.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst:657,Deployability,pipeline,pipeline,657,".. _sec-datasets:. ========; Datasets; ========. .. warning::; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; :ref:`Requester Pays<GCP Requester Pays>` buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via `Open Data on AWS <https://aws.amazon.com/opendata/>`__ and are in buckets; in the US region. Check out the :func:`.load_dataset` function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified. .. rubric:: Schemas for Available Datasets. .. toctree::; :maxdepth: 1. datasets/schemas.rst. .. raw:: html; :file: _static/datasets/datasets.html; ",MatchSource.DOCS,hail/python/hail/docs/datasets.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst:844,Energy Efficiency,charge,charges,844,".. _sec-datasets:. ========; Datasets; ========. .. warning::; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; :ref:`Requester Pays<GCP Requester Pays>` buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via `Open Data on AWS <https://aws.amazon.com/opendata/>`__ and are in buckets; in the US region. Check out the :func:`.load_dataset` function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified. .. rubric:: Schemas for Available Datasets. .. toctree::; :maxdepth: 1. datasets/schemas.rst. .. raw:: html; :file: _static/datasets/datasets.html; ",MatchSource.DOCS,hail/python/hail/docs/datasets.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst:617,Performance,load,load,617,".. _sec-datasets:. ========; Datasets; ========. .. warning::; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; :ref:`Requester Pays<GCP Requester Pays>` buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via `Open Data on AWS <https://aws.amazon.com/opendata/>`__ and are in buckets; in the US region. Check out the :func:`.load_dataset` function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified. .. rubric:: Schemas for Available Datasets. .. toctree::; :maxdepth: 1. datasets/schemas.rst. .. raw:: html; :file: _static/datasets/datasets.html; ",MatchSource.DOCS,hail/python/hail/docs/datasets.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/fs_api.rst:224,Security,access,access,224,.. _sec-fs-api:. =====================; hailtop.fs Python API; =====================. This is the API documentation for Hail's cloud-agnostic file system implementation in ``hailtop.fs``. Use ``import hailtop.fs as hfs`` to access this functionality. Top-Level Functions; ~~~~~~~~~~~~~~~~~~~. .. currentmodule:: hailtop.fs. .. autosummary::. copy; exists; is_dir; is_file; ls; mkdir; open; remove; rmtree; stat. .. autofunction:: copy; .. autofunction:: exists; .. autofunction:: is_dir; .. autofunction:: is_file; .. autofunction:: ls; .. autofunction:: mkdir; .. autofunction:: open; .. autofunction:: remove; .. autofunction:: rmtree; .. autofunction:: stat; ,MatchSource.DOCS,hail/python/hail/docs/fs_api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/fs_api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:8,Deployability,install,installation,8,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:112,Deployability,install,install,112,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:140,Deployability,install,install,140,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:177,Deployability,install,install,177,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:217,Deployability,install,install,217,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:259,Deployability,install,install,259,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:293,Deployability,install,installation,293,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst:334,Deployability,install,install,334,".. _sec-installation:. ===============; Installing Hail; ===============. .. toctree::; :maxdepth: 1. Mac OS X <install/macosx.rst>; Linux <install/linux.rst>; Google Dataproc <install/dataproc.rst>; Azure HDInsight <install/azure.rst>; Other Spark Clusters <install/other-cluster.rst>; After installation, try your first Hail query <install/try.rst>; ",MatchSource.DOCS,hail/python/hail/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:336,Deployability,install,installation,336,"For Software Developers; -----------------------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A cou",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:434,Deployability,install,install,434,"For Software Developers; -----------------------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A cou",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:506,Deployability,install,installed,506,"For Software Developers; -----------------------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A cou",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:676,Deployability,install,install,676,"For Software Developers; -----------------------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A cou",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:974,Deployability,release,releases,974,"For Software Developers; -----------------------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A cou",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1183,Deployability,install,install,1183,". If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 tes",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1252,Deployability,install,install,1252,"cture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1357,Deployability,install,install,1357,"rted <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1970,Deployability,install,install-dev-requirements,1970,". If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:3255,Deployability,release,released,3255,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1507,Integrability,depend,dependencies,1507," the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitt",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1943,Integrability,depend,dependencies,1943," with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2962,Integrability,message,message,2962,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:3149,Integrability,message,message,3149,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1054,Modifiability,variab,variable,1054,"-------. Hail is an open-source project. We welcome contributions to the repository. Requirements; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1106,Modifiability,variab,variable,1106,"nts; ~~~~~~~~~~~~. - `Java 11 JDK <https://adoptopenjdk.net/index.html>`_ . If you have a Mac, you must use a; compatible architecture (``uname -m`` prints your architecture). - The Python and non-pip installation requirements in `Getting Started <getting_started.html>`_.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_.",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1897,Testability,test,tests,1897," native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2010,Testability,test,tests,2010,"e OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2156,Testability,test,test,2156,"ource. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR tit",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2206,Testability,test,test,2206," from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should b",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2736,Testability,test,tests,2736,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:2801,Testability,test,tests,2801,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:3229,Testability,log,log,3229,"OMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multiple features should be split into multiple PRs.; - Before submitting your PR, you should rebase onto the latest main.; - PRs must pass all tests before being merged. See the section above on `Running the tests`_ locally.; - PRs require a review before being merged. We will assign someone from our dev team to review your PR.; - When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; - For user facing changes (new functions, etc), include ""CHANGELOG"" in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released.; ",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1546,Usability,guid,guide,1546," the JRE. - If you are setting `HAIL_COMPILE_NATIVES=1`, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitt",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst:1623,Usability,guid,guide,1623,"ed the LZ4 library; header files. On Debian and Ubuntu machines run: `apt-get install liblz4-dev`. Building Hail; ~~~~~~~~~~~~~. The Hail source code is hosted `on GitHub <https://github.com/hail-is/hail>`_::. git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you're not using one of these OSes, set; the environment (or Make) variable `HAIL_COMPILE_NATIVES` to any value. This; variable tells GNU Make to build the native libraries from source. Build and install a wheel file from source with local-mode ``pyspark``::. make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions::. make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Install build dependencies listed in the `docs style guide <https://github.com/hail-is/hail/blob/main/hail/python/hail/docs/style-guide.txt>`_. Build without rendering the notebooks (which is slow)::. make hail-docs-do-not-render-notebooks. Build while rendering the notebooks::. make hail-docs. Serve the built website on http://localhost:8000/ ::. (cd build/www && python3 -m http.server). Running the tests; ~~~~~~~~~~~~~~~~~. Install development dependencies::. make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. - `PLINK 1.9 <https://www.cog-genomics.org/plink2>`_. Execute every Hail test using at most 8 parallel threads::. make -j8 test. Contributing; ~~~~~~~~~~~~. Chat with the dev team on our `Zulip chatroom <https://hail.zulipchat.com>`_ or; `development forum <https://dev.hail.is>`_ if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging. Keep in mind the following principles when submitting a pull request:. - A PR should focus on a single feature. Multipl",MatchSource.DOCS,hail/python/hail/docs/getting_started_developing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/getting_started_developing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:8,Usability,guid,guides,8,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:160,Usability,guid,guides,160,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:179,Usability,feedback,feedback,179,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:237,Usability,guid,guide,237,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:293,Usability,guid,guides,293,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:385,Usability,guid,guides,385,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:430,Usability,guid,guides,430,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst:464,Usability,guid,guides,464,".. _sec-guides:. =============; How-To Guides; =============. .. note::; Hail's How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you'd like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. .. toctree::. Aggregation <guides/agg.rst>; Annotation (Adding Fields) <guides/annotation.rst>; Genetics <guides/genetics.rst>",MatchSource.DOCS,hail/python/hail/docs/guides.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst:384,Deployability,install,installation,384,"========; Hail 0.2; ========. Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the `overview <overview/index.html>`_ for; a high-level walkthrough of the library, the `GWAS tutorial; <tutorials/01-genome-wide-association-study.html>`_ for a simple; example of conducting a genome-wide association study, and the `installation page <getting_started.html>`_ to get started; using Hail. ========; Contents; ========. .. toctree::; :maxdepth: 2. Installation <getting_started>; Hail on the Cloud <hail_on_the_cloud>; Tutorials <tutorials-landing>; Reference (Python API) <root_api>; Configuration Reference <configuration_reference>; Overview <overview/index>; How-To Guides <guides>; Cheatsheets <cheatsheets>; Datasets <datasets>; Annotation Database <annotation_database_ui>; Libraries <libraries>; For Software Developers <getting_started_developing>; Other Resources <other_resources>; Change Log And Version Policy <change_log>. ==================; Indices and tables; ==================. * :ref:`genindex`. If you would like to refer to our Hail v0.1 (deprecated) docs, please view `Hail 0.1 docs </docs/0.1/index.html>`_; ",MatchSource.DOCS,hail/python/hail/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst:65,Performance,scalab,scalable,65,"========; Hail 0.2; ========. Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the `overview <overview/index.html>`_ for; a high-level walkthrough of the library, the `GWAS tutorial; <tutorials/01-genome-wide-association-study.html>`_ for a simple; example of conducting a genome-wide association study, and the `installation page <getting_started.html>`_ to get started; using Hail. ========; Contents; ========. .. toctree::; :maxdepth: 2. Installation <getting_started>; Hail on the Cloud <hail_on_the_cloud>; Tutorials <tutorials-landing>; Reference (Python API) <root_api>; Configuration Reference <configuration_reference>; Overview <overview/index>; How-To Guides <guides>; Cheatsheets <cheatsheets>; Datasets <datasets>; Annotation Database <annotation_database_ui>; Libraries <libraries>; For Software Developers <getting_started_developing>; Other Resources <other_resources>; Change Log And Version Policy <change_log>. ==================; Indices and tables; ==================. * :ref:`genindex`. If you would like to refer to our Hail v0.1 (deprecated) docs, please view `Hail 0.1 docs </docs/0.1/index.html>`_; ",MatchSource.DOCS,hail/python/hail/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst:312,Usability,simpl,simple,312,"========; Hail 0.2; ========. Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the `overview <overview/index.html>`_ for; a high-level walkthrough of the library, the `GWAS tutorial; <tutorials/01-genome-wide-association-study.html>`_ for a simple; example of conducting a genome-wide association study, and the `installation page <getting_started.html>`_ to get started; using Hail. ========; Contents; ========. .. toctree::; :maxdepth: 2. Installation <getting_started>; Hail on the Cloud <hail_on_the_cloud>; Tutorials <tutorials-landing>; Reference (Python API) <root_api>; Configuration Reference <configuration_reference>; Overview <overview/index>; How-To Guides <guides>; Cheatsheets <cheatsheets>; Datasets <datasets>; Annotation Database <annotation_database_ui>; Libraries <libraries>; For Software Developers <getting_started_developing>; Other Resources <other_resources>; Change Log And Version Policy <change_log>. ==================; Indices and tables; ==================. * :ref:`genindex`. If you would like to refer to our Hail v0.1 (deprecated) docs, please view `Hail 0.1 docs </docs/0.1/index.html>`_; ",MatchSource.DOCS,hail/python/hail/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst:743,Usability,guid,guides,743,"========; Hail 0.2; ========. Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the `overview <overview/index.html>`_ for; a high-level walkthrough of the library, the `GWAS tutorial; <tutorials/01-genome-wide-association-study.html>`_ for a simple; example of conducting a genome-wide association study, and the `installation page <getting_started.html>`_ to get started; using Hail. ========; Contents; ========. .. toctree::; :maxdepth: 2. Installation <getting_started>; Hail on the Cloud <hail_on_the_cloud>; Tutorials <tutorials-landing>; Reference (Python API) <root_api>; Configuration Reference <configuration_reference>; Overview <overview/index>; How-To Guides <guides>; Cheatsheets <cheatsheets>; Datasets <datasets>; Annotation Database <annotation_database_ui>; Libraries <libraries>; For Software Developers <getting_started_developing>; Other Resources <other_resources>; Change Log And Version Policy <change_log>. ==================; Indices and tables; ==================. * :ref:`genindex`. If you would like to refer to our Hail v0.1 (deprecated) docs, please view `Hail 0.1 docs </docs/0.1/index.html>`_; ",MatchSource.DOCS,hail/python/hail/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/libraries.rst:624,Deployability,install,install,624,".. _sec-libraries:. ===================; Libraries; ===================. This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. --------. gnomad (Hail Utilities for gnomAD); ----------------------------------. This repo contains a number of Hail utility functions and scripts for the `gnomAD <https://gnomad.broadinstitute.org>`_ project and the `Translational Genomics Group <https://the-tgg.org/>`_. Install with ``pip install gnomad``. More info can be found in the `documentation <https://broadinstitute.github.io/gnomad_methods/>`_ or on the `PyPI project page <https://pypi.org/project/gnomad/>`_. ",MatchSource.DOCS,hail/python/hail/docs/libraries.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/libraries.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/plot.rst:368,Modifiability,extend,extend,368,"Plot; ====. .. warning::; Plotting functionality is in early stages and is experimental. Interfaces will change regularly. Plotting in Hail is easy. Hail's plot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples. Plot functions in Hail accept data in the form of either Python objects or :class:`.Table` and :class:`.MatrixTable` fields. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.plot. .. autosummary::; :nosignatures:. cdf; pdf; smoothed_pdf; histogram; cumulative_histogram; histogram2d; scatter; qq; manhattan; output_notebook; visualize_missingness. .. autofunction:: cdf; .. autofunction:: pdf; .. autofunction:: smoothed_pdf; .. autofunction:: histogram; .. autofunction:: cumulative_histogram; .. autofunction:: histogram2d; .. autofunction:: scatter; .. autofunction:: qq; .. autofunction:: manhattan; .. autofunction:: output_notebook; .. autofunction:: visualize_missingness",MatchSource.DOCS,hail/python/hail/docs/plot.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/plot.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst:141,Deployability,rolling,rolling,141,".. _sec-scan:. Scans; ===========. The ``scan`` module is exposed as ``hl.scan``, e.g. ``hl.scan.sum``. The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported. For example, the ``count`` aggregator can be used as ``hl.scan.count`` to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:. >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:. >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row's value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:. >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; |",MatchSource.DOCS,hail/python/hail/docs/scans.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst:133,Performance,perform,perform,133,".. _sec-scan:. Scans; ===========. The ``scan`` module is exposed as ``hl.scan``, e.g. ``hl.scan.sum``. The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported. For example, the ``count`` aggregator can be used as ``hl.scan.count`` to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:. >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:. >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row's value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:. >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; |",MatchSource.DOCS,hail/python/hail/docs/scans.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst:58,Security,expose,exposed,58,".. _sec-scan:. Scans; ===========. The ``scan`` module is exposed as ``hl.scan``, e.g. ``hl.scan.sum``. The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported. For example, the ``count`` aggregator can be used as ``hl.scan.count`` to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:. >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:. >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row's value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:. >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; |",MatchSource.DOCS,hail/python/hail/docs/scans.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/scans.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst:150,Availability,down,download,150,".. _sec-tutorials2:. ==============; Hail Tutorials; ==============. .. raw:: html. <!-- for some reason, Safari is confused if we do not include the download attribute in the; anchor. At time of writing, there does not appear to be a way to tell Sphinx to include that; attribute. -->; <p>To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; <a class=""reference external"" href=""tutorials.tar.gz"" download>downloading the archive (.tar.gz)</a>; and running the following:</p>. ::. pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. .. toctree::; :maxdepth: 1. Genome-Wide Association Study (GWAS) Tutorial <tutorials/01-genome-wide-association-study.ipynb>; Table Tutorial <tutorials/03-tables.ipynb>; Aggregation Tutorial <tutorials/04-aggregation.ipynb>; Filtering and Annotation Tutorial <tutorials/05-filter-annotate.ipynb>; Table Joins Tutorial <tutorials/06-joins>; MatrixTable Tutorial <tutorials/07-matrixtable.ipynb>; Plotting Tutorial<tutorials/08-plotting.ipynb>; GGPlot Tutorial<tutorials/09-ggplot.ipynb>; ",MatchSource.DOCS,hail/python/hail/docs/tutorials-landing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst:518,Availability,down,download,518,".. _sec-tutorials2:. ==============; Hail Tutorials; ==============. .. raw:: html. <!-- for some reason, Safari is confused if we do not include the download attribute in the; anchor. At time of writing, there does not appear to be a way to tell Sphinx to include that; attribute. -->; <p>To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; <a class=""reference external"" href=""tutorials.tar.gz"" download>downloading the archive (.tar.gz)</a>; and running the following:</p>. ::. pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. .. toctree::; :maxdepth: 1. Genome-Wide Association Study (GWAS) Tutorial <tutorials/01-genome-wide-association-study.ipynb>; Table Tutorial <tutorials/03-tables.ipynb>; Aggregation Tutorial <tutorials/04-aggregation.ipynb>; Filtering and Annotation Tutorial <tutorials/05-filter-annotate.ipynb>; Table Joins Tutorial <tutorials/06-joins>; MatrixTable Tutorial <tutorials/07-matrixtable.ipynb>; Plotting Tutorial<tutorials/08-plotting.ipynb>; GGPlot Tutorial<tutorials/09-ggplot.ipynb>; ",MatchSource.DOCS,hail/python/hail/docs/tutorials-landing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst:527,Availability,down,downloading,527,".. _sec-tutorials2:. ==============; Hail Tutorials; ==============. .. raw:: html. <!-- for some reason, Safari is confused if we do not include the download attribute in the; anchor. At time of writing, there does not appear to be a way to tell Sphinx to include that; attribute. -->; <p>To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; <a class=""reference external"" href=""tutorials.tar.gz"" download>downloading the archive (.tar.gz)</a>; and running the following:</p>. ::. pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. .. toctree::; :maxdepth: 1. Genome-Wide Association Study (GWAS) Tutorial <tutorials/01-genome-wide-association-study.ipynb>; Table Tutorial <tutorials/03-tables.ipynb>; Aggregation Tutorial <tutorials/04-aggregation.ipynb>; Filtering and Annotation Tutorial <tutorials/05-filter-annotate.ipynb>; Table Joins Tutorial <tutorials/06-joins>; MatrixTable Tutorial <tutorials/07-matrixtable.ipynb>; Plotting Tutorial<tutorials/08-plotting.ipynb>; GGPlot Tutorial<tutorials/09-ggplot.ipynb>; ",MatchSource.DOCS,hail/python/hail/docs/tutorials-landing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst:606,Deployability,install,install,606,".. _sec-tutorials2:. ==============; Hail Tutorials; ==============. .. raw:: html. <!-- for some reason, Safari is confused if we do not include the download attribute in the; anchor. At time of writing, there does not appear to be a way to tell Sphinx to include that; attribute. -->; <p>To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; <a class=""reference external"" href=""tutorials.tar.gz"" download>downloading the archive (.tar.gz)</a>; and running the following:</p>. ::. pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. .. toctree::; :maxdepth: 1. Genome-Wide Association Study (GWAS) Tutorial <tutorials/01-genome-wide-association-study.ipynb>; Table Tutorial <tutorials/03-tables.ipynb>; Aggregation Tutorial <tutorials/04-aggregation.ipynb>; Filtering and Annotation Tutorial <tutorials/05-filter-annotate.ipynb>; Table Joins Tutorial <tutorials/06-joins>; MatrixTable Tutorial <tutorials/07-matrixtable.ipynb>; Plotting Tutorial<tutorials/08-plotting.ipynb>; GGPlot Tutorial<tutorials/09-ggplot.ipynb>; ",MatchSource.DOCS,hail/python/hail/docs/tutorials-landing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst:309,Testability,test,test,309,".. _sec-tutorials2:. ==============; Hail Tutorials; ==============. .. raw:: html. <!-- for some reason, Safari is confused if we do not include the download attribute in the; anchor. At time of writing, there does not appear to be a way to tell Sphinx to include that; attribute. -->; <p>To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; <a class=""reference external"" href=""tutorials.tar.gz"" download>downloading the archive (.tar.gz)</a>; and running the following:</p>. ::. pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. .. toctree::; :maxdepth: 1. Genome-Wide Association Study (GWAS) Tutorial <tutorials/01-genome-wide-association-study.ipynb>; Table Tutorial <tutorials/03-tables.ipynb>; Aggregation Tutorial <tutorials/04-aggregation.ipynb>; Filtering and Annotation Tutorial <tutorials/05-filter-annotate.ipynb>; Table Joins Tutorial <tutorials/06-joins>; MatrixTable Tutorial <tutorials/07-matrixtable.ipynb>; Plotting Tutorial<tutorials/08-plotting.ipynb>; GGPlot Tutorial<tutorials/09-ggplot.ipynb>; ",MatchSource.DOCS,hail/python/hail/docs/tutorials-landing.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/tutorials-landing.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:1724,Availability,down,down,1724,"er is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using ``gcloud storage cp``,; ``gs://hail-us-central1-vep/loftee-beta/GRCh37.tar`` and ``gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar``,; extracting them into a local folder, and uploading that folder to your storage account usi",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:2467,Availability,down,downloading,2467,"xt. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using ``gcloud storage cp``,; ``gs://hail-us-central1-vep/loftee-beta/GRCh37.tar`` and ``gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar``,; extracting them into a local folder, and uploading that folder to your storage account using ``az; storage copy``. The hail-us-central1-vep Google Cloud Storage bucket is a *requester pays* bucket which means; *you* must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism. Hail also supports VEP for GRCh38 variants. The required tar files are located at; ``gs://hail-REGION-vep/loftee-beta/GRCh38.tar`` and; ``gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar``. A cluster started without the ``--vep`` argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using ``--vep``.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:123,Deployability,install,installations,123,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:440,Deployability,install,install-azure-cli,440,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:1960,Deployability,configurat,configuration,1960,"create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using ``gcloud storage cp``,; ``gs://hail-us-central1-vep/loftee-beta/GRCh37.tar`` and ``gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar``,; extracting them into a local folder, and uploading that folder to your storage account using ``az; storage copy``. The hail-us-central1-vep Google Cloud Storage bucket is a *requester pays* bucket which means; *you* must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivale",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:340,Modifiability,config,configured,340,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:620,Modifiability,config,configured,620,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:1960,Modifiability,config,configuration,1960,"create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using ``gcloud storage cp``,; ``gs://hail-us-central1-vep/loftee-beta/GRCh37.tar`` and ``gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar``,; extracting them into a local folder, and uploading that folder to your storage account using ``az; storage copy``. The hail-us-central1-vep Google Cloud Storage bucket is a *requester pays* bucket which means; *you* must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivale",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:763,Security,password,password,763,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:802,Security,password,password,802,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst:967,Security,access,access,967,"===============; Microsoft Azure; ===============. ``hailctl hdinsight``; ---------------------. As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, ``hailctl; hdinsight`` for working with `Microsoft Azure HDInsight Spark; <https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview>`__ clusters configured for; Hail. This tool requires the `Azure CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli>`__. An HDInsight cluster always consists of two ""head"" nodes, two or more ""worker"" nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; ``https://CLUSTER_NAME.azurehdinsight.net/jupyter`` . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created. Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using ``hailctl hdinsight stop``,; this container will be deleted. To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group. .. code-block:: text. hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:. .. code-block:: text. hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. .. _vep_hdinsight:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in ",MatchSource.DOCS,hail/python/hail/docs/cloud/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst:836,Availability,avail,available,836,"; ==========; Databricks; ==========. The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks. Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the `Project Glow Dockerhub; <https://hub.docker.com/r/projectglow/databricks-hail/tags?page=1&ordering=last_updated>`__. Docker; files to build your own Hail container on Databricks can be found in the Glow `Github repository; <https://github.com/projectglow/glow/tree/master/docker>`__. Install Hail via Docker with `Databricks Container Services; <https://docs.databricks.com/clusters/custom-containers.html>`__. Use the Docker Image URL, ``projectglow/databricks-hail:<hail_version>``, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; ----------------------. For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; ---------------. When initializing Hail, pass in the pre-created `SparkContext` and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. :**note**:. - Enable ``skip_logging_configuration`` to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; - Hail is not supported with `Credential passthrough; <https://docs.databricks.com/security/credential-passthrough/index.html>`__. :**code**:. >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) # doctest: +SKIP. Display Bokeh plots; -------------------. Hail uses the `Bokeh <https://docs.bokeh.org/en/latest/>`__ library to create plots. The `show`; function built into Bokeh does not work in D",MatchSource.DOCS,hail/python/hail/docs/cloud/databricks.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst:165,Deployability,install,installed,165,"; ==========; Databricks; ==========. The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks. Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the `Project Glow Dockerhub; <https://hub.docker.com/r/projectglow/databricks-hail/tags?page=1&ordering=last_updated>`__. Docker; files to build your own Hail container on Databricks can be found in the Glow `Github repository; <https://github.com/projectglow/glow/tree/master/docker>`__. Install Hail via Docker with `Databricks Container Services; <https://docs.databricks.com/clusters/custom-containers.html>`__. Use the Docker Image URL, ``projectglow/databricks-hail:<hail_version>``, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; ----------------------. For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; ---------------. When initializing Hail, pass in the pre-created `SparkContext` and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. :**note**:. - Enable ``skip_logging_configuration`` to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; - Hail is not supported with `Credential passthrough; <https://docs.databricks.com/security/credential-passthrough/index.html>`__. :**code**:. >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) # doctest: +SKIP. Display Bokeh plots; -------------------. Hail uses the `Bokeh <https://docs.bokeh.org/en/latest/>`__ library to create plots. The `show`; function built into Bokeh does not work in D",MatchSource.DOCS,hail/python/hail/docs/cloud/databricks.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst:1468,Deployability,rolling,rolling,1468,"les to build your own Hail container on Databricks can be found in the Glow `Github repository; <https://github.com/projectglow/glow/tree/master/docker>`__. Install Hail via Docker with `Databricks Container Services; <https://docs.databricks.com/clusters/custom-containers.html>`__. Use the Docker Image URL, ``projectglow/databricks-hail:<hail_version>``, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; ----------------------. For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; ---------------. When initializing Hail, pass in the pre-created `SparkContext` and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. :**note**:. - Enable ``skip_logging_configuration`` to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; - Hail is not supported with `Credential passthrough; <https://docs.databricks.com/security/credential-passthrough/index.html>`__. :**code**:. >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) # doctest: +SKIP. Display Bokeh plots; -------------------. Hail uses the `Bokeh <https://docs.bokeh.org/en/latest/>`__ library to create plots. The `show`; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:. >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function `displayHTML` with `html` as its argument. See Databricks' `Bokeh docs <https://docs.databricks.com/note",MatchSource.DOCS,hail/python/hail/docs/cloud/databricks.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst:1639,Security,secur,security,1639,"und in the Glow `Github repository; <https://github.com/projectglow/glow/tree/master/docker>`__. Install Hail via Docker with `Databricks Container Services; <https://docs.databricks.com/clusters/custom-containers.html>`__. Use the Docker Image URL, ``projectglow/databricks-hail:<hail_version>``, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; ----------------------. For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; ---------------. When initializing Hail, pass in the pre-created `SparkContext` and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. :**note**:. - Enable ``skip_logging_configuration`` to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; - Hail is not supported with `Credential passthrough; <https://docs.databricks.com/security/credential-passthrough/index.html>`__. :**code**:. >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) # doctest: +SKIP. Display Bokeh plots; -------------------. Hail uses the `Bokeh <https://docs.bokeh.org/en/latest/>`__ library to create plots. The `show`; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:. >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function `displayHTML` with `html` as its argument. See Databricks' `Bokeh docs <https://docs.databricks.com/notebooks/visualizations/bokeh.html>`__ for; more information. ",MatchSource.DOCS,hail/python/hail/docs/cloud/databricks.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst:1456,Testability,log,logs,1456,"les to build your own Hail container on Databricks can be found in the Glow `Github repository; <https://github.com/projectglow/glow/tree/master/docker>`__. Install Hail via Docker with `Databricks Container Services; <https://docs.databricks.com/clusters/custom-containers.html>`__. Use the Docker Image URL, ``projectglow/databricks-hail:<hail_version>``, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; ----------------------. For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; ---------------. When initializing Hail, pass in the pre-created `SparkContext` and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. :**note**:. - Enable ``skip_logging_configuration`` to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; - Hail is not supported with `Credential passthrough; <https://docs.databricks.com/security/credential-passthrough/index.html>`__. :**code**:. >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) # doctest: +SKIP. Display Bokeh plots; -------------------. Hail uses the `Bokeh <https://docs.bokeh.org/en/latest/>`__ library to create plots. The `show`; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:. >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function `displayHTML` with `html` as its argument. See Databricks' `Bokeh docs <https://docs.databricks.com/note",MatchSource.DOCS,hail/python/hail/docs/cloud/databricks.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/databricks.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst:2233,Energy Efficiency,charge,charges,2233,"`` to read a small fraction of the data:. .. code-block:: python. test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like ``pca`` or ``BlockMatrix`` multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it's scaling linearly. Estimating cost; ---------------. Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers. Google charges by the core-hour, so we can convert so-called ""wall clock time"" (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar ""dataproc premium"" fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:. .. code-block:: text. 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD. There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as ex",MatchSource.DOCS,hail/python/hail/docs/cloud/general_advice.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst:3036,Energy Efficiency,charge,charges,3036,"s a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like ``pca`` or ``BlockMatrix`` multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it's scaling linearly. Estimating cost; ---------------. Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers. Google charges by the core-hour, so we can convert so-called ""wall clock time"" (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar ""dataproc premium"" fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:. .. code-block:: text. 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD. There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive. In general, once you know the wall clock time of your job, you can enter your cluster parameters into the ; `Google Cloud Pricing Calculator <https://cloud.google.com/products/calculator/>`_. and get a precise estimate; of cost using the latest prices.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/general_advice.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst:900,Modifiability,config,configurable,900,"==============; General Advice; ==============. Start Small; -----------. The cloud has a reputation for easily burning lots of money. You don't want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it's easy to not be that person!. Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small ; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with ``filter_rows``.; Hail will make sure not to load data for other chromosomes. .. code-block:: python. import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail's ``hl.balding_nichols_model`` creates a random genotype dataset with configurable numbers of rows and columns. ; You can use these datasets for experimentation. As you'll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; ---------------. Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use ``filter_rows`` to read a small fraction of the data:. .. code-block:: python. test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like ``pca`` or ``BlockMatrix`` multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size o",MatchSource.DOCS,hail/python/hail/docs/cloud/general_advice.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst:663,Performance,load,load,663,"==============; General Advice; ==============. Start Small; -----------. The cloud has a reputation for easily burning lots of money. You don't want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it's easy to not be that person!. Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small ; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with ``filter_rows``.; Hail will make sure not to load data for other chromosomes. .. code-block:: python. import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail's ``hl.balding_nichols_model`` creates a random genotype dataset with configurable numbers of rows and columns. ; You can use these datasets for experimentation. As you'll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; ---------------. Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use ``filter_rows`` to read a small fraction of the data:. .. code-block:: python. test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like ``pca`` or ``BlockMatrix`` multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size o",MatchSource.DOCS,hail/python/hail/docs/cloud/general_advice.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst:1305,Usability,simpl,simple,1305,"Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small ; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with ``filter_rows``.; Hail will make sure not to load data for other chromosomes. .. code-block:: python. import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail's ``hl.balding_nichols_model`` creates a random genotype dataset with configurable numbers of rows and columns. ; You can use these datasets for experimentation. As you'll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; ---------------. Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use ``filter_rows`` to read a small fraction of the data:. .. code-block:: python. test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like ``pca`` or ``BlockMatrix`` multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it's scaling linearly. Estimating cost; ---------------. Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers. Google charges by the core-hour, so we can convert so",MatchSource.DOCS,hail/python/hail/docs/cloud/general_advice.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/general_advice.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:1513,Availability,down,down,1513,"oogle Dataproc <https://cloud.google.com/dataproc/>`__ clusters configured for Hail. This tool requires the `Google Cloud SDK <https://cloud.google.com/sdk/gcloud/>`__. Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:. .. code-block:: text. hailctl dataproc. It is possible to print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2530,Availability,down,down,2530,"oc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of t",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:380,Deployability,install,installations,380,"=====================; Google Cloud Platform; =====================. If you're new to Google Cloud in general, and would like an overview, linked ; `here <https://github.com/danking/hail-cloud-docs/blob/master/how-to-cloud.md>`__.; is a document written to onboard new users within our lab to cloud computing. ``hailctl dataproc``; --------------------. As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, ``hailctl``. This tool has a submodule called ``dataproc`` for working with; `Google Dataproc <https://cloud.google.com/dataproc/>`__ clusters configured for Hail. This tool requires the `Google Cloud SDK <https://cloud.google.com/sdk/gcloud/>`__. Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:. .. code-block:: text. hailctl dataproc. It is possible to print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/clou",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:1898,Deployability,install,install,1898,"print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For th",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2148,Deployability,install,install-gcs-connector,2148,"-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2195,Deployability,install,installed,2195,"gs to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:4002,Deployability,configurat,configuration,4002,"pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use ``--requester-pays-allow-all`` from above; or use the special ``--requester-pays-allow-annotation-db`` to enable the specific list of buckets that the annotation database; relies on. .. _vep_dataproc:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument ``--vep GRCh38``. A cluster started without the ``--vep`` argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using ``--vep``.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2490,Energy Efficiency,charge,charges,2490,"ext. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2539,Energy Efficiency,charge,charges,2539,"oc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of t",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2604,Energy Efficiency,charge,charges,2604,"oc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of t",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2736,Energy Efficiency,charge,charges,2736,"dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use ``--requester",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:736,Integrability,interface,interface,736,"=====================; Google Cloud Platform; =====================. If you're new to Google Cloud in general, and would like an overview, linked ; `here <https://github.com/danking/hail-cloud-docs/blob/master/how-to-cloud.md>`__.; is a document written to onboard new users within our lab to cloud computing. ``hailctl dataproc``; --------------------. As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, ``hailctl``. This tool has a submodule called ``dataproc`` for working with; `Google Dataproc <https://cloud.google.com/dataproc/>`__ clusters configured for Hail. This tool requires the `Google Cloud SDK <https://cloud.google.com/sdk/gcloud/>`__. Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:. .. code-block:: text. hailctl dataproc. It is possible to print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/clou",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:585,Modifiability,config,configured,585,"=====================; Google Cloud Platform; =====================. If you're new to Google Cloud in general, and would like an overview, linked ; `here <https://github.com/danking/hail-cloud-docs/blob/master/how-to-cloud.md>`__.; is a document written to onboard new users within our lab to cloud computing. ``hailctl dataproc``; --------------------. As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, ``hailctl``. This tool has a submodule called ``dataproc`` for working with; `Google Dataproc <https://cloud.google.com/dataproc/>`__ clusters configured for Hail. This tool requires the `Google Cloud SDK <https://cloud.google.com/sdk/gcloud/>`__. Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:. .. code-block:: text. hailctl dataproc. It is possible to print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/clou",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:1759,Modifiability,config,configured,1759,", we encourage; you to run the following command to see the list of modules:. .. code-block:: text. hailctl dataproc. It is possible to print help for a specific command using the ``help`` flag:. .. code-block:: text. hailctl dataproc start --help. To start a cluster, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:. .. code-block:: text. hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:. .. code-block:: text. hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:. .. code-block:: text. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should ",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:4002,Modifiability,config,configuration,4002,"pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use ``--requester-pays-allow-all`` from above; or use the special ``--requester-pays-allow-annotation-db`` to enable the specific list of buckets that the annotation database; relies on. .. _vep_dataproc:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument ``--vep GRCh38``. A cluster started without the ``--vep`` argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using ``--vep``.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:3186,Safety,avoid,avoid,3186," from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the :ref:`Annotation Database` will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use ``--requester-pays-allow-all`` from above; or use the special ``--requester-pays-allow-annotation-db`` to enable the specific list of buckets that the annotation database; relies on. .. _vep_dataproc:. Variant Effect Predictor (VEP); ------------------------------. The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:. .. code-block:: text. hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you ",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst:2464,Security,access,accessing,2464,"ext. hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:. .. code-block:: text. hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; ---------------------------------. A dataproc cluster created through ``hailctl dataproc`` will automatically be configured to allow hail to read files from ; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the ; `Cloud Storage Connector <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage>`_. The easiest way to do that is to; run the following script from your command line:. .. code-block:: text. curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you'll be able to read from paths beginning with ``gs`` directly from you laptop. .. _GCP Requester Pays:. Requester Pays; --------------. Some google cloud buckets are `Requester Pays <https://cloud.google.com/storage/docs/requester-pays>`_, meaning ; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are `Network Charges <https://cloud.google.com/storage/pricing#network-pricing>`_.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable ; requester pays on your ``hailctl dataproc`` cluster if you'd like to use it. To allow your cluster to read from any requester pays bucket, use:. .. code-block:: text. hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; ``--requester-pays-allow-buckets``. If you'd like to enable only reading from buckets named; ``hail-bucket`` and ``big-data``, you can specify the following:. .. code-block:: text. hailctl dataproc start my-",MatchSource.DOCS,hail/python/hail/docs/cloud/google_cloud.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/google_cloud.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:906,Availability,avail,available,906,"===================; Hail Query-on-Batch; ===================. .. warning::. Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please `contact us <https://discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Pre",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1515,Availability,avail,available,1515,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:577,Deployability,deploy,deploying,577,"===================; Hail Query-on-Batch; ===================. .. warning::. Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please `contact us <https://discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Pre",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:796,Deployability,install,install,796,"===================; Hail Query-on-Batch; ===================. .. warning::. Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please `contact us <https://discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Pre",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1825,Deployability,install,install,1825,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1267,Modifiability,config,config,1267,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1619,Modifiability,config,config,1619,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1755,Modifiability,config,config,1755,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1483,Performance,load,loaded,1483,"/discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predictor (VEP); ------------------------------. More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our `discussion forum <https://discuss.hail.is>`__.; ",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst:1007,Testability,log,login,1007,"================; Hail Query-on-Batch; ===================. .. warning::. Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please `contact us <https://discuss.hail.is>`__ if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the `Hail; Batch docs <https://hail.is/docs/batch/>`__. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our `discussion forum <https://discuss.hail.is>`__. Getting Started; ---------------. 1. Install Hail version 0.2.93 or later:. .. code-block:: text. pip install 'hail>=0.2.93'. 2. `Sign up for a Hail Batch account <https://auth.hail.is/signup>`__ (currently only available to; Broad affiliates). 3. Authenticate with Hail Batch. .. code-block:: text. hailctl auth login. 3. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with `automatic deletion after a set period of time; <https://cloud.google.com/storage/docs/lifecycle>`__. .. code-block:: text. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. 4. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the `Hail User; account page <https://auth.hail.is/user>`__. .. code-block:: text. hailctl config set batch/billing_project my-billing-project. 5. Set the default Hail Query backend to ``batch``:. .. code-block:: text. hailctl config set query/backend batch. 6. Now you are ready to `try Hail <../install/try.rst>`__! If you want to switch back to; Query-on-Spark, run the previous command again with ""spark"" in place of ""batch"". .. _vep_query_on_batch:. Variant Effect Predi",MatchSource.DOCS,hail/python/hail/docs/cloud/query_on_batch.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/cloud/query_on_batch.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst:513,Integrability,depend,depend,513,"Experimental; ============. This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions. At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. .. warning::. The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. .. toctree::; :maxdepth: 1. ldscsim. Contribution Guidelines; -----------------------; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. 1. Function docstrings are required. Hail uses; `NumPy style docstrings <https://www.sphinx-doc.org/en/stable/usage/extensions/example_numpy.html>`__.; 2. Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on ``Tests`` in; ``python/tests/experimental/test_experimental.py``; 3. Code style is not strictly enforced, aside from egregious violations. We do; recommend using `autopep8 <https://pypi.org/project/autopep8/>`__ though!. .. currentmodule:: hail.experimental. Annotation Database; -------------------. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.experimental.DB. Genetics Methods; ----------------. .. autosummary::. load_dataset; ld_score; ld_score_regression; write_expression; read_expression; filtering_allele_frequency; hail_metadata; plot_roc_curve; phase_by_transmission; phase_trio_matrix_by_transmission; explode_trio_matrix; import_gtf; get_gene_intervals; export_entries_by_col; pc_project. `dplyr`-inspired Methods; ------------",MatchSource.DOCS,hail/python/hail/docs/experimental/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst:1124,Testability,test,tests,1124,"package, and as a library of lightly reviewed; community submissions. At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. .. warning::. The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. .. toctree::; :maxdepth: 1. ldscsim. Contribution Guidelines; -----------------------; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. 1. Function docstrings are required. Hail uses; `NumPy style docstrings <https://www.sphinx-doc.org/en/stable/usage/extensions/example_numpy.html>`__.; 2. Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on ``Tests`` in; ``python/tests/experimental/test_experimental.py``; 3. Code style is not strictly enforced, aside from egregious violations. We do; recommend using `autopep8 <https://pypi.org/project/autopep8/>`__ though!. .. currentmodule:: hail.experimental. Annotation Database; -------------------. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.experimental.DB. Genetics Methods; ----------------. .. autosummary::. load_dataset; ld_score; ld_score_regression; write_expression; read_expression; filtering_allele_frequency; hail_metadata; plot_roc_curve; phase_by_transmission; phase_trio_matrix_by_transmission; explode_trio_matrix; import_gtf; get_gene_intervals; export_entries_by_col; pc_project. `dplyr`-inspired Methods; ------------------------. .. autosummary::. gather; separate; spread. Functions; ---------. .. autofunction:: load_dataset; .. autofunction:: ld_score; ",MatchSource.DOCS,hail/python/hail/docs/experimental/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst:1183,Testability,test,tests,1183,"he experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. .. warning::. The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. .. toctree::; :maxdepth: 1. ldscsim. Contribution Guidelines; -----------------------; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. 1. Function docstrings are required. Hail uses; `NumPy style docstrings <https://www.sphinx-doc.org/en/stable/usage/extensions/example_numpy.html>`__.; 2. Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on ``Tests`` in; ``python/tests/experimental/test_experimental.py``; 3. Code style is not strictly enforced, aside from egregious violations. We do; recommend using `autopep8 <https://pypi.org/project/autopep8/>`__ though!. .. currentmodule:: hail.experimental. Annotation Database; -------------------. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.experimental.DB. Genetics Methods; ----------------. .. autosummary::. load_dataset; ld_score; ld_score_regression; write_expression; read_expression; filtering_allele_frequency; hail_metadata; plot_roc_curve; phase_by_transmission; phase_trio_matrix_by_transmission; explode_trio_matrix; import_gtf; get_gene_intervals; export_entries_by_col; pc_project. `dplyr`-inspired Methods; ------------------------. .. autosummary::. gather; separate; spread. Functions; ---------. .. autofunction:: load_dataset; .. autofunction:: ld_score; .. autofunction:: ld_score_regression; .. autofunction:: write_expression; .. auto",MatchSource.DOCS,hail/python/hail/docs/experimental/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst:1233,Testability,test,tests,1233,"he experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. .. warning::. The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. .. toctree::; :maxdepth: 1. ldscsim. Contribution Guidelines; -----------------------; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. 1. Function docstrings are required. Hail uses; `NumPy style docstrings <https://www.sphinx-doc.org/en/stable/usage/extensions/example_numpy.html>`__.; 2. Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on ``Tests`` in; ``python/tests/experimental/test_experimental.py``; 3. Code style is not strictly enforced, aside from egregious violations. We do; recommend using `autopep8 <https://pypi.org/project/autopep8/>`__ though!. .. currentmodule:: hail.experimental. Annotation Database; -------------------. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. hail.experimental.DB. Genetics Methods; ----------------. .. autosummary::. load_dataset; ld_score; ld_score_regression; write_expression; read_expression; filtering_allele_frequency; hail_metadata; plot_roc_curve; phase_by_transmission; phase_trio_matrix_by_transmission; explode_trio_matrix; import_gtf; get_gene_intervals; export_entries_by_col; pc_project. `dplyr`-inspired Methods; ------------------------. .. autosummary::. gather; separate; spread. Functions; ---------. .. autofunction:: load_dataset; .. autofunction:: ld_score; .. autofunction:: ld_score_regression; .. autofunction:: write_expression; .. auto",MatchSource.DOCS,hail/python/hail/docs/experimental/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/experimental/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/genetics.rst:262,Availability,down,downcode,262,Genetics functions; ------------------. .. currentmodule:: hail.expr.functions. .. autosummary::. locus; locus_from_global_position; locus_interval; parse_locus; parse_variant; parse_locus_interval; variant_str; call; unphased_diploid_gt_index_call; parse_call; downcode; triangle; is_snp; is_mnp; is_transition; is_transversion; is_insertion; is_deletion; is_indel; is_star; is_complex; is_strand_ambiguous; is_valid_contig; is_valid_locus; contig_length; allele_type; numeric_allele_type; pl_dosage; gp_dosage; get_sequence; mendel_error_code; liftover; min_rep; reverse_complement. .. autofunction:: locus; .. autofunction:: locus_from_global_position; .. autofunction:: locus_interval; .. autofunction:: parse_locus; .. autofunction:: parse_variant; .. autofunction:: parse_locus_interval; .. autofunction:: variant_str; .. autofunction:: call; .. autofunction:: unphased_diploid_gt_index_call; .. autofunction:: parse_call; .. autofunction:: downcode; .. autofunction:: triangle; .. autofunction:: is_snp; .. autofunction:: is_mnp; .. autofunction:: is_transition; .. autofunction:: is_transversion; .. autofunction:: is_insertion; .. autofunction:: is_deletion; .. autofunction:: is_indel; .. autofunction:: is_star; .. autofunction:: is_complex; .. autofunction:: is_strand_ambiguous; .. autofunction:: is_valid_contig; .. autofunction:: is_valid_locus; .. autofunction:: contig_length; .. autofunction:: allele_type; .. autofunction:: numeric_allele_type; .. autofunction:: pl_dosage; .. autofunction:: gp_dosage; .. autofunction:: get_sequence; .. autofunction:: mendel_error_code; .. autofunction:: liftover; .. autofunction:: min_rep; .. autofunction:: reverse_complement; ,MatchSource.DOCS,hail/python/hail/docs/functions/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/genetics.rst:947,Availability,down,downcode,947,Genetics functions; ------------------. .. currentmodule:: hail.expr.functions. .. autosummary::. locus; locus_from_global_position; locus_interval; parse_locus; parse_variant; parse_locus_interval; variant_str; call; unphased_diploid_gt_index_call; parse_call; downcode; triangle; is_snp; is_mnp; is_transition; is_transversion; is_insertion; is_deletion; is_indel; is_star; is_complex; is_strand_ambiguous; is_valid_contig; is_valid_locus; contig_length; allele_type; numeric_allele_type; pl_dosage; gp_dosage; get_sequence; mendel_error_code; liftover; min_rep; reverse_complement. .. autofunction:: locus; .. autofunction:: locus_from_global_position; .. autofunction:: locus_interval; .. autofunction:: parse_locus; .. autofunction:: parse_variant; .. autofunction:: parse_locus_interval; .. autofunction:: variant_str; .. autofunction:: call; .. autofunction:: unphased_diploid_gt_index_call; .. autofunction:: parse_call; .. autofunction:: downcode; .. autofunction:: triangle; .. autofunction:: is_snp; .. autofunction:: is_mnp; .. autofunction:: is_transition; .. autofunction:: is_transversion; .. autofunction:: is_insertion; .. autofunction:: is_deletion; .. autofunction:: is_indel; .. autofunction:: is_star; .. autofunction:: is_complex; .. autofunction:: is_strand_ambiguous; .. autofunction:: is_valid_contig; .. autofunction:: is_valid_locus; .. autofunction:: contig_length; .. autofunction:: allele_type; .. autofunction:: numeric_allele_type; .. autofunction:: pl_dosage; .. autofunction:: gp_dosage; .. autofunction:: get_sequence; .. autofunction:: mendel_error_code; .. autofunction:: liftover; .. autofunction:: min_rep; .. autofunction:: reverse_complement; ,MatchSource.DOCS,hail/python/hail/docs/functions/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst:2138,Availability,down,downcode,2138,r_missing; range; query_table. .. rubric:: Constructors. .. autosummary::. bool; float; float32; float64; int; int32; int64; interval; str; struct; tuple. .. rubric:: Collection constructors. .. autosummary::. array; empty_array; set; empty_set; dict; empty_dict. .. rubric:: Collection functions. .. autosummary::. len; map; flatmap; zip; enumerate; zip_with_index; flatten; any; all; filter; sorted; find; group_by; fold; array_scan; reversed; keyed_intersection; keyed_union. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. rubric:: String functions. .. autosummary::. format; json; parse_json; hamming; delimit; entropy; parse_int; parse_int32; parse_int64; parse_float; parse_float32; parse_float64. .. rubric:: Statistical functions. .. autosummary::. chi_squared_test; fisher_exact_test; contingency_table_test; cochran_mantel_haenszel_test; dbeta; dpois; hardy_weinberg_test; pchisqtail; pnorm; ppois; qchisqtail; qnorm; qpois. .. rubric:: Randomness. .. autosummary::. rand_bool; rand_beta; rand_cat; rand_dirichlet; rand_gamma; rand_norm; rand_pois; rand_unif; rand_int32; rand_int64; shuffle. .. rubric:: Genetics functions. .. autosummary::. locus; locus_from_global_position; locus_interval; parse_locus; parse_variant; parse_locus_interval; variant_str; call; unphased_diploid_gt_index_call; parse_call; downcode; triangle; is_snp; is_mnp; is_transition; is_transversion; is_insertion; is_deletion; is_indel; is_star; is_complex; is_valid_contig; is_valid_locus; contig_length; allele_type; pl_dosage; gp_dosage; get_sequence; mendel_error_code; liftover; min_rep; reverse_complement; ,MatchSource.DOCS,hail/python/hail/docs/functions/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst:62,Security,expose,exposed,62,".. _sec-functions:. Functions; =========. These functions are exposed at the top level of the module, e.g. ``hl.case``. .. currentmodule:: hail.expr.functions. .. toctree::; :maxdepth: 2. core; constructors; collections; numeric; string; stats; random; genetics. .. rubric:: Core language functions. .. autosummary::. literal; cond; if_else; switch; case; bind; rbind; null; is_missing; is_defined; coalesce; or_else; or_missing; range; query_table. .. rubric:: Constructors. .. autosummary::. bool; float; float32; float64; int; int32; int64; interval; str; struct; tuple. .. rubric:: Collection constructors. .. autosummary::. array; empty_array; set; empty_set; dict; empty_dict. .. rubric:: Collection functions. .. autosummary::. len; map; flatmap; zip; enumerate; zip_with_index; flatten; any; all; filter; sorted; find; group_by; fold; array_scan; reversed; keyed_intersection; keyed_union. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. rubric:: String functions. .. autosummary::. format; json; parse_json; hamming; delimit; entropy; parse_int; parse_int32; parse_int64; parse_float; parse_float32; parse_float64. .. rubric:: Statistical functions. .. autosummary::. chi_squared_test; fisher_exact_test; contingency_table_test; cochran_mantel_haenszel_test; dbeta; dpois; hardy_weinberg_test; pchisqtail; pnorm; ppois; qchisqtail; qnorm; qpois. .. rubric:: Randomness. .. autosummary::. rand_bool; rand_beta; rand_cat; rand_dirichlet; rand_gamma; rand_norm; rand_pois; rand_unif; rand_int32; rand_int64; shuffle. .. rubric:: Genetics functions. .. autosummary::. locus; locus_from_global_po",MatchSource.DOCS,hail/python/hail/docs/functions/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst:1080,Testability,log,log,1080," exposed at the top level of the module, e.g. ``hl.case``. .. currentmodule:: hail.expr.functions. .. toctree::; :maxdepth: 2. core; constructors; collections; numeric; string; stats; random; genetics. .. rubric:: Core language functions. .. autosummary::. literal; cond; if_else; switch; case; bind; rbind; null; is_missing; is_defined; coalesce; or_else; or_missing; range; query_table. .. rubric:: Constructors. .. autosummary::. bool; float; float32; float64; int; int32; int64; interval; str; struct; tuple. .. rubric:: Collection constructors. .. autosummary::. array; empty_array; set; empty_set; dict; empty_dict. .. rubric:: Collection functions. .. autosummary::. len; map; flatmap; zip; enumerate; zip_with_index; flatten; any; all; filter; sorted; find; group_by; fold; array_scan; reversed; keyed_intersection; keyed_union. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. rubric:: String functions. .. autosummary::. format; json; parse_json; hamming; delimit; entropy; parse_int; parse_int32; parse_int64; parse_float; parse_float32; parse_float64. .. rubric:: Statistical functions. .. autosummary::. chi_squared_test; fisher_exact_test; contingency_table_test; cochran_mantel_haenszel_test; dbeta; dpois; hardy_weinberg_test; pchisqtail; pnorm; ppois; qchisqtail; qnorm; qpois. .. rubric:: Randomness. .. autosummary::. rand_bool; rand_beta; rand_cat; rand_dirichlet; rand_gamma; rand_norm; rand_pois; rand_unif; rand_int32; rand_int64; shuffle. .. rubric:: Genetics functions. .. autosummary::. locus; locus_from_global_position; locus_interval; parse_locus; parse_variant; parse_lo",MatchSource.DOCS,hail/python/hail/docs/functions/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst:1092,Testability,log,logit,1092," exposed at the top level of the module, e.g. ``hl.case``. .. currentmodule:: hail.expr.functions. .. toctree::; :maxdepth: 2. core; constructors; collections; numeric; string; stats; random; genetics. .. rubric:: Core language functions. .. autosummary::. literal; cond; if_else; switch; case; bind; rbind; null; is_missing; is_defined; coalesce; or_else; or_missing; range; query_table. .. rubric:: Constructors. .. autosummary::. bool; float; float32; float64; int; int32; int64; interval; str; struct; tuple. .. rubric:: Collection constructors. .. autosummary::. array; empty_array; set; empty_set; dict; empty_dict. .. rubric:: Collection functions. .. autosummary::. len; map; flatmap; zip; enumerate; zip_with_index; flatten; any; all; filter; sorted; find; group_by; fold; array_scan; reversed; keyed_intersection; keyed_union. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. rubric:: String functions. .. autosummary::. format; json; parse_json; hamming; delimit; entropy; parse_int; parse_int32; parse_int64; parse_float; parse_float32; parse_float64. .. rubric:: Statistical functions. .. autosummary::. chi_squared_test; fisher_exact_test; contingency_table_test; cochran_mantel_haenszel_test; dbeta; dpois; hardy_weinberg_test; pchisqtail; pnorm; ppois; qchisqtail; qnorm; qpois. .. rubric:: Randomness. .. autosummary::. rand_bool; rand_beta; rand_cat; rand_dirichlet; rand_gamma; rand_norm; rand_pois; rand_unif; rand_int32; rand_int64; shuffle. .. rubric:: Genetics functions. .. autosummary::. locus; locus_from_global_position; locus_interval; parse_locus; parse_variant; parse_lo",MatchSource.DOCS,hail/python/hail/docs/functions/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst:260,Testability,log,log,260,Numeric functions; -----------------; .. currentmodule:: hail.expr.functions. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. autofunction:: abs; .. autofunction:: approx_equal; .. autofunction:: bit_and; .. autofunction:: bit_or; .. autofunction:: bit_xor; .. autofunction:: bit_lshift; .. autofunction:: bit_rshift; .. autofunction:: bit_not; .. autofunction:: bit_count; .. autofunction:: exp; .. autofunction:: expit; .. autofunction:: is_nan; .. autofunction:: is_finite; .. autofunction:: is_infinite; .. autofunction:: log; .. autofunction:: log10; .. autofunction:: logit; .. autofunction:: floor; .. autofunction:: ceil; .. autofunction:: sqrt; .. autofunction:: sign; .. autofunction:: min; .. autofunction:: nanmin; .. autofunction:: max; .. autofunction:: nanmax; .. autofunction:: mean; .. autofunction:: median; .. autofunction:: product; .. autofunction:: sum; .. autofunction:: cumulative_sum; .. autofunction:: argmin; .. autofunction:: argmax; .. autofunction:: corr; .. autofunction:: uniroot; .. autofunction:: binary_search; ,MatchSource.DOCS,hail/python/hail/docs/functions/numeric.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst:272,Testability,log,logit,272,Numeric functions; -----------------; .. currentmodule:: hail.expr.functions. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. autofunction:: abs; .. autofunction:: approx_equal; .. autofunction:: bit_and; .. autofunction:: bit_or; .. autofunction:: bit_xor; .. autofunction:: bit_lshift; .. autofunction:: bit_rshift; .. autofunction:: bit_not; .. autofunction:: bit_count; .. autofunction:: exp; .. autofunction:: expit; .. autofunction:: is_nan; .. autofunction:: is_finite; .. autofunction:: is_infinite; .. autofunction:: log; .. autofunction:: log10; .. autofunction:: logit; .. autofunction:: floor; .. autofunction:: ceil; .. autofunction:: sqrt; .. autofunction:: sign; .. autofunction:: min; .. autofunction:: nanmin; .. autofunction:: max; .. autofunction:: nanmax; .. autofunction:: mean; .. autofunction:: median; .. autofunction:: product; .. autofunction:: sum; .. autofunction:: cumulative_sum; .. autofunction:: argmin; .. autofunction:: argmax; .. autofunction:: corr; .. autofunction:: uniroot; .. autofunction:: binary_search; ,MatchSource.DOCS,hail/python/hail/docs/functions/numeric.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst:927,Testability,log,log,927,Numeric functions; -----------------; .. currentmodule:: hail.expr.functions. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. autofunction:: abs; .. autofunction:: approx_equal; .. autofunction:: bit_and; .. autofunction:: bit_or; .. autofunction:: bit_xor; .. autofunction:: bit_lshift; .. autofunction:: bit_rshift; .. autofunction:: bit_not; .. autofunction:: bit_count; .. autofunction:: exp; .. autofunction:: expit; .. autofunction:: is_nan; .. autofunction:: is_finite; .. autofunction:: is_infinite; .. autofunction:: log; .. autofunction:: log10; .. autofunction:: logit; .. autofunction:: floor; .. autofunction:: ceil; .. autofunction:: sqrt; .. autofunction:: sign; .. autofunction:: min; .. autofunction:: nanmin; .. autofunction:: max; .. autofunction:: nanmax; .. autofunction:: mean; .. autofunction:: median; .. autofunction:: product; .. autofunction:: sum; .. autofunction:: cumulative_sum; .. autofunction:: argmin; .. autofunction:: argmax; .. autofunction:: corr; .. autofunction:: uniroot; .. autofunction:: binary_search; ,MatchSource.DOCS,hail/python/hail/docs/functions/numeric.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst:975,Testability,log,logit,975,Numeric functions; -----------------; .. currentmodule:: hail.expr.functions. .. rubric:: Numeric functions. .. autosummary::. abs; approx_equal; bit_and; bit_or; bit_xor; bit_lshift; bit_rshift; bit_not; bit_count; exp; expit; is_nan; is_finite; is_infinite; log; log10; logit; sign; sqrt; int; int32; int64; float; float32; float64; floor; ceil; uniroot. .. rubric:: Numeric collection functions. .. autosummary::. min; nanmin; max; nanmax; mean; median; product; sum; cumulative_sum; argmin; argmax; corr; binary_search. .. autofunction:: abs; .. autofunction:: approx_equal; .. autofunction:: bit_and; .. autofunction:: bit_or; .. autofunction:: bit_xor; .. autofunction:: bit_lshift; .. autofunction:: bit_rshift; .. autofunction:: bit_not; .. autofunction:: bit_count; .. autofunction:: exp; .. autofunction:: expit; .. autofunction:: is_nan; .. autofunction:: is_finite; .. autofunction:: is_infinite; .. autofunction:: log; .. autofunction:: log10; .. autofunction:: logit; .. autofunction:: floor; .. autofunction:: ceil; .. autofunction:: sqrt; .. autofunction:: sign; .. autofunction:: min; .. autofunction:: nanmin; .. autofunction:: max; .. autofunction:: nanmax; .. autofunction:: mean; .. autofunction:: median; .. autofunction:: product; .. autofunction:: sum; .. autofunction:: cumulative_sum; .. autofunction:: argmin; .. autofunction:: argmax; .. autofunction:: corr; .. autofunction:: uniroot; .. autofunction:: binary_search; ,MatchSource.DOCS,hail/python/hail/docs/functions/numeric.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/numeric.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst:4288,Deployability,pipeline,pipeline,4288,".rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; ===============================. The values of a random function are fully determined by three things:. * The seed set on the function itself. If not specified, these are simply; generated sequentially.; * Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a :func:`.range_table`,; this data is simply the row id, as suggested by the previous examples.; * The global seed. This is fixed for the entire session, and can only be set; using the ``global_seed`` argument to :func:`.init`. To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; :func:`.reset_global_randomness` at the start of a pipeline, which resets the; counter used to generate seeds. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; `global_seed` in :func:`.init`. If not specified, the global seed is chosen; randomly. All documentation examples were computed using ``global_seed=0``. >>> hl.stop() # doctest: +SKIP; >>> hl.init(global_seed=0) # doctest: +SKIP; >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +SKIP; [0.9828239225846387, 0.49094525115847415]. .. autosummary::. rand_bool; rand_beta; rand_cat; rand_dirichlet; rand_gamma; rand_norm; rand_pois; rand_unif; rand_int32; rand_int64; shuffle. .. autofunction:: rand_bool; .. autofunction:: rand_beta; .. autofunction:: rand_cat; ..",MatchSource.DOCS,hail/python/hail/docs/functions/random.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst:3700,Usability,simpl,simply,3700,"_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876]. However, moving it to a sufficiently different context will produce different; results:. >>> table = hl.utils.range_table(7, 1); >>> table = table.filter(table.idx >= 2).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of. >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; ===============================. The values of a random function are fully determined by three things:. * The seed set on the function itself. If not specified, these are simply; generated sequentially.; * Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a :func:`.range_table`,; this data is simply the row id, as suggested by the previous examples.; * The global seed. This is fixed for the entire session, and can only be set; using the ``global_seed`` argument to :func:`.init`. To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; :func:`.reset_global_randomness` at the start of a pipeline, which resets the; counter used to generate seeds. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition speci",MatchSource.DOCS,hail/python/hail/docs/functions/random.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst:3903,Usability,simpl,simply,3903,"duce different; results:. >>> table = hl.utils.range_table(7, 1); >>> table = table.filter(table.idx >= 2).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of. >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; ===============================. The values of a random function are fully determined by three things:. * The seed set on the function itself. If not specified, these are simply; generated sequentially.; * Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a :func:`.range_table`,; this data is simply the row id, as suggested by the previous examples.; * The global seed. This is fixed for the entire session, and can only be set; using the ``global_seed`` argument to :func:`.init`. To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; :func:`.reset_global_randomness` at the start of a pipeline, which resets the; counter used to generate seeds. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; `global_seed` in :func:`.init`. If not specified, the global seed is chosen; randomly. All documentation examples were computed using ``global_seed=0``. >>> hl.stop() # doctest: +SKIP; >>> hl.init(global_seed=",MatchSource.DOCS,hail/python/hail/docs/functions/random.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/functions/random.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/ggplot/index.rst:451,Modifiability,flexible,flexible,451,"----------------------------------; Plotting With hail.ggplot Overview; ----------------------------------. .. warning::; Plotting functionality is in early stages and is experimental. The ``hl.ggplot`` module is designed based on R's tidyverse ``ggplot2`` library. This module provides a subset of ``ggplot2``'s; functionality to allow users to generate plots in much the same way they would in ``ggplot2``. This module is intended to be a new, more flexible way of plotting compared to the ``hl.plot`` module. This module; currently uses plotly to generate plots, as opposed to ``hl.plot``, which uses bokeh. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.ggplot. .. rubric:: Core functions. .. autosummary::; :nosignatures:. ggplot; aes; coord_cartesian. .. autofunction:: ggplot; .. autofunction:: aes; .. autofunction:: coord_cartesian. .. rubric:: Geoms. .. autosummary::; :nosignatures:. geom_point; geom_line; geom_text; geom_bar; geom_col; geom_histogram; geom_density; geom_hline; geom_vline; geom_area; geom_ribbon. .. autofunction:: geom_point; .. autofunction:: geom_line; .. autofunction:: geom_text; .. autofunction:: geom_bar; .. autofunction:: geom_col; .. autofunction:: geom_histogram; .. autofunction:: geom_density; .. autofunction:: geom_hline; .. autofunction:: geom_vline; .. autofunction:: geom_area; .. autofunction:: geom_ribbon. .. rubric:: Scales. .. autosummary::; :nosignatures:. scale_x_continuous; scale_x_discrete; scale_x_genomic; scale_x_log10; scale_x_reverse; scale_y_continuous; scale_y_discrete; scale_y_log10; scale_y_reverse; scale_color_continuous; scale_color_discrete; scale_color_hue; scale_color_manual; scale_color_identity; scale_fill_continuous; scale_fill_discrete; scale_fill_hue; scale_fill_manual; scale_fill_identity. .. autofunction:: scale_x_continuous; .. autofunction:: scale_x_discrete; .. autofunction:: scale_x_genomic; .. autofunction:: scale_x_log10; .. autofunction:: scale_x_reverse; .. autofunction:: scale_y_continuous; .. autof",MatchSource.DOCS,hail/python/hail/docs/ggplot/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/ggplot/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:5590,Deployability,toggle,toggle,5590,"ute the mean of the entry-indexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:. >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). :**dependencies**: :meth:`.MatrixTable.aggregate_entries`, :func:`.aggregators.mean`, :func:`.aggregators.fraction`, :class:`.StructExpression`. Aggregate Per Column Group; ~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Group the columns of the matrix table by the column-indexed; field ``cohort`` and compute the call rate per cohort. :**code**:. >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). :**dependencies**: :meth:`.MatrixTable.group_rows_",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:5613,Deployability,toggle,toggle-content,5613,"ndexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:. >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). :**dependencies**: :meth:`.MatrixTable.aggregate_entries`, :func:`.aggregators.mean`, :func:`.aggregators.fraction`, :class:`.StructExpression`. Aggregate Per Column Group; ~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Group the columns of the matrix table by the column-indexed; field ``cohort`` and compute the call rate per cohort. :**code**:. >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). :**dependencies**: :meth:`.MatrixTable.group_rows_by`, :class:`.GroupedMatrix",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:6697,Deployability,toggle,toggle,6697,"l_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). :**dependencies**: :meth:`.MatrixTable.group_rows_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the rows of the matrix table by the row-indexed field ``gene``; using :meth:`.MatrixTable.group_rows_by`, which returns a; :class:`.GroupedMatrixTable`. Then use :meth:`.GroupedMatrixTable.aggregate`; to compute an aggregation per grouped row. The result is a matrix table with an entry field ``n_non_ref`` that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to ``group_rows_by``, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to ``aggregate``. Other row fields and entry fields; are dropped.; ",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:6720,Deployability,toggle,toggle-content,6720,"l_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). :**dependencies**: :meth:`.MatrixTable.group_rows_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the rows of the matrix table by the row-indexed field ``gene``; using :meth:`.MatrixTable.group_rows_by`, which returns a; :class:`.GroupedMatrixTable`. Then use :meth:`.GroupedMatrixTable.aggregate`; to compute an aggregation per grouped row. The result is a matrix table with an entry field ``n_non_ref`` that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to ``group_rows_by``, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to ``aggregate``. Other row fields and entry fields; are dropped.; ",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:437,Integrability,depend,dependencies,437,"Aggregation; ===========. For a full list of aggregators, see the :ref:`aggregators <sec-aggregators>`; section of the API reference. Table Aggregations; ------------------. Aggregate Over Rows Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the fraction of rows where ``SEX == 'M'`` in a table. :**code**:. >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Compute two aggregation statistics, the fraction of rows where; ``SEX == 'M'`` and the mean value of ``X``, from the rows of a table. :**code**:. >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Per Group; ~~~~~~~~~~~~~~~~~~~. :**description**: Group the table ``ht`` by ``ID`` and compute the mean value of ``X`` per group. :**code**:. >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). :**dependencies**: :meth:`.Table.group_by`, :meth:`.GroupedTable.aggregate`, :func:`.aggregators.mean`. Matrix Table Aggregations; -------------------------. Aggregate Entries Per Row (Over Columns); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Count the number of occurrences of each unique ``GT`` field per row, i.e.; aggregate over the columns of the matrix table. Methods :meth:`.MatrixTable.filter_rows`, :meth:`.MatrixTable.select_rows`,; and :meth:`.MatrixTable.transmute_rows` also support aggregation over columns. :**code**:. >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:871,Integrability,depend,dependencies,871,"Aggregation; ===========. For a full list of aggregators, see the :ref:`aggregators <sec-aggregators>`; section of the API reference. Table Aggregations; ------------------. Aggregate Over Rows Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the fraction of rows where ``SEX == 'M'`` in a table. :**code**:. >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Compute two aggregation statistics, the fraction of rows where; ``SEX == 'M'`` and the mean value of ``X``, from the rows of a table. :**code**:. >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Per Group; ~~~~~~~~~~~~~~~~~~~. :**description**: Group the table ``ht`` by ``ID`` and compute the mean value of ``X`` per group. :**code**:. >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). :**dependencies**: :meth:`.Table.group_by`, :meth:`.GroupedTable.aggregate`, :func:`.aggregators.mean`. Matrix Table Aggregations; -------------------------. Aggregate Entries Per Row (Over Columns); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Count the number of occurrences of each unique ``GT`` field per row, i.e.; aggregate over the columns of the matrix table. Methods :meth:`.MatrixTable.filter_rows`, :meth:`.MatrixTable.select_rows`,; and :meth:`.MatrixTable.transmute_rows` also support aggregation over columns. :**code**:. >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:1226,Integrability,depend,dependencies,1226,"~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the fraction of rows where ``SEX == 'M'`` in a table. :**code**:. >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Compute two aggregation statistics, the fraction of rows where; ``SEX == 'M'`` and the mean value of ``X``, from the rows of a table. :**code**:. >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Per Group; ~~~~~~~~~~~~~~~~~~~. :**description**: Group the table ``ht`` by ``ID`` and compute the mean value of ``X`` per group. :**code**:. >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). :**dependencies**: :meth:`.Table.group_by`, :meth:`.GroupedTable.aggregate`, :func:`.aggregators.mean`. Matrix Table Aggregations; -------------------------. Aggregate Entries Per Row (Over Columns); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Count the number of occurrences of each unique ``GT`` field per row, i.e.; aggregate over the columns of the matrix table. Methods :meth:`.MatrixTable.filter_rows`, :meth:`.MatrixTable.select_rows`,; and :meth:`.MatrixTable.transmute_rows` also support aggregation over columns. :**code**:. >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the ``GQ`` field per column, i.e. aggregate over the rows; of the MatrixTable. Methods :meth:`.MatrixTable.filter_cols`, :meth:`.MatrixTable.select_cols`,; and :meth:`.MatrixTable.trans",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:1846,Integrability,depend,dependencies,1846,"ean_x=6.5). :**dependencies**: :meth:`.Table.aggregate`, :func:`.aggregators.fraction`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Per Group; ~~~~~~~~~~~~~~~~~~~. :**description**: Group the table ``ht`` by ``ID`` and compute the mean value of ``X`` per group. :**code**:. >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). :**dependencies**: :meth:`.Table.group_by`, :meth:`.GroupedTable.aggregate`, :func:`.aggregators.mean`. Matrix Table Aggregations; -------------------------. Aggregate Entries Per Row (Over Columns); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Count the number of occurrences of each unique ``GT`` field per row, i.e.; aggregate over the columns of the matrix table. Methods :meth:`.MatrixTable.filter_rows`, :meth:`.MatrixTable.select_rows`,; and :meth:`.MatrixTable.transmute_rows` also support aggregation over columns. :**code**:. >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the ``GQ`` field per column, i.e. aggregate over the rows; of the MatrixTable. Methods :meth:`.MatrixTable.filter_cols`, :meth:`.MatrixTable.select_cols`,; and :meth:`.MatrixTable.transmute_cols` also support aggregation over rows. :**code**:. >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). :**dependencies**: :meth:`.MatrixTable.annotate_cols`, :func:`.aggregators.mean`. Aggregate Column Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**:. Aggregate over the column-indexed field ``pheno.is_female`` to compute the; fraction of female samples in the matrix table. :**code**:. >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:2360,Integrability,depend,dependencies,2360,"---------. Aggregate Entries Per Row (Over Columns); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Count the number of occurrences of each unique ``GT`` field per row, i.e.; aggregate over the columns of the matrix table. Methods :meth:`.MatrixTable.filter_rows`, :meth:`.MatrixTable.select_rows`,; and :meth:`.MatrixTable.transmute_rows` also support aggregation over columns. :**code**:. >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the ``GQ`` field per column, i.e. aggregate over the rows; of the MatrixTable. Methods :meth:`.MatrixTable.filter_cols`, :meth:`.MatrixTable.select_cols`,; and :meth:`.MatrixTable.transmute_cols` also support aggregation over rows. :**code**:. >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). :**dependencies**: :meth:`.MatrixTable.annotate_cols`, :func:`.aggregators.mean`. Aggregate Column Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**:. Aggregate over the column-indexed field ``pheno.is_female`` to compute the; fraction of female samples in the matrix table. :**code**:. >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, ``fraction_female`` and ``case_ratio``. :**code**:. >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). :**dependencies**: :meth:`.MatrixT",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:2784,Integrability,depend,dependencies,2784,"annotate_rows(gt_counter=hl.agg.counter(mt.GT)). :**dependencies**: :meth:`.MatrixTable.annotate_rows`, :func:`.aggregators.counter`. Aggregate Entries Per Column (Over Rows); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the ``GQ`` field per column, i.e. aggregate over the rows; of the MatrixTable. Methods :meth:`.MatrixTable.filter_cols`, :meth:`.MatrixTable.select_cols`,; and :meth:`.MatrixTable.transmute_cols` also support aggregation over rows. :**code**:. >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). :**dependencies**: :meth:`.MatrixTable.annotate_cols`, :func:`.aggregators.mean`. Aggregate Column Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**:. Aggregate over the column-indexed field ``pheno.is_female`` to compute the; fraction of female samples in the matrix table. :**code**:. >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, ``fraction_female`` and ``case_ratio``. :**code**:. >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`, :func:`.aggregators.count_where`, :class:`.StructExpression`. Aggregate Row Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the mean value of the row-indexed field ``qual``. :**code**:. >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. :**dependencies**: :meth:`.MatrixTable.ag",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:3339,Integrability,depend,dependencies,3339,"t.GQ)). :**dependencies**: :meth:`.MatrixTable.annotate_cols`, :func:`.aggregators.mean`. Aggregate Column Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**:. Aggregate over the column-indexed field ``pheno.is_female`` to compute the; fraction of female samples in the matrix table. :**code**:. >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, ``fraction_female`` and ``case_ratio``. :**code**:. >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`, :func:`.aggregators.count_where`, :class:`.StructExpression`. Aggregate Row Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the mean value of the row-indexed field ``qual``. :**code**:. >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.mean`. Multiple aggregations; ..................... :**description**:. Perform two row aggregations: count the number of row values of ``qual``; that are greater than 40, and compute the mean value of ``qual``.; The result is a single struct containing two nested fields, ``n_high_quality`` and ``mean_qual``. :**code**:. >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). :**dependencies*",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:3756,Integrability,depend,dependencies,3756,"emale)); 0.44. :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`. Multiple aggregations; ..................... :**description**: Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, ``fraction_female`` and ``case_ratio``. :**code**:. >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). :**dependencies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`, :func:`.aggregators.count_where`, :class:`.StructExpression`. Aggregate Row Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the mean value of the row-indexed field ``qual``. :**code**:. >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.mean`. Multiple aggregations; ..................... :**description**:. Perform two row aggregations: count the number of row values of ``qual``; that are greater than 40, and compute the mean value of ``qual``.; The result is a single struct containing two nested fields, ``n_high_quality`` and ``mean_qual``. :**code**:. >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.count_where`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Entry Values Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the entry-indexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:.",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:4336,Integrability,depend,dependencies,4336,"ncies**: :meth:`.MatrixTable.aggregate_cols`, :func:`.aggregators.fraction`, :func:`.aggregators.count_where`, :class:`.StructExpression`. Aggregate Row Values Into a Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. One aggregation; ............... :**description**: Compute the mean value of the row-indexed field ``qual``. :**code**:. >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.mean`. Multiple aggregations; ..................... :**description**:. Perform two row aggregations: count the number of row values of ``qual``; that are greater than 40, and compute the mean value of ``qual``.; The result is a single struct containing two nested fields, ``n_high_quality`` and ``mean_qual``. :**code**:. >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.count_where`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Entry Values Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the entry-indexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:. >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). :**dependencies**: :meth:`.MatrixTable.aggregate_entries`, :func:`.aggregators.mean`, :func:`.aggregators.fraction`, :class:`.StructExpression`. Aggregate Per Column Group; ~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Group the columns of the matrix table by the column-indexed; field ``cohort`` and compute the call rate per cohort. :**code**:. >>> result_mt = (mt.group_co",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:4974,Integrability,depend,dependencies,4974,"reater than 40, and compute the mean value of ``qual``.; The result is a single struct containing two nested fields, ``n_high_quality`` and ``mean_qual``. :**code**:. >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). :**dependencies**: :meth:`.MatrixTable.aggregate_rows`, :func:`.aggregators.count_where`, :func:`.aggregators.mean`, :class:`.StructExpression`. Aggregate Entry Values Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the entry-indexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:. >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). :**dependencies**: :meth:`.MatrixTable.aggregate_entries`, :func:`.aggregators.mean`, :func:`.aggregators.fraction`, :class:`.StructExpression`. Aggregate Per Column Group; ~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Group the columns of the matrix table by the column-indexed; field ``cohort`` and compute the call rate per cohort. :**code**:. >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:5433,Integrability,depend,dependencies,5433,"mean`, :class:`.StructExpression`. Aggregate Entry Values Into A Local Value; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**:. Compute the mean of the entry-indexed field ``GQ`` and the call rate of; the entry-indexed field ``GT``. The result is returned as a single struct with; two nested fields. :**code**:. >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). :**dependencies**: :meth:`.MatrixTable.aggregate_entries`, :func:`.aggregators.mean`, :func:`.aggregators.fraction`, :class:`.StructExpression`. Aggregate Per Column Group; ~~~~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Group the columns of the matrix table by the column-indexed; field ``cohort`` and compute the call rate per cohort. :**code**:. >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst:6540,Integrability,depend,dependencies,6540,"l_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). :**dependencies**: :meth:`.MatrixTable.group_cols_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the columns of the matrix table by; the column-indexed field ``cohort`` using :meth:`.MatrixTable.group_cols_by`,; which returns a :class:`.GroupedMatrixTable`. Then use; :meth:`.GroupedMatrixTable.aggregate` to compute an aggregation per column; group. The result is a matrix table with an entry field ``call_rate`` that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; ``group_cols_by``, and an entry schema determined by the expression passed to; ``aggregate``. Other column fields and entry fields are dropped. Aggregate Per Row Group; ~~~~~~~~~~~~~~~~~~~~~~~. :**description**: Compute the number of calls with one or more non-reference; alleles per gene group. :**code**:. >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). :**dependencies**: :meth:`.MatrixTable.group_rows_by`, :class:`.GroupedMatrixTable`, :meth:`.GroupedMatrixTable.aggregate`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Group the rows of the matrix table by the row-indexed field ``gene``; using :meth:`.MatrixTable.group_rows_by`, which returns a; :class:`.GroupedMatrixTable`. Then use :meth:`.GroupedMatrixTable.aggregate`; to compute an aggregation per grouped row. The result is a matrix table with an entry field ``n_non_ref`` that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to ``group_rows_by``, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to ``aggregate``. Other row fields and entry fields; are dropped.; ",MatchSource.DOCS,hail/python/hail/docs/guides/agg.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/agg.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst:464,Deployability,toggle,toggle,464,"Annotation; ==========. Annotations are Hail's way of adding data fields to Hail's tables and matrix; tables. Create a nested annotation; --------------------------. :**description**: Add a new field ``gq_mean`` as a nested field inside ``info``. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). :**dependencies**: :meth:`.StructExpression.annotate`, :meth:`.MatrixTable.annotate_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To add a new field ``gq_mean`` as a nested field inside ``info``,; instead of a top-level field, we need to annotate the ``info`` field itself. Construct an expression ``mt.info.annotate(gq_mean=...)`` which adds the field; to ``info``. Then, reassign this expression to ``info`` using; :meth:`.MatrixTable.annotate_rows`. Remove a nested annotation; --------------------------. :**description**: Drop a field ``AF``, which is nested inside the ``info`` field. To drop a nested field ``AF``, construct an expression ``mt.info.drop('AF')``; which drops the field from its parent field, ``info``. Then, reassign this; expression to ``info`` using :meth:`.MatrixTable.annotate_rows`. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). :**dependencies**: :meth:`.StructExpression.drop`, :meth:`.MatrixTable.annotate_rows`",MatchSource.DOCS,hail/python/hail/docs/guides/annotation.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst:487,Deployability,toggle,toggle-content,487,"Annotation; ==========. Annotations are Hail's way of adding data fields to Hail's tables and matrix; tables. Create a nested annotation; --------------------------. :**description**: Add a new field ``gq_mean`` as a nested field inside ``info``. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). :**dependencies**: :meth:`.StructExpression.annotate`, :meth:`.MatrixTable.annotate_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To add a new field ``gq_mean`` as a nested field inside ``info``,; instead of a top-level field, we need to annotate the ``info`` field itself. Construct an expression ``mt.info.annotate(gq_mean=...)`` which adds the field; to ``info``. Then, reassign this expression to ``info`` using; :meth:`.MatrixTable.annotate_rows`. Remove a nested annotation; --------------------------. :**description**: Drop a field ``AF``, which is nested inside the ``info`` field. To drop a nested field ``AF``, construct an expression ``mt.info.drop('AF')``; which drops the field from its parent field, ``info``. Then, reassign this; expression to ``info`` using :meth:`.MatrixTable.annotate_rows`. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). :**dependencies**: :meth:`.StructExpression.drop`, :meth:`.MatrixTable.annotate_rows`",MatchSource.DOCS,hail/python/hail/docs/guides/annotation.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst:340,Integrability,depend,dependencies,340,"Annotation; ==========. Annotations are Hail's way of adding data fields to Hail's tables and matrix; tables. Create a nested annotation; --------------------------. :**description**: Add a new field ``gq_mean`` as a nested field inside ``info``. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). :**dependencies**: :meth:`.StructExpression.annotate`, :meth:`.MatrixTable.annotate_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To add a new field ``gq_mean`` as a nested field inside ``info``,; instead of a top-level field, we need to annotate the ``info`` field itself. Construct an expression ``mt.info.annotate(gq_mean=...)`` which adds the field; to ``info``. Then, reassign this expression to ``info`` using; :meth:`.MatrixTable.annotate_rows`. Remove a nested annotation; --------------------------. :**description**: Drop a field ``AF``, which is nested inside the ``info`` field. To drop a nested field ``AF``, construct an expression ``mt.info.drop('AF')``; which drops the field from its parent field, ``info``. Then, reassign this; expression to ``info`` using :meth:`.MatrixTable.annotate_rows`. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). :**dependencies**: :meth:`.StructExpression.drop`, :meth:`.MatrixTable.annotate_rows`",MatchSource.DOCS,hail/python/hail/docs/guides/annotation.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst:1251,Integrability,depend,dependencies,1251,"Annotation; ==========. Annotations are Hail's way of adding data fields to Hail's tables and matrix; tables. Create a nested annotation; --------------------------. :**description**: Add a new field ``gq_mean`` as a nested field inside ``info``. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). :**dependencies**: :meth:`.StructExpression.annotate`, :meth:`.MatrixTable.annotate_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To add a new field ``gq_mean`` as a nested field inside ``info``,; instead of a top-level field, we need to annotate the ``info`` field itself. Construct an expression ``mt.info.annotate(gq_mean=...)`` which adds the field; to ``info``. Then, reassign this expression to ``info`` using; :meth:`.MatrixTable.annotate_rows`. Remove a nested annotation; --------------------------. :**description**: Drop a field ``AF``, which is nested inside the ``info`` field. To drop a nested field ``AF``, construct an expression ``mt.info.drop('AF')``; which drops the field from its parent field, ``info``. Then, reassign this; expression to ``info`` using :meth:`.MatrixTable.annotate_rows`. :**code**:. >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). :**dependencies**: :meth:`.StructExpression.drop`, :meth:`.MatrixTable.annotate_rows`",MatchSource.DOCS,hail/python/hail/docs/guides/annotation.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/annotation.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:113,Deployability,pipeline,pipelines,113,"Genetics; ========. This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the :ref:`genetics methods page <methods_genetics>`. Formatting; ~~~~~~~~~~. Convert variants in string format to separate locus and allele fields; ..................................................................... ..; >>> # this sets up ht for doctest below; >>> ht = hl.import_table('data/variant-lof.tsv'); >>> ht = ht.transmute(variant = ht.v). :**code**:. >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). :**dependencies**: :func:`.parse_variant`, :meth:`.key_by`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. If your variants are strings of the format 'chr:pos:ref:alt', you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles. ``hl.parse_variant(ht.variant)`` constructs a :class:`.StructExpression`; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, ``locus`` and; ``alleles``. .. _liftover_howto:. Liftover variants from one coordinate system to another; ....................................................... :**tags**: liftover. :**description**: Liftover a Table or MatrixTable from one reference genome to another. :**code**:. First, we need to set up the two reference genomes (source and destination):. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.anno",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:711,Deployability,toggle,toggle,711,"Genetics; ========. This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the :ref:`genetics methods page <methods_genetics>`. Formatting; ~~~~~~~~~~. Convert variants in string format to separate locus and allele fields; ..................................................................... ..; >>> # this sets up ht for doctest below; >>> ht = hl.import_table('data/variant-lof.tsv'); >>> ht = ht.transmute(variant = ht.v). :**code**:. >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). :**dependencies**: :func:`.parse_variant`, :meth:`.key_by`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. If your variants are strings of the format 'chr:pos:ref:alt', you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles. ``hl.parse_variant(ht.variant)`` constructs a :class:`.StructExpression`; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, ``locus`` and; ``alleles``. .. _liftover_howto:. Liftover variants from one coordinate system to another; ....................................................... :**tags**: liftover. :**description**: Liftover a Table or MatrixTable from one reference genome to another. :**code**:. First, we need to set up the two reference genomes (source and destination):. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.anno",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:734,Deployability,toggle,toggle-content,734,"Genetics; ========. This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the :ref:`genetics methods page <methods_genetics>`. Formatting; ~~~~~~~~~~. Convert variants in string format to separate locus and allele fields; ..................................................................... ..; >>> # this sets up ht for doctest below; >>> ht = hl.import_table('data/variant-lof.tsv'); >>> ht = ht.transmute(variant = ht.v). :**code**:. >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). :**dependencies**: :func:`.parse_variant`, :meth:`.key_by`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. If your variants are strings of the format 'chr:pos:ref:alt', you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles. ``hl.parse_variant(ht.variant)`` constructs a :class:`.StructExpression`; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, ``locus`` and; ``alleles``. .. _liftover_howto:. Liftover variants from one coordinate system to another; ....................................................... :**tags**: liftover. :**description**: Liftover a Table or MatrixTable from one reference genome to another. :**code**:. First, we need to set up the two reference genomes (source and destination):. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.anno",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:3486,Deployability,toggle,toggle,3486,"),; ... old_locus=ht.locus) # doctest: +SKIP; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) # doctest: +SKIP; >>> ht = ht.key_by(locus=ht.new_locus.result) # doctest: +SKIP. :**dependencies**: :func:`.liftover`, :meth:`.add_liftover`, :func:`.get_reference`. Filtering and Pruning; ~~~~~~~~~~~~~~~~~~~~~. Remove related individuals from a dataset; ......................................... :**tags**: kinship. :**description**: Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. :**code**:. >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). :**dependencies**: :func:`.pc_relate`, :func:`.maximal_independent_set`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using :func:`.pc_relate`. We filter this; result based on a kinship threshold, which gives us a table of related pairs. From this table of pairs, we can compute the complement of the maximal; independent set using :func:`.maximal_independent_set`. The parameter; ``keep=False`` in ``maximal_independent_set`` specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals; ........................................ From a table of intervals; ++++++++++++++++++++++++",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:3509,Deployability,toggle,toggle-content,3509," # doctest: +SKIP; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) # doctest: +SKIP; >>> ht = ht.key_by(locus=ht.new_locus.result) # doctest: +SKIP. :**dependencies**: :func:`.liftover`, :meth:`.add_liftover`, :func:`.get_reference`. Filtering and Pruning; ~~~~~~~~~~~~~~~~~~~~~. Remove related individuals from a dataset; ......................................... :**tags**: kinship. :**description**: Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. :**code**:. >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). :**dependencies**: :func:`.pc_relate`, :func:`.maximal_independent_set`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using :func:`.pc_relate`. We filter this; result based on a kinship threshold, which gives us a table of related pairs. From this table of pairs, we can compute the complement of the maximal; independent set using :func:`.maximal_independent_set`. The parameter; ``keep=False`` in ``maximal_independent_set`` specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals; ........................................ From a table of intervals; +++++++++++++++++++++++++. :**tags**: genomic regio",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:4966,Deployability,toggle,toggle,4966,"nt the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals; ........................................ From a table of intervals; +++++++++++++++++++++++++. :**tags**: genomic region, genomic range. :**description**: Import a text file of locus intervals as a table, then use; this table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_locus_intervals('data/gene.interval_list', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_locus_intervals`, :meth:`.MatrixTable.filter_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have a matrix table ``mt`` containing the loci we would like to filter, and a; list of locus intervals stored in a file. We can import the intervals into a; table with :func:`.import_locus_intervals`. Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table. ``interval_table[mt.locus]`` joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals. To do our filtering, we can filter to the rows of our matrix table where the; struct expression ``interval_table[mt.locus]`` is defined. This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file; ++++++++++++++++++++. :**description**: Import a UCSC BED file as a tab",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:4989,Deployability,toggle,toggle-content,4989,"et (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals; ........................................ From a table of intervals; +++++++++++++++++++++++++. :**tags**: genomic region, genomic range. :**description**: Import a text file of locus intervals as a table, then use; this table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_locus_intervals('data/gene.interval_list', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_locus_intervals`, :meth:`.MatrixTable.filter_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have a matrix table ``mt`` containing the loci we would like to filter, and a; list of locus intervals stored in a file. We can import the intervals into a; table with :func:`.import_locus_intervals`. Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table. ``interval_table[mt.locus]`` joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals. To do our filtering, we can filter to the rows of our matrix table where the; struct expression ``interval_table[mt.locus]`` is defined. This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file; ++++++++++++++++++++. :**description**: Import a UCSC BED file as a table of intervals, then use t",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:7507,Deployability,toggle,toggle,7507,"interval'].collect()). :**dependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Hail's :func:`.ld_prune` method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use th",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:7530,Deployability,toggle,toggle-content,7530,"ependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Hail's :func:`.ld_prune` method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linre",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:8773,Deployability,toggle,toggle,8773,"c; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:8796,Deployability,toggle,toggle-content,8796," table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Appr",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:10186,Deployability,toggle,toggle,10186," regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:10209,Deployability,toggle,toggle-content,10209,"multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation from the genotype dosage f",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:13072,Deployability,toggle,toggle,13072,"ription**: Compute linear regression statistics for a single phenotype stratified by group. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for each group. >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.group_by` and :func:`.aggregators.linreg` aggregators. >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.group_by`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the :func:`.linear_regression_rows` method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression gener",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:13095,Deployability,toggle,toggle-content,13095,"egression statistics for a single phenotype stratified by group. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for each group. >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.group_by` and :func:`.aggregators.linreg` aggregators. >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.group_by`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the :func:`.linear_regression_rows` method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a k",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:7936,Energy Efficiency,efficient,efficient,7936,"_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Hail's :func:`.ld_prune` method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple c",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:8863,Energy Efficiency,efficient,efficient,8863,"we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:10276,Energy Efficiency,efficient,efficient,10276,"inear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collec",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:14377,Energy Efficiency,efficient,efficient,14377,"s for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table. The :func:`.linear_regression_rows` method is more efficient than the :func:`.aggregators.linreg`; aggregator and can be extended to multiple phenotypes, but the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions; ~~~~~~~~~~~~~~~~~. Polygenic Score Calculation; ........................... :**plink**:. >>> plink --bfile data --score scores.txt sum # doctest: +SKIP. :**tags**: PRS. :**description**: This command is analogous to plink's --score command with the; `sum` option. Biallelic variants are required. :**code**:. >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1]",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:618,Integrability,depend,dependencies,618,"Genetics; ========. This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the :ref:`genetics methods page <methods_genetics>`. Formatting; ~~~~~~~~~~. Convert variants in string format to separate locus and allele fields; ..................................................................... ..; >>> # this sets up ht for doctest below; >>> ht = hl.import_table('data/variant-lof.tsv'); >>> ht = ht.transmute(variant = ht.v). :**code**:. >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). :**dependencies**: :func:`.parse_variant`, :meth:`.key_by`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. If your variants are strings of the format 'chr:pos:ref:alt', you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles. ``hl.parse_variant(ht.variant)`` constructs a :class:`.StructExpression`; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, ``locus`` and; ``alleles``. .. _liftover_howto:. Liftover variants from one coordinate system to another; ....................................................... :**tags**: liftover. :**description**: Liftover a Table or MatrixTable from one reference genome to another. :**code**:. First, we need to set up the two reference genomes (source and destination):. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.anno",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:2697,Integrability,depend,dependencies,2697,"rence('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38')) # doctest: +SKIP; >>> ht = ht.filter(hl.is_defined(ht.new_locus)) # doctest: +SKIP; >>> ht = ht.key_by(locus=ht.new_locus) # doctest: +SKIP. Note that this approach does not retain the old locus, nor does it verify; that the allele has not changed strand. We can keep the old one for; reference and filter out any liftover that changed strands using:. >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) # doctest: +SKIP; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) # doctest: +SKIP; >>> ht = ht.key_by(locus=ht.new_locus.result) # doctest: +SKIP. :**dependencies**: :func:`.liftover`, :meth:`.add_liftover`, :func:`.get_reference`. Filtering and Pruning; ~~~~~~~~~~~~~~~~~~~~~. Remove related individuals from a dataset; ......................................... :**tags**: kinship. :**description**: Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. :**code**:. >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). :**dependencies**: :func:`.pc_relate`, :func:`.maximal_independent_set`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using :func:`.pc_relate`. We filter this; result based on a kinship thres",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:3380,Integrability,depend,dependencies,3380,"ds using:. >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) # doctest: +SKIP; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) # doctest: +SKIP; >>> ht = ht.key_by(locus=ht.new_locus.result) # doctest: +SKIP. :**dependencies**: :func:`.liftover`, :meth:`.add_liftover`, :func:`.get_reference`. Filtering and Pruning; ~~~~~~~~~~~~~~~~~~~~~. Remove related individuals from a dataset; ......................................... :**tags**: kinship. :**description**: Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. :**code**:. >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). :**dependencies**: :func:`.pc_relate`, :func:`.maximal_independent_set`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using :func:`.pc_relate`. We filter this; result based on a kinship threshold, which gives us a table of related pairs. From this table of pairs, we can compute the complement of the maximal; independent set using :func:`.maximal_independent_set`. The parameter; ``keep=False`` in ``maximal_independent_set`` specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals;",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:4847,Integrability,depend,dependencies,4847,"aximal_independent_set`. The parameter; ``keep=False`` in ``maximal_independent_set`` specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It's important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals. Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals; ........................................ From a table of intervals; +++++++++++++++++++++++++. :**tags**: genomic region, genomic range. :**description**: Import a text file of locus intervals as a table, then use; this table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_locus_intervals('data/gene.interval_list', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_locus_intervals`, :meth:`.MatrixTable.filter_rows`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have a matrix table ``mt`` containing the loci we would like to filter, and a; list of locus intervals stored in a file. We can import the intervals into a; table with :func:`.import_locus_intervals`. Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table. ``interval_table[mt.locus]`` joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals. To do our filtering, we can filter to the rows of our matrix table where the; struct expression ``interval_table[mt.locus]`` is defined. This method will also work to filter a table of loci, as well as a ma",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:6210,Integrability,depend,dependencies,6210,"ts implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table. ``interval_table[mt.locus]`` joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals. To do our filtering, we can filter to the rows of our matrix table where the; struct expression ``interval_table[mt.locus]`` is defined. This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file; ++++++++++++++++++++. :**description**: Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_bed`, :meth:`.MatrixTable.filter_rows`. Using ``hl.filter_intervals``; +++++++++++++++++++++++++++++. :**description**: Filter using an interval table, suitable for a small list of; intervals. :**code**:. >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). :**dependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:6530,Integrability,depend,dependencies,6530,"ch; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals. To do our filtering, we can filter to the rows of our matrix table where the; struct expression ``interval_table[mt.locus]`` is defined. This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file; ++++++++++++++++++++. :**description**: Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_bed`, :meth:`.MatrixTable.filter_rows`. Using ``hl.filter_intervals``; +++++++++++++++++++++++++++++. :**description**: Filter using an interval table, suitable for a small list of; intervals. :**code**:. >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). :**dependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-con",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:6938,Integrability,depend,dependencies,6938,"D file as a table of intervals, then use this; table to filter the loci in a matrix table. :**code**:. >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). :**dependencies**: :func:`.import_bed`, :meth:`.MatrixTable.filter_rows`. Using ``hl.filter_intervals``; +++++++++++++++++++++++++++++. :**description**: Filter using an interval table, suitable for a small list of; intervals. :**code**:. >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). :**dependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Hail's :func:`.ld_prune` method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:7436,Integrability,depend,dependencies,7436,">>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). :**dependencies**: :func:`.methods.filter_intervals`. Declaring intervals with ``hl.parse_locus_interval``; ++++++++++++++++++++++++++++++++++++++++++++++++++++. :**description**: Filter to declared intervals. :**code**:. >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). :**dependencies**: :func:`.methods.filter_intervals`, :func:`.parse_locus_interval`. Pruning Variants in Linkage Disequilibrium; .......................................... :**tags**: LD Prune. :**description**: Remove correlated variants from a matrix table. :**code**:. >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). :**dependencies**: :func:`.ld_prune`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. Hail's :func:`.ld_prune` method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:8659,Integrability,depend,dependencies,8659,"with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using :func:`.ld_prune`,; which we can use to filter the rows of our original dataset. Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariat",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:10072,Integrability,depend,dependencies,10072,"tiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosag",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:11135,Integrability,rout,routines,11135,"nc:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:. >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with :func:`.linear_regression_rows` using variant dosages as covariates:. >>> gwas = hl.linear_regression_rows( # doctest: +SKIP; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.collect`, :func:`.parse_variant`, :func:`.variant_str`. Stratified by Group; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute lin",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:11865,Integrability,depend,dependencies,11865,"sion statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:. >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with :func:`.linear_regression_rows` using variant dosages as covariates:. >>> gwas = hl.linear_regression_rows( # doctest: +SKIP; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.collect`, :func:`.parse_variant`, :func:`.variant_str`. Stratified by Group; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype stratified by group. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for each group. >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.group_by` and :func:`.aggregators.linreg` aggregators. >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.phe",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:12927,Integrability,depend,dependencies,12927,"lect`, :func:`.parse_variant`, :func:`.variant_str`. Stratified by Group; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype stratified by group. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for each group. >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.group_by` and :func:`.aggregators.linreg` aggregators. >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.group_by`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the :func:`.linear_regression_rows` method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The s",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:15712,Integrability,depend,dependencies,15712,"e; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table. The :func:`.linear_regression_rows` method is more efficient than the :func:`.aggregators.linreg`; aggregator and can be extended to multiple phenotypes, but the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions; ~~~~~~~~~~~~~~~~~. Polygenic Score Calculation; ........................... :**plink**:. >>> plink --bfile data --score scores.txt sum # doctest: +SKIP. :**tags**: PRS. :**description**: This command is analogous to plink's --score command with the; `sum` option. Biallelic variants are required. :**code**:. >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). :**dependencies**:. :func:`.import_plink`, :func:`.variant_qc`, :func:`.import_table`,; :func:`.coalesce`, :func:`.case`, :func:`.cond`, :meth:`.Call.n_alt_alleles`. ",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:8989,Modifiability,flexible,flexible,8989,"e biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis; ~~~~~~~~. Linear Regression; ................. Single Phenotype; ++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for a single phenotype. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method. >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator. However, the :func:`.aggregators.linreg` aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes; +++++++++++++++++++. :**tags**: Linear Regression. :**description**: Compute linear regression statistics for multiple phenotypes. :**code**:. Approach #1: Use the :func:`.linear_regression_rows` method for all phenotypes simultaneously. >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.bloo",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:10446,Modifiability,flexible,flexible,10446,". Approach #2: Use the :func:`.linear_regression_rows` method for each phenotype sequentially. >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the :func:`.aggregators.linreg` aggregator. >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. The :func:`.linear_regression_rows` method is more efficient than using the :func:`.aggregators.linreg`; aggregator, especially when analyzing many phenotypes. However, the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The :func:`.linear_regression_rows` method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates; +++++++++++++++++++++++++++++++++++. :**tags**: sample genotypes covariate. :**description**: Use sample genotype dosage at specific variant(s) as covariates in regression routines. :**code**:. Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:. >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.p",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:13208,Modifiability,variab,variable,13208,"*code**:. Approach #1: Use the :func:`.linear_regression_rows` method for each group. >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the :func:`.aggregators.group_by` and :func:`.aggregators.linreg` aggregators. >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). :**dependencies**: :func:`.linear_regression_rows`, :func:`.aggregators.group_by`, :func:`.aggregators.linreg`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the :func:`.linear_regression_rows` method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the correspondin",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:14134,Modifiability,variab,variable,14134,"to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the :func:`.linear_regression_rows` method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the :func:`.linear_regression_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table. The :func:`.linear_regression_rows` method is more efficient than the :func:`.aggregators.linreg`; aggregator and can be extended to multiple phenotypes, but the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions; ~~~~~~~~~~~~~~~~~. Polygenic Score Calculation; ........................... :**plink**:. >>> plink --bfile data --score scores.txt sum # doctest: +SKIP. :**tags**: PRS. :**description**: This command is analogous to plink's --score command with the; `sum` option. Biallelic variants are required. :**code**:. >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.impor",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:14447,Modifiability,extend,extended,14447,"ion_rows` method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table. The :func:`.linear_regression_rows` method is more efficient than the :func:`.aggregators.linreg`; aggregator and can be extended to multiple phenotypes, but the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions; ~~~~~~~~~~~~~~~~~. Polygenic Score Calculation; ........................... :**plink**:. >>> plink --bfile data --score scores.txt sum # doctest: +SKIP. :**tags**: PRS. :**description**: This command is analogous to plink's --score command with the; `sum` option. Biallelic variants are required. :**code**:. >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_row",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:14536,Modifiability,flexible,flexible,14536,"y exclusive,; such as 'Male' and 'Female', no samples remain! Note that we cannot define `male_pheno = ~female_pheno`; because we subsequently need `male_pheno` to be an expression on the `mt_linreg` matrix table; rather than `mt`. Lastly, the argument to `root` must be specified for both cases -- otherwise; the 'Male' output will overwrite the 'Female' output. The second approach uses the :func:`.aggregators.group_by` and :func:`.aggregators.linreg`; aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table. The :func:`.linear_regression_rows` method is more efficient than the :func:`.aggregators.linreg`; aggregator and can be extended to multiple phenotypes, but the :func:`.aggregators.linreg`; aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions; ~~~~~~~~~~~~~~~~~. Polygenic Score Calculation; ........................... :**plink**:. >>> plink --bfile data --score scores.txt sum # doctest: +SKIP. :**tags**: PRS. :**description**: This command is analogous to plink's --score command with the; `sum` option. Biallelic variants are required. :**code**:. >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ..",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst:46,Usability,guid,guides,46,"Genetics; ========. This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the :ref:`genetics methods page <methods_genetics>`. Formatting; ~~~~~~~~~~. Convert variants in string format to separate locus and allele fields; ..................................................................... ..; >>> # this sets up ht for doctest below; >>> ht = hl.import_table('data/variant-lof.tsv'); >>> ht = ht.transmute(variant = ht.v). :**code**:. >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). :**dependencies**: :func:`.parse_variant`, :meth:`.key_by`. :**understanding**:. .. container:: toggle. .. container:: toggle-content. If your variants are strings of the format 'chr:pos:ref:alt', you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles. ``hl.parse_variant(ht.variant)`` constructs a :class:`.StructExpression`; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, ``locus`` and; ``alleles``. .. _liftover_howto:. Liftover variants from one coordinate system to another; ....................................................... :**tags**: liftover. :**description**: Liftover a Table or MatrixTable from one reference genome to another. :**code**:. First, we need to set up the two reference genomes (source and destination):. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Then we can liftover the locus coordinates in a Table or MatrixTable (here, `ht`); from reference genome ``'GRCh37'`` to ``'GRCh38'``:. >>> ht = ht.anno",MatchSource.DOCS,hail/python/hail/docs/guides/genetics.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/guides/genetics.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst:94,Deployability,install,install,94,"===========================; Use Hail on Azure HDInsight; ===========================. First, install Hail on your `Mac OS X <macosx.rst>`__ or `Linux <linux.rst>`__ laptop or; desktop. The Hail pip package includes a tool called ``hailctl hdinsight`` which starts, stops, and; manipulates Hail-enabled HDInsight clusters. Start an HDInsight cluster named ""my-first-cluster"". Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group. .. code-block:: sh. hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster. Create a file called ""hail-script.py"" and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute. .. code-block:: sh. hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you'll see 25 rows of variant association; results. You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter. When you are finished with the cluster stop it:. .. code-block:: sh. hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps; """""""""""""""""""". - Read more about Hail on `Azure HDInsight <../cloud/azure.rst>`__; - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.",MatchSource.DOCS,hail/python/hail/docs/install/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst:649,Security,password,password,649,"===========================; Use Hail on Azure HDInsight; ===========================. First, install Hail on your `Mac OS X <macosx.rst>`__ or `Linux <linux.rst>`__ laptop or; desktop. The Hail pip package includes a tool called ``hailctl hdinsight`` which starts, stops, and; manipulates Hail-enabled HDInsight clusters. Start an HDInsight cluster named ""my-first-cluster"". Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group. .. code-block:: sh. hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster. Create a file called ""hail-script.py"" and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute. .. code-block:: sh. hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you'll see 25 rows of variant association; results. You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter. When you are finished with the cluster stop it:. .. code-block:: sh. hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps; """""""""""""""""""". - Read more about Hail on `Azure HDInsight <../cloud/azure.rst>`__; - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.",MatchSource.DOCS,hail/python/hail/docs/install/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst:674,Security,access,access,674,"===========================; Use Hail on Azure HDInsight; ===========================. First, install Hail on your `Mac OS X <macosx.rst>`__ or `Linux <linux.rst>`__ laptop or; desktop. The Hail pip package includes a tool called ``hailctl hdinsight`` which starts, stops, and; manipulates Hail-enabled HDInsight clusters. Start an HDInsight cluster named ""my-first-cluster"". Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group. .. code-block:: sh. hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster. Create a file called ""hail-script.py"" and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute. .. code-block:: sh. hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you'll see 25 rows of variant association; results. You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter. When you are finished with the cluster stop it:. .. code-block:: sh. hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps; """""""""""""""""""". - Read more about Hail on `Azure HDInsight <../cloud/azure.rst>`__; - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.",MatchSource.DOCS,hail/python/hail/docs/install/azure.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/azure.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/dataproc.rst:94,Deployability,install,install,94,"===========================; Use Hail on Google Dataproc; ===========================. First, install Hail on your `Mac OS X <macosx.rst>`__ or `Linux <linux.rst>`__ laptop or; desktop. The Hail pip package includes a tool called ``hailctl dataproc`` which starts, stops, and; manipulates Hail-enabled Dataproc clusters. Start a dataproc cluster named ""my-first-cluster"". Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes. .. code-block:: sh. hailctl dataproc start my-first-cluster. Create a file called ""hail-script.py"" and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute. .. code-block:: sh. hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you'll see 25 rows of variant association; results. You can also start a Jupyter Notebook running on the cluster:. .. code-block:: sh. hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:. .. code-block:: sh. hailctl dataproc stop my-first-cluster. Next Steps; """""""""""""""""""". - Read more about Hail on `Google Cloud <../cloud/google_cloud.rst>`__; - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/dataproc.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/dataproc.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/linux.rst:402,Deployability,install,install,402,"=========================; Install Hail on GNU/Linux; =========================. - Install Java 11.; - Install Python 3.9 or later.; - Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; - Install BLAS and LAPACK.; - Install Hail using pip. On a recent Debian-like system, the following should suffice:. .. code-block:: sh. apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. `Now let's take Hail for a spin! <try.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/linux.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/linux.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/linux.rst:521,Deployability,install,install,521,"=========================; Install Hail on GNU/Linux; =========================. - Install Java 11.; - Install Python 3.9 or later.; - Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; - Install BLAS and LAPACK.; - Install Hail using pip. On a recent Debian-like system, the following should suffice:. .. code-block:: sh. apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. `Now let's take Hail for a spin! <try.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/linux.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/linux.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:174,Availability,down,downloads,174,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:128,Deployability,install,installation,128,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:413,Deployability,install,install,413,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:461,Deployability,install,installation,461,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:842,Deployability,install,installers,842,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:897,Deployability,install,install,897,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:995,Deployability,install,install,995,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:1223,Deployability,install,install-completion,1223,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:942,Integrability,message,message,942,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst:1283,Modifiability,config,config,1283,"========================; Install Hail on Mac OS X; ========================. - Install Java 11. We recommend using a `packaged installation from Azul; <https://www.azul.com/downloads/?version=java-11-lts&os=macos&package=jdk&show-old-builds=true>`__; (make sure the OS version and architecture match your system) or using `Homebrew; <https://brew.sh/>`__:. .. code-block::. brew tap homebrew/cask-versions; brew install --cask temurin8. You *must* pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an ""arm64"" Java, otherwise you must use an ""x86_64"" Java. You can check if you have; an M1 or M2 either in the ""Apple Menu > About This Mac"" or by running ``uname -m`` Terminal.app. - Install Python 3.9 or later. We recommend `Miniconda <https://docs.conda.io/en/latest/miniconda.html#macosx-installers>`__.; - Open Terminal.app and execute ``pip install hail``. If this command fails with a message about ""Rust"", please try this instead: ``pip install hail --only-binary=:all:``.; - `Run your first Hail query! <try.rst>`__. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; hailctl Autocompletion (Optional); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. - Install autocompletion with ``hailctl --install-completion zsh``; - Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal. .. code-block::. autoload -Uz compinit && compinit; ",MatchSource.DOCS,hail/python/hail/docs/install/macosx.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/macosx.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:852,Availability,down,downloads,852,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:696,Deployability,update,update,696,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:712,Deployability,install,install,712,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:875,Deployability,install,installs,875,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:988,Deployability,install,install-on-cluster,988,"==========================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1090,Deployability,install,install,1090,"roc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drin",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1143,Deployability,install,install-on-cluster,1143,"roc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drin",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1275,Deployability,install,installed,1275,"roc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drin",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1351,Deployability,install,install-on-cluster,1351,"cala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1439,Deployability,install,install,1439,"1 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-blo",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1612,Deployability,install,install,1612,"-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location ",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1656,Deployability,install,installed,1656," apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spa",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1750,Deployability,configurat,configuration,1750," apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spa",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:2497,Deployability,configurat,configuration,2497,"`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps; """""""""""""""""""". - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:1750,Modifiability,config,configuration,1750," apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spa",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:2497,Modifiability,config,configuration,2497,"`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants. .. code-block:: python3. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute. .. code-block:: sh. python3 hail-script.py. Slightly more configuration is necessary to ``spark-submit`` a Hail script:. .. code-block:: sh. HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps; """""""""""""""""""". - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:151,Usability,simpl,simpler,151,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst:243,Usability,simpl,simpler,243,"===============================; Install Hail on a Spark Cluster; ===============================. If you are using Google Dataproc, please see `these simpler instructions <dataproc.rst>`__. If you; are using Azure HDInsight please see `these simpler instructions <azure.rst>`__. Hail should work with any Spark 3.5.x cluster built with Scala 2.12. Hail needs to be built from source on the leader node. Building Hail from source; requires:. - Java 11 JDK.; - Python 3.9 or later.; - A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; - The LZ4 library.; - BLAS and LAPACK. On a Debian-like system, the following should suffice:. .. code-block:: sh. apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source. .. code-block:: sh. git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running `make install-on-cluster`, it's possible; to get into a bad state where `make` insists you don't have a requirement that you have in fact installed.; Try doing `make clean` and then a fresh invocation of the `make install-on-cluster` line if this happens. On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node. .. code-block:: sh. apt-get install libopenblas liblapack3. Hail is now installed! You can use ``ipython``, ``python``, and ``jupyter; notebook`` without any further configuration. We recommend against using the; ``pyspark`` command. Let's take Hail for a spin! Create a file called ""hail-script.py"" and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million varia",MatchSource.DOCS,hail/python/hail/docs/install/other-cluster.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/other-cluster.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/try.rst:155,Deployability,install,install,155,"=====================; Your First Hail Query; =====================. We recommend using IPython, a super-powered Python terminal:. .. code-block:: sh. pip install ipython. Start an IPython session by copy-pasting the below into your Terminal. .. code-block:: sh. ipython. Let's randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations. .. code-block:: sh. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, ``mt.show()``, displays the dataset in a tabular form. .. code-block:: sh. 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+------+------+------+------+; showing top 11 rows; showing the first 4 of 10 columns</code></pre>. Next Steps; """""""""""""""""""". - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.rst>`__; - Learn how to use `Hail on Google Cloud <../cloud/google_cloud.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/try.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/try.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/try.rst:105,Energy Efficiency,power,powered,105,"=====================; Your First Hail Query; =====================. We recommend using IPython, a super-powered Python terminal:. .. code-block:: sh. pip install ipython. Start an IPython session by copy-pasting the below into your Terminal. .. code-block:: sh. ipython. Let's randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations. .. code-block:: sh. import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, ``mt.show()``, displays the dataset in a tabular form. .. code-block:: sh. 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+------+------+------+------+; showing top 11 rows; showing the first 4 of 10 columns</code></pre>. Next Steps; """""""""""""""""""". - Get the `Hail cheatsheets <../cheatsheets.rst>`__; - Follow the Hail `GWAS Tutorial <../tutorials/01-genome-wide-association-study.rst>`__; - Learn how to use `Hail on Google Cloud <../cloud/google_cloud.rst>`__; ",MatchSource.DOCS,hail/python/hail/docs/install/try.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/install/try.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/linalg/index.rst:139,Deployability,pipeline,pipelines,139,linalg; ======. **File formats and interface for numeric matrices are experimental**.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.linalg. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. BlockMatrix. .. rubric:: Modules. .. toctree::; :maxdepth: 1. utils <utils/index>,MatchSource.DOCS,hail/python/hail/docs/linalg/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/linalg/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/linalg/index.rst:35,Integrability,interface,interface,35,linalg; ======. **File formats and interface for numeric matrices are experimental**.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.linalg. .. rubric:: Classes. .. autosummary::; :nosignatures:; :toctree: ./; :template: class.rst. BlockMatrix. .. rubric:: Modules. .. toctree::; :maxdepth: 1. utils <utils/index>,MatchSource.DOCS,hail/python/hail/docs/linalg/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/linalg/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/impex.rst:337,Performance,perform,performance,337,".. _methods_impex:. Import / Export; ===============. .. _methods_impex_export:. .. currentmodule:: hail.methods. .. toctree::; :maxdepth: 2. This page describes functionality for moving data in and out of Hail. Hail has a suite of functionality for importing and exporting data to and from; general-purpose, genetics-specific, and high-performance native file formats. Native file formats; -------------------. .. _methods_impex_read:. When saving data to disk with the intent to later use Hail, we highly recommend; that you use the native file formats to store :class:`.Table` and; :class:`.MatrixTable` objects. These binary formats not only smaller than other formats; (especially textual ones) in most cases, but also are significantly faster to; read into Hail later. These files can be created with methods on the :class:`.Table` and; :class:`.MatrixTable` objects:. - :meth:`.Table.write`; - :meth:`.MatrixTable.write`. These files can be read into a Hail session later using the following methods:. .. autosummary::. read_matrix_table; read_table. Import; ------. General purpose; ~~~~~~~~~~~~~~~. The :func:`.import_table` function is widely-used to import textual data; into a Hail :class:`.Table`. :func:`.import_matrix_table` is used to import; two-dimensional matrix data in textual representations into a Hail; :class:`.MatrixTable`. Finally, it is possible to create a Hail Table; from a :mod:`pandas` DataFrame with :meth:`.Table.from_pandas`. .. autosummary::. import_table; import_matrix_table; import_lines. Genetics; ~~~~~~~~. Hail has several functions to import genetics-specific file formats into Hail; :class:`.MatrixTable` or :class:`.Table` objects:. .. autosummary::. import_vcf; import_plink; import_bed; import_bgen; index_bgen; import_gen; import_fam; import_locus_intervals. Export; ------. General purpose; ~~~~~~~~~~~~~~~. Some of the most widely-used export functionality is found as class methods; on the :class:`.Table` and :class:`.Expression` objects:. - :meth:",MatchSource.DOCS,hail/python/hail/docs/methods/impex.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/impex.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst:294,Modifiability,inherit,inherited,294,"Relatedness; -----------. .. currentmodule:: hail.methods. The *relatedness* of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles ""inherited identically by descent"". Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same ""recent,"" common ancestor. The term ""recent"" distinguishes; alleles shared IBD from family members from alleles shared IBD from ""distant""; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness. Relatedness is usually quantified by two quantities: kinship coefficient; (:math:`\phi` or ``PI_HAT``) and probability-of-identity-by-descent-zero; (:math:`\pi_0` or ``Z0``). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent. Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1]_, KING [2]_, and PC-Relate [3]_. - :func:`.identity_by_descent` is appropriate for datasets containing one; homogeneous population.; - :func:`.king` is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using :func:`.pc_relate`.; - :func:`.pc_relate` is appropriate for datasets containing multiple; homogeneous populations and admixture. .. toctree::; :maxdepth: 2. .. autosummary::. identity_by_descent; king; pc_relate; simulate_random_mating. .. autofunction:: identity_by_descent; .. autofunction:: king; .. autofunctio",MatchSource.DOCS,hail/python/hail/docs/methods/relatedness.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst:374,Modifiability,inherit,inherited,374,"Relatedness; -----------. .. currentmodule:: hail.methods. The *relatedness* of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles ""inherited identically by descent"". Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same ""recent,"" common ancestor. The term ""recent"" distinguishes; alleles shared IBD from family members from alleles shared IBD from ""distant""; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness. Relatedness is usually quantified by two quantities: kinship coefficient; (:math:`\phi` or ``PI_HAT``) and probability-of-identity-by-descent-zero; (:math:`\pi_0` or ``Z0``). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent. Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1]_, KING [2]_, and PC-Relate [3]_. - :func:`.identity_by_descent` is appropriate for datasets containing one; homogeneous population.; - :func:`.king` is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using :func:`.pc_relate`.; - :func:`.pc_relate` is appropriate for datasets containing multiple; homogeneous populations and admixture. .. toctree::; :maxdepth: 2. .. autosummary::. identity_by_descent; king; pc_relate; simulate_random_mating. .. autofunction:: identity_by_descent; .. autofunction:: king; .. autofunctio",MatchSource.DOCS,hail/python/hail/docs/methods/relatedness.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst:429,Modifiability,inherit,inherited,429,"Relatedness; -----------. .. currentmodule:: hail.methods. The *relatedness* of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles ""inherited identically by descent"". Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same ""recent,"" common ancestor. The term ""recent"" distinguishes; alleles shared IBD from family members from alleles shared IBD from ""distant""; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness. Relatedness is usually quantified by two quantities: kinship coefficient; (:math:`\phi` or ``PI_HAT``) and probability-of-identity-by-descent-zero; (:math:`\pi_0` or ``Z0``). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent. Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1]_, KING [2]_, and PC-Relate [3]_. - :func:`.identity_by_descent` is appropriate for datasets containing one; homogeneous population.; - :func:`.king` is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using :func:`.pc_relate`.; - :func:`.pc_relate` is appropriate for datasets containing multiple; homogeneous populations and admixture. .. toctree::; :maxdepth: 2. .. autosummary::. identity_by_descent; king; pc_relate; simulate_random_mating. .. autofunction:: identity_by_descent; .. autofunction:: king; .. autofunctio",MatchSource.DOCS,hail/python/hail/docs/methods/relatedness.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst:1258,Modifiability,inherit,inherited,1258,"notions of relatedness implemented in Hail are rooted in; the idea of alleles ""inherited identically by descent"". Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same ""recent,"" common ancestor. The term ""recent"" distinguishes; alleles shared IBD from family members from alleles shared IBD from ""distant""; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness. Relatedness is usually quantified by two quantities: kinship coefficient; (:math:`\phi` or ``PI_HAT``) and probability-of-identity-by-descent-zero; (:math:`\pi_0` or ``Z0``). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent. Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1]_, KING [2]_, and PC-Relate [3]_. - :func:`.identity_by_descent` is appropriate for datasets containing one; homogeneous population.; - :func:`.king` is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using :func:`.pc_relate`.; - :func:`.pc_relate` is appropriate for datasets containing multiple; homogeneous populations and admixture. .. toctree::; :maxdepth: 2. .. autosummary::. identity_by_descent; king; pc_relate; simulate_random_mating. .. autofunction:: identity_by_descent; .. autofunction:: king; .. autofunction:: pc_relate; .. autofunction:: simulate_random_mating. .. [1] Purcell, Shaun et al. PLINK: a tool set for whole-genome association and; population-based linkage analyses. American journal of human genetics; vol",MatchSource.DOCS,hail/python/hail/docs/methods/relatedness.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/methods/relatedness.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/nd/index.rst:304,Integrability,interface,interface,304,"nd; ========. .. toctree::; :maxdepth: 2. .. currentmodule:: hail.nd. .. rubric:: NDArray Functions. Notes; _____; This is a recently added, experimental module. We would love to hear what use cases you have for this as we expand this functionality.; As much as possible, we try to mimic the numpy array interface. .. autosummary::. array; arange; full; zeros; ones; diagonal; solve; solve_triangular; qr; svd; inv; concatenate; hstack; vstack; eye; identity; maximum; minimum. .. autofunction:: array; .. autofunction:: arange; .. autofunction:: full; .. autofunction:: zeros; .. autofunction:: ones; .. autofunction:: diagonal; .. autofunction:: solve; .. autofunction:: solve_triangular; .. autofunction:: qr; .. autofunction:: svd; .. autofunction:: inv; .. autofunction:: concatenate; .. autofunction:: hstack; .. autofunction:: vstack; .. autofunction:: eye; .. autofunction:: identity; .. autofunction:: maximum; .. autofunction:: minimum. ",MatchSource.DOCS,hail/python/hail/docs/nd/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/nd/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst:2708,Integrability,wrap,wrapped,2708,"ult of the expression is computed only when it is needed. So ``z`` is; an expression representing the computation of ``x + y``, but not the actual; value. To peek at the value of this computation, there are two options:; :func:`~hail.expr.eval`, which returns a Python value, and :meth:`.Expression.show`,; which prints a human-readable representation of an expression. >>> hl.eval(z); 11; >>> z.show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 11 |; +--------+. Hail's expressions are especially important for interacting with fields in; tables and matrix tables. Throughout Hail documentation and tutorials, you will; see code like this:. >>> ht2 = ht.annotate(C4 = ht.C3 + 3 * ht.C2 ** 2). This snippet of code is adding a field, ``C4``, to a table, ``ht``, and; returning the result as a new table, ``ht2``. The code passed to the; :meth:`.Table.annotate` method is an expression that references the fields; ``C3`` and ``C2`` in ``ht``. Notice that ``3`` and ``2`` are not wrapped in constructor functions like; ``hl.int32(3)``. In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects. For example, we can add a Python :obj:`int` to an :class:`.Int32Expression`. >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an :class:`.Int32Expression` to an; :obj:`int`. >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like :mod:`numpy`; or :mod:`scipy`. Hail has many subclasses of :class:`.Expression` -- one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail :class:`.ArrayNumericExpression` with; :func:`~hail.expr.functions.array`:. >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. :class:`.Expression` objects keep track of their data type, whi",MatchSource.DOCS,hail/python/hail/docs/overview/expressions.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst:6823,Integrability,depend,depending,6823,"ression` which can be used in more; computations. We can add the conditional expression to our array ``a`` from; earlier:. >>> a + hl.if_else(x > 0, 1, 0); <ArrayNumericExpression of type array<int32>>. Case Statements; ~~~~~~~~~~~~~~~. More complicated conditional statements can be constructed with :func:`.case`.; For example, we might want to return ``1`` if ``x < -1``, ``2`` if; ``-1 <= x <= 2`` and ``3`` if ``x > 2``. >>> (hl.case(); ... .when(x < -1, 1); ... .when((x >= -1) & (x <= 2), 2); ... .when(x > 2, 3); ... .or_missing()); <Int32Expression of type int32>. Notice that this expression ends with a call to :meth:`~hail.expr.builders.CaseBuilder.or_missing`,; which means that if none of the conditions are met, a missing value is returned. Cases started with :func:`.case` can end with a call to; :meth:`~hail.expr.builders.CaseBuilder.or_missing`, :meth:`~hail.expr.builders.CaseBuilder.default`, or; :meth:`~hail.expr.builders.CaseBuilder.or_error`, depending on what you want to happen if none; of the *when* clauses are met. It's important to note that missingness propagates up in Hail, so if the value; of the discriminant in a case statement is missing, then the result will be; missing as well. >>> y = hl.missing(hl.tint32); >>> result = hl.case().when(y > 0, 1).default(-1); >>> hl.eval(result). The value of ``result`` will be missing, not ``1`` or ``-1``, because the; discriminant, ``y``, is missing. Switch Statements; ~~~~~~~~~~~~~~~~~. Finally, Hail has the :func:`.switch` function to build a conditional tree based; on the value of an expression. In the example below, ``csq`` is a; :class:`.StringExpression` representing the functional consequence of a; mutation. If ``csq`` does not match one of the cases specified by; :meth:`~hail.expr.builders.SwitchBuilder.when`, it is set to missing with; :meth:`~hail.expr.builders.SwitchBuilder.or_missing`. Other switch statements are documented in the; :class:`.SwitchBuilder` class. >>> csq = hl.str('nonsense'). >>> (hl",MatchSource.DOCS,hail/python/hail/docs/overview/expressions.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst:3828,Security,access,accessed,3828,"ether; via operations like addition and multiplication, they can also be combined with; Python objects. For example, we can add a Python :obj:`int` to an :class:`.Int32Expression`. >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an :class:`.Int32Expression` to an; :obj:`int`. >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like :mod:`numpy`; or :mod:`scipy`. Hail has many subclasses of :class:`.Expression` -- one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail :class:`.ArrayNumericExpression` with; :func:`~hail.expr.functions.array`:. >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. :class:`.Expression` objects keep track of their data type, which is; why we can see that ``a`` is of type ``array<int32>`` in the output above. An; expression's type can also be accessed with :meth:`.Expression.dtype`. >>> a.dtype; dtype('array<int32>'). Hail arrays can be indexed and sliced like Python lists or :mod:`numpy` arrays:. >>> a[1]; <Int32Expression of type int32>. >>> a[1:-1]; <ArrayNumericExpression of type array<int32>>. In addition to constructor methods like :func:`~hail.expr.functions.array` and :func:`.bool`,; Hail expressions can also be constructed with the :func:`.literal` method,; which will impute the type of of the expression. >>> hl.literal([0,1,2]); <ArrayNumericExpression of type array<int32>>. Boolean Logic; =============. Unlike Python, a Hail :class:`.BooleanExpression` cannot be used with the Python; keywords ``and``, ``or``, and ``not``. The Hail substitutes are ``&``, ``|``,; and ``~``. >>> s1 = hl.int32(3) == 4; >>> s2 = hl.int32(3) != 4. >>> s1 & s2; <BooleanExpression of type bool>. >>> s1 | s2; <BooleanExpression of type bool>. >>> ~s1; <BooleanExpression of type bool>. Remember that you can use :func:`~hail.expr.eval",MatchSource.DOCS,hail/python/hail/docs/overview/expressions.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst:9164,Security,expose,exposed,9164,"ld a conditional tree based; on the value of an expression. In the example below, ``csq`` is a; :class:`.StringExpression` representing the functional consequence of a; mutation. If ``csq`` does not match one of the cases specified by; :meth:`~hail.expr.builders.SwitchBuilder.when`, it is set to missing with; :meth:`~hail.expr.builders.SwitchBuilder.or_missing`. Other switch statements are documented in the; :class:`.SwitchBuilder` class. >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of ``csq`` to the missing value; ``hl.missing(hl.tstr)``, then the result of the switch statement above would also; be missing. Missingness; ===========. In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the :func:`.missing` function, which; takes the type as its single argument. An example of generating a :class:`.Float64Expression` that is missing is:. >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don't satisfy a condition:. >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is ``None``. For example, if; we define ``cnull`` to be a missing value with type :obj:`.tcall`, calling; the method `is_het` will return ``None`` and not ``False``. >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; =========. In addition to the methods exposed on each :class:`.Expression`, Hail also has; numerous functions that can be applied to expressions, which also return an; expression. Take a look at the :ref:`sec-functions` page for full documentation.; ",MatchSource.DOCS,hail/python/hail/docs/overview/expressions.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/expressions.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:6086,Deployability,update,update,6086,"rray<str>; ----------------------------------------. Common Operations; =================. Like tables, Hail provides a number of methods for manipulating data in a; matrix table. **Filter**. :class:`.MatrixTable` has three methods to filter based on expressions:. - :meth:`.MatrixTable.filter_rows`; - :meth:`.MatrixTable.filter_cols`; - :meth:`.MatrixTable.filter_entries`. Filter methods take a :class:`.BooleanExpression` argument. These expressions; are generated by applying computations to the fields of the matrix table:. >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the :meth:`.MatrixTable.filter_cols`; example above aggregates entries per column of the matrix table to compute the; mean of the `GQ` field, and removes columns where the result is smaller than 20. **Annotate**. :class:`.MatrixTable` has four methods to add new fields or update existing fields:. - :meth:`.MatrixTable.annotate_globals`; - :meth:`.MatrixTable.annotate_rows`; - :meth:`.MatrixTable.annotate_cols`; - :meth:`.MatrixTable.annotate_entries`. Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added. The simplest example is adding a new global field `foo` that just contains the constant; 5. >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field `call_rate` which computes the fraction; of non-missing entries `GT` per row:. >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if `GQ` is less than 20, we can do the following:. >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). **Select",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:6899,Deployability,update,update,6899,"of the `GQ` field, and removes columns where the result is smaller than 20. **Annotate**. :class:`.MatrixTable` has four methods to add new fields or update existing fields:. - :meth:`.MatrixTable.annotate_globals`; - :meth:`.MatrixTable.annotate_rows`; - :meth:`.MatrixTable.annotate_cols`; - :meth:`.MatrixTable.annotate_entries`. Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added. The simplest example is adding a new global field `foo` that just contains the constant; 5. >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field `call_rate` which computes the fraction; of non-missing entries `GT` per row:. >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if `GQ` is less than 20, we can do the following:. >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). **Select**. Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the `GT` field; we can do the following:. >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. :class:`.MatrixTable` has four select methods that select and create new fields:. - :meth:`.MatrixTable.select_globals`; - :meth:`.MatrixTable.select_rows`; - :meth:`.MatrixTable.select_cols`; - :meth:`.MatrixTable.select_entries`. Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; ``KEY=VALUE`` to compute new fields. The",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:1229,Energy Efficiency,efficient,efficiently,1229,"x table consists of four components:. 1. a two-dimensional matrix of **entry fields** where each entry is indexed by; row key(s) and column key(s); 2. a corresponding rows table that stores all of the **row fields** that are; constant for every column in the dataset; 3. a corresponding columns table that stores all of the **column fields** that; are constant for; every row in the dataset; 4. a set of **global fields** that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, :class:`.Table` has :meth:`.Table.select` and; :meth:`.Table.select_globals`, while :class:`.MatrixTable` has; :meth:`.MatrixTable.select_rows`, :meth:`.MatrixTable.select_cols`,; :meth:`.MatrixTable.select_entries`, and :meth:`.MatrixTable.select_globals`. It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the :class:`.MatrixTable` represents; this data far more efficiently and exposes natural interfaces for computing on; it. The :meth:`.MatrixTable.rows` and :meth:`.MatrixTable.cols` methods return the; row and column fields as separate tables. The :meth:`.MatrixTable.entries`; method returns the matrix as a table in coordinate form -- use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Re",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:10579,Energy Efficiency,efficient,efficient,10579,":10019093 | [""A"",""G""] | 2 |; | 20:10026348 | [""A"",""G""] | 1 |; | 20:10026348 | [""A"",""G""] | 2 |; | 20:10026357 | [""T"",""C""] | 1 |; | 20:10026357 | [""T"",""C""] | 2 |; | 20:10030188 | [""T"",""A""] | 1 |; | 20:10030188 | [""T"",""A""] | 2 |; | 20:10030452 | [""G"",""A""] | 1 |; | 20:10030452 | [""G"",""A""] | 2 |; +---------------+------------+---------------+; showing top 10 rows. Aggregation; ===========. :class:`.MatrixTable` has three methods to compute aggregate statistics. - :meth:`.MatrixTable.aggregate_rows`; - :meth:`.MatrixTable.aggregate_cols`; - :meth:`.MatrixTable.aggregate_entries`. These methods take an aggregated expression and evaluate it, returning; a Python value. An example of querying entries is to compute the global mean of field `GQ`:. >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) # doctest: +SKIP_OUTPUT_CHECK; 67.73196915777027. It is possible to compute multiple values simultaneously by; creating a tuple or struct. This is encouraged, because grouping two; computations together is far more efficient by traversing the dataset only once; rather than twice. >>> mt.aggregate_entries((hl.agg.stats(mt.DP), hl.agg.stats(mt.GQ))) # doctest: +SKIP_OUTPUT_CHECK; (Struct(mean=41.83915800445897, stdev=41.93057654787303, min=0.0, max=450.0, n=34537, sum=1444998.9999999995),; Struct(mean=67.73196915777027, stdev=29.80840934057741, min=0.0, max=99.0, n=33720, sum=2283922.0000000135)). See the :ref:`sec-aggregators` page for the complete list of aggregator; functions. Group-By; ========. Matrix tables can be aggregated along the row or column axis to produce a new; matrix table. - :meth:`.MatrixTable.group_rows_by`; - :meth:`.MatrixTable.group_cols_by`. First let's add a random phenotype as a new column field `case_status` and then; compute statistics about the entry field `GQ` for each grouping of `case_status`. >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by `case_status` and aggregate:. >>> mt",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:1261,Integrability,interface,interfaces,1261,"x table consists of four components:. 1. a two-dimensional matrix of **entry fields** where each entry is indexed by; row key(s) and column key(s); 2. a corresponding rows table that stores all of the **row fields** that are; constant for every column in the dataset; 3. a corresponding columns table that stores all of the **column fields** that; are constant for; every row in the dataset; 4. a set of **global fields** that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, :class:`.Table` has :meth:`.Table.select` and; :meth:`.Table.select_globals`, while :class:`.MatrixTable` has; :meth:`.MatrixTable.select_rows`, :meth:`.MatrixTable.select_cols`,; :meth:`.MatrixTable.select_entries`, and :meth:`.MatrixTable.select_globals`. It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the :class:`.MatrixTable` represents; this data far more efficiently and exposes natural interfaces for computing on; it. The :meth:`.MatrixTable.rows` and :meth:`.MatrixTable.cols` methods return the; row and column fields as separate tables. The :meth:`.MatrixTable.entries`; method returns the matrix as a table in coordinate form -- use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Re",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:12313,Performance,perform,performing,12313,"bout the entry field `GQ` for each grouping of `case_status`. >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by `case_status` and aggregate:. >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; =====. Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table. :class:`.MatrixTable` has methods for concatenating rows or columns:. - :meth:`.MatrixTable.union_cols`; - :meth:`.MatrixTable.union_rows`. :meth:`.MatrixTable.union_cols` joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to `paste` in; Unix). Likewise, :meth:`.MatrixTable.union_rows` performs an inner join on; columns while concatenating rows together (similar to `cat` in Unix). In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys. If the keys are compatible, joins can then be performed using Python's bracket; notation ``[]``. This looks like ``right_table[left_table.key]``. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys. For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let `gnomad_data` be a; :class:`.Table` keyed by two row fields with type; ``locus`` and ``array<str>``, which mat",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:12457,Performance,perform,performs,12457,"TROL"")). Next we group the columns by `case_status` and aggregate:. >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; =====. Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table. :class:`.MatrixTable` has methods for concatenating rows or columns:. - :meth:`.MatrixTable.union_cols`; - :meth:`.MatrixTable.union_rows`. :meth:`.MatrixTable.union_cols` joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to `paste` in; Unix). Likewise, :meth:`.MatrixTable.union_rows` performs an inner join on; columns while concatenating rows together (similar to `cat` in Unix). In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys. If the keys are compatible, joins can then be performed using Python's bracket; notation ``[]``. This looks like ``right_table[left_table.key]``. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys. For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let `gnomad_data` be a; :class:`.Table` keyed by two row fields with type; ``locus`` and ``array<str>``, which matches the row keys of `mt`:. >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field su",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:12848,Performance,perform,performed,12848,">> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; =====. Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table. :class:`.MatrixTable` has methods for concatenating rows or columns:. - :meth:`.MatrixTable.union_cols`; - :meth:`.MatrixTable.union_rows`. :meth:`.MatrixTable.union_cols` joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to `paste` in; Unix). Likewise, :meth:`.MatrixTable.union_rows` performs an inner join on; columns while concatenating rows together (similar to `cat` in Unix). In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys. If the keys are compatible, joins can then be performed using Python's bracket; notation ``[]``. This looks like ``right_table[left_table.key]``. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys. For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let `gnomad_data` be a; :class:`.Table` keyed by two row fields with type; ``locus`` and ``array<str>``, which matches the row keys of `mt`:. >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as `AF` from `gnomad_data`,; we could do the following:. >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to :meth:`.MatrixTable.annotate_rows`:. >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus,",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:1245,Security,expose,exposes,1245,"x table consists of four components:. 1. a two-dimensional matrix of **entry fields** where each entry is indexed by; row key(s) and column key(s); 2. a corresponding rows table that stores all of the **row fields** that are; constant for every column in the dataset; 3. a corresponding columns table that stores all of the **column fields** that; are constant for; every row in the dataset; 4. a set of **global fields** that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, :class:`.Table` has :meth:`.Table.select` and; :meth:`.Table.select_globals`, while :class:`.MatrixTable` has; :meth:`.MatrixTable.select_rows`, :meth:`.MatrixTable.select_cols`,; :meth:`.MatrixTable.select_entries`, and :meth:`.MatrixTable.select_globals`. It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the :class:`.MatrixTable` represents; this data far more efficiently and exposes natural interfaces for computing on; it. The :meth:`.MatrixTable.rows` and :meth:`.MatrixTable.cols` methods return the; row and column fields as separate tables. The :meth:`.MatrixTable.entries`; method returns the matrix as a table in coordinate form -- use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Re",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:1944,Security,access,accessed,1944,":`.MatrixTable.select_cols`,; :meth:`.MatrixTable.select_entries`, and :meth:`.MatrixTable.select_globals`. It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the :class:`.MatrixTable` represents; this data far more efficiently and exposes natural interfaces for computing on; it. The :meth:`.MatrixTable.rows` and :meth:`.MatrixTable.cols` methods return the; row and column fields as separate tables. The :meth:`.MatrixTable.entries`; method returns the matrix as a table in coordinate form -- use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; ==================. All fields (row, column, global, entry) are top-level and exposed as attributes; on the :class:`.MatrixTable` object. For example, if the matrix table `mt` had a; row field `locus`, this field could be referenced with either ``mt.locus`` or; ``mt['locus']``. The former access pattern does not work with field names with; spaces or punctuation. The result of referencing a field from a matrix table is an :class:`.Expression`; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression. When evaluated in a Python inter",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:2335,Security,expose,exposed,2335,"nd :meth:`.MatrixTable.cols` methods return the; row and column fields as separate tables. The :meth:`.MatrixTable.entries`; method returns the matrix as a table in coordinate form -- use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; ==================. All fields (row, column, global, entry) are top-level and exposed as attributes; on the :class:`.MatrixTable` object. For example, if the matrix table `mt` had a; row field `locus`, this field could be referenced with either ``mt.locus`` or; ``mt['locus']``. The former access pattern does not work with field names with; spaces or punctuation. The result of referencing a field from a matrix table is an :class:`.Expression`; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression. When evaluated in a Python interpreter, we can see ``mt.locus`` is a; :class:`.LocusExpression` with type ``locus<GRCh37>``. >>> mt # doctest: +SKIP_OUTPUT_CHECK; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus # doctest: +SKIP_OUTPUT_CHECK; <LocusExpression of type locus<GRCh37>>. Likewise, ``mt.DP`` is an :class:`.Int32Expression` with type ``int32``; and is an entry field of ``mt``. Hail express",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:2547,Security,access,access,2547,"to compute and is significantly; larger in memory. Keys; ====. Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; :attr:`.MatrixTable.row_key` and :attr:`.MatrixTable.col_key`. It is possible to; change the keys with :meth:`.MatrixTable.key_rows_by` and; :meth:`.MatrixTable.key_cols_by`. Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; ==================. All fields (row, column, global, entry) are top-level and exposed as attributes; on the :class:`.MatrixTable` object. For example, if the matrix table `mt` had a; row field `locus`, this field could be referenced with either ``mt.locus`` or; ``mt['locus']``. The former access pattern does not work with field names with; spaces or punctuation. The result of referencing a field from a matrix table is an :class:`.Expression`; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression. When evaluated in a Python interpreter, we can see ``mt.locus`` is a; :class:`.LocusExpression` with type ``locus<GRCh37>``. >>> mt # doctest: +SKIP_OUTPUT_CHECK; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus # doctest: +SKIP_OUTPUT_CHECK; <LocusExpression of type locus<GRCh37>>. Likewise, ``mt.DP`` is an :class:`.Int32Expression` with type ``int32``; and is an entry field of ``mt``. Hail expressions can also :meth:`.Expression.describe` themselves, providing; information about their source matrix table or table and which keys index the; expression, if any. For example, ``mt.DP.describe()`` tells us that ``mt.DP``; has type ``int32`` and is an ",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:7840,Security,access,accessing,7840,"ate methods are also useful for updating values. For example, to update the; GT entry field to be missing if `GQ` is less than 20, we can do the following:. >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). **Select**. Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the `GT` field; we can do the following:. >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. :class:`.MatrixTable` has four select methods that select and create new fields:. - :meth:`.MatrixTable.select_globals`; - :meth:`.MatrixTable.select_rows`; - :meth:`.MatrixTable.select_cols`; - :meth:`.MatrixTable.select_entries`. Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; ``KEY=VALUE`` to compute new fields. The Python unpack operator ``**`` can be; used to specify that all fields of a Struct should become top level fields.; However, be aware that all top-level field names must be unique. In the; following example, `**mt['info']` would fail if `DP` already exists as an entry; field. >>> mt_new = mt.select_rows(**mt['info']) # doctest: +SKIP. The example below adds two new row fields. Keys are always preserved, so the; row keys ``locus`` and ``alleles`` will also be present in the new table.; ``AC = mt.info.AC`` turns the subfield ``AC`` into a top-level field. >>> mt_new = mt.select_rows(AC = mt.info.AC,; ... n_filters = hl.len(mt['filters'])). The order of the fields entered as arguments will be maintained in the new; matrix table. **Drop**. The complement of `select` methods, :meth:`.MatrixTable.drop` can remove any top; level field. An example of removing the `GQ` entry field is:. >>> mt_n",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst:6428,Usability,simpl,simplest,6428," :class:`.BooleanExpression` argument. These expressions; are generated by applying computations to the fields of the matrix table:. >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the :meth:`.MatrixTable.filter_cols`; example above aggregates entries per column of the matrix table to compute the; mean of the `GQ` field, and removes columns where the result is smaller than 20. **Annotate**. :class:`.MatrixTable` has four methods to add new fields or update existing fields:. - :meth:`.MatrixTable.annotate_globals`; - :meth:`.MatrixTable.annotate_rows`; - :meth:`.MatrixTable.annotate_cols`; - :meth:`.MatrixTable.annotate_entries`. Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added. The simplest example is adding a new global field `foo` that just contains the constant; 5. >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field `call_rate` which computes the fraction; of non-missing entries `GT` per row:. >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if `GQ` is less than 20, we can do the following:. >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). **Select**. Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the `GT` field; we can do the following:. >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtyp",MatchSource.DOCS,hail/python/hail/docs/overview/matrix_table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/matrix_table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:8831,Availability,down,downstream,8831,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:4150,Deployability,update,update,4150,". This means that the row field `ID` can be accessed; from table `ht` with ``ht.Sample`` or ``ht['Sample']``. If `ht` also had a; global field `G`, then it could be accessed by either ``ht.G`` or ``ht['G']``.; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; :class:`.Expression` that also contains context about its type and source, in; this case a row field of table `ht`. >>> ht # doctest: +SKIP_OUTPUT_CHECK; <hail.table.Table at 0x110791a20>. >>> ht.ID # doctest: +SKIP_OUTPUT_CHECK; <Int32Expression of type int32>. Updating Fields; ===============. Add or remove row fields from a Table with :meth:`.Table.select` and; :meth:`.Table.drop`. >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use :meth:`.Table.annotate` to add new row fields or update the values of; existing row fields and use :meth:`.Table.filter` to either keep or remove; rows based on a condition:. >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; ===========. To compute an aggregate statistic over the rows of; a dataset, Hail provides an :meth:`.Table.aggregate` method which can be passed; a wide variety of aggregator functions (see :ref:`sec-aggregators`):. >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'F')); 0.5. We also might want to compute the mean value of `HT` for each sex. This is; possible with a combination of :meth:`.Table.group_by` and; :meth:`.GroupedTable.aggregate`:. >>> ht_agg = (ht.group_by(ht.SEX); ... .aggregate(mean = hl.agg.mean(ht.HT))); >>> ht_agg.show(); +-----+----------+; | SEX | mean |; +-----+----------+; | str | float64 |; +-----+----------+; | ""F"" | 6.50e+01 |; | ""M"" | 6.85e+01 |; +-----+----------+. Note that the result of ``ht.group_by(...).aggregate(...)`` is a new; :class:`.Ta",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:8769,Deployability,pipeline,pipelines,8769,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:803,Modifiability,variab,variable,803,"--------------; Table Overview; --------------. A :class:`.Table` is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; ======. Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the :func:`.import_table` function. >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; :func:`.import_locus_intervals`, :func:`.import_fam`, and :func:`.import_bed`.; Many Hail methods also return tables. An example of a table is below. We recommend `ht` as a variable name for; tables, referring to a ""Hail table"". >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; =============. In addition to row fields, Hail tables also have global fields. You can think of; globals as extra fields in the table whose values are identical for every row.; For example, the same table above with the global field ``G = 5`` can be thought; of as. .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | G |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:423,Performance,load,load,423,"--------------; Table Overview; --------------. A :class:`.Table` is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; ======. Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the :func:`.import_table` function. >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; :func:`.import_locus_intervals`, :func:`.import_fam`, and :func:`.import_bed`.; Many Hail methods also return tables. An example of a table is below. We recommend `ht` as a variable name for; tables, referring to a ""Hail table"". >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; =============. In addition to row fields, Hail tables also have global fields. You can think of; globals as extra fields in the table whose values are identical for every row.; For example, the same table above with the global field ``G = 5`` can be thought; of as. .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | G |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:7151,Performance,perform,perform,7151,"T | SEX | X | Z | C1 | C2 | C3 | A | B |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 65 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 72 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 70 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 60 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; <BLANKLINE>. In addition to the :meth:`.Table.join` method, Hail provides another; join syntax using Python's bracket indexing syntax. The syntax looks like; ``right_table[left_table.key]``, which will return an :class:`.Expression`; instead of a :class:`.Table`. This expression is a dictionary mapping the; keys in the left table to the rows in the right table.; We can annotate the left table with this expression to perform a left join:; ``left_table.annotate(x = right_table[left_table.key].x]``. For example, below; we add the field 'B' from `ht2` to `ht`:. >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebo",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:3205,Security,access,accessed,3205,"-10 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. but the value ``5`` is only stored once for the entire dataset and NOT once per; row of the table. The output of :meth:`.Table.describe` lists what all of the row; fields and global fields are. >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; ====. Row fields can be specified to be the key of the table with the method; :meth:`.Table.key_by`. Keys are important for joining tables together (discussed; below). Referencing Fields; ==================. Each :class:`.Table` object has all of its row fields and global fields as; attributes in its namespace. This means that the row field `ID` can be accessed; from table `ht` with ``ht.Sample`` or ``ht['Sample']``. If `ht` also had a; global field `G`, then it could be accessed by either ``ht.G`` or ``ht['G']``.; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; :class:`.Expression` that also contains context about its type and source, in; this case a row field of table `ht`. >>> ht # doctest: +SKIP_OUTPUT_CHECK; <hail.table.Table at 0x110791a20>. >>> ht.ID # doctest: +SKIP_OUTPUT_CHECK; <Int32Expression of type int32>. Updating Fields; ===============. Add or remove row fields from a Table with :meth:`.Table.select` and; :meth:`.Table.drop`. >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use :meth:`.Table.annotate` to add new row fields or update the values of; existing row fields and use :m",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:3326,Security,access,accessed,3326,"ly stored once for the entire dataset and NOT once per; row of the table. The output of :meth:`.Table.describe` lists what all of the row; fields and global fields are. >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; ====. Row fields can be specified to be the key of the table with the method; :meth:`.Table.key_by`. Keys are important for joining tables together (discussed; below). Referencing Fields; ==================. Each :class:`.Table` object has all of its row fields and global fields as; attributes in its namespace. This means that the row field `ID` can be accessed; from table `ht` with ``ht.Sample`` or ``ht['Sample']``. If `ht` also had a; global field `G`, then it could be accessed by either ``ht.G`` or ``ht['G']``.; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; :class:`.Expression` that also contains context about its type and source, in; this case a row field of table `ht`. >>> ht # doctest: +SKIP_OUTPUT_CHECK; <hail.table.Table at 0x110791a20>. >>> ht.ID # doctest: +SKIP_OUTPUT_CHECK; <Int32Expression of type int32>. Updating Fields; ===============. Add or remove row fields from a Table with :meth:`.Table.select` and; :meth:`.Table.drop`. >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use :meth:`.Table.annotate` to add new row fields or update the values of; existing row fields and use :meth:`.Table.filter` to either keep or remove; rows based on a condition:. >>> ht_new = ht.filter(ht['C1'] >=",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:3441,Security,access,accessing,3441,"ribe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; ====. Row fields can be specified to be the key of the table with the method; :meth:`.Table.key_by`. Keys are important for joining tables together (discussed; below). Referencing Fields; ==================. Each :class:`.Table` object has all of its row fields and global fields as; attributes in its namespace. This means that the row field `ID` can be accessed; from table `ht` with ``ht.Sample`` or ``ht['Sample']``. If `ht` also had a; global field `G`, then it could be accessed by either ``ht.G`` or ``ht['G']``.; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; :class:`.Expression` that also contains context about its type and source, in; this case a row field of table `ht`. >>> ht # doctest: +SKIP_OUTPUT_CHECK; <hail.table.Table at 0x110791a20>. >>> ht.ID # doctest: +SKIP_OUTPUT_CHECK; <Int32Expression of type int32>. Updating Fields; ===============. Add or remove row fields from a Table with :meth:`.Table.select` and; :meth:`.Table.drop`. >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use :meth:`.Table.annotate` to add new row fields or update the values of; existing row fields and use :meth:`.Table.filter` to either keep or remove; rows based on a condition:. >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; ===========. To compute an aggregate statistic over the rows of; a dataset, Hail provides an :meth:`.Ta",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:8609,Security,access,accessed,8609,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:9012,Security,access,accessed,9012,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:9104,Security,access,accessed,9104,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:9210,Security,access,accessed,9210,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst:8761,Testability,test,testing,8761,"t.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; ===============================. Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the :meth:`.Table.show` method to see the first few rows; of a table. :meth:`.Table.take` will collect the first `n` rows of a table into a local; Python list:. >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a :class:`.Struct` whose elements can be; accessed using Python's get attribute or get item notation:. >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The :meth:`.Table.head` method is helpful for testing pipelines. It subsets a; table to the first `n` rows, causing downstream operations to run much more; quickly. :meth:`.Table.describe` is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ``ht.ID.dtype``), and the full row and global types can be accessed with; ``ht.row.dtype`` and ``ht.globals.dtype``. The row fields that are part of the; key can be accessed with :attr:`.Table.key`. The :meth:`.Table.count` method; returns the number of rows.; ",MatchSource.DOCS,hail/python/hail/docs/overview/table.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/overview/table.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:5052,Availability,error,error,5052,"nt information content. Component tables; ^^^^^^^^^^^^^^^^. The :class:`.VariantDataset` is made up of two component matrix tables -- the; ``reference_data`` and the ``variant_data``. The ``reference_data`` matrix table is a sparse matrix of reference blocks. The; ``reference_data`` matrix table has row key ``locus``, but; does not have an ``alleles`` key or field. The column key is the sample ID. The; entries indicate regions of reference calls with similar sequencing metadata; (depth, quality, etc), starting from ``vds.reference_data.locus.position`` and; ending at ``vds.reference_data.END`` (inclusive!). There is no ``GT`` call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. ",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:6082,Deployability,pipeline,pipelines,6082,"). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. **Analyses that use variant data and/or reference data separately.** Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with ``vds.variant_data`` and; ``vds.reference_data``. 3. **Analyses that use the full variant-by-sample matrix with variant and reference data**.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; :func:`.vds.to_dense_mt` and :func:`.vds.to_merged_sparse_mt`.; ",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:6334,Deployability,pipeline,pipelines,6334,"). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. **Analyses that use variant data and/or reference data separately.** Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with ``vds.variant_data`` and; ``vds.reference_data``. 3. **Analyses that use the full variant-by-sample matrix with variant and reference data**.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; :func:`.vds.to_dense_mt` and :func:`.vds.to_merged_sparse_mt`.; ",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:6560,Deployability,pipeline,pipelines,6560,"). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. **Analyses that use variant data and/or reference data separately.** Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with ``vds.variant_data`` and; ``vds.reference_data``. 3. **Analyses that use the full variant-by-sample matrix with variant and reference data**.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; :func:`.vds.to_dense_mt` and :func:`.vds.to_merged_sparse_mt`.; ",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:602,Integrability,interface,interfaces,602,".. _sec-vds:. Variant Dataset; ===============. The :class:`.VariantDataset` is an extra layer of abstraction of the Hail Matrix Table for working; with large sequencing datasets. It was initially developed in response to the gnomAD project's need; to combine, represent, and analyze 150,000 whole genomes. It has since been used on datasets as; large as 955,000 whole exomes. The :class:`.VariantDatasetCombiner` produces a; :class:`.VariantDataset` by combining any number of GVCF and/or :class:`.VariantDataset` files. .. warning::. Hail 0.1 also had a Variant Dataset class. Although pieces of the interfaces are similar, they should not; be considered interchangeable and do not represent the same data. .. currentmodule:: hail.vds. .. rubric:: Variant Dataset. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VariantDataset. .. autosummary::; :toctree: ./. read_vds; filter_samples; filter_variants; filter_intervals; filter_chromosomes; sample_qc; split_multi; interval_coverage; impute_sex_chromosome_ploidy; impute_sex_chr_ploidy_from_interval_coverage; to_dense_mt; to_merged_sparse_mt; truncate_reference_blocks; merge_reference_blocks; lgt_to_gt; local_to_global; store_ref_block_max_length. .. currentmodule:: hail.vds.combiner. .. rubric:: Variant Dataset Combiner. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VDSMetadata; VariantDatasetCombiner. .. autosummary::; :toctree: ./. new_combiner; load_combiner. The data model of :class:`.VariantDataset`; ------------------------------------------. A VariantDataset is the Hail implementation of a data structure called the; ""scalable variant call representation"", or SVCR. The Scalable Variant Call Representation (SVCR); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. 1. The scalable variant call",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:5545,Integrability,depend,depending,5545,"ding at ``vds.reference_data.END`` (inclusive!). There is no ``GT`` call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. **Analyses that use variant data and/or reference data separately.** Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with ``vds.variant_data`` and; ``vds.reference_data``. 3. **Analyses that use the full variant-by-sample matrix with variant and reference data**.; Many pipelines req",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:1637,Performance,scalab,scalable,1637,"lar, they should not; be considered interchangeable and do not represent the same data. .. currentmodule:: hail.vds. .. rubric:: Variant Dataset. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VariantDataset. .. autosummary::; :toctree: ./. read_vds; filter_samples; filter_variants; filter_intervals; filter_chromosomes; sample_qc; split_multi; interval_coverage; impute_sex_chromosome_ploidy; impute_sex_chr_ploidy_from_interval_coverage; to_dense_mt; to_merged_sparse_mt; truncate_reference_blocks; merge_reference_blocks; lgt_to_gt; local_to_global; store_ref_block_max_length. .. currentmodule:: hail.vds.combiner. .. rubric:: Variant Dataset Combiner. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VDSMetadata; VariantDatasetCombiner. .. autosummary::; :toctree: ./. new_combiner; load_combiner. The data model of :class:`.VariantDataset`; ------------------------------------------. A VariantDataset is the Hail implementation of a data structure called the; ""scalable variant call representation"", or SVCR. The Scalable Variant Call Representation (SVCR); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. 1. The scalable variant call representation is **sparse**. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores **less data but more information**, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; ``vds.reference_data``, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and all",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:1843,Performance,scalab,scalable,1843,". .. autosummary::; :toctree: ./. read_vds; filter_samples; filter_variants; filter_intervals; filter_chromosomes; sample_qc; split_multi; interval_coverage; impute_sex_chromosome_ploidy; impute_sex_chr_ploidy_from_interval_coverage; to_dense_mt; to_merged_sparse_mt; truncate_reference_blocks; merge_reference_blocks; lgt_to_gt; local_to_global; store_ref_block_max_length. .. currentmodule:: hail.vds.combiner. .. rubric:: Variant Dataset Combiner. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VDSMetadata; VariantDatasetCombiner. .. autosummary::; :toctree: ./. new_combiner; load_combiner. The data model of :class:`.VariantDataset`; ------------------------------------------. A VariantDataset is the Hail implementation of a data structure called the; ""scalable variant call representation"", or SVCR. The Scalable Variant Call Representation (SVCR); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. 1. The scalable variant call representation is **sparse**. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores **less data but more information**, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; ``vds.reference_data``, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; ``END`` field which denotes the last position included in the current; reference block. 2. The scalable variant call representation uses **local alleles**. In a VCF,; the fields GT, AD, PL, etc contain info",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:1980,Performance,scalab,scalable,1980,"erage; impute_sex_chromosome_ploidy; impute_sex_chr_ploidy_from_interval_coverage; to_dense_mt; to_merged_sparse_mt; truncate_reference_blocks; merge_reference_blocks; lgt_to_gt; local_to_global; store_ref_block_max_length. .. currentmodule:: hail.vds.combiner. .. rubric:: Variant Dataset Combiner. .. autosummary::; :nosignatures:; :toctree: ./; :template: class2.rst. VDSMetadata; VariantDatasetCombiner. .. autosummary::; :toctree: ./. new_combiner; load_combiner. The data model of :class:`.VariantDataset`; ------------------------------------------. A VariantDataset is the Hail implementation of a data structure called the; ""scalable variant call representation"", or SVCR. The Scalable Variant Call Representation (SVCR); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. 1. The scalable variant call representation is **sparse**. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores **less data but more information**, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; ``vds.reference_data``, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; ``END`` field which denotes the last position included in the current; reference block. 2. The scalable variant call representation uses **local alleles**. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers,",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:2741,Performance,scalab,scalable,2741,"^^^^^^^^^^^^^. Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. 1. The scalable variant call representation is **sparse**. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores **less data but more information**, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; ``vds.reference_data``, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; ``END`` field which denotes the last position included in the current; reference block. 2. The scalable variant call representation uses **local alleles**. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, **even though the information content; does not change**. To avoid this superlinear scaling, the SVCR renames these; fields to their ""local"" versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list. For instance, if a sample has the following information in its GVCF:. .. code::. Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample's; entry would look like:. .. code::. LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The ""1"" allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (**C**),",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:3065,Safety,avoid,avoid,3065,"blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores **less data but more information**, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; ``vds.reference_data``, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; ``END`` field which denotes the last position included in the current; reference block. 2. The scalable variant call representation uses **local alleles**. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, **even though the information content; does not change**. To avoid this superlinear scaling, the SVCR renames these; fields to their ""local"" versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list. For instance, if a sample has the following information in its GVCF:. .. code::. Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample's; entry would look like:. .. code::. LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The ""1"" allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (**C**), but rather the allele whose index is in position 1 of the LA list.; The *index* at position 2 of the LA list is 2, and the allele with absolute; index 2 is **T**. Local alleles make it possible to keep the data small to; match its inherent information content. Component tables; ^^^^^^^^^^^^^^^^. The :class:`.VariantDataset` is made up of two component matrix tables -",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst:6214,Testability,test,tests,6214,"). A record from a component GVCF is included in the; ``reference_data`` if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner). The ``variant_data`` matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the ``variant_data`` if; it does not define the END INFO field. This means that some records of the; ``variant_data`` can be no-call (``./.``) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the :class:`.VariantDataset`; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. 1. **Analyses that use prebuilt methods**. Some analyses can be supported by using; only the utility functions defined in the ``hl.vds`` module, like; :func:`.vds.sample_qc`. 2. **Analyses that use variant data and/or reference data separately.** Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with ``vds.variant_data`` and; ``vds.reference_data``. 3. **Analyses that use the full variant-by-sample matrix with variant and reference data**.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; :func:`.vds.to_dense_mt` and :func:`.vds.to_merged_sparse_mt`.; ",MatchSource.DOCS,hail/python/hail/docs/vds/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/vds/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/experimental.rst:112,Integrability,interface,interface,112,.. DANGER::. This functionality is *experimental*. It may not be tested as; well as other parts of Hail and the interface is subject to; change.; ,MatchSource.DOCS,hail/python/hail/docs/_templates/experimental.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/experimental.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/experimental.rst:65,Testability,test,tested,65,.. DANGER::. This functionality is *experimental*. It may not be tested as; well as other parts of Hail and the interface is subject to; change.; ,MatchSource.DOCS,hail/python/hail/docs/_templates/experimental.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/experimental.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/write_warning.rst:29,Availability,checkpoint,checkpoint,29,.. DANGER::. Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss.; ,MatchSource.DOCS,hail/python/hail/docs/_templates/write_warning.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/write_warning.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2008,Safety,predict,prediction,2008,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2067,Safety,predict,prediction,2067,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2136,Safety,predict,prediction,2136,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2204,Safety,predict,prediction,2204,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2272,Safety,predict,prediction,2272,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2335,Safety,predict,prediction,2335,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2397,Safety,predict,prediction,2397,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2459,Safety,predict,prediction,2459,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2525,Safety,predict,prediction,2525,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst:2597,Safety,predict,prediction,2597,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/dbNSFP_genes.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/gnomad_pca_variant_loadings.rst:431,Performance,load,loadings,431,".. _gnomad_pca_variant_loadings:. gnomad_pca_variant_loadings; ===========================. * **Versions:** 2.1, 3.1; * **Reference genome builds:** GRCh37, GRCh38; * **Type:** :class:`hail.Table`. Schema (3.1, GRCh38); ~~~~~~~~~~~~~~~~~~~~. .. code-block:: text. ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------; ",MatchSource.DOCS,hail/python/hail/docs/datasets/schemas/gnomad_pca_variant_loadings.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/datasets/schemas/gnomad_pca_variant_loadings.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class.rst:116,Modifiability,inherit,inherited-members,116,"{{ objname | escape | underline }}. .. currentmodule:: {{ module }}. .. autoclass:: {{ objname }}(); :members:; :no-inherited-members:. {% block attributes %}; {% if attributes %}; .. rubric:: Attributes. .. autosummary::; :nosignatures:. {% for item in attributes %}; {% if item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}. {% block methods %}; {% if (methods | reject('in', inherited_members) | list | count) != 0 %}. .. rubric:: Methods. .. autosummary::; :nosignatures:. {% for item in methods %}; {% if item not in inherited_members and item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}; ",MatchSource.DOCS,hail/python/hail/docs/_templates/_autosummary/class.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class2.rst:132,Modifiability,inherit,inherited-members,132,"{{ objname | escape | underline }}. .. currentmodule:: {{ module }}. .. autoclass:: {{ objname }}(); :members:; :special-members:; :inherited-members:; :no-undoc-members:. {% block attributes %}; {% if attributes %}; .. rubric:: Attributes. .. autosummary::; :nosignatures:. {% for item in attributes %}; {% if item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}. {% block methods %}; {% if (methods | reject('in', inherited_members) | list | count) != 0 %}. .. rubric:: Methods. .. autosummary::; :nosignatures:. {% for item in methods %}; {% if item not in inherited_members and item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}; ",MatchSource.DOCS,hail/python/hail/docs/_templates/_autosummary/class2.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class2.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class2.rst:156,Usability,undo,undoc-members,156,"{{ objname | escape | underline }}. .. currentmodule:: {{ module }}. .. autoclass:: {{ objname }}(); :members:; :special-members:; :inherited-members:; :no-undoc-members:. {% block attributes %}; {% if attributes %}; .. rubric:: Attributes. .. autosummary::; :nosignatures:. {% for item in attributes %}; {% if item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}. {% block methods %}; {% if (methods | reject('in', inherited_members) | list | count) != 0 %}. .. rubric:: Methods. .. autosummary::; :nosignatures:. {% for item in methods %}; {% if item not in inherited_members and item[0] != '_' %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}; ",MatchSource.DOCS,hail/python/hail/docs/_templates/_autosummary/class2.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/_autosummary/class2.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst:2530,Availability,failure,failure,2530,"words; ^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - cost; - float; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - duration; - float; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - Values are rounded to the millisecond; * - start_time; - date; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - ISO-8601 datetime string; * - end_time; - date; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - ISO-8601 datetime string. **Example:** ``cost >= 1.00``. **Example:** ``duration > 5``. **Example:** ``start_time >= 2023-02-24T17:15:25Z``. Keywords specific to searching for batches; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - batch_id; - int; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - state; - str; - ``=``, ``==``, ``!=``; - Allowed values are `running`, `complete`, `success`, `failure`, `cancelled`, `open`, `closed`; * - user; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; -; * - billing_project; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; -. **Example:** ``state = running``. **Example:** ``user = johndoe``. **Example:** ``billing_project = johndoe-trial``. Keywords specific to searching for jobs in a batch; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - job_id; - int; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - state; - str; - ``=``, ``==``, ``!=``; - Allowed values are `pending`, `ready`, `creating`, `running`, `live`, `cancelled`, `error`, `failed`, `bad`, `success`, `done`; * - instance; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; - use this to search for all jobs that ran on a given worker; * - instance_collection; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; - use this to ",MatchSource.DOCS,hail/python/hailtop/batch/docs/advanced_search_help.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst:3244,Availability,error,error,3244,"widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - batch_id; - int; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - state; - str; - ``=``, ``==``, ``!=``; - Allowed values are `running`, `complete`, `success`, `failure`, `cancelled`, `open`, `closed`; * - user; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; -; * - billing_project; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; -. **Example:** ``state = running``. **Example:** ``user = johndoe``. **Example:** ``billing_project = johndoe-trial``. Keywords specific to searching for jobs in a batch; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - job_id; - int; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - state; - str; - ``=``, ``==``, ``!=``; - Allowed values are `pending`, `ready`, `creating`, `running`, `live`, `cancelled`, `error`, `failed`, `bad`, `success`, `done`; * - instance; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; - use this to search for all jobs that ran on a given worker; * - instance_collection; - str; - ``=``, ``==``, ``!=``, ``=~``, ``!~``; - use this to search for all jobs in a given pool. **Example:** ``user = johndoe``. **Example:** ``billing_project = johndoe-trial``. **Example:** ``instance_collection = standard``. Combining Multiple Statements; -----------------------------. **Example:** Searching for batches in a time window. .. code-block::. start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. **Example:** Searching for batches that have run since June 2023 that cost more than $5 submitted by a given user. .. code-block::. start_time >= 2023-06-01; cost > 5.00; user = johndoe. **Example:** Searching for failed batches where the batch name contains pca. .. code-block::. state = failed; name =~ pca. **Example:** Searching for jobs within a given range of ids. .. code-block::. ",MatchSource.DOCS,hail/python/hailtop/batch/docs/advanced_search_help.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst:1292,Integrability,depend,dependent,1292,"ression; ----------------------. A single word enclosed with double quotes that is an exact match for either the name or; value of an attribute. **Example:** ``""pca_pipeline""``. Partial Match Expression; ------------------------. A single word without any quotes that is a partial match for either the name or the value; of an attribute. **Example:** ``pipe``. Keyword Expression; ------------------. The left hand side of the statement is the name of the attribute and the right hand side; is the value to search against. Allowed operators are ``=``, ``==``, ``!=``, ``=~``, and; ``!~`` where the operators with tildes are looking for partial matches. **Example:** ``name = pca_pipeline``. **Example:** ``name =~ pca``. Predefined Keyword Expression; -----------------------------. The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``, ``=~``, ``!~``.; The right hand side is the value to search against. Keywords; ^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * - cost; - float; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; -; * - duration; - float; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - Values are rounded to the millisecond; * - start_time; - date; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - ISO-8601 datetime string; * - end_time; - date; - ``=``, ``==``, ``!=``, ``>``, ``>=``, ``<``, ``<=``; - ISO-8601 datetime string. **Example:** ``cost >= 1.00``. **Example:** ``duration > 5``. **Example:** ``start_time >= 2023-02-24T17:15:25Z``. Keywords specific to searching for batches; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. list-table::; :widths: 25 25 50 50; :header-rows: 1. * - Keyword; - Value Type; - Allowed Operators; - Extra; * -",MatchSource.DOCS,hail/python/hailtop/batch/docs/advanced_search_help.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/advanced_search_help.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:152,Integrability,interface,interface,152,".. _sec-api:. ==========; Python API; ==========. This is the API documentation for Batch, and provides detailed information; on the Python programming interface. Use ``import hailtop.batch`` to access this functionality. .. currentmodule:: hailtop.batch. Batches; ~~~~~~~. A :class:`.Batch` is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each :class:`.Job` has; an image in which to execute commands and settings for storage,; memory, and CPU. A :class:`.BashJob` is a subclass of :class:`.Job`; that runs bash commands while a :class:`.PythonJob` executes Python; functions. .. autosummary::; :toctree: api/batch/; :nosignatures:; :template: class.rst. batch.Batch; job.Job; job.BashJob; job.PythonJob. Resources; ~~~~~~~~~. A :class:`.Resource` is an abstract class that represents files in a :class:`.Batch` and; has two subtypes: :class:`.ResourceFile` and :class:`.ResourceGroup`. A single file is represented by a :class:`.ResourceFile` which has two subtypes:; :class:`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; stand",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:362,Integrability,depend,dependencies,362,".. _sec-api:. ==========; Python API; ==========. This is the API documentation for Batch, and provides detailed information; on the Python programming interface. Use ``import hailtop.batch`` to access this functionality. .. currentmodule:: hailtop.batch. Batches; ~~~~~~~. A :class:`.Batch` is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each :class:`.Job` has; an image in which to execute commands and settings for storage,; memory, and CPU. A :class:`.BashJob` is a subclass of :class:`.Job`; that runs bash commands while a :class:`.PythonJob` executes Python; functions. .. autosummary::; :toctree: api/batch/; :nosignatures:; :template: class.rst. batch.Batch; job.Job; job.BashJob; job.PythonJob. Resources; ~~~~~~~~~. A :class:`.Resource` is an abstract class that represents files in a :class:`.Batch` and; has two subtypes: :class:`.ResourceFile` and :class:`.ResourceGroup`. A single file is represented by a :class:`.ResourceFile` which has two subtypes:; :class:`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; stand",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:1377,Integrability,depend,dependency,1377,"he order or dependencies between the jobs. Each :class:`.Job` has; an image in which to execute commands and settings for storage,; memory, and CPU. A :class:`.BashJob` is a subclass of :class:`.Job`; that runs bash commands while a :class:`.PythonJob` executes Python; functions. .. autosummary::; :toctree: api/batch/; :nosignatures:; :template: class.rst. batch.Batch; job.Job; job.BashJob; job.PythonJob. Resources; ~~~~~~~~~. A :class:`.Resource` is an abstract class that represents files in a :class:`.Batch` and; has two subtypes: :class:`.ResourceFile` and :class:`.ResourceGroup`. A single file is represented by a :class:`.ResourceFile` which has two subtypes:; :class:`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; standard library's :class:`.concurrent.futures.Executor`. It facilitates; executing arbitrary Python functions in the cloud. .. autosummary::; :toctree: api/batch_pool_executor/; :nosignatures:; :template: class.rst. batch_pool_executor.BatchPoolExecutor; batch_pool_executor.BatchPoolFuture. Backends; ~~~~~~~~. A :class:`.Backend` is an abstract class ",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:1971,Integrability,interface,interface,1971,":class:`.ResourceFile` which has two subtypes:; :class:`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; standard library's :class:`.concurrent.futures.Executor`. It facilitates; executing arbitrary Python functions in the cloud. .. autosummary::; :toctree: api/batch_pool_executor/; :nosignatures:; :template: class.rst. batch_pool_executor.BatchPoolExecutor; batch_pool_executor.BatchPoolFuture. Backends; ~~~~~~~~. A :class:`.Backend` is an abstract class that can execute a :class:`.Batch`. Currently,; there are two types of backends: :class:`.LocalBackend` and :class:`.ServiceBackend`. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (:ref:`Batch Service <sec-service>`). You can access the UI for the Batch Service; at `<https://batch.hail.is>`__. .. autosummary::; :toctree: api/backend/; :nosignatures:; :template: class.rst. backend.RunningBatchType; backend.Backend; backend.LocalBackend; backend.ServiceBackend. Utilities; ~~~~~~~~~. .. au",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:2024,Performance,concurren,concurrent,2024,":`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; standard library's :class:`.concurrent.futures.Executor`. It facilitates; executing arbitrary Python functions in the cloud. .. autosummary::; :toctree: api/batch_pool_executor/; :nosignatures:; :template: class.rst. batch_pool_executor.BatchPoolExecutor; batch_pool_executor.BatchPoolFuture. Backends; ~~~~~~~~. A :class:`.Backend` is an abstract class that can execute a :class:`.Batch`. Currently,; there are two types of backends: :class:`.LocalBackend` and :class:`.ServiceBackend`. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (:ref:`Batch Service <sec-service>`). You can access the UI for the Batch Service; at `<https://batch.hail.is>`__. .. autosummary::; :toctree: api/backend/; :nosignatures:; :template: class.rst. backend.RunningBatchType; backend.Backend; backend.LocalBackend; backend.ServiceBackend. Utilities; ~~~~~~~~~. .. autosummary::; :toctree: api/utils/; :nosignatures:. dock",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:195,Security,access,access,195,".. _sec-api:. ==========; Python API; ==========. This is the API documentation for Batch, and provides detailed information; on the Python programming interface. Use ``import hailtop.batch`` to access this functionality. .. currentmodule:: hailtop.batch. Batches; ~~~~~~~. A :class:`.Batch` is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each :class:`.Job` has; an image in which to execute commands and settings for storage,; memory, and CPU. A :class:`.BashJob` is a subclass of :class:`.Job`; that runs bash commands while a :class:`.PythonJob` executes Python; functions. .. autosummary::; :toctree: api/batch/; :nosignatures:; :template: class.rst. batch.Batch; job.Job; job.BashJob; job.PythonJob. Resources; ~~~~~~~~~. A :class:`.Resource` is an abstract class that represents files in a :class:`.Batch` and; has two subtypes: :class:`.ResourceFile` and :class:`.ResourceGroup`. A single file is represented by a :class:`.ResourceFile` which has two subtypes:; :class:`.InputResourceFile` and :class:`.JobResourceFile`. An InputResourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; stand",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst:2710,Security,access,access,2710,"esourceFile is used; to specify files that are inputs to a :class:`.Batch`. These files are not generated as outputs from a; :class:`.Job`. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs. A :class:`.ResourceGroup` represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension. A :class:`.PythonResult` stores the output from running a :class:`.PythonJob`. .. autosummary::; :toctree: api/resource/; :nosignatures:; :template: class.rst. resource.Resource; resource.ResourceFile; resource.InputResourceFile; resource.JobResourceFile; resource.ResourceGroup; resource.PythonResult. Batch Pool Executor; ~~~~~~~~~~~~~~~~~~~. A :class:`.BatchPoolExecutor` provides roughly the same interface as the Python; standard library's :class:`.concurrent.futures.Executor`. It facilitates; executing arbitrary Python functions in the cloud. .. autosummary::; :toctree: api/batch_pool_executor/; :nosignatures:; :template: class.rst. batch_pool_executor.BatchPoolExecutor; batch_pool_executor.BatchPoolFuture. Backends; ~~~~~~~~. A :class:`.Backend` is an abstract class that can execute a :class:`.Batch`. Currently,; there are two types of backends: :class:`.LocalBackend` and :class:`.ServiceBackend`. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (:ref:`Batch Service <sec-service>`). You can access the UI for the Batch Service; at `<https://batch.hail.is>`__. .. autosummary::; :toctree: api/backend/; :nosignatures:; :template: class.rst. backend.RunningBatchType; backend.Backend; backend.LocalBackend; backend.ServiceBackend. Utilities; ~~~~~~~~~. .. autosummary::; :toctree: api/utils/; :nosignatures:. docker.build_python_image; utils.concatenate; utils.plink_merge; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/api.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/api.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:2225,Availability,error,error,2225," cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. **Version 0.2.122**. - (`#13565 <https://github.com/hail-is/hail/pull/13565>`__) Users can now use VEP images from the `hailgenetics` DockerHub; in Hail Batch. **Version 0.2.121**. - (`#13396 <https://github.com/hail-is/hail/pull/13396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hail/pull/13007>`__) Memory and storage request strings may now be optionally terminated with a `B` for bytes.; - (`#13051 <https://github.com/hail-is/hail/pull/13051>`__) Azure Blob Storage `https` URLs are now supported. **Version 0.2.115**. - (`#12731 <https://github.com/hail-is/hail/pull/12731>`__) Introduced `hailtop.fs` that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by `import hailtop.fs as hfs`.; - (`#12918 <https://github.com/hail-is/hail/pull/12918>`__) Fixed a combinatorial explosion in cancellation ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:4067,Availability,error,error,4067," abs. This can be used by `import hailtop.fs as hfs`.; - (`#12918 <https://github.com/hail-is/hail/pull/12918>`__) Fixed a combinatorial explosion in cancellation calculation in the :class:`.LocalBackend`; - (`#12917 <https://github.com/hail-is/hail/pull/12917>`__) ABS blob URIs in the form of `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>` are now supported when running in Azure. The `hail-az` scheme for referencing ABS blobs is now deprecated and will be removed in a future release. **Version 0.2.114**. - (`#12780 <https://github.com/hail-is/hail/pull/12881>`__) PythonJobs now handle arguments with resources nested inside dicts and lists.; - (`#12900 <https://github.com/hail-is/hail/pull/12900>`__) Reading data from public blobs is now supported in Azure. **Version 0.2.113**. - (`#12780 <https://github.com/hail-is/hail/pull/12780>`__) The LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; - (`#12845 <https://github.com/hail-is/hail/pull/12845>`__) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :cl",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:4846,Availability,error,errors,4846,"*Version 0.2.113**. - (`#12780 <https://github.com/hail-is/hail/pull/12780>`__) The LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; - (`#12845 <https://github.com/hail-is/hail/pull/12845>`__) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the co",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:4930,Availability,error,erroring,4930,"*Version 0.2.113**. - (`#12780 <https://github.com/hail-is/hail/pull/12780>`__) The LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; - (`#12845 <https://github.com/hail-is/hail/pull/12845>`__) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the co",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:5492,Availability,avail,available,5492,"ll/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **V",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:6048,Availability,error,errors,6048,"il/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:6777,Availability,error,error,6777," 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core job",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:6852,Availability,error,error,6852," 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core job",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:8225,Availability,error,error,8225,"mory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; - Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; - Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job's CPU and memory requests. **Version 0.2.58**. - Added concatenate and plink_merge functions that use tree aggregation when merging.; - BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. **Version 0.2.56**. - Fix ``LocalBackend.run()`` succeeding when intermediate command fails. **Version 0.2.55**. - Attempts are now sorted by attempt time in the Batch Service UI. **Version 0.2.53**. - Implement and document ``BatchPoolExecutor``. **Version 0.2.50**. - Add ``requester_pays_project`` as a new parameter on batches. **Version 0.2.43**. - Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. **Version 0.2.42**. - Fixed the documentation for job memory and storage requests to have default units in bytes.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:8265,Availability,error,errors,8265,"mory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; - Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; - Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job's CPU and memory requests. **Version 0.2.58**. - Added concatenate and plink_merge functions that use tree aggregation when merging.; - BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. **Version 0.2.56**. - Fix ``LocalBackend.run()`` succeeding when intermediate command fails. **Version 0.2.55**. - Attempts are now sorted by attempt time in the Batch Service UI. **Version 0.2.53**. - Implement and document ``BatchPoolExecutor``. **Version 0.2.50**. - Add ``requester_pays_project`` as a new parameter on batches. **Version 0.2.43**. - Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. **Version 0.2.42**. - Fixed the documentation for job memory and storage requests to have default units in bytes.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:314,Deployability,release,released,314,".. _sec-change-log:. Python Version Compatibility Policy; ===================================. Hail complies with `NumPy's compatibility policy <https://numpy.org/neps/nep-0029-deprecation_policy.html#implementation>`__ on Python; versions. In particular, Hail officially supports:. - All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions. - All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; ==========. **Version 0.2.132**. - (`#14576 <https://github.com/hail-is/hail/pull/14576>`__) Fixed bug where; submitting many Python jobs would fail with `RecursionError`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:432,Deployability,release,released,432,".. _sec-change-log:. Python Version Compatibility Policy; ===================================. Hail complies with `NumPy's compatibility policy <https://numpy.org/neps/nep-0029-deprecation_policy.html#implementation>`__ on Python; versions. In particular, Hail officially supports:. - All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions. - All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; ==========. **Version 0.2.132**. - (`#14576 <https://github.com/hail-is/hail/pull/14576>`__) Fixed bug where; submitting many Python jobs would fail with `RecursionError`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:1754,Deployability,configurat,configuration,1754,Error`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. **Version 0.2.122**. - (`#13565 <https://github.com/hail-is/hail/pull/13565>`__) Users can now use VEP images from the `hailgenetics` DockerHub; in Hail Batch. **Version 0.2.121**. - (`#13396 <https://github.com/hail-is/hail/pull/13396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hai,MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:2247,Deployability,pipeline,pipeline,2247," cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. **Version 0.2.122**. - (`#13565 <https://github.com/hail-is/hail/pull/13565>`__) Users can now use VEP images from the `hailgenetics` DockerHub; in Hail Batch. **Version 0.2.121**. - (`#13396 <https://github.com/hail-is/hail/pull/13396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hail/pull/13007>`__) Memory and storage request strings may now be optionally terminated with a `B` for bytes.; - (`#13051 <https://github.com/hail-is/hail/pull/13051>`__) Azure Blob Storage `https` URLs are now supported. **Version 0.2.115**. - (`#12731 <https://github.com/hail-is/hail/pull/12731>`__) Introduced `hailtop.fs` that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by `import hailtop.fs as hfs`.; - (`#12918 <https://github.com/hail-is/hail/pull/12918>`__) Fixed a combinatorial explosion in cancellation ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:3606,Deployability,release,release,3606,"3396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hail/pull/13007>`__) Memory and storage request strings may now be optionally terminated with a `B` for bytes.; - (`#13051 <https://github.com/hail-is/hail/pull/13051>`__) Azure Blob Storage `https` URLs are now supported. **Version 0.2.115**. - (`#12731 <https://github.com/hail-is/hail/pull/12731>`__) Introduced `hailtop.fs` that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by `import hailtop.fs as hfs`.; - (`#12918 <https://github.com/hail-is/hail/pull/12918>`__) Fixed a combinatorial explosion in cancellation calculation in the :class:`.LocalBackend`; - (`#12917 <https://github.com/hail-is/hail/pull/12917>`__) ABS blob URIs in the form of `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>` are now supported when running in Azure. The `hail-az` scheme for referencing ABS blobs is now deprecated and will be removed in a future release. **Version 0.2.114**. - (`#12780 <https://github.com/hail-is/hail/pull/12881>`__) PythonJobs now handle arguments with resources nested inside dicts and lists.; - (`#12900 <https://github.com/hail-is/hail/pull/12900>`__) Reading data from public blobs is now supported in Azure. **Version 0.2.113**. - (`#12780 <https://github.com/hail-is/hail/pull/12780>`__) The LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; - (`#12845 <https://github.com/hail-is/hail/pull/12845>`__) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs b",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:4514,Deployability,update,update,4514,"az` scheme for referencing ABS blobs is now deprecated and will be removed in a future release. **Version 0.2.114**. - (`#12780 <https://github.com/hail-is/hail/pull/12881>`__) PythonJobs now handle arguments with resources nested inside dicts and lists.; - (`#12900 <https://github.com/hail-is/hail/pull/12900>`__) Reading data from public blobs is now supported in Azure. **Version 0.2.113**. - (`#12780 <https://github.com/hail-is/hail/pull/12780>`__) The LocalBackend now supports `always_run` jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; - (`#12845 <https://github.com/hail-is/hail/pull/12845>`__) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:7709,Energy Efficiency,allocate,allocated,7709,"when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; - Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; - Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job's CPU and memory requests. **Version 0.2.58**. - Added concatenate and plink_merge functions that use tree aggregation when merging.; - BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. **Version 0.2.56**. - Fix ``LocalBackend.run()`` succeeding when intermediate command fails. **Version 0.2.55**. - Attempts are now sorted by attempt time in the Batch Service UI. **Version 0.2.53**. - Implement and document ``BatchPoolExecutor``. **Version 0.2.50**. - Add ``requester_pays_project`` as a new parameter on batches. **Version 0.2.43**. - Add support for a user-specified, at-most-once HTTP PO",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:8231,Integrability,message,message,8231,"mory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ``PythonJob``; - Added new ``Job.memory`` inputs `lowmem`, `standard`, and `highmem` corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; - ``Job.storage`` is now interpreted as the desired extra storage mounted at `/io` in addition to the default root filesystem `/` when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; - Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; - Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job's CPU and memory requests. **Version 0.2.58**. - Added concatenate and plink_merge functions that use tree aggregation when merging.; - BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. **Version 0.2.56**. - Fix ``LocalBackend.run()`` succeeding when intermediate command fails. **Version 0.2.55**. - Attempts are now sorted by attempt time in the Batch Service UI. **Version 0.2.53**. - Implement and document ``BatchPoolExecutor``. **Version 0.2.50**. - Add ``requester_pays_project`` as a new parameter on batches. **Version 0.2.43**. - Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. **Version 0.2.42**. - Fixed the documentation for job memory and storage requests to have default units in bytes.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:1754,Modifiability,config,configuration,1754,Error`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. **Version 0.2.122**. - (`#13565 <https://github.com/hail-is/hail/pull/13565>`__) Users can now use VEP images from the `hailgenetics` DockerHub; in Hail Batch. **Version 0.2.121**. - (`#13396 <https://github.com/hail-is/hail/pull/13396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hai,MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:5355,Modifiability,config,configurable,5355," ServiceBackend jobs. **Version 0.2.111**. - (`#12530 <https://github.com/hail-is/hail/pull/12530>`__) Added the ability to update an existing batch with additional jobs by calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:5931,Modifiability,variab,variables,5931,"d arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where me",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:1219,Performance,queue,queued,1219," Python; versions. In particular, Hail officially supports:. - All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions. - All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; ==========. **Version 0.2.132**. - (`#14576 <https://github.com/hail-is/hail/pull/14576>`__) Fixed bug where; submitting many Python jobs would fail with `RecursionError`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:6123,Safety,avoid,avoid,6123,"12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_fai",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:6361,Safety,timeout,timeout,6361,"ameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` when using the ``ServiceBackend``. **Version 0.2.68**. - Fixed copying a directory from GCS when using the ``LocalBackend``; - Fixed writing files to GCS when the bucket name starts with a ""g"" or an ""s""; - Fixed the error ""Argument list too long"" when using the ``LocalBackend``; - Fixed an error where memory is set to None when using the ``LocalBackend``. **Version 0.2.66**. - Removed the need for the ``project`` argument in ``Batch()`` unless you are creating a PythonJob; - Set the default for ``Job.memory`` to be 'standard'; - Added the `cancel_after_n_failures` option to ``Batch()``; - Fixed executing a job with ``Job.memory`` set to 'lowmem', 'standard', and 'highmem' when using the; ``LocalBackend``; - Fixed executing a ``PythonJob`` when using the ``LocalBackend``. **Version 0.2.65**. - Added ",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:5551,Security,authoriz,authorization,5551,"calling :meth:`.Batch.run` more than once. The method :meth:`.Batch.from_batch_id`; can be used to construct a :class:`.Batch` from a previously submitted batch. **Version 0.2.110**. - (`#12734 <https://github.com/hail-is/hail/pull/12734>`__) :meth:`.PythonJob.call` now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; - (`#12726 <https://github.com/hail-is/hail/pull/12726>`__) :class:`.PythonJob` now supports intermediate file resources the same as :class:`.BashJob`.; - (`#12684 <https://github.com/hail-is/hail/pull/12684>`__) :class:`.PythonJob` now correctly uses the default region when a specific region for the job is not given. **Version 0.2.103**. - Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. **Version 0.2.89**. - Support passing an authorization token to the ``ServiceBackend``. **Version 0.2.79**. - The `bucket` parameter in the ``ServiceBackend`` has been deprecated. Use `remote_tmpdir` instead. **Version 0.2.75**. - Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; - Made resource files be represented as an explicit path in the command rather than using environment; variables; - Fixed ``Backend.close`` to be idempotent; - Fixed ``BatchPoolExecutor`` to always cancel all batches on errors. **Version 0.2.74**. - Large job commands are now written to GCS to avoid Linux argument length and number limitations. **Version 0.2.72**. - Made failed Python Jobs have non-zero exit codes. **Version 0.2.71**. - Added the ability to set values for ``Job.cpu``, ``Job.memory``, ``Job.storage``, and ``Job.timeout`` to `None`. **Version 0.2.70**. - Made submitting ``PythonJob`` faster when using the ``ServiceBackend``. **Version 0.2.69**. - Added the option to specify either `remote_tmpdir` or `bucket` wh",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:15,Testability,log,log,15,".. _sec-change-log:. Python Version Compatibility Policy; ===================================. Hail complies with `NumPy's compatibility policy <https://numpy.org/neps/nep-0029-deprecation_policy.html#implementation>`__ on Python; versions. In particular, Hail officially supports:. - All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions. - All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; ==========. **Version 0.2.132**. - (`#14576 <https://github.com/hail-is/hail/pull/14576>`__) Fixed bug where; submitting many Python jobs would fail with `RecursionError`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614",MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst:1708,Testability,log,login,1708,Error`. **Version 0.2.131**. - (`#14544 <https://github.com/hail-is/hail/pull/14544>`__) `batch.read_input`; and `batch.read_input_group` now accept `os.PathLike` objects as well as strings.; - (`#14328 <https://github.com/hail-is/hail/pull/14328>`__) Job resource usage; data can now be retrieved from the Batch API. **Version 0.2.130**. - (`#14425 <https://github.com/hail-is/hail/pull/14425>`__) A job's 'always run'; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; - (`#14437 <https://github.com/hail-is/hail/pull/14437>`__) The billing page now; reports users' spend on the batch service. **Version 0.2.128**. - (`#14224 <https://github.com/hail-is/hail/pull/14224>`__) `hb.Batch` now accepts a; `default_regions` argument which is the default for all jobs in the Batch. **Version 0.2.124**. - (`#13681 <https://github.com/hail-is/hail/pull/13681>`__) Fix `hailctl batch init` and `hailctl auth login` for; new users who have never set up a configuration before. **Version 0.2.123**. - (`#13643 <https://github.com/hail-is/hail/pull/13643>`__) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; - (`#13614 <https://github.com/hail-is/hail/pull/13614>`__) Fixed a bug that broke the `LocalBackend` when run inside a; Jupyter notebook.; - (`#13200 <https://github.com/hail-is/hail/pull/13200>`__) `hailtop.batch` will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. **Version 0.2.122**. - (`#13565 <https://github.com/hail-is/hail/pull/13565>`__) Users can now use VEP images from the `hailgenetics` DockerHub; in Hail Batch. **Version 0.2.121**. - (`#13396 <https://github.com/hail-is/hail/pull/13396>`__) Non-spot instances can be requested via the :meth:`.Job.spot` method. **Version 0.2.117**. - (`#13007 <https://github.com/hail-is/hai,MatchSource.DOCS,hail/python/hailtop/batch/docs/change_log.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/change_log.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/configuration_reference.rst:8,Deployability,configurat,configuration-reference,8,.. _sec-configuration-reference:. Configuration Reference; =======================. See `the query documentation <https://hail.is/docs/0.2/configuration_reference.html>`__.; ,MatchSource.DOCS,hail/python/hailtop/batch/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/configuration_reference.rst:8,Modifiability,config,configuration-reference,8,.. _sec-configuration-reference:. Configuration Reference; =======================. See `the query documentation <https://hail.is/docs/0.2/configuration_reference.html>`__.; ,MatchSource.DOCS,hail/python/hailtop/batch/docs/configuration_reference.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/configuration_reference.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1679,Availability,down,download,1679,"tall/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/reference/builder/. Building Images; ---------------. To create a Docker image, use. .. code-block:: sh. docker b",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1765,Availability,down,download,1765,"r-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/reference/builder/. Building Images; ---------------. To create a Docker image, use. .. code-block:: sh. docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfil",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:605,Deployability,install,install,605,".. _sec-docker-resources:. ================; Docker Resources; ================. What is Docker?; ---------------; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we're only going to focus on building images. Installation; ------------. You can install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 ",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:707,Deployability,install,install,707,".. _sec-docker-resources:. ================; Docker Resources; ================. What is Docker?; ---------------; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we're only going to focus on building images. Installation; ------------. You can install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 ",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:760,Deployability,install,install,760,".. _sec-docker-resources:. ================; Docker Resources; ================. What is Docker?; ---------------; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we're only going to focus on building images. Installation; ------------. You can install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 ",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1185,Deployability,install,installed,1185,"ing systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we're only going to focus on building images. Installation; ------------. You can install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-as",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1367,Deployability,install,installed,1367,"mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we're only going to focus on building images. Installation; ------------. You can install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recurs",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1576,Deployability,install,installing,1576,"an install Docker by following the instructions for either `Macs <https://docs.docker.com/docker-for-mac/install/>`__; or for `Linux <https://docs.docker.com/install/linux/docker-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/refer",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1778,Deployability,install,install,1778,"r-ce/ubuntu/>`__. Creating a Dockerfile; ---------------------. A Dockerfile contains the instructions for creating an image and is typically called `Dockerfile`.; The first directive at the top of each Dockerfile is `FROM` which states what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/reference/builder/. Building Images; ---------------. To create a Docker image, use. .. code-block:: sh. docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfil",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1961,Deployability,update,update,1961,"tes what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/reference/builder/. Building Images; ---------------. To create a Docker image, use. .. code-block:: sh. docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <http",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst:1979,Deployability,install,install,1979,"tes what image to create this; image on top of. For example, we can build off of `ubuntu:22.04` which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; one that has `gcloud` installed is `google/cloud-sdk:slim`. Be careful when choosing images from; unknown sources!. In the example below, we create a Dockerfile that is based on `ubuntu:22.04`. In this file, we show an; example of installing PLINK in the image with the `RUN` directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using `apt-get`. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the `COPY` directive. .. code-block:: text. FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; - https://docs.docker.com/engine/reference/builder/. Building Images; ---------------. To create a Docker image, use. .. code-block:: sh. docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <http",MatchSource.DOCS,hail/python/hailtop/batch/docs/docker_resources.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/docker_resources.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:130,Availability,avail,available,130,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:1262,Availability,echo,echo,1262,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:487,Deployability,install,install,487,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:662,Deployability,install,install,662,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:681,Deployability,install,install-macos-silent,681,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:787,Deployability,install,install,787,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:970,Deployability,install,install,970,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:1034,Deployability,install,install,1034,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:231,Integrability,depend,depends,231,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:424,Usability,guid,guide,424,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst:656,Usability,guid,guide,656,".. _sec-getting_started:. ===============; Getting Started; ===============. Installation; ------------. Batch is a Python module available inside the Hail Python package located at `hailtop.batch`. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Create a `conda enviroment; <https://conda.io/docs/user-guide/concepts.html#conda-environments>`__ named; ``hail`` and install the Hail python library in that environment. If ``conda activate`` doesn't work, `please read these instructions <https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent>`_. .. code-block:: sh. conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; `install the Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`__. Try it out!; ~~~~~~~~~~~. To try `batch` out, open iPython or a Jupyter notebook and run:. .. code-block:: python. >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You're now all set to run the :ref:`tutorial <sec-tutorial>`!; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/getting_started.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/getting_started.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/index.rst:245,Deployability,pipeline,pipelines,245,Batch; =====. Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the :ref:`Batch Service <sec-service>`. .. image:: _static/images/dags/dags.008.png. Contents; ========. .. toctree::; :maxdepth: 2. Getting Started <getting_started>; Tutorial <tutorial>; Docker Resources <docker_resources>; Batch Service <service>; Cookbooks <cookbook>; Reference (Python API) <api>; Configuration Reference <configuration_reference>; Advanced UI Search Help <advanced_search_help>; Change Log And Version Policy <change_log>. Indices and tables; ==================. * :ref:`genindex`; * :ref:`search`; ,MatchSource.DOCS,hail/python/hailtop/batch/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/index.rst:284,Integrability,depend,dependencies,284,Batch; =====. Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the :ref:`Batch Service <sec-service>`. .. image:: _static/images/dags/dags.008.png. Contents; ========. .. toctree::; :maxdepth: 2. Getting Started <getting_started>; Tutorial <tutorial>; Docker Resources <docker_resources>; Batch Service <service>; Cookbooks <cookbook>; Reference (Python API) <api>; Configuration Reference <configuration_reference>; Advanced UI Search Help <advanced_search_help>; Change Log And Version Policy <change_log>. Indices and tables; ==================. * :ref:`genindex`; * :ref:`search`; ,MatchSource.DOCS,hail/python/hailtop/batch/docs/index.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/index.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:113,Availability,avail,available,113,".. _sec-service:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. Fi",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:949,Availability,avail,available,949,".. _sec-service:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. Fi",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2134,Availability,down,downloads,2134,"will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVI",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2313,Availability,down,downloaded,2313,"ct is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-co",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2657,Availability,down,downstream,2657,"m <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2878,Availability,down,download,2878,"roject modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact re",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:3808,Availability,avail,available,3808,"------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repository you want to grant access to and has a path that; has the following prefix `us-docker.pkg.dev/<MY_PROJECT>`:. .. code-block:: sh. gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; -------. The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. - Compute cost. .. caution::. The price",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:9042,Availability,error,error,9042,"he time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:10096,Availability,echo,echo,10096,"; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13375,Availability,error,error,13375,"any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - A",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13721,Availability,error,error,13721," registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:14269,Availability,failure,failure,14269,"t code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:14509,Availability,failure,failure,14509,"a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:14548,Availability,failure,failure,14548,"a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:328,Deployability,install,installed,328,".. _sec-service:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. Fi",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:7374,Deployability,configurat,configuration,7374,"ers, and **$0.02429905** per core/hour for highcpu spot workers. There is also an additional; cost of **$0.00023** per GB per hour of extra storage requested. At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is **$0.02774** plus; **$0.00023** per GB per hour storage of extra storage requested. For jobs that run on non-preemptible machines, the costs are **$0.06449725** per core/hour for standard workers, **$0.076149** per core/hour; for highmem workers, and **$0.0524218** per core/hour for highcpu workers. .. note::. If the memory is specified as either 'lowmem', 'standard', or 'highmem', then the corresponding worker types; used are 'highcpu', 'standard', and 'highmem'. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batc",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8404,Deployability,install,installed,8404,"per worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write acce",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11896,Deployability,configurat,configuration,11896,"r writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:756,Energy Efficiency,schedul,scheduler,756,".. _sec-service:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. Fi",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:6710,Energy Efficiency,schedul,scheduled,6710,"30.4375. Cost per GB per month = $0.048. Cost per core per hour = $0.048 * 375 / 30.4375 / 24 / 16. = $0.001685 per core per hour. - Storage. .. code-block:: text. Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.17. Cost per GB per hour = $0.17 / 30.4375 / 24. - IP network cost; = $0.0003125 per core per hour for **nonpreemptible** worker types. = $0.00015625 per core per hour for **spot** worker types. - Service cost; = $0.01 per core per hour. - Logs, Specs, and Firewall Fee; = $0.005 per core per hour. The sum of these costs is **$0.02684125** per core/hour for standard spot workers, **$0.02929425** per core/hour; for highmem spot workers, and **$0.02429905** per core/hour for highcpu spot workers. There is also an additional; cost of **$0.00023** per GB per hour of extra storage requested. At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is **$0.02774** plus; **$0.00023** per GB per hour storage of extra storage requested. For jobs that run on non-preemptible machines, the costs are **$0.06449725** per core/hour for standard workers, **$0.076149** per core/hour; for highmem workers, and **$0.0524218** per core/hour for highcpu workers. .. note::. If the memory is specified as either 'lowmem', 'standard', or 'highmem', then the corresponding worker types; used are 'highcpu', 'standard', and 'highmem'. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request m",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:7433,Energy Efficiency,power,power,7433,"ers, and **$0.02429905** per core/hour for highcpu spot workers. There is also an additional; cost of **$0.00023** per GB per hour of extra storage requested. At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is **$0.02774** plus; **$0.00023** per GB per hour storage of extra storage requested. For jobs that run on non-preemptible machines, the costs are **$0.06449725** per core/hour for standard workers, **$0.076149** per core/hour; for highmem workers, and **$0.0524218** per core/hour for highcpu workers. .. note::. If the memory is specified as either 'lowmem', 'standard', or 'highmem', then the corresponding worker types; used are 'highcpu', 'standard', and 'highmem'. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batc",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8306,Energy Efficiency,schedul,scheduled,8306,"e cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execut",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:10950,Energy Efficiency,charge,charges,10950,"ject', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` con",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11038,Energy Efficiency,charge,charges,11038,">>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:12367,Energy Efficiency,charge,charges,12367,"kend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:12661,Energy Efficiency,charge,charges,12661,"egions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13159,Energy Efficiency,schedul,scheduled,13159,"any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - A",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13204,Energy Efficiency,schedul,scheduled,13204,"any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - A",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:15264,Energy Efficiency,charge,charges,15264,"a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2294,Integrability,depend,dependent,2294," account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:2640,Integrability,depend,dependencies,2640,"m <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:4445,Integrability,depend,depends,4445,"CKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repository you want to grant access to and has a path that; has the following prefix `us-docker.pkg.dev/<MY_PROJECT>`:. .. code-block:: sh. gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; -------. The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. - Compute cost. .. caution::. The prices shown below are **approximate** prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use :meth:`.Job.regions` to specify which regions to run a job in. = $0.01 per core per hour for **spot standard** worker types. = $0.012453 per core per hour for **spot highmem** worker types. = $0.0074578 per core per hour for **spot highcpu** worker types. = $0.04749975 per core per hour for **nonpreemptible standard** worker types. = $0.0591515 per core per hour for **nonpreemptible highmem** worker types. = $0.0354243 per core per hour for **nonpreemptible highcpu** worker types. - Disk cost; - Boot Disk. .. code-bloc",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8971,Integrability,message,message,8971," is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:9048,Integrability,message,messages,9048,"he time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13054,Integrability,depend,dependencies,13054,"any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - A",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13105,Integrability,depend,dependencies,13105,"any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - A",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13727,Integrability,message,messages,13727," registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:14581,Integrability,depend,depend,14581,"a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:7374,Modifiability,config,configuration,7374,"ers, and **$0.02429905** per core/hour for highcpu spot workers. There is also an additional; cost of **$0.00023** per GB per hour of extra storage requested. At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is **$0.02774** plus; **$0.00023** per GB per hour storage of extra storage requested. For jobs that run on non-preemptible machines, the costs are **$0.06449725** per core/hour for standard workers, **$0.076149** per core/hour; for highmem workers, and **$0.0524218** per core/hour for highcpu workers. .. note::. If the memory is specified as either 'lowmem', 'standard', or 'highmem', then the corresponding worker types; used are 'highcpu', 'standard', and 'highmem'. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. .. note::. The storage for the root file system (`/`) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batc",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:10325,Modifiability,config,config,10325," billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.n",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:10386,Modifiability,config,config,10386," billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.n",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11774,Modifiability,variab,variable,11774,"n of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11896,Modifiability,config,configuration,11896,"r writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11910,Modifiability,variab,variable,11910,"r writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11949,Modifiability,config,config,11949,"ogle.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your job may run in *any* region!. In Google Cloud Platform, the location of a multi-region bucket is considered *different* from any; region within that multi-region. For example, if a VM in the `us-central1` region reads data from a; bucket in the `us` multi-region, this incurs network charges becuse `us` is not considered equal to; `us-central1`. Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. T",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:9168,Safety,avoid,avoid,9168,", but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:11024,Safety,avoid,avoid,11024,">>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-pricing>`__. To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in `us-central1`. The options are; listed from highest to lowest precedence. 1. :meth:`.Job.regions`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). 2. The ``default_regions`` parameter of :class:`.Batch`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). 3. The ``regions`` parameter of :class:`.ServiceBackend`:. .. code-block:: python. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). 4. The ``HAIL_BATCH_REGIONS`` environment variable:. .. code-block:: sh. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. 5. The ``batch/region`` configuration variable:. .. code-block:: sh. hailctl config set batch/regions us-central1; python3 my-batch-script.py. .. warning::. If none of the five options above are specified, your",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:15241,Safety,avoid,avoid,15241,"a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the ""Cancel""; button next to the row for that batch. You can also delete a batch with the ""Delete"" button. .. warning::. Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the :ref:`Advanced Search Help <sec-advanced_search_help>` page. Important Notes; ---------------. .. warning::. To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region.; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:1034,Security,access,access,1034,"e:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localizatio",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:1170,Security,authenticat,authenticate,1170,"tute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localization; -----------------. A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These in",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:3072,Security,access,access,3072,"ainer; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repos",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:3358,Security,access,access-control,3358," a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repository you want to grant access to and has a path that; has the following prefix `us-docker.pkg.dev/<MY_PROJECT>`:. .. code-block:: sh. gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artif",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:3405,Security,access,access,3405," a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repository you want to grant access to and has a path that; has the following prefix `us-docker.pkg.dev/<MY_PROJECT>`:. .. code-block:: sh. gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artif",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:4151,Security,access,access,4151," and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repository you want to grant access to and has a path that; has the following prefix `us-docker.pkg.dev/<MY_PROJECT>`:. .. code-block:: sh. gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; -------. The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. - Compute cost. .. caution::. The prices shown below are **approximate** prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use :meth:`.Job.regions` to specify which regions to run a job in. = $0.01 per core per hour fo",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8599,Security,authenticat,authenticate,8599,"t receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""H",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8900,Security,authenticat,authenticate,8900,"ncremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as h",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8995,Security,authenticat,authenticated,8995," is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:9483,Security,access,access,9483,":`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:1041,Testability,log,logs,1041,"e:. =============; Batch Service; =============. .. warning::. The Batch Service is currently only available to Broad Institute affiliates. Please `contact us; <mailto:hail-team@broadinstitute.org>`__ if you are interested in hosting a copy of the Batch; Service at your institution. .. warning::. Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; :ref:`Getting Started <sec-getting_started>`. What is the Batch Service?; --------------------------. Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at `<https://batch.hail.is>`__; that allows a user to see job progress and access logs. Sign Up; -------. For Broad Institute users, you can sign up at `<https://auth.hail.is/signup>`__.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A :ref:`Google Service Account <service-accounts>` is created; on your behalf. A trial Batch billing project is also created for you at; :code:`<USERNAME>-trial`. You can view these at `<https://auth.hail.is/user>`__. To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this `billing project creation form <https://docs.google.com/forms/u/0/d/e/1FAIpQLSc1DoqSZKtt1VjVhJjNzzFL8Wfoi5QAFLHuSPwGLnamdtDzHg/viewform>`__.; To modify an existing Hail Batch billing project, send an inquiry using this; `billing project modification form <https://docs.google.com/forms/d/e/1FAIpQLSdOdrYE2ZlT6GmMI8ShSoR8uKyePkZ8UJ2Hel7dWaHYAC-TBA/viewform>`__. .. _file-localization:. File Localizatio",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:3210,Testability,test,test,3210,"ainer; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user's code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; :meth:`.Batch.write_output` or are file dependencies for downstream jobs. .. image:: _static/images/file_localization.png. .. _service-accounts:. Service Accounts; ----------------. A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; `<https://auth.hail.is/user>`__. To give the service account read and write access to a Google Storage bucket, run the following command substituting; `SERVICE_ACCOUNT_NAME` with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and `BUCKET_NAME`; with your bucket name. See this `page <https://cloud.google.com/container-registry/docs/access-control>`__; for more information about access control. .. code-block:: sh. gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn't be publically available. If you have an artifact registry `associated with your project; <https://cloud.google.com/artifact-registry/docs/>`__, then you can enable the service account to; view Docker images with the command below where `SERVICE_ACCOUNT_NAME` is your full service account; name, and `<REPO>` is the name of your repos",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8752,Testability,log,login,8752,"xplicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyte",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8772,Testability,log,login,8772,"xplicitly with the :meth:`.Job.storage` method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyte",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:8821,Testability,log,login,8821,"ncremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at `/io`. .. note::. If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; -----. We assume you've already installed Batch and the Google Cloud SDK as described in the :ref:`Getting; Started <sec-getting_started>` section and we have created a user account for you and given you a; billing project. To authenticate your computer with the Batch service, run the following; command in a terminal window:. .. code-block:: sh. gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message ""hailctl is now authenticated."" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as h",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:10006,Testability,test,test,10006,""" in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service; ---------------------------------. .. warning::. To avoid substantial network costs, ensure your jobs and data reside in the same `region`_. To execute a batch on the Batch service rather than locally, first; construct a :class:`.ServiceBackend` object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket. Next, pass the :class:`.ServiceBackend` object to the :class:`.Batch` constructor; with the parameter name `backend`. An example of running ""Hello World"" on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:. .. code-block:: python. >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') # doctest: +SKIP; >>> b = hb.Batch(backend=backend, name='test') # doctest: +SKIP; >>> j = b.new_job(name='hello') # doctest: +SKIP; >>> j.command('echo ""hello world""') # doctest: +SKIP; >>> b.run(open=True) # doctest: +SKIP. You may elide the ``billing_project`` and ``remote_tmpdir`` parameters if you; have previously set them with ``hailctl``:. .. code-block:: sh. hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. .. note::. A trial billing project is automatically created for you with the name {USERNAME}-trial. .. _region:. Regions; -------. Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. ``gcloud`` can determine the location of a; bucket::. gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial `network charges <https://cloud.google.com/storage/pricing#network-p",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst:13462,Testability,log,logs,13462," registry, which at time of writing, despite being; ""multi-regional"", does not incur network charges in the manner described above. Using the UI; ------------. If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. - Pending - A job is waiting for its dependencies to complete; - Ready - All of a job's dependencies have completed, but the job has not been scheduled to run; - Running - A job has been scheduled to run on a worker; - Success - A job finished with exit code 0; - Failure - A job finished with exit code not equal to 0; - Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (:ref:`see above <file-localization>`) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages. To see all batches you've submitted, go to `<https://batch.hail.is>`__. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. - open - Not all jobs in the batch have been successfully submitted.; - running - All jobs in the batch have been successfully submitted.; - success - All jobs in the batch have completed with state ""Success""; - failure - Any job has completed with state ""Failure"" or ""Error""; - cancelled - Any job has been cancelled and no jobs have completed with state ""Failure"" or ""Error"". .. note::; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; 'failure', other jobs ",MatchSource.DOCS,hail/python/hailtop/batch/docs/service.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/service.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:2279,Availability,echo,echo,2279,"g the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:2746,Availability,echo,echo,2746,"rary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both t",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3509,Availability,echo,echo,3509,"un`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have spe",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3578,Availability,echo,echo,3578,"` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previ",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3954,Availability,echo,echo,3954," Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get t",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4023,Availability,echo,echo,4023,"uted by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. How",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4356,Availability,down,downstream,4356,"`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python in",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:5946,Availability,echo,echo,5946,"le`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:6582,Availability,echo,echo,6582,"print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: pytho",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:6753,Availability,echo,echo,6753,"ocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I w",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7657,Availability,echo,echo,7657," >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7754,Availability,echo,echo,7754,"me}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in [",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:8266,Availability,echo,echo,8266,"e single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; ---------------. We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'A",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:8840,Availability,echo,echo,8840,"o create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; ---------------. We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as a",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:9752,Availability,echo,echo,9752,"ice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; ---------------. We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The `do_chores` function takes a :class:`.Batch` object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for ",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:10440,Availability,echo,echo,10440,"ies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The `do_chores` function takes a :class:`.Batch` object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. .. image:: _static/images/dags/dags.008.png. .. code-block:: python. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink);",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:11060,Availability,echo,echo,11060,"wever, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. .. image:: _static/images/dags/dags.008.png. .. code-block:: python. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink); >>> final_sink = b.new_job(name='final-sink'); >>> final_sink.depends_on(*user_sinks); >>> b.run(). .. _input-files:. Input Files; -----------. Previously, we discussed that :class:`.JobResourceFile` are temporary files and; are created from :class:`.Job` objects. However, in order to read a file that; was not generated by executing jobs (input file), we use the method; :meth:`.Batch.read_input` to create an :class:`.InputResourceFile`. An input; resource file can be used exactly in the same way as a; :class:`.JobResourceFile`. We can refer to an input resource file in a command; using an f-string. In the example",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:13369,Availability,down,downstream,13369,"ob(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(). Why do we need to explicitly add input files to batches rather than referring; directly to the path in the command? You could refer directly to the path when using the; :class:`.LocalBackend`, but only if you are not specifying a docker image to use when running; the command with :meth:`.BashJob.image`. This is because Batch copies any input files to a special; temporary directory which gets mounted to the Docker container. When using the :class:`.ServiceBackend`,; input files would be files in Google Storage. Many commands do not know how to handle file; paths in Google Storage. Therefore, we suggest explicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; ------------. All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs' use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call :meth:`.Batch.write_output`.; The first argument to :meth:`.Batch.write_output` can be any type of :class:`.ResourceFile` which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the :class:`.LocalBackend`.; For the :class:`.ServiceBackend`, the second argument must be a google storage file path. .. code-block:: python. >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; ---------------. Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, `PLINK <http",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:14102,Availability,echo,echo,14102,"e sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; ------------. All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs' use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call :meth:`.Batch.write_output`.; The first argument to :meth:`.Batch.write_output` can be any type of :class:`.ResourceFile` which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the :class:`.LocalBackend`.; For the :class:`.ServiceBackend`, the second argument must be a google storage file path. .. code-block:: python. >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; ---------------. Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, `PLINK <https://www.cog-genomics.org/plink/>`_; stores genetic data in three files: `*.bed` has the genotype data,; `*.bim` has the variant information, and `*.fam` has the sample information.; PLINK can take as an input the path to the files expecting there will be three; files with the appropriate extensions. It also writes files with a common file root and; specific file extensions including when writing out a new dataset or outputting summary statistics. To enable Batch to work with file groups, we added a :class:`.ResourceGroup` object; that is essentially a dictionary from file extension name to file path. When creating; a :class:`.ResourceGroup` in a :class:`.Job` (equivalent to a :class:`.JobResourceFile`),; you first need to use the method :met",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:194,Deployability,install,installed,194,".. _sec-tutorial:. ========; Tutorial; ========. This tutorial goes through the basic concepts of Batch with examples. Import; ------. Batch is located inside the `hailtop` module, which can be installed; as described in the :ref:`Getting Started <sec-getting_started>` section. .. code-block:: python. >>> import hailtop.batch as hb. .. _f-strings:. f-strings; ---------. f-strings were added to Python in version 3.6 and are denoted by the 'f' character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces `{...}` using the current variable scope. When Python compiles; the example below, the string 'Alice' is substituted for `{name}` because the variable; `name` is set to 'Alice' in the line above. .. code-block:: python. >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4282,Deployability,pipeline,pipelines,4282,"`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python in",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:10258,Deployability,pipeline,pipelines,10258,"e example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The `do_chores` function takes a :class:`.Batch` object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. .. image:: _static/images/dags/dags.008.png. .. code-block:: python. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:1824,Integrability,depend,dependencies,1824," >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now ",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:1841,Integrability,depend,dependency,1841,"he curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jo",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:1885,Integrability,depend,dependent,1885,"he curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jo",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3372,Integrability,depend,dependencies,3372," Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arguments to both :class:`.Batch` and; :class:`.Job` are used in the :ref:`Batch Service UI <sec-service>`. .. code-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generat",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3627,Integrability,depend,dependency,3627,"de-block:: python. >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this t",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:3731,Integrability,depend,depends,3731,"(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call :meth:`.Batch.new_job`; twice to create two jobs `s` and `t` which both will print a variant of hello world to stdout.; Calling `b.run()` executes the batch. By default, batches are executed by the :class:`.LocalBackend`; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4090,Integrability,depend,dependencies,4090,"local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or pr",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4198,Integrability,depend,dependencies,4198,"lly. However, if batches are executed by the :class:`.ServiceBackend`; using the :ref:`Batch Service <sec-service>`, then `s` and `t` can be run in parallel as; there exist no dependencies between them. .. code-block:: python. >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4444,Integrability,depend,dependencies,4444,"= hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between `s` and `t`, we use the method; :meth:`.Job.depends_on` to explicitly state that `t` depends on `s`. In both the; :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path ",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:5669,Integrability,depend,depend,5669," temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:5712,Integrability,depend,dependency,5712," temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:6183,Integrability,depend,dependency,6183,"Job.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method t",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7006,Integrability,depend,depends,7006,"e}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7193,Integrability,depend,dependent,7193,"aphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strin",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7874,Integrability,depend,depends,7874,"f we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:7990,Integrability,depend,dependencies,7990,"f we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:8053,Integrability,depend,dependencies,8053,"f we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:8072,Integrability,depend,dependencies,8072,"f we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:9448,Integrability,depend,dependencies,9448,":class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; ---------------. We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The `do_chores` function takes a :class:`.Batch` object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:10189,Integrability,depend,dependency,10189,"e example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. .. image:: _static/images/dags/dags.007.png. .. code-block:: python. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The `do_chores` function takes a :class:`.Batch` object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines. .. code-block:: python. >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. .. image:: _static/images/dags/dags.008.png. .. code-block:: python. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:591,Modifiability,variab,variable,591,".. _sec-tutorial:. ========; Tutorial; ========. This tutorial goes through the basic concepts of Batch with examples. Import; ------. Batch is located inside the `hailtop` module, which can be installed; as described in the :ref:`Getting Started <sec-getting_started>` section. .. code-block:: python. >>> import hailtop.batch as hb. .. _f-strings:. f-strings; ---------. f-strings were added to Python in version 3.6 and are denoted by the 'f' character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces `{...}` using the current variable scope. When Python compiles; the example below, the string 'Alice' is substituted for `{name}` because the variable; `name` is set to 'Alice' in the line above. .. code-block:: python. >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:707,Modifiability,variab,variable,707,".. _sec-tutorial:. ========; Tutorial; ========. This tutorial goes through the basic concepts of Batch with examples. Import; ------. Batch is located inside the `hailtop` module, which can be installed; as described in the :ref:`Getting Started <sec-getting_started>` section. .. code-block:: python. >>> import hailtop.batch as hb. .. _f-strings:. f-strings; ---------. f-strings were added to Python in version 3.6 and are denoted by the 'f' character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces `{...}` using the current variable scope. When Python compiles; the example below, the string 'Alice' is substituted for `{name}` because the variable; `name` is set to 'Alice' in the line above. .. code-block:: python. >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:6879,Modifiability,variab,variable,6879," = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; ----------------. Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or ""sink"" that waits for all of the jobs in the scatter to be complete; before executing. In the example below, we use a for loop to create a job for each one of; 'Alice', 'Bob', and 'Dan' that prints the name of the user programatically; thereby scattering the echo command over users. .. code-block:: python. >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it `j` each time in the; for loop. However, if we want to add a final gather job (`sink`) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the :meth:`.Job.depends_on` method to explicitly link; the `sink` job to be dependent on the user jobs, which are stored in the; `jobs` array. The single asterisk before `jobs` is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case :meth:`.Job.depends_on`. .. image:: _static/images/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` jo",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:8479,Modifiability,inherit,inherits,8479,"ges/dags/dags.005.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a `sink` job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the `sink` job (see the section on; :ref:`file dependencies <file-dependencies>`). The changes from the previous; example to make this happen are each job `j` uses an :ref:`f-string <f-strings>`; to create a temporary output file `j.ofile` where the output to echo is redirected.; We then use all of the output files in the `sink` command by creating a string; with the temporary output file names for each job. A :class:`.JobResourceFile`; is a Batch-specific object that inherits from `str`. Therefore, you can use; :class:`.JobResourceFile` as if they were strings, which we do with the `join`; command for strings. .. image:: _static/images/dags/dags.006.png. .. code-block:: python. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; ---------------. We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new :class:`.Batch` object `b`. Then for each user in 'Alice', 'Bob', and 'Dan'; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; be",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:19337,Modifiability,variab,variable,19337,"ave a :meth:`.PythonJob.call` method; that takes a Python function to call and the positional arguments and key-word arguments to provide; to the function. The result of :meth:`.PythonJob.call` is a :class:`.PythonResult` which can be; used as either arguments to another :class:`.PythonJob` or to other :class:`.BashJob` by using one; of the methods to convert a :class:`.PythonResult` to a file: :meth:`.PythonResult.as_str`,; :meth:`.PythonResult.as_repr`, and :meth:`.PythonResult.as_json`. In the example below, we first define two Python functions: `hello_world()` and `upper()`.; Next, we create a batch and then create a new PythonJob with :meth:`.Batch.new_python_job`.; Then we use :meth:`.PythonJob.call` and pass the `hello_world` function that we want to call.; Notice we just passed the reference to the function and not ``hello_world()``. We also add; a Python string `alice` as an argument to the function. The result of the ``j.call()`` is; a :class:`.PythonResult` which we've assigned to the variable `hello_str`. We want to use the `hello_str` result and make all the letters in upper case. We call; :meth:`.PythonJob.call` and pass a reference to the `upper` function.; But now the argument is `hello_str` which holds the result from calling `hello_world`; above. We assign the new output to the variable `result`. At this point, we want to write out the transformed hello world result to a text file.; However, `result` is a :class:`.PythonResult`. Therefore, we need to use the :meth:`.PythonResult.as_str`; to convert `result` to a :class:`.JobResourceFile` with the string output `HELLO WORLD ALICE`. Now; we can write the result to a file. .. code-block:: python. def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; --------. There are two",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:19643,Modifiability,variab,variable,19643,"s:`.BashJob` by using one; of the methods to convert a :class:`.PythonResult` to a file: :meth:`.PythonResult.as_str`,; :meth:`.PythonResult.as_repr`, and :meth:`.PythonResult.as_json`. In the example below, we first define two Python functions: `hello_world()` and `upper()`.; Next, we create a batch and then create a new PythonJob with :meth:`.Batch.new_python_job`.; Then we use :meth:`.PythonJob.call` and pass the `hello_world` function that we want to call.; Notice we just passed the reference to the function and not ``hello_world()``. We also add; a Python string `alice` as an argument to the function. The result of the ``j.call()`` is; a :class:`.PythonResult` which we've assigned to the variable `hello_str`. We want to use the `hello_str` result and make all the letters in upper case. We call; :meth:`.PythonJob.call` and pass a reference to the `upper` function.; But now the argument is `hello_str` which holds the result from calling `hello_world`; above. We assign the new output to the variable `result`. At this point, we want to write out the transformed hello world result to a text file.; However, `result` is a :class:`.PythonResult`. Therefore, we need to use the :meth:`.PythonResult.as_str`; to convert `result` to a :class:`.JobResourceFile` with the string output `HELLO WORLD ALICE`. Now; we can write the result to a file. .. code-block:: python. def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; --------. There are two backends that execute batches: the :class:`.LocalBackend` and the; :class:`.ServiceBackend`. The local backend is used by default and executes jobs; on your local computer. The service backend executes jobs in a shared compute cluster; managed by the Hail team. To use the Batch Service, follow the direction",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4822,Security,access,accessed,4822,"`s` will always run before; `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:4905,Security,access,access,4905," >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). .. _file-dependencies:. File Dependencies; -----------------. So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files. In the example below, we have specified two jobs: `s` and `t`. `s` prints; ""hello world"" as in previous examples. However, instead of printing to stdout,; this time `s` redirects the output to a temporary file defined by `s.ofile`.; `s.ofile` is a Python object of type :class:`.JobResourceFile` that was created; on the fly when we accessed an attribute of a :class:`.Job` that does not already; exist. Any time we access the attribute again (in this example `ofile`), we get the; same :class:`.JobResourceFile` that was previously created. However, be aware that; you cannot use an existing method or property name of :class:`.Job` objects such; as :meth:`.BashJob.command` or :meth:`.BashJob.image`. Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so; when Python interpolates the :ref:`f-string <f-strings>`, it replaced the; :class:`.JobResourceFile` object with an actual file path into the command for `s`.; We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.; `s.ofile` is the same temporary file that was created in the command for `t`. Therefore,; Batch deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.; In both the :class:`.LocalBackend` and :class:`.ServiceBackend`, `s` will always run before `t`. .. code-block:: python. >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.com",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst:1509,Usability,learn,learn,1509,"; in single curly braces `{...}` using the current variable scope. When Python compiles; the example below, the string 'Alice' is substituted for `{name}` because the variable; `name` is set to 'Alice' in the line above. .. code-block:: python. >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate `x + 1` first before compiling; the string. Therefore, we get 'x = 6' as the resulting string. .. code-block:: python. >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, `{` becomes `{{` in the string definition,; but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`. .. code-block:: python. >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_. Hello World; -----------. A :class:`.Batch` consists of a set of :class:`.Job` to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs. In the example below, we have defined a :class:`.Batch` `b` with the name 'hello'.; We use the method :meth:`.Batch.new_job` to create a job object which we call `j` and then; use the method :meth:`.BashJob.command` to tell Batch that we want to execute `echo ""hello world""`.; However, at this point, Batch hasn't actually run the job to print ""hello world"". All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call :meth:`.Batch.run`. The `name` arg",MatchSource.DOCS,hail/python/hailtop/batch/docs/tutorial.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/tutorial.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1110,Availability,avail,available,1110,"--. After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like th",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:4161,Availability,avail,available,4161,": python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the :class:`.Job` that is created in the function, whic",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:5291,Availability,down,downstream,5291,"ivate Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the :class:`.Job` that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs. .. code-block:: python. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. - The `image` is the image created in the previous step. We copied the `run_gwas.py`; script into the root directory `/`. Therefore, to execute the `run_gwas.py` script, we; call `/run_gwas.py`. - The `run_gwas.py` script takes an output-file parameter and then creates files ending with; the extensions `.bed`, `.bim`, `.fam`, and `.assoc`. In order for Batch to know th",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:7502,Availability,avail,available,7502,"claring the resource group). Clumping By Chromosome; ~~~~~~~~~~~~~~~~~~~~~~. The second function performs clumping for a given chromosome. The input arguments are the :class:`.Batch`; for which to create a new :class:`.BashJob`, the PLINK binary file **root**, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new :class:`.BashJob` created. .. code-block:: python. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine's memory. PLINK's memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK. - PLINK creates a hard-coded file `plink.clumped`. We have to move that file to a temporary; Batch file `{c.clumped}` in order to use that file in downstream jobs. Merge Clumping Results; ~~~~~~~~~~~~~~~~~~~~~~. The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the :class:`.Batch` for which to create a new :class:`.BashJob`; and a list containing all of the individual clumping results files. We use the ``ubuntu:22.04``; Docker image for this job. The return value is the new :class:`.BashJob` c",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:7766,Availability,avail,available,7766,"ciation results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new :class:`.BashJob` created. .. code-block:: python. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine's memory. PLINK's memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK. - PLINK creates a hard-coded file `plink.clumped`. We have to move that file to a temporary; Batch file `{c.clumped}` in order to use that file in downstream jobs. Merge Clumping Results; ~~~~~~~~~~~~~~~~~~~~~~. The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the :class:`.Batch` for which to create a new :class:`.BashJob`; and a list containing all of the individual clumping results files. We use the ``ubuntu:22.04``; Docker image for this job. The return value is the new :class:`.BashJob` created. .. code-block:: python. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:8102,Availability,down,downstream,8102,"ew_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine's memory. PLINK's memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK. - PLINK creates a hard-coded file `plink.clumped`. We have to move that file to a temporary; Batch file `{c.clumped}` in order to use that file in downstream jobs. Merge Clumping Results; ~~~~~~~~~~~~~~~~~~~~~~. The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the :class:`.Batch` for which to create a new :class:`.BashJob`; and a list containing all of the individual clumping results files. We use the ``ubuntu:22.04``; Docker image for this job. The return value is the new :class:`.BashJob` created. .. code-block:: python. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. ~~~~~~~~~~~~; Control Code; ~~~~~~~~~~~~. The last thing we want to do is use the functions we wrote above to create new jobs; on a :class:`.Batch` which can",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1840,Deployability,install,installed,1840,"ociation for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P w",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3465,Deployability,install,installed,3465,"; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docke",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3752,Deployability,install,installed,3752,"inary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images i",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1657,Energy Efficiency,consumption,consumption,1657,"d; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:10778,Energy Efficiency,consumption,consumption,10778,"Backend`; and give it the name 'clumping'. .. code-block:: python. backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). We create :class:`.InputResourceFile` objects for the VCF file and; phenotypes file using the :meth:`.Batch.read_input` method. These; are the inputs to the entire Batch and are not outputs of a :class:`.BashJob`. .. code-block:: python. vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the ``gwas`` function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:. .. code-block:: python. g = gwas(batch, vcf, phenotypes). We call the ``clump`` function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the ``g`` job defined above; as inputs to the ``clump`` function:. .. code-block:: python. results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the ``merge`` function to concatenate the results into a single file; and then write this output to a permanent location using :meth:`.Batch.write_output`.; The inputs to the ``merge`` function are the clumped output files from each of the ``clump``; jobs. .. code-block:: python. m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:. .. code-block:: python. batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; --------. We provide the code used above in one place for your reference:. .. literalinclude:: files/run_gwas.py; :language: python; :caption: run_gwas.py. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile. .. literalinclude:: files/batch_clumping.py; :language: python; :caption: batch_clumping.py; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3349,Integrability,depend,dependencies,3349,"destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even i",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3506,Integrability,depend,depends,3506,"; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docke",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3559,Integrability,depend,dependencies,3559,"files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. R",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3641,Integrability,depend,dependencies,3641,"files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. R",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:902,Modifiability,flexible,flexible,902,".. _sec-cookbook-clumping:. =====================; Clumping GWAS Results; =====================. Introduction; ------------. After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:3766,Modifiability,extend,extend,3766,"so export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to d",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:131,Performance,perform,performing,131,".. _sec-cookbook-clumping:. =====================; Clumping GWAS Results; =====================. Introduction; ------------. After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:920,Performance,perform,performing,920,".. _sec-cookbook-clumping:. =====================; Clumping GWAS Results; =====================. Introduction; ------------. After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1009,Performance,scalab,scalable,1009,".. _sec-cookbook-clumping:. =====================; Clumping GWAS Results; =====================. Introduction; ------------. After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1351,Performance,scalab,scalable,1351," p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants. For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1399,Performance,perform,perform,1399,"ven a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of `1e-8`, SNP2 has a p-value of `1e-7`, and SNP3 has a; p-value of `1e-6`. The correlation between SNP1 and SNP2 is `0.95`, SNP1 and; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for per",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:1627,Performance,perform,performs,1627,"d; SNP3 is `0.8`, and SNP2 and SNP3 is `0.7`. We would want to report SNP1 is the; most associated variant with the phenotype and ""clump"" SNP2 and SNP3 with the; association for SNP1. `Hail <https://hail.is/index.html>`__ is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as `PLINK <https://www.cog-genomics.org/plink/>`__.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster. To demonstrate how to perform LD-based clumping with Batch, we'll use the; 1000 Genomes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:2443,Performance,perform,performing,2443,"omes dataset from the `Hail GWAS tutorial <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; First, we'll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we'll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we'll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. .. image:: ../_static/images/cookbook_clumping.png. Hail GWAS Script; ----------------. We wrote a stand-alone Python script `run_gwas.py` that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; `here <https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html>`__.; We export two sets of files to the file root defined by ``--output-file``. The first is; a `binary PLINK file <http://zzz.bwh.harvard.edu/plink/binary.shtml>`__ set with three files; ending in `.bed`, `.bim`, and `.fam`. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant. Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail mus",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:6632,Performance,perform,performs,6632,"'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. - The `image` is the image created in the previous step. We copied the `run_gwas.py`; script into the root directory `/`. Therefore, to execute the `run_gwas.py` script, we; call `/run_gwas.py`. - The `run_gwas.py` script takes an output-file parameter and then creates files ending with; the extensions `.bed`, `.bim`, `.fam`, and `.assoc`. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the :meth:`.BashJob.declare_resource_group`; method. We then pass ``g.ofile`` as the output file root to `run_gwas.py` as that represents the temporary file; root given to all files in the resource group (`{root}` when declaring the resource group). Clumping By Chromosome; ~~~~~~~~~~~~~~~~~~~~~~. The second function performs clumping for a given chromosome. The input arguments are the :class:`.Batch`; for which to create a new :class:`.BashJob`, the PLINK binary file **root**, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new :class:`.BashJob` created. .. code-block:: python. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We ex",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:7918,Performance,perform,performance,7918,"g for.; The return value is the new :class:`.BashJob` created. .. code-block:: python. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine's memory. PLINK's memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK. - PLINK creates a hard-coded file `plink.clumped`. We have to move that file to a temporary; Batch file `{c.clumped}` in order to use that file in downstream jobs. Merge Clumping Results; ~~~~~~~~~~~~~~~~~~~~~~. The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the :class:`.Batch` for which to create a new :class:`.BashJob`; and a list containing all of the individual clumping results files. We use the ``ubuntu:22.04``; Docker image for this job. The return value is the new :class:`.BashJob` created. .. code-block:: python. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.of",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:9915,Performance,perform,perform,9915,"; '''); return merger. ~~~~~~~~~~~~; Control Code; ~~~~~~~~~~~~. The last thing we want to do is use the functions we wrote above to create new jobs; on a :class:`.Batch` which can be executed with the :class:`.ServiceBackend`. First, we import the Batch module as ``hb``. .. code-block:: python. import hailtop.batch as hb. Next, we create a :class:`.Batch` specifying the backend is the :class:`.ServiceBackend`; and give it the name 'clumping'. .. code-block:: python. backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). We create :class:`.InputResourceFile` objects for the VCF file and; phenotypes file using the :meth:`.Batch.read_input` method. These; are the inputs to the entire Batch and are not outputs of a :class:`.BashJob`. .. code-block:: python. vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the ``gwas`` function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:. .. code-block:: python. g = gwas(batch, vcf, phenotypes). We call the ``clump`` function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the ``g`` job defined above; as inputs to the ``clump`` function:. .. code-block:: python. results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the ``merge`` function to concatenate the results into a single file; and then write this output to a permanent location using :meth:`.Batch.write_output`.; The inputs to the ``merge`` function are the clumped output files from each of the ``clump``; jobs. .. code-block:: python. m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:. .. code-block:: python. b",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:7776,Safety,detect,detection,7776,"ciation results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new :class:`.BashJob` created. .. code-block:: python. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. - We use the image ``hailgenetics/genetics`` which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK. - We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine's memory. PLINK's memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK. - PLINK creates a hard-coded file `plink.clumped`. We have to move that file to a temporary; Batch file `{c.clumped}` in order to use that file in downstream jobs. Merge Clumping Results; ~~~~~~~~~~~~~~~~~~~~~~. The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the :class:`.Batch` for which to create a new :class:`.BashJob`; and a list containing all of the individual clumping results files. We use the ``ubuntu:22.04``; Docker image for this job. The return value is the new :class:`.BashJob` created. .. code-block:: python. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:4090,Security,access,access,4090,"il can use based on the input argument ``--cores``. .. literalinclude:: files/run_gwas.py; :language: python; :emphasize-lines: 47-48; :caption: run_gwas.py; :name: run_gwas. Docker Image; ------------. A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the `run_gwas.py` script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:4693,Security,access,access,4693,"mage, ``hailgenetics/hail``, for public use with Hail already installed. We extend this; Docker image to include the `run_gwas.py` script. .. literalinclude:: files/Dockerfile; :language: docker; :caption: Dockerfile; :name: Dockerfile. The following Docker command builds this image:. .. code-block:: sh. docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project's private Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the :class:`.Job` that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs. .. code-block:: python. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; pyth",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:5228,Security,access,access,5228,"ivate Google Container Repository (GCR).; It is **not** advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the :class:`.Job` that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs. .. code-block:: python. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. - The `image` is the image created in the previous step. We copied the `run_gwas.py`; script into the root directory `/`. Therefore, to execute the `run_gwas.py` script, we; call `/run_gwas.py`. - The `run_gwas.py` script takes an output-file parameter and then creates files ending with; the extensions `.bed`, `.bim`, `.fam`, and `.assoc`. In order for Batch to know th",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst:5400,Testability,test,test,5400,"ository. The following Docker command pushes the image to GCR:. .. code-block:: sh. docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace ``<MY_PROJECT>`` with the name of your Google project. Ensure your Batch service account; :ref:`can access images in GCR <service-accounts>`. Batch Script; ------------. The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. ~~~~~~~~~; Functions; ~~~~~~~~~. GWAS; ~~~~. To start, we will write a function that creates a new :class:`.Job` on an existing :class:`.Batch` that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the :class:`.Job` that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs. .. code-block:: python. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. - The `image` is the image created in the previous step. We copied the `run_gwas.py`; script into the root directory `/`. Therefore, to execute the `run_gwas.py` script, we; call `/run_gwas.py`. - The `run_gwas.py` script takes an output-file parameter and then creates files ending with; the extensions `.bed`, `.bim`, `.fam`, and `.assoc`. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the :meth:`.BashJob.declare_resource_group`; method. We then pass ``g.ofi",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/clumping.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/clumping.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:976,Availability,checkpoint,checkpointing,976,".. _sec-cookbook-random_forest:. ===================; Random Forest Model; ===================. Introduction; ------------. We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features. To perform this analysis with Batch, we will first use a :class:`.PythonJob`; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pand",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8358,Availability,resilien,resilient,8358,"lt` to the results list. However,; `tsv_result` is a Python object. We use the :meth:`.PythonResult.as_str` method to convert the; Python object to a text file containing the `str()` output of the Python object. .. code-block:: python. for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results fil",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8417,Availability,checkpoint,checkpoint,8417,"as_str` method to convert the; Python object to a text file containing the `str()` output of the Python object. .. code-block:: python. for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window i",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8733,Availability,checkpoint,checkpoint,8733,"l(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, re",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8844,Availability,checkpoint,checkpoints,8844,"est results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(t",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9059,Availability,checkpoint,checkpointed,9059,"t location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9123,Availability,checkpoint,checkpointed,9123,"ut, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Buildi",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9257,Availability,checkpoint,checkpoint,9257,"The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, w",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9362,Availability,checkpoint,checkpoint,9362,"The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, w",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9485,Availability,checkpoint,checkpoint,9485," runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9537,Availability,checkpoint,checkpoint,9537,"by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9573,Availability,checkpoint,checkpoint,9573,"ed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs t",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:9806,Availability,checkpoint,checkpoint,9806,"_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. ..",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10030,Availability,checkpoint,checkpointing,10030,"r the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; cont",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10151,Availability,checkpoint,checkpointing,10151,"end; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :clas",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10523,Availability,checkpoint,checkpoint,10523,"); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10785,Availability,checkpoint,checkpoint,10785,"t); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; --------------------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10877,Availability,checkpoint,checkpoint,10877,"---------. If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10929,Availability,checkpoint,checkpoint,10929," might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with in",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:10965,Availability,checkpoint,checkpoint,10965,"windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline th",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:11035,Availability,checkpoint,checkpoint,11035,"eckpointing; mechanism to check if the result for the window has already completed. Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows. First, we create a results array that is the size of the number of windows. .. code-block:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:11500,Availability,checkpoint,checkpoint,11500,"k:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. .. literalinclude:: files/run_rf_simple.py; :language: python; :caption: run_rf_simple.py. .. literalinclude:: files/run_rf_checkpoint.py; :language: python; :caption: run_rf_checkpoint.py. .. literalinclude:: files/run_rf_checkpoint_batching.py; :language: python; :caption: run_rf_checkpoint_batching.py; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:11685,Availability,checkpoint,checkpoint,11685,"k:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. .. literalinclude:: files/run_rf_simple.py; :language: python; :caption: run_rf_simple.py. .. literalinclude:: files/run_rf_checkpoint.py; :language: python; :caption: run_rf_checkpoint.py. .. literalinclude:: files/run_rf_checkpoint_batching.py; :language: python; :caption: run_rf_checkpoint_batching.py; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:11799,Availability,checkpoint,checkpoint,11799,"k:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. .. literalinclude:: files/run_rf_simple.py; :language: python; :caption: run_rf_simple.py. .. literalinclude:: files/run_rf_checkpoint.py; :language: python; :caption: run_rf_checkpoint.py. .. literalinclude:: files/run_rf_checkpoint_batching.py; :language: python; :caption: run_rf_checkpoint_batching.py; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:3890,Deployability,install,installed,3890,"dex == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). ~~~~~~~~~~~~~~~~~~~~~~; Format Result Function; ~~~~~~~~~~~~~~~~~~~~~~. The function below takes the expected output of the function `random_forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class:`.PythonJob`, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package `dill` installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method :meth:`.PythonJob.image` or as the argument `default_python_image` when; constructing a Batch . We also provide a convenience function :func:`.docker.build_python_image`; for building an image that has the correct version of Python and `dill` installed; along with any desired Python packages. For running the random forest, we need both the `sklearn` and `pandas` Python; packages installed in the image. We use :func:`.docker.build_python_image` to build; an image and push it automatically to the location specified (ex: `us-docker.pkg.dev/hail-vdc/random-forest`). .. code-block:: python. image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). ~~~~~~~~~~~~; Control Code; ~",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:4329,Deployability,install,installed,4329,"~~~~~~~~~; Format Result Function; ~~~~~~~~~~~~~~~~~~~~~~. The function below takes the expected output of the function `random_forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class:`.PythonJob`, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package `dill` installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method :meth:`.PythonJob.image` or as the argument `default_python_image` when; constructing a Batch . We also provide a convenience function :func:`.docker.build_python_image`; for building an image that has the correct version of Python and `dill` installed; along with any desired Python packages. For running the random forest, we need both the `sklearn` and `pandas` Python; packages installed in the image. We use :func:`.docker.build_python_image` to build; an image and push it automatically to the location specified (ex: `us-docker.pkg.dev/hail-vdc/random-forest`). .. code-block:: python. image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). ~~~~~~~~~~~~; Control Code; ~~~~~~~~~~~~. We start by defining a backend. .. code-block:: python. backend = hb.ServiceBackend(). Second, we create a :class:`.Batch` and specify the default Python image to; use for Python jobs with `default_python_image`. `image` is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to. .. code-block:: python. b = hb.Batch(name='rf',; default_python_image=image). Next, we read the `y` dataframe locally in order t",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:4468,Deployability,install,installed,4468,"forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class:`.PythonJob`, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package `dill` installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method :meth:`.PythonJob.image` or as the argument `default_python_image` when; constructing a Batch . We also provide a convenience function :func:`.docker.build_python_image`; for building an image that has the correct version of Python and `dill` installed; along with any desired Python packages. For running the random forest, we need both the `sklearn` and `pandas` Python; packages installed in the image. We use :func:`.docker.build_python_image` to build; an image and push it automatically to the location specified (ex: `us-docker.pkg.dev/hail-vdc/random-forest`). .. code-block:: python. image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). ~~~~~~~~~~~~; Control Code; ~~~~~~~~~~~~. We start by defining a backend. .. code-block:: python. backend = hb.ServiceBackend(). Second, we create a :class:`.Batch` and specify the default Python image to; use for Python jobs with `default_python_image`. `image` is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to. .. code-block:: python. b = hb.Batch(name='rf',; default_python_image=image). Next, we read the `y` dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use the ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8327,Deployability,pipeline,pipeline,8327,"lt` to the results list. However,; `tsv_result` is a Python object. We use the :meth:`.PythonResult.as_str` method to convert the; Python object to a text file containing the `str()` output of the Python object. .. code-block:: python. for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results fil",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:8514,Deployability,pipeline,pipeline,8514,"as_str` method to convert the; Python object to a text file containing the `str()` output of the Python object. .. code-block:: python. for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the :func:`.concatenate` function and then; write the concatenated results file to a permanent output location. .. code-block:: python. output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call :meth:`.Batch.run` to execute the batch and then close the backend. .. code-block:: python. b.run(wait=False); backend.close(). Add Checkpointing; -----------------. The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven't already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function `hfs.exists` to check whether the file already; exists before adding that job to the DAG. First, we define the checkpoint path for each window. .. code-block:: python. def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we'll append to:. .. code-block:: python. results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a :class:`.InputResourceFile` using :meth:`.Batch.read_input` and append; the input to the results list. If the checkpoint doesn't exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using :meth:`.Batch.write_output`. .. code-block:: python. for window i",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:11970,Deployability,pipeline,pipeline,11970,"k:: python. indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path. .. code-block:: python. inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the `hailtop.grouped`; function to group the inputs into groups of 10 and create a; job for each group. Then we create a :class:`.PythonJob` and; use :meth:`.PythonJob.call` to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list. .. code-block:: python. for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we've only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; --------. We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. .. literalinclude:: files/run_rf_simple.py; :language: python; :caption: run_rf_simple.py. .. literalinclude:: files/run_rf_checkpoint.py; :language: python; :caption: run_rf_checkpoint.py. .. literalinclude:: files/run_rf_checkpoint_batching.py; :language: python; :caption: run_rf_checkpoint_batching.py; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:1845,Modifiability,variab,variables,1845,"ythonJob`; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio. We write a function that runs the random forest model and leaves the window; of interest out of the model `window_name`. An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below. .. code-block:: python; :emphasize-lines: 15. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.inde",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:2026,Modifiability,variab,variables,2026,"rest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio. We write a function that runs the random forest model and leaves the window; of interest out of the model `window_name`. An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below. .. code-block:: python; :emphasize-lines: 15. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:6829,Modifiability,variab,variable,6829,"ll Batch to localize these files; when they are referenced by a :class:`.Job`. .. code-block:: python. df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). We initialize a list to keep track of all of the output files to concatenate; later on. .. code-block:: python. results = []. We now have all of our inputs ready and can iterate through each window in the; `y` dataframe. For each window, we create a new :class:`.PythonJob` using the; method :meth:`.Batch.new_python_job`. We then use the method :meth:`.PythonJob.call`; to run the function `random_forest`. The inputs to `random_forest` are the Batch inputs; `df_x_input` and `df_y_input` as well as the window name. Notice that the first argument to; :meth:`.PythonJob.call` is the reference to the function to call (i.e `random_forest` and; not `random_forest(...)`. The rest of the arguments are the usual positional arguments and; key-word arguments to the function. Lastly, we assign the result of calling the function; to the variable `result` which is a :class:`.PythonResult`. A :class:`.PythonResult`; can be thought of as a Python object and used in subsequent calls to :meth:`.PythonJob.call`. Since the type of `result` is a tuple of (str, float, float), we need to convert the Python; tuple to a tab-delimited string that can later be concatenated. We use the `as_tsv` function; we wrote above to do so. The input to `as_tsv` is `result` and we assign the output to `tsv_result`. Lastly in the for loop for each window, we append the `tsv_result` to the results list. However,; `tsv_result` is a Python object. We use the :meth:`.PythonResult.as_str` method to convert the; Python object to a text file containing the `str()` output of the Python object. .. code-block:: python. for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have comput",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:799,Performance,perform,perform,799,".. _sec-cookbook-random_forest:. ===================; Random Forest Model; ===================. Introduction; ------------. We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features. To perform this analysis with Batch, we will first use a :class:`.PythonJob`; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pand",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:164,Safety,predict,predict,164,".. _sec-cookbook-random_forest:. ===================; Random Forest Model; ===================. Introduction; ------------. We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features. To perform this analysis with Batch, we will first use a :class:`.PythonJob`; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pand",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:734,Safety,predict,predict,734,".. _sec-cookbook-random_forest:. ===================; Random Forest Model; ===================. Introduction; ------------. We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features. To perform this analysis with Batch, we will first use a :class:`.PythonJob`; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code; ----------. ~~~~~~~; Imports; ~~~~~~~. We import all the modules we will need. The random forest model code comes; from the `sklearn` package. .. code-block:: python. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. ~~~~~~~~~~~~~~~~~~~~~~; Random Forest Function; ~~~~~~~~~~~~~~~~~~~~~~. The inputs to the random forest function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pand",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:3148,Safety,predict,predict,3148,"del and leaves the window; of interest out of the model `window_name`. An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below. .. code-block:: python; :emphasize-lines: 15. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). ~~~~~~~~~~~~~~~~~~~~~~; Format Result Function; ~~~~~~~~~~~~~~~~~~~~~~. The function below takes the expected output of the function `random_forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class:`.PythonJob`, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package `dill` installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method :meth:`.PythonJob.image` or as the argument `default_python_image` whe",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:2718,Testability,test,testing,2718,"est function are two data frame files. `df_x`; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. `df_y` is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio. We write a function that runs the random forest model and leaves the window; of interest out of the model `window_name`. An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below. .. code-block:: python; :emphasize-lines: 15. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). ~~~~~~~~~~~~~~~~~~~~~~; Format Result Function; ~~~~~~~~~~~~~~~~~~~~~~. The function below takes the expected output of the function `random_forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst:3122,Testability,test,testing,3122," function that runs the random forest model and leaves the window; of interest out of the model `window_name`. An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below. .. code-block:: python; :emphasize-lines: 15. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). ~~~~~~~~~~~~~~~~~~~~~~; Format Result Function; ~~~~~~~~~~~~~~~~~~~~~~. The function below takes the expected output of the function `random_forest`; and returns a tab-delimited string that will be used later on when concatenating results. .. code-block:: python. def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). ~~~~~~~~~~~~~~~~~~; Build Python Image; ~~~~~~~~~~~~~~~~~~. In order to run a :class:`.PythonJob`, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package `dill` installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method :meth:`.PythonJob.image` or as",MatchSource.DOCS,hail/python/hailtop/batch/docs/cookbook/random_forest.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/random_forest.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst:107,Modifiability,inherit,inheritance,107,"{{ objname | escape | underline }}. .. currentmodule:: {{ module }}. .. autoclass:: {{ objname }}(); :show-inheritance:; :members:; :no-inherited-members:. {% block attributes %}; {% if (attributes | reject('in', inherited_members) | list | count) != 0 %}; .. rubric:: Attributes. .. autosummary::; :nosignatures:. {% for item in attributes %}; {% if item not in inherited_members %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}. {% block methods %}; {% if (methods | reject('in', inherited_members) | list | count) != 0 %}. .. rubric:: Methods. .. autosummary::; :nosignatures:. {% for item in methods %}; {% if item not in inherited_members %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst:136,Modifiability,inherit,inherited-members,136,"{{ objname | escape | underline }}. .. currentmodule:: {{ module }}. .. autoclass:: {{ objname }}(); :show-inheritance:; :members:; :no-inherited-members:. {% block attributes %}; {% if (attributes | reject('in', inherited_members) | list | count) != 0 %}; .. rubric:: Attributes. .. autosummary::; :nosignatures:. {% for item in attributes %}; {% if item not in inherited_members %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}. {% block methods %}; {% if (methods | reject('in', inherited_members) | list | count) != 0 %}. .. rubric:: Methods. .. autosummary::; :nosignatures:. {% for item in methods %}; {% if item not in inherited_members %}; ~{{ name }}.{{ item }}; {% endif %}; {%- endfor %}; {% endif %}; {% endblock %}; ",MatchSource.DOCS,hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/_templates/_autosummary/class.rst
https://github.com/hail-is/hail/tree/0.2.133/batch/pinned-requirements.txt:775,Safety,timeout,timeout,775,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux batch/requirements.txt --output-file=batch/pinned-requirements.txt; aiodocker==0.22.2; # via -r batch/requirements.txt; aiohttp==3.9.5; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # aiodocker; aiorwlock==1.4.0; # via -r batch/requirements.txt; aiosignal==1.3.1; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # aiohttp; async-timeout==4.0.3; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # -r batch/requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # aiohttp; dictdiffer==0.9.0; # via -r batch/requirements.txt; frozenlist==1.4.1; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # aiohttp; # aiosignal; idna==3.7; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pinned-requirements.txt; # yarl; multidict==6.0.5; # via; # -c batch/../gear/pinned-requirements.txt; # -c batch/../hail/python/dev/pinned-requirements.txt; # -c batch/../hail/python/pinned-requirements.txt; # -c batch/../web_common/pi,MatchSource.DOCS,batch/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/batch/requirements.txt:295,Safety,timeout,timeout,295,"-c ../hail/python/pinned-requirements.txt; -c ../hail/python/dev/pinned-requirements.txt; -c ../gear/pinned-requirements.txt; -c ../web_common/pinned-requirements.txt; dictdiffer>=0.8.1,<1; pandas>=2,<3; plotly>=5.18.0,<6; # Worker requirements; aiodocker>=0.17.0,<1; aiorwlock>=1.0.0,<2; async-timeout>=4.0.2,<5; ",MatchSource.DOCS,batch/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt:5468,Integrability,wrap,wrapt,5468,sql; pyparsing==3.1.2; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # httplib2; python-dateutil==2.9.0.post0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; pyyaml==6.0.1; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; requests==2.32.3; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # google-api-core; # google-cloud-profiler; rsa==4.9; # via; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # google-auth; setuptools==71.1.0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # kubernetes-asyncio; six==1.16.0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; # python-dateutil; sortedcontainers==2.4.0; # via; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # -r gear/requirements.txt; uritemplate==4.1.1; # via google-api-python-client; urllib3==1.26.19; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; # requests; wrapt==1.16.0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # prometheus-async; yarl==1.9.4; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; ,MatchSource.DOCS,gear/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt:1119,Performance,cache,cachetools,1119,m linux gear/requirements.txt --output-file=gear/pinned-requirements.txt; aiohttp==3.9.5; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp-session; # kubernetes-asyncio; aiohttp-session==2.12.0; # via -r gear/requirements.txt; aiomysql==0.2.0; # via -r gear/requirements.txt; aiosignal==1.3.1; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; async-timeout==4.0.3; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; cachetools==5.4.0; # via; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # google-auth; certifi==2024.7.4; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; # requests; charset-normalizer==3.3.2; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # requests; frozenlist==1.4.1; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; # aiosignal; google-api-core==2.19.1; # via google-api-python-client; google-api-python-client==2.137.0; # via google-cloud-profiler; google-auth==2.32.0; # via; # -c gear/../hail/python/hailtop/pinned-requirements.txt; #,MatchSource.DOCS,gear/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt:728,Safety,timeout,timeout,728,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux gear/requirements.txt --output-file=gear/pinned-requirements.txt; aiohttp==3.9.5; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp-session; # kubernetes-asyncio; aiohttp-session==2.12.0; # via -r gear/requirements.txt; aiomysql==0.2.0; # via -r gear/requirements.txt; aiosignal==1.3.1; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; async-timeout==4.0.3; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; cachetools==5.4.0; # via; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # google-auth; certifi==2024.7.4; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # kubernetes-asyncio; # requests; charset-normalizer==3.3.2; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # requests; frozenlist==1.4.1; # via; # -c gear/../hail/python/dev/pinned-requirements.txt; # -c gear/../hail/python/hailtop/pinned-requirements.txt; # -c gear/../hail/python/pinned-requirements.txt; # aiohttp; # aiosignal; google-api-core==2.19.1; # via google-api-python-client; google-api-python-client==2.137.0;,MatchSource.DOCS,gear/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt:13,Deployability,install,installed,13,"# hailtop is installed in every service so we must be compatible with it; -c ../hail/python/hailtop/pinned-requirements.txt; # ci-utils includes gear and is used by test_dataproc which installs hail ergo we must be compatible; # with hail; -c ../hail/python/pinned-requirements.txt; # dev is installed in the batch tests; -c ../hail/python/dev/pinned-requirements.txt. aiohttp_session>=2.7,<2.13; aiomysql>=0.0.20,<1; google-cloud-profiler<4.0.0; # google-cloud-profiler<4 is incompatible with protobuf 4 but does not place an upper bound on its pin; protobuf==3.20.2; kubernetes-asyncio>=19.15.1,<20; prometheus_async>=19.2.0,<20; prometheus_client>=0.11.0,<1; PyMySQL>=1,<2; sortedcontainers>=2.4.0,<3; ",MatchSource.DOCS,gear/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt:185,Deployability,install,installs,185,"# hailtop is installed in every service so we must be compatible with it; -c ../hail/python/hailtop/pinned-requirements.txt; # ci-utils includes gear and is used by test_dataproc which installs hail ergo we must be compatible; # with hail; -c ../hail/python/pinned-requirements.txt; # dev is installed in the batch tests; -c ../hail/python/dev/pinned-requirements.txt. aiohttp_session>=2.7,<2.13; aiomysql>=0.0.20,<1; google-cloud-profiler<4.0.0; # google-cloud-profiler<4 is incompatible with protobuf 4 but does not place an upper bound on its pin; protobuf==3.20.2; kubernetes-asyncio>=19.15.1,<20; prometheus_async>=19.2.0,<20; prometheus_client>=0.11.0,<1; PyMySQL>=1,<2; sortedcontainers>=2.4.0,<3; ",MatchSource.DOCS,gear/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt:292,Deployability,install,installed,292,"# hailtop is installed in every service so we must be compatible with it; -c ../hail/python/hailtop/pinned-requirements.txt; # ci-utils includes gear and is used by test_dataproc which installs hail ergo we must be compatible; # with hail; -c ../hail/python/pinned-requirements.txt; # dev is installed in the batch tests; -c ../hail/python/dev/pinned-requirements.txt. aiohttp_session>=2.7,<2.13; aiomysql>=0.0.20,<1; google-cloud-profiler<4.0.0; # google-cloud-profiler<4 is incompatible with protobuf 4 but does not place an upper bound on its pin; protobuf==3.20.2; kubernetes-asyncio>=19.15.1,<20; prometheus_async>=19.2.0,<20; prometheus_client>=0.11.0,<1; PyMySQL>=1,<2; sortedcontainers>=2.4.0,<3; ",MatchSource.DOCS,gear/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt:315,Testability,test,tests,315,"# hailtop is installed in every service so we must be compatible with it; -c ../hail/python/hailtop/pinned-requirements.txt; # ci-utils includes gear and is used by test_dataproc which installs hail ergo we must be compatible; # with hail; -c ../hail/python/pinned-requirements.txt; # dev is installed in the batch tests; -c ../hail/python/dev/pinned-requirements.txt. aiohttp_session>=2.7,<2.13; aiomysql>=0.0.20,<1; google-cloud-profiler<4.0.0; # google-cloud-profiler<4 is incompatible with protobuf 4 but does not place an upper bound on its pin; protobuf==3.20.2; kubernetes-asyncio>=19.15.1,<20; prometheus_async>=19.2.0,<20; prometheus_client>=0.11.0,<1; PyMySQL>=1,<2; sortedcontainers>=2.4.0,<3; ",MatchSource.DOCS,gear/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/letsencrypt/subdomains.txt:36,Energy Efficiency,monitor,monitoring,36,ci; www; batch; batch-driver; blog; monitoring; auth; ukbb-rg; guide-analysis; grafana; prometheus; hello; ,MatchSource.DOCS,letsencrypt/subdomains.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/letsencrypt/subdomains.txt
https://github.com/hail-is/hail/tree/0.2.133/letsencrypt/subdomains.txt:63,Usability,guid,guide-analysis,63,ci; www; batch; batch-driver; blog; monitoring; auth; ukbb-rg; guide-analysis; grafana; prometheus; hello; ,MatchSource.DOCS,letsencrypt/subdomains.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/letsencrypt/subdomains.txt
https://github.com/hail-is/hail/tree/0.2.133/web_common/pinned-requirements.txt:676,Safety,timeout,timeout,676,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux web_common/requirements.txt --output-file=web_common/pinned-requirements.txt; aiohttp==3.9.5; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # aiohttp-jinja2; aiohttp-jinja2==1.6; # via -r web_common/requirements.txt; aiosignal==1.3.1; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # aiohttp; async-timeout==4.0.3; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # aiohttp; frozenlist==1.4.1; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # aiohttp; # aiosignal; idna==3.7; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # yarl; jinja2==3.1.4; # via; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # -r web_common/requirements.txt; # aiohttp-jinja2; libsass==0.23.0; # via -r web_common/requirements.txt; markupsafe==2.1.5; # via; # -c web_common/../hail/python/dev/pinned-requirements.txt; # -c web_common/../hail/python/pinned-requirements.txt; # jinja2; multidict==6.0.5; # via; # -c web_common/../gear/pinned-requirements.txt; # -c web_common/../hail/python/dev/pinned-requireme,MatchSource.DOCS,web_common/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dataproc-pre-installed-requirements.txt:70,Deployability,release,release-,70,# https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.2; #; # 2.2.5-debian12; pyspark==3.5.0; ,MatchSource.DOCS,hail/python/dataproc-pre-installed-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dataproc-pre-installed-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt:5690,Deployability,install,installed-requirements,5690,ckaging==24.1; # via; # bokeh; # plotly; pandas==2.2.2; # via; # -r hail/python/requirements.txt; # bokeh; parsimonious==0.10.0; # via -r hail/python/requirements.txt; pillow==10.4.0; # via bokeh; plotly==5.22.0; # via -r hail/python/requirements.txt; portalocker==2.10.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msal-extensions; protobuf==3.20.2; # via -r hail/python/requirements.txt; py4j==0.10.9.7; # via pyspark; pyasn1==0.6.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # pyasn1-modules; # rsa; pyasn1-modules==0.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; pycares==4.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiodns; pycparser==2.22; # via; # -c hail/python/hailtop/pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # rich; pyjwt==2.8.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msal; pyspark==3.5.0; # via; # -c hail/python/dataproc-pre-installed-requirements.txt; # -r hail/python/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # botocore; # pandas; python-json-logger==2.0.7; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; pytz==2024.1; # via pandas; pyyaml==6.0.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # bokeh; regex==2024.5.15; # via parsimonious; requests==2.32.3; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/requirements.txt; # azure-core; # msal; # msrest; # requests-oauthlib; requests-oauthlib==2.0.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth-oauthlib; # msrest; rich==12.6.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # typer; rsa==4.9; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; s3transfer==0.10.2,MatchSource.DOCS,hail/python/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt:7837,Integrability,wrap,wrapt,7837,; pytz==2024.1; # via pandas; pyyaml==6.0.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # bokeh; regex==2024.5.15; # via parsimonious; requests==2.32.3; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/requirements.txt; # azure-core; # msal; # msrest; # requests-oauthlib; requests-oauthlib==2.0.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth-oauthlib; # msrest; rich==12.6.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # typer; rsa==4.9; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; s3transfer==0.10.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # boto3; scipy==1.11.4; # via -r hail/python/requirements.txt; shellingham==1.5.4; # via; # -c hail/python/hailtop/pinned-requirements.txt; # typer; six==1.16.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-core; # isodate; # jproperties; # python-dateutil; sortedcontainers==2.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; tabulate==0.9.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; tenacity==8.5.0; # via plotly; tornado==6.4.1; # via bokeh; typer==0.12.3; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; typing-extensions==4.12.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-core; # azure-identity; # azure-storage-blob; # janus; # typer; tzdata==2024.1; # via pandas; urllib3==1.26.19; # via; # -c hail/python/hailtop/pinned-requirements.txt; # botocore; # requests; uvloop==0.19.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; wrapt==1.16.0; # via deprecated; xyzservices==2024.6.0; # via bokeh; yarl==1.9.4; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiohttp; ,MatchSource.DOCS,hail/python/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt:1793,Performance,cache,cachetools,1793,l/python/hailtop/pinned-requirements.txt; # azure-mgmt-storage; azure-core==1.30.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-identity; # azure-mgmt-core; # azure-storage-blob; # msrest; azure-identity==1.17.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; azure-mgmt-core==1.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-mgmt-storage; azure-mgmt-storage==20.1.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; azure-storage-blob==12.21.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; bokeh==3.3.4; # via -r hail/python/requirements.txt; boto3==1.34.145; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; botocore==1.34.145; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # boto3; # s3transfer; cachetools==5.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; certifi==2024.7.4; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msrest; # requests; cffi==1.16.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # cryptography; # pycares; charset-normalizer==3.3.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # requests; click==8.1.7; # via; # -c hail/python/hailtop/pinned-requirements.txt; # typer; commonmark==0.9.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # rich; contourpy==1.2.1; # via bokeh; cryptography==43.0.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-identity; # azure-storage-blob; # msal; # pyjwt; decorator==4.4.2; # via -r hail/python/requirements.txt; deprecated==1.2.14; # via -r hail/python/requirements.txt; dill==0.3.8; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; frozenlist==1.4.1; # via; # -c hail/python/hailtop,MatchSource.DOCS,hail/python/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt:529,Safety,timeout,timeout,529,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux hail/python/requirements.txt --output-file=hail/python/pinned-requirements.txt; aiodns==2.0.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; aiohttp==3.9.5; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; aiosignal==1.3.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiohttp; async-timeout==4.0.3; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiohttp; avro==1.11.3; # via -r hail/python/requirements.txt; azure-common==1.1.28; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-mgmt-storage; azure-core==1.30.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-identity; # azure-mgmt-core; # azure-storage-blob; # msrest; azure-identity==1.17.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; azure-mgmt-core==1.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # azure-mgmt-storage; azure-mgmt-storage==20.1.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; azure-storage-blob==12.21.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; bokeh==3.3.4; # via -r hail/python/requirements.txt; boto3==1.34.145; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; botocore==1.34.145; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # boto3; # s3transfer; cachetools==5.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; certifi==2024.7.4; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msrest; # requests; cffi==1.16.0; # via,MatchSource.DOCS,hail/python/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt:5874,Testability,log,logger,5874,.4.0; # via bokeh; plotly==5.22.0; # via -r hail/python/requirements.txt; portalocker==2.10.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msal-extensions; protobuf==3.20.2; # via -r hail/python/requirements.txt; py4j==0.10.9.7; # via pyspark; pyasn1==0.6.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # pyasn1-modules; # rsa; pyasn1-modules==0.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; pycares==4.4.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # aiodns; pycparser==2.22; # via; # -c hail/python/hailtop/pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # rich; pyjwt==2.8.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # msal; pyspark==3.5.0; # via; # -c hail/python/dataproc-pre-installed-requirements.txt; # -r hail/python/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # botocore; # pandas; python-json-logger==2.0.7; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; pytz==2024.1; # via pandas; pyyaml==6.0.1; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # bokeh; regex==2024.5.15; # via parsimonious; requests==2.32.3; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/requirements.txt; # azure-core; # msal; # msrest; # requests-oauthlib; requests-oauthlib==2.0.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth-oauthlib; # msrest; rich==12.6.0; # via; # -c hail/python/hailtop/pinned-requirements.txt; # -r hail/python/hailtop/requirements.txt; # typer; rsa==4.9; # via; # -c hail/python/hailtop/pinned-requirements.txt; # google-auth; s3transfer==0.10.2; # via; # -c hail/python/hailtop/pinned-requirements.txt; # boto3; scipy==1.11.4; # via -r hail/python/requirements.txt; shellingham==1.5.4; # via; # -c hail/python/hailtop/pinn,MatchSource.DOCS,hail/python/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/requirements.txt:52,Deployability,install,installed-requirements,52,"-c hailtop/pinned-requirements.txt; -c dataproc-pre-installed-requirements.txt; -r hailtop/requirements.txt. avro>=1.10,<1.12; bokeh>=3,<3.4; decorator<5; Deprecated>=1.2.10,<1.3; numpy<2; pandas>=2,<3; parsimonious<1; plotly>=5.18.0,<6; protobuf==3.20.2; pyspark>=3.5.0,<3.6; requests>=2.31.0,<3; scipy>1.2,<1.12; ",MatchSource.DOCS,hail/python/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:12050,Integrability,wrap,wrapt,12050,-server; # jupyterlab; # notebook; # terminado; traitlets==5.14.3; # via; # comm; # ipykernel; # ipython; # ipywidgets; # jupyter-client; # jupyter-console; # jupyter-core; # jupyter-events; # jupyter-server; # jupyterlab; # matplotlib-inline; # nbclient; # nbconvert; # nbformat; # nbsphinx; # qtconsole; types-chardet==5.0.4.6; # via -r hail/python/dev/requirements.txt; types-decorator==5.1.8.20240310; # via -r hail/python/dev/requirements.txt; types-deprecated==1.2.9.20240311; # via -r hail/python/dev/requirements.txt; types-pymysql==1.1.0.20240524; # via -r hail/python/dev/requirements.txt; types-python-dateutil==2.9.0.20240316; # via; # -r hail/python/dev/requirements.txt; # arrow; types-pyyaml==6.0.12.20240311; # via -r hail/python/dev/requirements.txt; types-requests==2.31.0.6; # via -r hail/python/dev/requirements.txt; types-setuptools==71.0.0.20240722; # via -r hail/python/dev/requirements.txt; types-six==1.16.21.20240513; # via -r hail/python/dev/requirements.txt; types-tabulate==0.9.0.20240106; # via -r hail/python/dev/requirements.txt; types-urllib3==1.26.25.14; # via; # -r hail/python/dev/requirements.txt; # types-requests; typing-extensions==4.12.2; # via; # -c hail/python/dev/../pinned-requirements.txt; # anyio; # astroid; # async-lru; # ipython; # pylint; uri-template==1.3.0; # via jsonschema; urllib3==1.26.19; # via; # -c hail/python/dev/../pinned-requirements.txt; # requests; uv==0.2.27; # via -r hail/python/dev/requirements.txt; virtualenv==20.26.3; # via pre-commit; watchfiles==0.22.0; # via aiohttp-devtools; wcwidth==0.2.13; # via prompt-toolkit; webcolors==24.6.0; # via jsonschema; webencodings==0.5.1; # via; # bleach; # tinycss2; websocket-client==1.8.0; # via jupyter-server; widgetsnbextension==4.0.11; # via ipywidgets; wrapt==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # astroid; yarl==1.9.4; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp; zipp==3.19.2; # via; # importlib-metadata; # importlib-resources; ,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:803,Safety,timeout,timeout,803,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux hail/python/dev/requirements.txt --output-file=hail/python/dev/pinned-requirements.txt; aiohttp==3.9.5; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp-devtools; aiohttp-devtools==1.1.2; # via -r hail/python/dev/requirements.txt; aiosignal==1.3.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp; alabaster==0.7.16; # via sphinx; anyio==4.4.0; # via; # httpx; # jupyter-server; # watchfiles; argon2-cffi==23.1.0; # via jupyter-server; argon2-cffi-bindings==21.2.0; # via argon2-cffi; arrow==1.3.0; # via isoduration; astroid==2.15.8; # via pylint; asttokens==2.4.1; # via; # devtools; # stack-data; async-lru==2.0.4; # via jupyterlab; async-timeout==4.0.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp; attrs==23.2.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp; # curlylint; # jsonschema; # referencing; babel==2.15.0; # via; # jupyterlab-server; # sphinx; beautifulsoup4==4.12.3; # via nbconvert; bleach==6.1.0; # via nbconvert; build==1.1.1; # via -r hail/python/dev/requirements.txt; certifi==2024.7.4; # via; # -c hail/python/dev/../pinned-requirements.txt; # httpcore; # httpx; # requests; cffi==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # argon2-cffi-bindings; cfgv==3.4.0; # via pre-commit; charset-normalizer==3.3.2; # via; # -c hail/python/dev/../pinned-requirements.txt; # requests; click==8.1.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # -r hail/python/dev/requirements.txt; # aiohttp-devtools; # curlylint; comm==0.2.2; # via; # ipykernel; # ipywidgets; contourpy==1.2.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # matplotlib; curlylint==0.13.1; # via -r hail/python/dev/requirements.txt; cycler==0.12.1; # via matplotlib; debugpy==1.8.2; # via ipykernel; decorator==4.4.2; # via; # -c hail/python/dev/../pinned-requirements.txt; ,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:7357,Safety,timeout,timeout,7357,atformdirs==4.2.2; # via; # jupyter-core; # pylint; # virtualenv; pluggy==1.5.0; # via pytest; pre-commit==3.7.1; # via -r hail/python/dev/requirements.txt; prometheus-client==0.20.0; # via jupyter-server; prompt-toolkit==3.0.47; # via; # ipython; # jupyter-console; psutil==6.0.0; # via ipykernel; ptyprocess==0.7.0; # via; # pexpect; # terminado; pure-eval==0.2.3; # via stack-data; py==1.11.0; # via pytest-forked; pycparser==2.22; # via; # -c hail/python/dev/../pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp-devtools; # devtools; # ipython; # jupyter-console; # nbconvert; # qtconsole; # sphinx; pylint==2.17.7; # via -r hail/python/dev/requirements.txt; pyparsing==3.1.2; # via matplotlib; pyproject-hooks==1.1.0; # via build; pyright==1.1.372; # via -r hail/python/dev/requirements.txt; pytest==7.4.4; # via; # -r hail/python/dev/requirements.txt; # pytest-asyncio; # pytest-forked; # pytest-html; # pytest-instafail; # pytest-metadata; # pytest-mock; # pytest-timeout; # pytest-xdist; pytest-asyncio==0.21.2; # via -r hail/python/dev/requirements.txt; pytest-forked==1.6.0; # via pytest-xdist; pytest-html==1.22.1; # via -r hail/python/dev/requirements.txt; pytest-instafail==0.5.0; # via -r hail/python/dev/requirements.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jup,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:7737,Safety,timeout,timeout,7737, pycparser==2.22; # via; # -c hail/python/dev/../pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp-devtools; # devtools; # ipython; # jupyter-console; # nbconvert; # qtconsole; # sphinx; pylint==2.17.7; # via -r hail/python/dev/requirements.txt; pyparsing==3.1.2; # via matplotlib; pyproject-hooks==1.1.0; # via build; pyright==1.1.372; # via -r hail/python/dev/requirements.txt; pytest==7.4.4; # via; # -r hail/python/dev/requirements.txt; # pytest-asyncio; # pytest-forked; # pytest-html; # pytest-instafail; # pytest-metadata; # pytest-mock; # pytest-timeout; # pytest-xdist; pytest-asyncio==0.21.2; # via -r hail/python/dev/requirements.txt; pytest-forked==1.6.0; # via pytest-xdist; pytest-html==1.22.1; # via -r hail/python/dev/requirements.txt; pytest-instafail==0.5.0; # via -r hail/python/dev/requirements.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonschema; # jupyter-events; rfc3986-validator==0.1.1; # via; # jsonschema; # j,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:8630,Security,validat,validator,8630,nts.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonschema; # jupyter-events; rfc3986-validator==0.1.1; # via; # jsonschema; # jupyter-events; rpds-py==0.19.0; # via; # jsonschema; # referencing; ruff==0.1.13; # via -r hail/python/dev/requirements.txt; send2trash==1.8.3; # via jupyter-server; setuptools==71.1.0; # via jupyterlab; six==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # asttokens; # bleach; # python-dateutil; # rfc3339-validator; sniffio==1.3.1; # via; # anyio; # httpx; snowballstemmer==2.2.0; # via sphinx; soupsieve==2.5; # via beautifulsoup4; sphinx==6.2.1; # via; # -r hail/python/dev/requirements.txt; # nbsphinx; # sphinx-autodoc-typehints; # sphinx-rtd-theme; # sphinxcontrib-jquery; # sphinxcontrib-katex; sphinx-autodoc-typehints==1.23.0; # via -r hail/python/dev/requirements.txt; sphinx-rtd-theme==1.3.0; # via -r hail/python/dev/requirements.txt; sphinxcontrib-applehelp==1.0.8; # via sphinx; sphinxcontrib-devhelp==1.0.6; # via sphinx; sphinxcontrib-htmlhelp,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:8695,Security,validat,validator,8695,k==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonschema; # jupyter-events; rfc3986-validator==0.1.1; # via; # jsonschema; # jupyter-events; rpds-py==0.19.0; # via; # jsonschema; # referencing; ruff==0.1.13; # via -r hail/python/dev/requirements.txt; send2trash==1.8.3; # via jupyter-server; setuptools==71.1.0; # via jupyterlab; six==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # asttokens; # bleach; # python-dateutil; # rfc3339-validator; sniffio==1.3.1; # via; # anyio; # httpx; snowballstemmer==2.2.0; # via sphinx; soupsieve==2.5; # via beautifulsoup4; sphinx==6.2.1; # via; # -r hail/python/dev/requirements.txt; # nbsphinx; # sphinx-autodoc-typehints; # sphinx-rtd-theme; # sphinxcontrib-jquery; # sphinxcontrib-katex; sphinx-autodoc-typehints==1.23.0; # via -r hail/python/dev/requirements.txt; sphinx-rtd-theme==1.3.0; # via -r hail/python/dev/requirements.txt; sphinxcontrib-applehelp==1.0.8; # via sphinx; sphinxcontrib-devhelp==1.0.6; # via sphinx; sphinxcontrib-htmlhelp==2.0.6; # via sphinx; sphinxcontrib-jquery==4.1; # via sphinx,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:9062,Security,validat,validator,9062,# matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonschema; # jupyter-events; rfc3986-validator==0.1.1; # via; # jsonschema; # jupyter-events; rpds-py==0.19.0; # via; # jsonschema; # referencing; ruff==0.1.13; # via -r hail/python/dev/requirements.txt; send2trash==1.8.3; # via jupyter-server; setuptools==71.1.0; # via jupyterlab; six==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # asttokens; # bleach; # python-dateutil; # rfc3339-validator; sniffio==1.3.1; # via; # anyio; # httpx; snowballstemmer==2.2.0; # via sphinx; soupsieve==2.5; # via beautifulsoup4; sphinx==6.2.1; # via; # -r hail/python/dev/requirements.txt; # nbsphinx; # sphinx-autodoc-typehints; # sphinx-rtd-theme; # sphinxcontrib-jquery; # sphinxcontrib-katex; sphinx-autodoc-typehints==1.23.0; # via -r hail/python/dev/requirements.txt; sphinx-rtd-theme==1.3.0; # via -r hail/python/dev/requirements.txt; sphinxcontrib-applehelp==1.0.8; # via sphinx; sphinxcontrib-devhelp==1.0.6; # via sphinx; sphinxcontrib-htmlhelp==2.0.6; # via sphinx; sphinxcontrib-jquery==4.1; # via sphinx-rtd-theme; sphinxcontrib-jsmath==1.0.1; # via sphinx; sphinxcontrib-katex==0.9.10; # via -r hail/python/dev/requirements.txt; sphinxcontrib-qthelp==1.0.8; # via sphinx; sphinxcontrib-serializinghtml==1.1.10; # via sphinx; stack-data==0.6.3; # via ipython; terminado==0.18.1; # via; # jupyter-server; # jupyter-server-terminals; tinycss2==1.3.0; # via nbconvert; toml=,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:5021,Testability,mock,mock,5021, # qtconsole; jupyter-console==6.6.3; # via jupyter; jupyter-core==5.7.2; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # jupyterlab; # nbclient; # nbconvert; # nbformat; # qtconsole; jupyter-events==0.10.0; # via jupyter-server; jupyter-lsp==2.2.5; # via jupyterlab; jupyter-server==2.14.2; # via; # jupyter-lsp; # jupyterlab; # jupyterlab-server; # notebook; # notebook-shim; jupyter-server-terminals==0.5.3; # via jupyter-server; jupyterlab==4.2.4; # via notebook; jupyterlab-pygments==0.3.0; # via nbconvert; jupyterlab-server==2.27.3; # via; # jupyterlab; # notebook; jupyterlab-widgets==3.0.11; # via ipywidgets; kiwisolver==1.4.5; # via matplotlib; lazy-object-proxy==1.10.0; # via astroid; markupsafe==2.1.5; # via; # -c hail/python/dev/../pinned-requirements.txt; # jinja2; # nbconvert; matplotlib==3.9.1; # via -r hail/python/dev/requirements.txt; matplotlib-inline==0.1.7; # via; # ipykernel; # ipython; mccabe==0.7.0; # via pylint; mistune==3.0.2; # via nbconvert; mock==5.1.0; # via -r hail/python/dev/requirements.txt; multidict==6.0.5; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp; # yarl; nbclient==0.10.0; # via nbconvert; nbconvert==7.13.1; # via; # -r hail/python/dev/requirements.txt; # jupyter; # jupyter-server; # nbsphinx; nbformat==5.10.4; # via; # jupyter-server; # nbclient; # nbconvert; # nbsphinx; nbsphinx==0.9.4; # via -r hail/python/dev/requirements.txt; nest-asyncio==1.6.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # ipykernel; nodeenv==1.9.1; # via; # pre-commit; # pyright; notebook==7.2.1; # via jupyter; notebook-shim==0.2.4; # via; # jupyterlab; # notebook; numpy==1.26.4; # via; # -c hail/python/dev/../pinned-requirements.txt; # contourpy; # matplotlib; overrides==7.7.0; # via jupyter-server; packaging==24.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # build; # ipykernel; # jupyter-server; # jupyterlab; # jupyterlab-server; # matplotlib; # nbconvert; # pytest; # qtconsole; # q,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:7342,Testability,mock,mock,7342,atformdirs==4.2.2; # via; # jupyter-core; # pylint; # virtualenv; pluggy==1.5.0; # via pytest; pre-commit==3.7.1; # via -r hail/python/dev/requirements.txt; prometheus-client==0.20.0; # via jupyter-server; prompt-toolkit==3.0.47; # via; # ipython; # jupyter-console; psutil==6.0.0; # via ipykernel; ptyprocess==0.7.0; # via; # pexpect; # terminado; pure-eval==0.2.3; # via stack-data; py==1.11.0; # via pytest-forked; pycparser==2.22; # via; # -c hail/python/dev/../pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp-devtools; # devtools; # ipython; # jupyter-console; # nbconvert; # qtconsole; # sphinx; pylint==2.17.7; # via -r hail/python/dev/requirements.txt; pyparsing==3.1.2; # via matplotlib; pyproject-hooks==1.1.0; # via build; pyright==1.1.372; # via -r hail/python/dev/requirements.txt; pytest==7.4.4; # via; # -r hail/python/dev/requirements.txt; # pytest-asyncio; # pytest-forked; # pytest-html; # pytest-instafail; # pytest-metadata; # pytest-mock; # pytest-timeout; # pytest-xdist; pytest-asyncio==0.21.2; # via -r hail/python/dev/requirements.txt; pytest-forked==1.6.0; # via pytest-xdist; pytest-html==1.22.1; # via -r hail/python/dev/requirements.txt; pytest-instafail==0.5.0; # via -r hail/python/dev/requirements.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jup,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:7673,Testability,mock,mock,7673,nado; pure-eval==0.2.3; # via stack-data; py==1.11.0; # via pytest-forked; pycparser==2.22; # via; # -c hail/python/dev/../pinned-requirements.txt; # cffi; pygments==2.18.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # aiohttp-devtools; # devtools; # ipython; # jupyter-console; # nbconvert; # qtconsole; # sphinx; pylint==2.17.7; # via -r hail/python/dev/requirements.txt; pyparsing==3.1.2; # via matplotlib; pyproject-hooks==1.1.0; # via build; pyright==1.1.372; # via -r hail/python/dev/requirements.txt; pytest==7.4.4; # via; # -r hail/python/dev/requirements.txt; # pytest-asyncio; # pytest-forked; # pytest-html; # pytest-instafail; # pytest-metadata; # pytest-mock; # pytest-timeout; # pytest-xdist; pytest-asyncio==0.21.2; # via -r hail/python/dev/requirements.txt; pytest-forked==1.6.0; # via pytest-xdist; pytest-html==1.22.1; # via -r hail/python/dev/requirements.txt; pytest-instafail==0.5.0; # via -r hail/python/dev/requirements.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonsc,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt:8070,Testability,log,logger,8070,t; pyparsing==3.1.2; # via matplotlib; pyproject-hooks==1.1.0; # via build; pyright==1.1.372; # via -r hail/python/dev/requirements.txt; pytest==7.4.4; # via; # -r hail/python/dev/requirements.txt; # pytest-asyncio; # pytest-forked; # pytest-html; # pytest-instafail; # pytest-metadata; # pytest-mock; # pytest-timeout; # pytest-xdist; pytest-asyncio==0.21.2; # via -r hail/python/dev/requirements.txt; pytest-forked==1.6.0; # via pytest-xdist; pytest-html==1.22.1; # via -r hail/python/dev/requirements.txt; pytest-instafail==0.5.0; # via -r hail/python/dev/requirements.txt; pytest-metadata==3.1.1; # via pytest-html; pytest-mock==3.14.0; # via -r hail/python/dev/requirements.txt; pytest-timeout==2.3.1; # via -r hail/python/dev/requirements.txt; pytest-timestamper==0.0.10; # via -r hail/python/dev/requirements.txt; pytest-xdist==2.5.0; # via -r hail/python/dev/requirements.txt; python-dateutil==2.9.0.post0; # via; # -c hail/python/dev/../pinned-requirements.txt; # arrow; # jupyter-client; # matplotlib; python-json-logger==2.0.7; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; pyyaml==6.0.1; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyter-events; # pre-commit; pyzmq==26.0.3; # via; # ipykernel; # jupyter-client; # jupyter-console; # jupyter-server; # qtconsole; qtconsole==5.5.2; # via jupyter; qtpy==2.4.1; # via qtconsole; referencing==0.35.1; # via; # jsonschema; # jsonschema-specifications; # jupyter-events; requests==2.32.3; # via; # -c hail/python/dev/../pinned-requirements.txt; # jupyterlab-server; # sphinx; rfc3339-validator==0.1.4; # via; # jsonschema; # jupyter-events; rfc3986-validator==0.1.1; # via; # jsonschema; # jupyter-events; rpds-py==0.19.0; # via; # jsonschema; # referencing; ruff==0.1.13; # via -r hail/python/dev/requirements.txt; send2trash==1.8.3; # via jupyter-server; setuptools==71.1.0; # via jupyterlab; six==1.16.0; # via; # -c hail/python/dev/../pinned-requirements.txt; # asttokens; # bleach; # python-date,MatchSource.DOCS,hail/python/dev/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:410,Safety,timeout,timeout,410,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:1056,Safety,avoid,avoid,1056,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:185,Testability,mock,mock,185,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:434,Testability,mock,mock,434,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:720,Testability,stub,stubs,720,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:876,Testability,stub,stubs,876,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt:1104,Testability,benchmark,benchmarks,1104,"-c ../pinned-requirements.txt. aiohttp-devtools>=1.1,<2; build>=1.1,<1.2; pylint>=2.13.5,<3; pre-commit>=3.3.3,<4; ruff==0.1.13; uv>=0.1.38,<0.3; curlylint>=0.13.1,<1; click>=8.1.2,<9; mock>=5.1,<5.2; pytest>=7.1.3,<8; pytest-html>=1.20.0,<2; pytest-xdist>=2.2.1,<3; pytest-instafail>=0.4.2,<1; # https://github.com/hail-is/hail/issues/14130; pytest-asyncio>=0.14.0,<0.23; pytest-timestamper>=0.0.9,<1; pytest-timeout>=2.1,<3; pytest-mock>=3.14,<4; pyright>=1.1.349,<1.2; sphinx>=6,<7; sphinx-autodoc-typehints==1.23.0; nbsphinx>=0.8.8,<1; sphinx_rtd_theme>=1.0.0,<2; jupyter>=1.0.0,<2; sphinxcontrib.katex>=0.9.0,<1; fswatch>=0.1.1,<1; # https://github.com/jupyter/nbconvert/issues/2092; nbconvert<7.14. # library type stubs; types-Deprecated; types-PyMySQL; types-PyYAML; types-chardet; types-decorator; types-python-dateutil; # https://github.com/python/typeshed/blob/main/stubs/requests/METADATA.toml#L5-L10; types-requests<2.31.0.7; types-setuptools; types-six; types-tabulate; types-urllib3; pillow>=10.0.1 # not directly required, pinned by Snyk to avoid a vulnerability. matplotlib>=3.5,<4 # for benchmarks; ",MatchSource.DOCS,hail/python/dev/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/dev/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt:1020,Performance,cache,cachetools,1020,as autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux hail/python/hailtop/requirements.txt --output-file=hail/python/hailtop/pinned-requirements.txt; aiodns==2.0.0; # via -r hail/python/hailtop/requirements.txt; aiohttp==3.9.5; # via -r hail/python/hailtop/requirements.txt; aiosignal==1.3.1; # via aiohttp; async-timeout==4.0.3; # via aiohttp; attrs==23.2.0; # via aiohttp; azure-common==1.1.28; # via azure-mgmt-storage; azure-core==1.30.2; # via; # azure-identity; # azure-mgmt-core; # azure-storage-blob; # msrest; azure-identity==1.17.1; # via -r hail/python/hailtop/requirements.txt; azure-mgmt-core==1.4.0; # via azure-mgmt-storage; azure-mgmt-storage==20.1.0; # via -r hail/python/hailtop/requirements.txt; azure-storage-blob==12.21.0; # via -r hail/python/hailtop/requirements.txt; boto3==1.34.145; # via -r hail/python/hailtop/requirements.txt; botocore==1.34.145; # via; # -r hail/python/hailtop/requirements.txt; # boto3; # s3transfer; cachetools==5.4.0; # via google-auth; certifi==2024.7.4; # via; # msrest; # requests; cffi==1.16.0; # via; # cryptography; # pycares; charset-normalizer==3.3.2; # via requests; click==8.1.7; # via typer; commonmark==0.9.1; # via rich; cryptography==43.0.0; # via; # azure-identity; # azure-storage-blob; # msal; # pyjwt; dill==0.3.8; # via -r hail/python/hailtop/requirements.txt; frozenlist==1.4.1; # via; # -r hail/python/hailtop/requirements.txt; # aiohttp; # aiosignal; google-auth==2.32.0; # via; # -r hail/python/hailtop/requirements.txt; # google-auth-oauthlib; google-auth-oauthlib==0.8.0; # via -r hail/python/hailtop/requirements.txt; humanize==4.10.0; # via -r hail/python/hailtop/requirements.txt; idna==3.7; # via; # requests; # yarl; isodate==0.6.1; # via; # azure-storage-blob; # msrest; janus==1.0.0; # via -r hail/python/hailtop/requirements.txt; jmespath==1.0.1; # via; # boto3; # botocore; jproperties==2.1.2; # via -r hail/python/hailtop/requirements.txt; msal==1.30.0; # v,MatchSource.DOCS,hail/python/hailtop/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt:386,Safety,timeout,timeout,386,# This file was autogenerated by uv via the following command:; # uv pip compile --python-version 3.9 --python-platform linux hail/python/hailtop/requirements.txt --output-file=hail/python/hailtop/pinned-requirements.txt; aiodns==2.0.0; # via -r hail/python/hailtop/requirements.txt; aiohttp==3.9.5; # via -r hail/python/hailtop/requirements.txt; aiosignal==1.3.1; # via aiohttp; async-timeout==4.0.3; # via aiohttp; attrs==23.2.0; # via aiohttp; azure-common==1.1.28; # via azure-mgmt-storage; azure-core==1.30.2; # via; # azure-identity; # azure-mgmt-core; # azure-storage-blob; # msrest; azure-identity==1.17.1; # via -r hail/python/hailtop/requirements.txt; azure-mgmt-core==1.4.0; # via azure-mgmt-storage; azure-mgmt-storage==20.1.0; # via -r hail/python/hailtop/requirements.txt; azure-storage-blob==12.21.0; # via -r hail/python/hailtop/requirements.txt; boto3==1.34.145; # via -r hail/python/hailtop/requirements.txt; botocore==1.34.145; # via; # -r hail/python/hailtop/requirements.txt; # boto3; # s3transfer; cachetools==5.4.0; # via google-auth; certifi==2024.7.4; # via; # msrest; # requests; cffi==1.16.0; # via; # cryptography; # pycares; charset-normalizer==3.3.2; # via requests; click==8.1.7; # via typer; commonmark==0.9.1; # via rich; cryptography==43.0.0; # via; # azure-identity; # azure-storage-blob; # msal; # pyjwt; dill==0.3.8; # via -r hail/python/hailtop/requirements.txt; frozenlist==1.4.1; # via; # -r hail/python/hailtop/requirements.txt; # aiohttp; # aiosignal; google-auth==2.32.0; # via; # -r hail/python/hailtop/requirements.txt; # google-auth-oauthlib; google-auth-oauthlib==0.8.0; # via -r hail/python/hailtop/requirements.txt; humanize==4.10.0; # via -r hail/python/hailtop/requirements.txt; idna==3.7; # via; # requests; # yarl; isodate==0.6.1; # via; # azure-storage-blob; # msrest; janus==1.0.0; # via -r hail/python/hailtop/requirements.txt; jmespath==1.0.1; # via; # boto3; # botocore; jproperties==2.1.2; # via -r hail/python/hailtop/requirements.txt; msal=,MatchSource.DOCS,hail/python/hailtop/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt:2664,Testability,log,logger,2664,-r hail/python/hailtop/requirements.txt; humanize==4.10.0; # via -r hail/python/hailtop/requirements.txt; idna==3.7; # via; # requests; # yarl; isodate==0.6.1; # via; # azure-storage-blob; # msrest; janus==1.0.0; # via -r hail/python/hailtop/requirements.txt; jmespath==1.0.1; # via; # boto3; # botocore; jproperties==2.1.2; # via -r hail/python/hailtop/requirements.txt; msal==1.30.0; # via; # azure-identity; # msal-extensions; msal-extensions==1.2.0; # via azure-identity; msrest==0.7.1; # via azure-mgmt-storage; multidict==6.0.5; # via; # aiohttp; # yarl; nest-asyncio==1.6.0; # via -r hail/python/hailtop/requirements.txt; oauthlib==3.2.2; # via requests-oauthlib; orjson==3.10.6; # via -r hail/python/hailtop/requirements.txt; portalocker==2.10.1; # via msal-extensions; pyasn1==0.6.0; # via; # pyasn1-modules; # rsa; pyasn1-modules==0.4.0; # via google-auth; pycares==4.4.0; # via aiodns; pycparser==2.22; # via cffi; pygments==2.18.0; # via rich; pyjwt==2.8.0; # via msal; python-dateutil==2.9.0.post0; # via botocore; python-json-logger==2.0.7; # via -r hail/python/hailtop/requirements.txt; pyyaml==6.0.1; # via -r hail/python/hailtop/requirements.txt; requests==2.32.3; # via; # azure-core; # msal; # msrest; # requests-oauthlib; requests-oauthlib==2.0.0; # via; # google-auth-oauthlib; # msrest; rich==12.6.0; # via; # -r hail/python/hailtop/requirements.txt; # typer; rsa==4.9; # via google-auth; s3transfer==0.10.2; # via boto3; shellingham==1.5.4; # via typer; six==1.16.0; # via; # azure-core; # isodate; # jproperties; # python-dateutil; sortedcontainers==2.4.0; # via -r hail/python/hailtop/requirements.txt; tabulate==0.9.0; # via -r hail/python/hailtop/requirements.txt; typer==0.12.3; # via -r hail/python/hailtop/requirements.txt; typing-extensions==4.12.2; # via; # azure-core; # azure-identity; # azure-storage-blob; # janus; # typer; urllib3==1.26.19; # via; # botocore; # requests; uvloop==0.19.0; # via -r hail/python/hailtop/requirements.txt; yarl==1.9.4; # via aiohttp; ,MatchSource.DOCS,hail/python/hailtop/pinned-requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/pinned-requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/requirements.txt:382,Testability,log,logger,382,"aiodns>=2.0.0,<3; aiohttp>=3.9,<4; azure-identity>=1.6.0,<2; azure-mgmt-storage==20.1.0; azure-storage-blob>=12.11.0,<13; boto3>=1.17,<2.0; botocore>=1.20,<2.0; dill>=0.3.6,<0.4; frozenlist>=1.3.1,<2; google-auth>=2.14.1,<3; google-auth-oauthlib>=0.5.2,<1; humanize>=4.0,<5; janus>=0.6,<1.1; nest_asyncio>=1.5.8,<2; orjson>=3.9.15,<4; rich>=12.6.0,<13; typer>=0.9.0,<1; python-json-logger>=2.0.2,<3; pyyaml>=6.0,<7.0; sortedcontainers>=2.4.0,<3; tabulate>=0.8.9,<1; uvloop>=0.19.0,<1; sys_platform!='win32'; jproperties>=2.1.1,<3; ",MatchSource.DOCS,hail/python/hailtop/requirements.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/requirements.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:1914,Availability,error,errors,1914,"d use the heading `**Notes**`. Additional subsections can be added.; - Put short python expressions or code snippets and Hail expressions; in double-``. ### Annotations; - For commands that create annotations, have ""Annotations"" section that lists the annotations. Use a bulleted list with the following format:. ```; **Annotations**. - **annotation** (*Type*) -- description; ```. ### Parameter and Return Type Specification; - Use :param:, :rtype: (if not None), :return:.; - :return: gives a short description of what is being returned. Example: `A VariantDataset that has been annotated.`. ### General Style; - links: `this is a description <with a url>`_.; - subsections: use `**`, we'll look into something better; - All function/command references should use a Sphinx directive to link ```:py:meth:`~hail.VariantDataset.vep````. ## Code Examples; - All examples are automatically tested with the [Sphinx doctest extension](https://www.sphinx-doc.org/en/stable/ext/doctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be placed in `python/hail/docs/data`. When referencing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessa",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:2799,Availability,avail,available,2799," tested with the [Sphinx doctest extension](https://www.sphinx-doc.org/en/stable/ext/doctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be placed in `python/hail/docs/data`. When referencing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessary.; - Example MT files that currently exist in `python/hail/docs/data` were generated as follows:. **example1.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sampl",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:5390,Availability,down,downloaded,5390,"norm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/example_burden.tsv', 'Sample', root = 'sa.burden',; ... config = TextTableConfig(impute=True)); ... .annotate_variants_expr('va.weight = v.start.toDouble'); ... .variant_qc(); ... .annotate_variants_intervals('python/hail/docs/data/genes.interval_list', 'va.genes', all=True); ... .annotate_variants_intervals('python/hail/docs/data/gene.interval_list', 'va.gene', all=False); ... .write('python/hail/docs/data/example_burden.mt', overwrite=True)); ```. ## Tutorial Setup. If building the docs on your local computer, use `-Dtutorial.home=/path/hail-tutorial-files/` to specify where the tutorial files have been previously downloaded to avoid downloading the files using `wget` each time.; ",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:5410,Availability,down,downloading,5410,"norm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/example_burden.tsv', 'Sample', root = 'sa.burden',; ... config = TextTableConfig(impute=True)); ... .annotate_variants_expr('va.weight = v.start.toDouble'); ... .variant_qc(); ... .annotate_variants_intervals('python/hail/docs/data/genes.interval_list', 'va.genes', all=True); ... .annotate_variants_intervals('python/hail/docs/data/gene.interval_list', 'va.gene', all=False); ... .write('python/hail/docs/data/example_burden.mt', overwrite=True)); ```. ## Tutorial Setup. If building the docs on your local computer, use `-Dtutorial.home=/path/hail-tutorial-files/` to specify where the tutorial files have been previously downloaded to avoid downloading the files using `wget` each time.; ",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:85,Deployability,install,installing,85,"# Hail Python Docs Style Guide. ## Build Dependencies; - [pandoc](https://pandoc.org/installing.html); - [sphinx v1.5.4](https://www.sphinx-doc.org/); - [nbsphinx](https://nbsphinx.readthedocs.io/); - [read the docs Sphinx theme v0.1.9](https://github.com/snide/sphinx_rtd_theme); - pandas; - numpy. ## Function Documentation Structure; - Description; - Examples; - Notes (if needed); - Other subsections (if needed); - Annotations (if needed); - Parameter Specification. ### Description; - Start with a short description. ### Examples; - Create an examples section with this structure:. ```; **Examples**. Short description of example 1:. >>> python code example. Short description of example 2:. >>> python code example; ```. - The first example should be the most common use case.; - Try to keep example descriptions short and concise. ### Additional Information; - This is not required. If needed use the heading `**Notes**`. Additional subsections can be added.; - Put short python expressions or code snippets and Hail expressions; in double-``. ### Annotations; - For commands that create annotations, have ""Annotations"" section that lists the annotations. Use a bulleted list with the following format:. ```; **Annotations**. - **annotation** (*Type*) -- description; ```. ### Parameter and Return Type Specification; - Use :param:, :rtype: (if not None), :return:.; - :return: gives a short description of what is being returned. Example: `A VariantDataset that has been annotated.`. ### General Style; - links: `this is a description <with a url>`_.; - subsections: use `**`, we'll look into something better; - All function/command references should use a Sphinx directive to link ```:py:meth:`~hail.VariantDataset.vep````. ## Code Examples; - All examples are automatically tested with the [Sphinx doctest extension](https://www.sphinx-doc.org/en/stable/ext/doctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be p",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:2661,Modifiability,variab,variable,2661,"ences should use a Sphinx directive to link ```:py:meth:`~hail.VariantDataset.vep````. ## Code Examples; - All examples are automatically tested with the [Sphinx doctest extension](https://www.sphinx-doc.org/en/stable/ext/doctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be placed in `python/hail/docs/data`. When referencing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessary.; - Example MT files that currently exist in `python/hail/docs/data` were generated as follows:. **example1.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:2865,Modifiability,variab,variables,2865,"ctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be placed in `python/hail/docs/data`. When referencing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessary.; - Example MT files that currently exist in `python/hail/docs/data` were generated as follows:. **example1.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; v",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:4822,Modifiability,config,config,4822,"norm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/example_burden.tsv', 'Sample', root = 'sa.burden',; ... config = TextTableConfig(impute=True)); ... .annotate_variants_expr('va.weight = v.start.toDouble'); ... .variant_qc(); ... .annotate_variants_intervals('python/hail/docs/data/genes.interval_list', 'va.genes', all=True); ... .annotate_variants_intervals('python/hail/docs/data/gene.interval_list', 'va.gene', all=False); ... .write('python/hail/docs/data/example_burden.mt', overwrite=True)); ```. ## Tutorial Setup. If building the docs on your local computer, use `-Dtutorial.home=/path/hail-tutorial-files/` to specify where the tutorial files have been previously downloaded to avoid downloading the files using `wget` each time.; ",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:5404,Safety,avoid,avoid,5404,"norm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/example_burden.tsv', 'Sample', root = 'sa.burden',; ... config = TextTableConfig(impute=True)); ... .annotate_variants_expr('va.weight = v.start.toDouble'); ... .variant_qc(); ... .annotate_variants_intervals('python/hail/docs/data/genes.interval_list', 'va.genes', all=True); ... .annotate_variants_intervals('python/hail/docs/data/gene.interval_list', 'va.gene', all=False); ... .write('python/hail/docs/data/example_burden.mt', overwrite=True)); ```. ## Tutorial Setup. If building the docs on your local computer, use `-Dtutorial.home=/path/hail-tutorial-files/` to specify where the tutorial files have been previously downloaded to avoid downloading the files using `wget` each time.; ",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:1786,Testability,test,tested,1786,"Try to keep example descriptions short and concise. ### Additional Information; - This is not required. If needed use the heading `**Notes**`. Additional subsections can be added.; - Put short python expressions or code snippets and Hail expressions; in double-``. ### Annotations; - For commands that create annotations, have ""Annotations"" section that lists the annotations. Use a bulleted list with the following format:. ```; **Annotations**. - **annotation** (*Type*) -- description; ```. ### Parameter and Return Type Specification; - Use :param:, :rtype: (if not None), :return:.; - :return: gives a short description of what is being returned. Example: `A VariantDataset that has been annotated.`. ### General Style; - links: `this is a description <with a url>`_.; - subsections: use `**`, we'll look into something better; - All function/command references should use a Sphinx directive to link ```:py:meth:`~hail.VariantDataset.vep````. ## Code Examples; - All examples are automatically tested with the [Sphinx doctest extension](https://www.sphinx-doc.org/en/stable/ext/doctest.html) to make sure they run with no errors. The content of the result is not checked.; - All input files required must be placed in `python/hail/docs/data`. When referencing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring w",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:3042,Testability,test,test,3042,"cing the files in the code example, the input directory is `data/` and the output directory is `output/`.; - Each command should start with `>>>`. If the command statement is on multiple lines, use `...` for each subsequent line.; - To skip execution of a command, see the `vep` example in `dataset.py`. Please try not to use this unless absolutely necessary.; - The HailContext `hc` is in scope and the following import statements have been run:. ```; from hail import *; from hail.genetics import *; from hail.expr import *; from hail.stats import *; ```. - Make sure you do not assign a result to the protected variable names `mt` in `dataset.py` and `kt1`, `kt2` in keytable.py.; - Variables not specified in the module-level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessary.; - Example MT files that currently exist in `python/hail/docs/data` were generated as follows:. **example1.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**.",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:3766,Testability,test,test,3766,"level docstring will not be available in the scope for other functions. Try not to add global variables unless absolutely necessary.; - Example MT files that currently exist in `python/hail/docs/data` were generated as follows:. **example1.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/examp",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt:4077,Testability,test,test,4077,"downsample_variants(10); ... .annotate_variants_expr('va.useInKinship = pcoin(0.9),; va.panel_maf = 0.1,; va.anno1 = 5,; va.anno2 = 0,; va.consequence = ""LOF"",; va.gene = ""A"",; va.score = 5.0'); ... .split_multi(); ... .variant_qc(); ... .sample_qc(); ... .annotate_samples_expr('sa.isCase = true,; sa.pheno.isCase = pcoin(0.5),; sa.pheno.isFemale = pcoin(0.5),; sa.pheno.age=rnorm(65, 10),; sa.cov.PC1 = rnorm(0,1),; sa.pheno.height = rnorm(70, 10),; sa.cov1 = rnorm(0, 1),; sa.cov2 = rnorm(0,1),; sa.pheno.bloodPressure= rnorm(120,20),; sa.pheno.cohortName = ""cohort1""'); ... .write(""python/hail/docs/data/example.mt"", overwrite=True)); ```. **example2.mt**. ```; >>> (hc.import(""src/test/resources/sample.vcf.bgz""); ... .downsample_variants(5); ... .annotate_variants_expr('va.anno1 = 5,; va.toKeep1 = true,; va.toKeep2 = false,; va.toKeep3 = true'); ... .split_multi(); ... .write(""python/hail/docs/data/example2.mt"", overwrite=True)); ```. **example_lmmreg.mt**. ```; >>> (hc.import_vcf('src/test/resources/sample.vcf'); ... .split_multi(); ... .variant_qc(); ... .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); ... .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); ... .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); ... .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); ... .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); ... .write(""python/hail/docs/data/example_lmmreg.mt"", overwrite=True)). **example_burden.mt**. ```; >>> (hc.import_vcf('python/hail/docs/data/example_burden.vcf'); ... .annotate_samples_table('python/hail/docs/data/example_burden.tsv', 'Sample', root = 'sa.burden',; ... config = TextTableConfig(impute=True)); ... .annotate_variants_expr('va.weight = v.start.toDouble'); ... .variant_qc(); ... .annotate_variants_intervals('python/hail/docs/data/genes.interval_list', 'va.genes', all=True); ... .annotate_variants_intervals('pyt",MatchSource.DOCS,hail/python/hail/docs/style-guide.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/style-guide.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt:24,Performance,scalab,scalable,24,"Hail is an open-source, scalable framework for exploring and analyzing genetic data. Starting from sequencing or microarray data in VCF and other formats, Hail can, for example:. generate variant annotations like call rate, Hardy-Weinberg equilibrium p-value, and population-specific allele count; generate sample annotations like mean depth, imputed sex, and TiTv ratio; load variant and sample annotations from text tables, JSON, VCF, VEP, and locus interval files; generate new annotations from existing annotations and the genotypes, and use these to filter samples, variants, and genotypes; find Mendelian violations in trios, analyze genetic similarity between samples via the GRM and IBD matrix, and compute sample scores and variant loadings using PCA; perform association analyses using linear, logistic, and linear mixed regression, and estimate heritability",MatchSource.DOCS,hail/python/hail/docs/data/file.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt:372,Performance,load,load,372,"Hail is an open-source, scalable framework for exploring and analyzing genetic data. Starting from sequencing or microarray data in VCF and other formats, Hail can, for example:. generate variant annotations like call rate, Hardy-Weinberg equilibrium p-value, and population-specific allele count; generate sample annotations like mean depth, imputed sex, and TiTv ratio; load variant and sample annotations from text tables, JSON, VCF, VEP, and locus interval files; generate new annotations from existing annotations and the genotypes, and use these to filter samples, variants, and genotypes; find Mendelian violations in trios, analyze genetic similarity between samples via the GRM and IBD matrix, and compute sample scores and variant loadings using PCA; perform association analyses using linear, logistic, and linear mixed regression, and estimate heritability",MatchSource.DOCS,hail/python/hail/docs/data/file.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt:741,Performance,load,loadings,741,"Hail is an open-source, scalable framework for exploring and analyzing genetic data. Starting from sequencing or microarray data in VCF and other formats, Hail can, for example:. generate variant annotations like call rate, Hardy-Weinberg equilibrium p-value, and population-specific allele count; generate sample annotations like mean depth, imputed sex, and TiTv ratio; load variant and sample annotations from text tables, JSON, VCF, VEP, and locus interval files; generate new annotations from existing annotations and the genotypes, and use these to filter samples, variants, and genotypes; find Mendelian violations in trios, analyze genetic similarity between samples via the GRM and IBD matrix, and compute sample scores and variant loadings using PCA; perform association analyses using linear, logistic, and linear mixed regression, and estimate heritability",MatchSource.DOCS,hail/python/hail/docs/data/file.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt:761,Performance,perform,perform,761,"Hail is an open-source, scalable framework for exploring and analyzing genetic data. Starting from sequencing or microarray data in VCF and other formats, Hail can, for example:. generate variant annotations like call rate, Hardy-Weinberg equilibrium p-value, and population-specific allele count; generate sample annotations like mean depth, imputed sex, and TiTv ratio; load variant and sample annotations from text tables, JSON, VCF, VEP, and locus interval files; generate new annotations from existing annotations and the genotypes, and use these to filter samples, variants, and genotypes; find Mendelian violations in trios, analyze genetic similarity between samples via the GRM and IBD matrix, and compute sample scores and variant loadings using PCA; perform association analyses using linear, logistic, and linear mixed regression, and estimate heritability",MatchSource.DOCS,hail/python/hail/docs/data/file.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt:804,Testability,log,logistic,804,"Hail is an open-source, scalable framework for exploring and analyzing genetic data. Starting from sequencing or microarray data in VCF and other formats, Hail can, for example:. generate variant annotations like call rate, Hardy-Weinberg equilibrium p-value, and population-specific allele count; generate sample annotations like mean depth, imputed sex, and TiTv ratio; load variant and sample annotations from text tables, JSON, VCF, VEP, and locus interval files; generate new annotations from existing annotations and the genotypes, and use these to filter samples, variants, and genotypes; find Mendelian violations in trios, analyze genetic similarity between samples via the GRM and IBD matrix, and compute sample scores and variant loadings using PCA; perform association analyses using linear, logistic, and linear mixed regression, and estimate heritability",MatchSource.DOCS,hail/python/hail/docs/data/file.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file1.txt:15,Usability,simpl,simply,15,"Lorem Ipsum is simply dummy text of the printing and typesetting industry.; Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.; It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.; ",MatchSource.DOCS,hail/python/hail/docs/data/file1.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file1.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file2.txt:41,Deployability,release,release,41,"It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.",MatchSource.DOCS,hail/python/hail/docs/data/file2.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/data/file2.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:117,Modifiability,extend,extending,117,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:879,Modifiability,rewrite,rewrite,879,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:1201,Modifiability,variab,variable,1201,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:818,Performance,optimiz,optimizations,818,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:457,Testability,test,test,457,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:514,Testability,test,test,514,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:545,Testability,test,test,545,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:621,Testability,test,test,621,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:740,Testability,test,test,740,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:768,Testability,test,test,768,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt:867,Usability,simpl,simplifying,867,"This is the list of things you need to do to add a new IR node. * All nodes. - Add the node in Scala as a case class extending IR / TableIR /; MatrixIR / BlockMatrixIR. Use scala collections so that case class equality works; as expected (no java arrays). - Add the node in ir.py / table_ir.py / matrix_ir.py / blockmatrix_ir.py. - Extend Pretty in Scala to print the IR. Add `render` in Python.; Add the IR to the IR parser. - Add an example to the Python test IRTests. - Add an IR generator to the pretty/parser test suite in IRSuite. - Add a test case to IRSuite, TableIRSuite, MatrixIRSuite, or BlockMatrixIRSuite to test; the nodes behavior. - Check all cases involving missingness. - Add support for the IR in PruneDeadFields. - Add test cases to PruneSuite to test the memoize and rebuild behaviors. - Add any optimizations to Optimize,; - In particular, any simplifying rewrite rules to Simplify. * (value) IR. - Add a rule to Typecheck. - It must define its type inference rule in InferType/InferPType. - Support it in Children and Copy. - Implement it in Interpret or add it to Interpretable as false. - Implement it in Emit (the compiler) or add it to Compilable as false. - If it binds a variable, add support to Bindings. - [Optional] Add a case in ExtractIntervalFilters. * MatrixIR. - Define copy, partitionCounts, columnCount, typ, and execute. - add to LiftLiterals if necessary. * TableIR. - Define copy, partitionCounts, typ, and execute. - add to LiftLiterals if necessary. * BlockMatrixIR. - Define copy, typ and execute. ",MatchSource.DOCS,hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/src/main/scala/is/hail/expr/ir/add-ir-checklist.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:568,Deployability,deploy,deploy,568,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:1783,Energy Efficiency,monitor,monitoring,1783,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:732,Performance,cache,cache-pr,732,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:895,Performance,cache,cache,895,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:395,Testability,test,test-pr,395,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:563,Testability,test,test-deploy,563,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt:1809,Testability,test,test-ci-utils,1809,"[; {; ""name"": ""delete_untagged"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""untagged""; }; },; {; ""name"": ""delete_dev"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""dev-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_pr"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-pr-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_test_deploy"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""test-deploy-""; ],; ""olderThan"": ""3d""; }; },; {; ""name"": ""delete_pr_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-pr-""; ],; ""olderThan"": ""7d""; }; },; {; ""name"": ""delete_cache"",; ""action"": {; ""type"": ""Delete""; },; ""condition"": {; ""tagState"": ""tagged"",; ""tagPrefixes"": [; ""cache-""; ],; ""olderThan"": ""30d""; }; },; {; ""name"": ""keep_third_party"",; ""action"": {; ""type"": ""Keep""; },; ""condition"": {; ""tagState"": ""any"",; ""packageNamePrefixes"": [; ""alpine"",; ""debian"",; ""envoyproxy/envoy"",; ""ghost"",; ""google/cloud-sdk"",; ""grafana/grafana"",; ""jupyter/scipy-notebook"",; ""moby/buildkit"",; ""python"",; ""redis"",; ""ubuntu""; ]; }; },; {; ""name"": ""keep_most_recent_deploy"",; ""action"": {; ""type"": ""Keep""; },; ""mostRecentVersions"": {; ""packageNamePrefixes"": [; ""admin-pod"",; ""auth"",; ""base"",; ""batch"",; ""batch-worker"",; ""blog_nginx"",; ""ci"",; ""ci-hello"",; ""ci-utils"",; ""create_certs_image"",; ""git-make-bash"",; ""gpu"",; ""hail-buildkit"",; ""hail-dev"",; ""hail-run"",; ""hail-ubuntu"",; ""hail-ubuntu-py-3-10"",; ""hail-ubuntu-python-3-10"",; ""hail-ubuntu-python-3-11"",; ""hailgenetics/hail"",; ""hailgenetics/hailtop"",; ""hailgenetics/vep-grch37-85"",; ""hailgenetics/vep-grch38-95"",; ""linting"",; ""monitoring"",; ""netcat"",; ""test-ci-utils"",; ""test_hello_create_certs_image"",; ""volume"",; ""website"",; ""workdir""; ],; ""keepCount"": 10; }; }; ]; ",MatchSource.DOCS,infra/gcp-broad/gcp-ar-cleanup-policy.txt,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/infra/gcp-broad/gcp-ar-cleanup-policy.txt
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/account-creating.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Creating Account{% endblock %}; {% block content %}. Creating account for {{ username }}. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/account-creating.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/account-creating.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/account-error.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Account Error{% endblock %}; {% block content %}. Account is in a bad state!. Please contact hail-team@broadinstitute.org. Username: {{ username }}; Login ID: {{ login_id }}; State: {{ state }}. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/account-error.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/account-error.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Copy Paste Token{% endblock %}; {% block content %}; ; {{ userdata['username'] }}; Copy Paste Token. This copy paste token is good for one use in the next five minutes.; ; {{ copy_paste_token }}. If you need to authenticate a Jupyter Notebook session (for example, a; Terra Jupyter Notebook), copy and paste this into your notebook:; ; from hailtop.auth import copy_paste_login; copy_paste_login('{{ copy_paste_token }}'). If you need to authenticate from a terminal, copy and paste this into your; terminal:; ; hailctl auth copy-paste-login ""{{ copy_paste_token }}"". Create another token for copy-paste login. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/copy-paste-token.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html:257,Security,authenticat,authenticate,257,"{% extends ""layout.html"" %}; {% block title %}Copy Paste Token{% endblock %}; {% block content %}; ; {{ userdata['username'] }}; Copy Paste Token. This copy paste token is good for one use in the next five minutes.; ; {{ copy_paste_token }}. If you need to authenticate a Jupyter Notebook session (for example, a; Terra Jupyter Notebook), copy and paste this into your notebook:; ; from hailtop.auth import copy_paste_login; copy_paste_login('{{ copy_paste_token }}'). If you need to authenticate from a terminal, copy and paste this into your; terminal:; ; hailctl auth copy-paste-login ""{{ copy_paste_token }}"". Create another token for copy-paste login. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/copy-paste-token.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html:484,Security,authenticat,authenticate,484,"{% extends ""layout.html"" %}; {% block title %}Copy Paste Token{% endblock %}; {% block content %}; ; {{ userdata['username'] }}; Copy Paste Token. This copy paste token is good for one use in the next five minutes.; ; {{ copy_paste_token }}. If you need to authenticate a Jupyter Notebook session (for example, a; Terra Jupyter Notebook), copy and paste this into your notebook:; ; from hailtop.auth import copy_paste_login; copy_paste_login('{{ copy_paste_token }}'). If you need to authenticate from a terminal, copy and paste this into your; terminal:; ; hailctl auth copy-paste-login ""{{ copy_paste_token }}"". Create another token for copy-paste login. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/copy-paste-token.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html:582,Testability,log,login,582,"{% extends ""layout.html"" %}; {% block title %}Copy Paste Token{% endblock %}; {% block content %}; ; {{ userdata['username'] }}; Copy Paste Token. This copy paste token is good for one use in the next five minutes.; ; {{ copy_paste_token }}. If you need to authenticate a Jupyter Notebook session (for example, a; Terra Jupyter Notebook), copy and paste this into your notebook:; ; from hailtop.auth import copy_paste_login; copy_paste_login('{{ copy_paste_token }}'). If you need to authenticate from a terminal, copy and paste this into your; terminal:; ; hailctl auth copy-paste-login ""{{ copy_paste_token }}"". Create another token for copy-paste login. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/copy-paste-token.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html:650,Testability,log,login,650,"{% extends ""layout.html"" %}; {% block title %}Copy Paste Token{% endblock %}; {% block content %}; ; {{ userdata['username'] }}; Copy Paste Token. This copy paste token is good for one use in the next five minutes.; ; {{ copy_paste_token }}. If you need to authenticate a Jupyter Notebook session (for example, a; Terra Jupyter Notebook), copy and paste this into your notebook:; ; from hailtop.auth import copy_paste_login; copy_paste_login('{{ copy_paste_token }}'). If you need to authenticate from a terminal, copy and paste this into your; terminal:; ; hailctl auth copy-paste-login ""{{ copy_paste_token }}"". Create another token for copy-paste login. {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/copy-paste-token.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/copy-paste-token.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/index.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Home{% endblock %}; {% block head %}. {% endblock %}; {% block content %}. Hail; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/index.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/roles.html:49,Modifiability,extend,extends,49,"{% from ""utils.html"" import submit_button %}; {% extends ""layout.html"" %}; {% block title %}Roles{% endblock %}; {% block content %}; Create Role. Name. {{ submit_button('Create') }}; . Roles. ID; Name. {% for role in roles %}; ; {{ role['id'] }}; {{ role['name'] }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/roles.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/roles.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html:49,Modifiability,extend,extends,49,"{% from ""utils.html"" import submit_button %}; {% extends ""layout.html"" %}; {% block title %}User{% endblock %}; {% block content %}. {{ userdata['username'] }}. {{ submit_button('Log out') }}; ; {% if cloud == ""gcp"" %}; Google Service Account: {{ userdata['display_name'] }}; {% endif %}; {% if cloud == ""azure"" %}; Azure Service Principal Display Name: {{ userdata['display_name'] }}; Azure Service Principal Application ID: {{ userdata['hail_identity'] }}; {% endif %}; Trial Billing Project: {{ userdata['trial_bp_name'] }}. {{ submit_button('Get a copy-paste login token') }}; . Notice: The Hail system records your email address and IP address. Your email address; is recorded so that we can authenticate you. Your IP address is tracked as part of our; surveillance of all traffic to and from the Hail system. This broad surveillance enables the; protection of the Hail system from malicious actors.; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/user.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html:697,Security,authenticat,authenticate,697,"{% from ""utils.html"" import submit_button %}; {% extends ""layout.html"" %}; {% block title %}User{% endblock %}; {% block content %}. {{ userdata['username'] }}. {{ submit_button('Log out') }}; ; {% if cloud == ""gcp"" %}; Google Service Account: {{ userdata['display_name'] }}; {% endif %}; {% if cloud == ""azure"" %}; Azure Service Principal Display Name: {{ userdata['display_name'] }}; Azure Service Principal Application ID: {{ userdata['hail_identity'] }}; {% endif %}; Trial Billing Project: {{ userdata['trial_bp_name'] }}. {{ submit_button('Get a copy-paste login token') }}; . Notice: The Hail system records your email address and IP address. Your email address; is recorded so that we can authenticate you. Your IP address is tracked as part of our; surveillance of all traffic to and from the Hail system. This broad surveillance enables the; protection of the Hail system from malicious actors.; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/user.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html:563,Testability,log,login,563,"{% from ""utils.html"" import submit_button %}; {% extends ""layout.html"" %}; {% block title %}User{% endblock %}; {% block content %}. {{ userdata['username'] }}. {{ submit_button('Log out') }}; ; {% if cloud == ""gcp"" %}; Google Service Account: {{ userdata['display_name'] }}; {% endif %}; {% if cloud == ""azure"" %}; Azure Service Principal Display Name: {{ userdata['display_name'] }}; Azure Service Principal Application ID: {{ userdata['hail_identity'] }}; {% endif %}; Trial Billing Project: {{ userdata['trial_bp_name'] }}. {{ submit_button('Get a copy-paste login token') }}; . Notice: The Hail system records your email address and IP address. Your email address; is recorded so that we can authenticate you. Your IP address is tracked as part of our; surveillance of all traffic to and from the Hail system. This broad surveillance enables the; protection of the Hail system from malicious actors.; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/user.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/user.html
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/users.html:91,Modifiability,extend,extends,91,"{% from ""utils.html"" import submit_button, danger_button, success_check, fail_cross %}; {% extends ""layout.html"" %}; {% block title %}Users{% endblock %}. {% macro check_or_cross(b) %}; {% if b %}; {{ success_check() }}; {% else %}; {{ fail_cross() }}; {% endif %}; {% endmacro %}. {% block content %}. Create User. Username. Login ID. Developer; Service Account. {{ submit_button('Create') }}; . Delete User. User ID. Username. {{ danger_button('Delete') }}; . Users. ID; Username; Login ID; Hail Identity; State; Developer; Robot. {% for user in users %}; ; {{ user['id'] }}; {{ user['username'] }}; {{ user['login_id'] }}; {{ user['hail_identity'] }}; {{ user['state'] }}; {{ check_or_cross(user['is_developer']) }}; {{ check_or_cross(user['is_service_account']) }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,auth/auth/templates/users.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/templates/users.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/index.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Batch Status{% endblock %}; {% block content %}; Globals. instance ID: {{ instance_id }}; ready cores: {{ ready_cores_mcpu / 1000 }}. {% if not frozen %}; . Freeze; . {% else %}; . Unfreeze; . {% endif %}; ; Feature Flags. compact_billing_tables: . oms_agent: . Update. Instance Collections. Name; Instances. Cores. Schedulable Cores. Pending; Active; Inactive; Deleted. Pending; Active; Inactive; Deleted. Free; Total; % Free. {% for pool in pools %}; ; {{ pool.name }}; {{ pool.all_versions_instances_by_state['pending'] }}; {{ pool.all_versions_instances_by_state['active'] }}; {{ pool.all_versions_instances_by_state['inactive'] }}; {{ pool.all_versions_instances_by_state['deleted'] }}. {{ pool.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. {{ pool.current_worker_version_stats.active_schedulable_free_cores_mcpu / 1000 }}; {{ pool.current_worker_version_stats.cores_mcpu_by_state['active'] / 1000 }}; {% if pool.current_worker_version_stats.cores_mcpu_by_state['active'] != 0 %}; {{ (pool.current_worker_version_stats.active_schedulable_free_cores_mcpu * 100 / pool.current_worker_version_stats.cores_mcpu_by_state['active']) | round(1)}}%; {% else %}; ; {% endif %}; ; {% endfor %}; ; {{ jpim.name }}; {{ jpim.all_versions_instances_by_state['pending'] }}; {{ jpim.all_versions_instances_by_state['active'] }}; {{ jpim.all_versions_instances_by_state['inactive'] }}; {{ jpim.all_versions_instances_by_state['deleted'] }}. {{ jpim.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. Total; {{ global_n_instances_by_state['pending'] }}; {{ global_n_instances_b",MatchSource.DOCS,batch/batch/driver/templates/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/index.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/job_private.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Job Private Instance Manager{% endblock %}; {% block content %}. {{ jpim.name }}; Configuration. Boot disk size (in GB): ; Max instances: ; Max live instances: ; Max new instances per autoscaler loop: ; Autoscaler loop period in seconds: ; Worker max idle time in seconds: . Update; . User Resources. User; Ready Jobs; Allocated Jobs; Creating Jobs; Running Jobs. {% for user in user_resources %}; ; {{ user['user'] }}; {{ user['n_ready_jobs'] }}; {{ user['n_allocated_jobs'] }}; {{ user['n_creating_jobs'] }}; {{ user['n_running_jobs'] }}. {% endfor %}; . Status. Ready jobs: {{ n_ready_jobs }}; Creating jobs: {{ n_creating_jobs }}; Running jobs: {{ n_running_jobs }}. Instances. Cores. Pending; Active; Inactive; Deleted. Pending; Active; Inactive; Deleted. {{ jpim.all_versions_instances_by_state['pending'] }}; {{ jpim.all_versions_instances_by_state['active'] }}; {{ jpim.all_versions_instances_by_state['inactive'] }}; {{ jpim.all_versions_instances_by_state['deleted'] }}. {{ jpim.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ jpim.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. Instances. Name; Machine Type; Preemptible; Location; Version; State; Free Cores; Failed Requests; Time Created; Last Updated. {% for instance in instances %}; ; {{ instance.name }}; {{ instance.machine_type }}; {{ instance.preemptible }}; {{ instance.location }}; {{ instance.version }}; {{ instance.state }}; {{ instance.free_cores_mcpu / 1000 }} / {{ instance.cores_mcpu / 1000 }}; {{ instance.failed_request_count }}; {{ instance.time_created_str() }}; {{ instance.last_updated_str() }} ago. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,batch/batch/driver/templates/job_private.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/job_private.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html:576,Energy Efficiency,schedul,scheduling,576,"{% extends ""layout.html"" %}; {% block title %}Pool{% endblock %}; {% block content %}. {{ pool.name }}; Configuration. Worker type: {{ pool.worker_type }}; Worker preemptible: {{ pool.preemptible }}; Worker cores: ; Worker boot disk size (in GB): . Worker should use a Local SSD: . Worker External SSD data disk size (in GB): ; Standing worker cores: ; Min instances: ; Max instances: ; Max live instances: ; Max new instances per autoscaler loop: ; Autoscaler loop period in seconds: ; Worker max idle time in seconds: ; Standing worker max idle time in seconds: ; Job queue scheduling window in seconds: . Update; . User Resources. User; Ready Jobs; Ready Cores; Allocated Cores; Running Jobs; Running Cores. {% for user in user_resources %}; ; {{ user['user'] }}; {{ user['n_ready_jobs'] }}; {{ user['ready_cores_mcpu'] / 1000 }}; {{ user['allocated_cores_mcpu'] / 1000 }}; {{ user['n_running_jobs'] }}; {{ user['running_cores_mcpu'] / 1000 }}. {% endfor %}; . Status. Ready cores: {{ ready_cores_mcpu / 1000 }}. Instances. Cores. Schedulable Cores. Pending; Active; Inactive; Deleted. Pending; Active; Inactive; Deleted. Free; Total; % Free. {{ pool.all_versions_instances_by_state['pending'] }}; {{ pool.all_versions_instances_by_state['active'] }}; {{ pool.all_versions_instances_by_state['inactive'] }}; {{ pool.all_versions_instances_by_state['deleted'] }}. {{ pool.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. {{ pool.current_worker_version_stats.active_schedulable_free_cores_mcpu / 1000 }}; {{ pool.current_worker_version_stats.cores_mcpu_by_state['active'] / 1000 }}; {% if pool.current_worker_version_stats.cores_mcpu_by_state['active'] != 0 %}; {{ (pool.current_worker_version_stats.active_schedulable_free_cores_mcpu * 100 / pool.current_worker_version_stats.cores_mcpu_by_state['active'",MatchSource.DOCS,batch/batch/driver/templates/pool.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Pool{% endblock %}; {% block content %}. {{ pool.name }}; Configuration. Worker type: {{ pool.worker_type }}; Worker preemptible: {{ pool.preemptible }}; Worker cores: ; Worker boot disk size (in GB): . Worker should use a Local SSD: . Worker External SSD data disk size (in GB): ; Standing worker cores: ; Min instances: ; Max instances: ; Max live instances: ; Max new instances per autoscaler loop: ; Autoscaler loop period in seconds: ; Worker max idle time in seconds: ; Standing worker max idle time in seconds: ; Job queue scheduling window in seconds: . Update; . User Resources. User; Ready Jobs; Ready Cores; Allocated Cores; Running Jobs; Running Cores. {% for user in user_resources %}; ; {{ user['user'] }}; {{ user['n_ready_jobs'] }}; {{ user['ready_cores_mcpu'] / 1000 }}; {{ user['allocated_cores_mcpu'] / 1000 }}; {{ user['n_running_jobs'] }}; {{ user['running_cores_mcpu'] / 1000 }}. {% endfor %}; . Status. Ready cores: {{ ready_cores_mcpu / 1000 }}. Instances. Cores. Schedulable Cores. Pending; Active; Inactive; Deleted. Pending; Active; Inactive; Deleted. Free; Total; % Free. {{ pool.all_versions_instances_by_state['pending'] }}; {{ pool.all_versions_instances_by_state['active'] }}; {{ pool.all_versions_instances_by_state['inactive'] }}; {{ pool.all_versions_instances_by_state['deleted'] }}. {{ pool.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. {{ pool.current_worker_version_stats.active_schedulable_free_cores_mcpu / 1000 }}; {{ pool.current_worker_version_stats.cores_mcpu_by_state['active'] / 1000 }}; {% if pool.current_worker_version_stats.cores_mcpu_by_state['active'] != 0 %}; {{ (pool.current_worker_version_stats.active_schedulable_free_cores_mcpu * 100 / pool.current_worker_version_stats.cores_mcpu_by_state['active'",MatchSource.DOCS,batch/batch/driver/templates/pool.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html:570,Performance,queue,queue,570,"{% extends ""layout.html"" %}; {% block title %}Pool{% endblock %}; {% block content %}. {{ pool.name }}; Configuration. Worker type: {{ pool.worker_type }}; Worker preemptible: {{ pool.preemptible }}; Worker cores: ; Worker boot disk size (in GB): . Worker should use a Local SSD: . Worker External SSD data disk size (in GB): ; Standing worker cores: ; Min instances: ; Max instances: ; Max live instances: ; Max new instances per autoscaler loop: ; Autoscaler loop period in seconds: ; Worker max idle time in seconds: ; Standing worker max idle time in seconds: ; Job queue scheduling window in seconds: . Update; . User Resources. User; Ready Jobs; Ready Cores; Allocated Cores; Running Jobs; Running Cores. {% for user in user_resources %}; ; {{ user['user'] }}; {{ user['n_ready_jobs'] }}; {{ user['ready_cores_mcpu'] / 1000 }}; {{ user['allocated_cores_mcpu'] / 1000 }}; {{ user['n_running_jobs'] }}; {{ user['running_cores_mcpu'] / 1000 }}. {% endfor %}; . Status. Ready cores: {{ ready_cores_mcpu / 1000 }}. Instances. Cores. Schedulable Cores. Pending; Active; Inactive; Deleted. Pending; Active; Inactive; Deleted. Free; Total; % Free. {{ pool.all_versions_instances_by_state['pending'] }}; {{ pool.all_versions_instances_by_state['active'] }}; {{ pool.all_versions_instances_by_state['inactive'] }}; {{ pool.all_versions_instances_by_state['deleted'] }}. {{ pool.all_versions_cores_mcpu_by_state['pending'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['active'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['inactive'] / 1000 }}; {{ pool.all_versions_cores_mcpu_by_state['deleted'] / 1000 }}. {{ pool.current_worker_version_stats.active_schedulable_free_cores_mcpu / 1000 }}; {{ pool.current_worker_version_stats.cores_mcpu_by_state['active'] / 1000 }}; {% if pool.current_worker_version_stats.cores_mcpu_by_state['active'] != 0 %}; {{ (pool.current_worker_version_stats.active_schedulable_free_cores_mcpu * 100 / pool.current_worker_version_stats.cores_mcpu_by_state['active'",MatchSource.DOCS,batch/batch/driver/templates/pool.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/pool.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/quotas.html:163,Availability,avail,available,163,"{% extends ""layout.html"" %}; {% block title %}Quotas{% endblock %}; {% block content %}. Quotas by Region; {% if plot_json is not none %}. {% else %}; Data is not available.; {% endif %}; {% endblock %}; ",MatchSource.DOCS,batch/batch/driver/templates/quotas.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/quotas.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/quotas.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Quotas{% endblock %}; {% block content %}. Quotas by Region; {% if plot_json is not none %}. {% else %}; Data is not available.; {% endif %}; {% endblock %}; ",MatchSource.DOCS,batch/batch/driver/templates/quotas.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/quotas.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/user_resources.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Batch User Resources{% endblock %}; {% block content %}. User Resources. Name; Ready Jobs; Ready Cores; Running Jobs; Running Cores. {% for user in user_resources %}; ; {{ user['user'] }}; {{ user['n_ready_jobs'] }}; {{ user['ready_cores_mcpu'] / 1000 }}; {{ user['n_running_jobs'] }}; {{ user['running_cores_mcpu'] / 1000 }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,batch/batch/driver/templates/user_resources.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/templates/user_resources.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/batch.html:280,Modifiability,extend,extends,280,"{% from ""table_search.html"" import table_search with context %}; {% from ""components/metadata_tables.html"" import kv_table, resource_cost_table, collapsible_li %}; {% from ""utils.html"" import; batch_state_indicator, job_state_indicator, danger_button, submit_button, link; %}; {% extends ""layout.html"" %}; {% block title %}Batch {{ batch['id'] }}{% endblock %}; {% block head %}. {% endblock %}; {% block content %}. Batch {{ batch['id'] }}; {{ batch_state_indicator(batch) }}; . {% if 'attributes' in batch and 'name' in batch['attributes'] %}; {{ batch['attributes']['name'] }}; {% endif %}; . Submitted by {{ batch['user'] }}; Billed to {{ batch['billing_project'] }}. {% if not batch['complete'] and batch['state'] != 'Cancelled' %}; . {% if q is not none %}; ; {% endif %}; {{ danger_button('Cancel') }}; ; {% elif batch['complete'] %}; . {{ danger_button('Delete') }}; ; {% endif %}; . {% call collapsible_li(true, 'Jobs', batch['n_jobs']) %}; {{ kv_table({; 'Incomplete (Blocked, Queued or Running)': batch['n_jobs'] - batch['n_completed'],; 'Succeeded': batch['n_succeeded'],; 'Failed': batch['n_failed'],; 'Cancelled': batch['n_cancelled']; })}}; {% endcall %}. {% if 'attributes' in batch and batch['attributes'] %}; {% call collapsible_li(false, 'Attributes', '') %}; {{ kv_table(batch['attributes']) }}; {% endcall %}; {% endif %}. {% call collapsible_li(false, 'Duration', batch.get('duration') or '') %}; {{ kv_table({; 'Created': batch.get('time_created') or '',; 'Completed': batch.get('time_completed') or '',; })}}; {% endcall %}. {% call collapsible_li(false, 'Cost', batch.get('cost')) %}; {{ resource_cost_table(batch['cost_breakdown'] or {}) }}; {% endcall %}; . {{ table_search(""job-search"", base_path ~ ""/batches/"" ~ batch[""id""]) }}; . ID. Name; Exit Code; Duration; Cost. {% for job in batch['jobs'] %}; . {{ link(base_path ~ '/batches/' ~ job['batch_id'] ~ '/jobs/' ~ job['job_id'], job['job_id']) }}; . {% if 'name' in job and job['name'] is not none %}; ; {{ link(base_path",MatchSource.DOCS,batch/batch/front_end/templates/batch.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/batch.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/batches.html:159,Modifiability,extend,extends,159,"{% from ""table_search.html"" import table_search with context %}; {% from ""utils.html"" import batch_state_indicator, submit_button, link, truncated_link %}; {% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block head %}. {% endblock %}; {% block content %}. Batches; {{ table_search(""batch-search"", base_path ~ ""/batches"") }}; . ID; Batch Name; Billing Project; Job Statuses; Created; Completed; Duration; Cost. {% for batch in batches %}; . {{ batch_state_indicator(batch) }}; . {{ link(base_path ~ ""/batches/"" ~ batch['id'], batch['id']) }}; . {% if 'attributes' in batch and 'name' in batch['attributes'] %}; {{ truncated_link(base_path ~ ""/batches/"" ~ batch['id'], batch['attributes']['name']) }}; {% else %}; ; {{ truncated_link(base_path ~ ""/batches/"" ~ batch['id'], 'no name') }}; ; {% endif %}; . {{ batch['billing_project'] }}. {% set statuses = [] %}; {% if batch['n_jobs'] - batch['n_completed'] != 0 %}; {% do statuses.append((batch['n_jobs'] - batch['n_completed'])|string ~ ' pending') %}; {% endif %}; {% if batch['n_succeeded'] != 0 %}; {% do statuses.append(batch['n_succeeded'] ~ ' succeeded') %}; {% endif %}; {% if batch['n_cancelled'] != 0 %}; {% do statuses.append(batch['n_cancelled'] ~ ' cancelled') %}; {% endif %}; {% if batch['n_failed'] != 0 %}; {% do statuses.append(batch['n_failed'] ~ ' failed') %}; {% endif %}; {{ statuses|join(', ') }}; . {{ batch.get('time_created') or '' }}; . {{ batch.get('time_completed') or '--' }}; . {{ batch.get('duration') or '' }}; . {{ batch.get('cost') or '' }}; . {% endfor %}; . {% if last_batch_id is not none %}; . {% if q is not none %}; ; {% endif %}; ; {{ submit_button('Next page') }}; . {% endif %}. {% endblock %}; ",MatchSource.DOCS,batch/batch/front_end/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing.html:49,Modifiability,extend,extends,49,"{% from ""utils.html"" import submit_button %}; {% extends ""layout.html"" %}; {% block title %}Billing{% endblock %}; {% block content %}. Start. End (inclusive). {{ submit_button('Submit') }}; . Start must be a date in the format MM/DD/YYYY. End is an optional date in the format; MM/DD/YYYY. Leave End empty to include currently running batches. If End is not empty,; then no currently running batches are included. All dates search for batches that have; completed within that time interval (inclusive). Total spend: {{ total_cost }}. {% if is_developer %}; ; By Billing Project; . By User; ; {% endif %}; ; By Billing Project and User; . {% if is_developer %}; . {% for row in billing_by_project %}; ; {{ row['billing_project'] }}; {{ row['cost'] }}. {% endfor %}; . {% for row in billing_by_user %}; ; {{ row['user'] }}; {{ row['cost'] }}. {% endfor %}; . {% endif %}; . {% for row in billing_by_project_user %}; ; {{ row['billing_project'] }}; {{ row['user'] }}; {{ row['cost'] }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,batch/batch/front_end/templates/billing.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing_limits.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Billing Limits{% endblock %}; {% block content %}; Billing Projects Limits. Billing Project; Accrued Cost; Limit. {% for row in open_billing_projects %}; ; {{ row['billing_project'] }}; {{ row['accrued_cost'] }}; {{ row['limit'] }}. {% endfor %}; ; Closed projects. {% for row in closed_billing_projects %}; ; {{ row['billing_project'] }}; {{ row['accrued_cost'] }}; {{ row['limit'] }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,batch/batch/front_end/templates/billing_limits.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing_limits.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing_projects.html:64,Modifiability,extend,extends,64,"{% from ""utils.html"" import danger_button, submit_button %}; {% extends ""layout.html"" %}; {% block title %}Billing Projects{% endblock %}; {% block content %}; Billing Projects. {{ submit_button('Create') }}. Billing Project; Limit; Users. {% for bp in billing_projects %}; . {{ bp['billing_project'] }}; . {{ submit_button('Update') }}; . {% for user in bp['users'] %}; ; {{ user }}. close. {% endfor %}; . {{ submit_button('Add') }}; . edit. {{ danger_button('Close') }}; . Cancel; . {% endfor %}; {% for bp in closed_projects %}; . {{ bp['billing_project'] }}; . {{ bp['limit'] }}; . {{ submit_button('Reopen') }}; . {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,batch/batch/front_end/templates/billing_projects.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/billing_projects.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/job.html:693,Availability,down,download,693,"{% from ""components/metadata_tables.html"" import collapsible_li, resource_cost_table, kv_table %}; {% from ""utils.html"" import success_check, fail_cross, progress_spinner, job_state_indicator %}; {% extends ""layout.html"" %}; {% block title %}Batch {{ batch_id }} Job {{ job_id }}{% endblock %}. {% macro code_block(code) %}. {{ code }}. {% endmacro %}. {% macro error_and_logs_panel(step) %}; {% if container_statuses[step]['short_error'] is not none %}. Errored with: {{ container_statuses[step]['short_error']|capitalize }}. {% endif %}; {% if step in step_errors and step_errors[step] is not none %}; Error; {{ code_block(step_errors[step]) }}; {% endif %}; {% if step in job_log %}. Logs. download; . {{ code_block(job_log[step]) }}; {% endif %}; {% endmacro %}. {% macro step_state_indicator(state) %}; {% if state == 'running' %}; {{ progress_spinner('text-sky-600') }}; {% elif state == 'succeeded' %}; {{ success_check() }}; {% elif state == 'failed' %}; {{ fail_cross() }}; {% endif %}; {% endmacro %}. {% macro step_runtime(step_name) %}; {% set step = container_statuses[step_name] %}; {% if step and step['timing']['running'] and step['timing']['running']['duration'] %}. Command runtime: {{ step['timing']['running']['duration'] / 1000.0 }}s. {% endif %}; {% endmacro %}. {% block content %}. Batch {{ batch_id }}; . Batch {{ batch_id }} Job {{ job_id }}; {% if 'exit_code' in job and job['exit_code'] is not none and job['exit_code'] > 0 %}; ; Exit {{ job['exit_code'] }}; {{ fail_cross() }}; ; {% else %}; {{ job_state_indicator(job) }}; {% endif %}; . {% if 'attributes' in job and 'name' in job['attributes'] %}; {{ job['attributes']['name'] }}; {% endif %}; . Submitted by {{ job['user'] }}; Billed to {{ job['billing_project'] }}. {% if 'always_run' in job and job['always_run'] %}; Always Run; {% endif %}; . {% call collapsible_li(false, 'Environment Variables', '') %}; . {% if job_specification %}; {% for envvar in job_specification['env'] %}; ; {{ envvar['name'] }}; {{ envvar",MatchSource.DOCS,batch/batch/front_end/templates/job.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/job.html
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/job.html:199,Modifiability,extend,extends,199,"{% from ""components/metadata_tables.html"" import collapsible_li, resource_cost_table, kv_table %}; {% from ""utils.html"" import success_check, fail_cross, progress_spinner, job_state_indicator %}; {% extends ""layout.html"" %}; {% block title %}Batch {{ batch_id }} Job {{ job_id }}{% endblock %}. {% macro code_block(code) %}. {{ code }}. {% endmacro %}. {% macro error_and_logs_panel(step) %}; {% if container_statuses[step]['short_error'] is not none %}. Errored with: {{ container_statuses[step]['short_error']|capitalize }}. {% endif %}; {% if step in step_errors and step_errors[step] is not none %}; Error; {{ code_block(step_errors[step]) }}; {% endif %}; {% if step in job_log %}. Logs. download; . {{ code_block(job_log[step]) }}; {% endif %}; {% endmacro %}. {% macro step_state_indicator(state) %}; {% if state == 'running' %}; {{ progress_spinner('text-sky-600') }}; {% elif state == 'succeeded' %}; {{ success_check() }}; {% elif state == 'failed' %}; {{ fail_cross() }}; {% endif %}; {% endmacro %}. {% macro step_runtime(step_name) %}; {% set step = container_statuses[step_name] %}; {% if step and step['timing']['running'] and step['timing']['running']['duration'] %}. Command runtime: {{ step['timing']['running']['duration'] / 1000.0 }}s. {% endif %}; {% endmacro %}. {% block content %}. Batch {{ batch_id }}; . Batch {{ batch_id }} Job {{ job_id }}; {% if 'exit_code' in job and job['exit_code'] is not none and job['exit_code'] > 0 %}; ; Exit {{ job['exit_code'] }}; {{ fail_cross() }}; ; {% else %}; {{ job_state_indicator(job) }}; {% endif %}; . {% if 'attributes' in job and 'name' in job['attributes'] %}; {{ job['attributes']['name'] }}; {% endif %}; . Submitted by {{ job['user'] }}; Billed to {{ job['billing_project'] }}. {% if 'always_run' in job and job['always_run'] %}; Always Run; {% endif %}; . {% call collapsible_li(false, 'Environment Variables', '') %}; . {% if job_specification %}; {% for envvar in job_specification['env'] %}; ; {{ envvar['name'] }}; {{ envvar",MatchSource.DOCS,batch/batch/front_end/templates/job.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/templates/job.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batch.html:70,Modifiability,extend,extends,70,"{% from ""filtered-jobs.html"" import filtered_jobs with context %}; {% extends ""layout.html"" %}; {% block title %}Batch {{ batch['id'] }}{% endblock %}; {% block head %}; ; {% endblock %}; {% block content %}; Batch {{ batch['id'] }}; Timing. {% for field in ['time_created', 'time_completed', 'duration'] %}; {% if field in batch %}; {{ field }}: {{ batch[field] }}; {% endif %}; {% endfor %}; . {% if 'attributes' in batch %}; Attributes. {% for name, value in batch['attributes'].items() %}; {% if name == ""pr"" and wb is not none %}; {{ name }}: {{ value }}; {% else %}; {{ name }}: {{ value }}; {% endif %}; {% endfor %}; ; {% endif %}. Jobs; {{ filtered_jobs(running, failed, pending, completed) }}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batch.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batch.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html:225,Deployability,deploy,deploy,225,"{% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block content %}; Batches; {% if batches %}; . id; type; state. 	{% for batch in batches %}; ; {{ batch['id'] }}. {% if 'attributes' in batch %}; {% if 'deploy' in batch['attributes'] %}; deploy; {% elif 'test' in batch['attributes'] %}; test; {% else %}; unknown; {% endif %}; {% endif %}; . {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; {% if not batch['complete'] %}; running; {% endif %}; . {% endfor %}; . {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html:260,Deployability,deploy,deploy,260,"{% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block content %}; Batches; {% if batches %}; . id; type; state. 	{% for batch in batches %}; ; {{ batch['id'] }}. {% if 'attributes' in batch %}; {% if 'deploy' in batch['attributes'] %}; deploy; {% elif 'test' in batch['attributes'] %}; test; {% else %}; unknown; {% endif %}; {% endif %}; . {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; {% if not batch['complete'] %}; running; {% endif %}; . {% endfor %}; . {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block content %}; Batches; {% if batches %}; . id; type; state. 	{% for batch in batches %}; ; {{ batch['id'] }}. {% if 'attributes' in batch %}; {% if 'deploy' in batch['attributes'] %}; deploy; {% elif 'test' in batch['attributes'] %}; test; {% else %}; unknown; {% endif %}; {% endif %}; . {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; {% if not batch['complete'] %}; running; {% endif %}; . {% endfor %}; . {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html:277,Testability,test,test,277,"{% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block content %}; Batches; {% if batches %}; . id; type; state. 	{% for batch in batches %}; ; {{ batch['id'] }}. {% if 'attributes' in batch %}; {% if 'deploy' in batch['attributes'] %}; deploy; {% elif 'test' in batch['attributes'] %}; test; {% else %}; unknown; {% endif %}; {% endif %}; . {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; {% if not batch['complete'] %}; running; {% endif %}; . {% endfor %}; . {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html:310,Testability,test,test,310,"{% extends ""layout.html"" %}; {% block title %}Batches{% endblock %}; {% block content %}; Batches; {% if batches %}; . id; type; state. 	{% for batch in batches %}; ; {{ batch['id'] }}. {% if 'attributes' in batch %}; {% if 'deploy' in batch['attributes'] %}; deploy; {% elif 'test' in batch['attributes'] %}; test; {% else %}; unknown; {% endif %}; {% endif %}; . {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; {% if not batch['complete'] %}; running; {% endif %}; . {% endfor %}; . {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/batches.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/batches.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/index.html:60,Modifiability,extend,extends,60,"{% from ""pr-table.html"" import pr_table with context %}; {% extends ""layout.html"" %}; {% block title %}CI{% endblock %}; {% block head %}; . {% endblock %}; {% block content %}; CI; {% for wb in watched_branches %}; {{ wb.branch }}. SHA:; 	{% if wb.sha is not none %}; 	{{ wb.sha }}; 	{% else %}; 	unknown; 	{% endif %}; ; Deploy State:; 	{% if wb.deploy_state is not none %}; 	{{ wb.deploy_state }}; 	{% endif %}; ; Deploy Batch:; 	{% if wb.deploy_batch_id is not none %}; 	{{ wb.deploy_batch_id }}; 	{% endif %}; ; Merge Candidate:; {% if wb.merge_candidate is not none %}; {{ wb.merge_candidate }}; {% endif %}; . {% if not frozen_merge_deploy %}; . Freeze Merges & Deploys; . {% else %}; . Unfreeze Merges & Deploys; . {% endif %}; ; PRs; {% if wb.prs is not none %}; {% if wb.prs|length > 0 %}; {{ pr_table(wb, ""prs"", ""prsSearchBar"") }}; {% else %}; No PRs.; {% endif %}; {% else %}; Unknown.; {% endif %}; {% endfor %}. Authorize SHA. SHA:. Authorize. {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/index.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/namespaces.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Active Namespaces{% endblock %}; {% block content %}; Active Namespaces. Namespace; Creation Time; Expiration Time; Services. Name; Inbound Requests Per Second Limit. {% for ns in namespaces %}; . {{ ns['namespace'] }}; . {{ ns['creation_time'] }}; . {{ ns['expiration_time'] }}; . {% for service in ns['services'] %}; . {{ service }}; . {% if ns['services'][service] %}; ; {% else %}; ; {% endif %}; ; Update; . {% endfor %}; . Add. {% endfor %}. Add. {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/namespaces.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/namespaces.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/pr.html:746,Availability,error,error,746,"{% from ""filtered-jobs.html"" import filtered_jobs with context %}; {% extends ""layout.html"" %}; {% block title %}PR {{ number }}{% endblock %}; {% block head %}; ; {% endblock %}; {% block content %}; {{ pr.title }} #{{ pr.number }}. {% if batch is defined %}; ; batch: {{ batch['id'] }}; artifacts:; {{ artifacts_uri }}; open_in_new. cost: {{ batch['cost'] }}; {% for name, value in batch['attributes'].items() %}; {{ name }}: {{ value }}; {% endfor %}; labels: {{ pr.labels|join("", "") }}. Retry. {% if logging_queries is not none %}; Logging Queries. {% for name, link in logging_queries.items() %}; {{ name }}; {% endfor %}; ; {% endif %}. Jobs; {{ filtered_jobs(running, failed, pending, completed) }}; {% elif exception is defined %}; Build error:. {{ exception }}; ; {% else %}; No build running.; {% endif %}. Build History; {% if history %}; . id; state. {% for batch in history %}; . {{ batch['id'] }}. {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; . {% endfor %}; . {% else %}; No builds.; {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/pr.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/pr.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/pr.html:70,Modifiability,extend,extends,70,"{% from ""filtered-jobs.html"" import filtered_jobs with context %}; {% extends ""layout.html"" %}; {% block title %}PR {{ number }}{% endblock %}; {% block head %}; ; {% endblock %}; {% block content %}; {{ pr.title }} #{{ pr.number }}. {% if batch is defined %}; ; batch: {{ batch['id'] }}; artifacts:; {{ artifacts_uri }}; open_in_new. cost: {{ batch['cost'] }}; {% for name, value in batch['attributes'].items() %}; {{ name }}: {{ value }}; {% endfor %}; labels: {{ pr.labels|join("", "") }}. Retry. {% if logging_queries is not none %}; Logging Queries. {% for name, link in logging_queries.items() %}; {{ name }}; {% endfor %}; ; {% endif %}. Jobs; {{ filtered_jobs(running, failed, pending, completed) }}; {% elif exception is defined %}; Build error:. {{ exception }}; ; {% else %}; No build running.; {% endif %}. Build History; {% if history %}; . id; state. {% for batch in history %}; . {{ batch['id'] }}. {% if 'state' in batch and batch['state'] %}; {{ batch['state'] }}; {% endif %}; . {% endfor %}; . {% else %}; No builds.; {% endif %}; {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/pr.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/pr.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/user.html:70,Deployability,deploy,deploy-table,70,"{% from ""pr-table.html"" import pr_table with context %}; {% from ""dev-deploy-table.html"" import dev_deploy_table with context %}; {% from ""team-table.html"" import team_table with context %}; {% extends ""layout.html"" %}. {% block title %}User Homepage{% endblock %}. {% block head %}. {% endblock %}. {% block content %}; Welcome, {{ username }}!. GitHub username: {{ gh_username }}. {% for wb in actionable_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} Awaiting Action; {{ pr_table(wb, ""actionitems"", ""actionitemsSearchBar"") }}; {% endif %}; {% endfor %}; . {% for wb in pr_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} PRs; {{ pr_table(wb, ""myprs"", ""myprsSearchBar"") }}; {% endif %}; {% endfor %}; . {% for wb in review_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} Reviews; {{ pr_table(wb, ""reviews"", ""reviewsSearchBar"") }}; {% endif %}; {% endfor %}; . Authorize SHA. SHA:. Authorize. {{ team_table(team_member) }}; . Dev Deploys; {{ dev_deploy_table(dev_deploys) }}; . {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/user.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/user.html
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/user.html:194,Modifiability,extend,extends,194,"{% from ""pr-table.html"" import pr_table with context %}; {% from ""dev-deploy-table.html"" import dev_deploy_table with context %}; {% from ""team-table.html"" import team_table with context %}; {% extends ""layout.html"" %}. {% block title %}User Homepage{% endblock %}. {% block head %}. {% endblock %}. {% block content %}; Welcome, {{ username }}!. GitHub username: {{ gh_username }}. {% for wb in actionable_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} Awaiting Action; {{ pr_table(wb, ""actionitems"", ""actionitemsSearchBar"") }}; {% endif %}; {% endfor %}; . {% for wb in pr_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} PRs; {{ pr_table(wb, ""myprs"", ""myprsSearchBar"") }}; {% endif %}; {% endfor %}; . {% for wb in review_wbs %}; {% if wb.prs is not none %}; {{ wb.branch }} Reviews; {{ pr_table(wb, ""reviews"", ""reviewsSearchBar"") }}; {% endif %}; {% endfor %}; . Authorize SHA. SHA:. Authorize. {{ team_table(team_member) }}; . Dev Deploys; {{ dev_deploy_table(dev_deploys) }}; . {% endblock %}; ",MatchSource.DOCS,ci/ci/templates/user.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/templates/user.html
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html:210,Deployability,toggle,toggle,210,"{% extends ""dynamic-base.html"" %}. {# TEMPLATE VAR SETTINGS #}; {%- set url_root = pathto('', 1) %}; {%- if url_root == '#' %}{% set url_root = '' %}{% endif %}. {%- set script_files = script_files + ['_static/toggle.js'] %}; {%- set css_files = css_files + ['_static/rtd_modifications.css'] %}. {% if pagename == ""annotation_database_ui"" %}; {%- set css_files = css_files + [ '_static/annotationdb/annotationdb.css'] %}; {%- set script_files = script_files + ['_static/hail_version.js','_static/annotationdb/annotationdb.js'] %}; {% endif %}. {% if pagename == ""datasets"" %}; {%- set css_files = css_files + [ '_static/annotationdb/annotationdb.css'] %}; {%- set script_files = script_files + ['_static/hail_version.js','_static/datasets/datasets.js'] %}; {% endif %}. {% block title %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block meta_description %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block head %}; {{ metatags }}; . {% for cssfile in css_files %}; ; {% endfor %}. {% for cssfile in extra_css_files %}; ; {% endfor %}. {%- block linktags %}; {%- if hasdoc('about') %}; ; {%- endif %}; {%- if hasdoc('genindex') %}; ; {%- endif %}; {%- if hasdoc('search') %}; ; {%- endif %}; {%- if hasdoc('copyright') %}; ; {%- endif %}; ; {%- if parents %}; ; {%- endif %}; {%- if next %}; ; {%- endif %}; {%- if prev %}; ; {%- endif %}; {%- endblock %}; {%- block extrahead %} {% endblock %}. {# Keep modernizr in head - http://modernizr.com/docs/#installing #}; . {% if pagename == ""annotationdb"" %}; ; {% endif %}. {% endblock %}. {% block content %}; . {# SIDE NAV, TOGGLES ON MOBILE #}; . {% block sidebartitle %}. {% include ""searchbox.html"" %}. {% endblock %}; . {% if not logo or not theme_logo_only %}; {{ project }} Docs; {% if theme_display_version %}; {%- set nav_version = version %}; {% if READTHEDOCS and current_version %}; {%- set nav_version = current_version %}; {% endif %}; {% if nav_version %}; ({{ nav_version }}); {% endif %}; {% endif %}",MatchSource.DOCS,hail/python/hail/docs/_templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html:1488,Deployability,install,installing,1488,"atic/annotationdb/annotationdb.js'] %}; {% endif %}. {% if pagename == ""datasets"" %}; {%- set css_files = css_files + [ '_static/annotationdb/annotationdb.css'] %}; {%- set script_files = script_files + ['_static/hail_version.js','_static/datasets/datasets.js'] %}; {% endif %}. {% block title %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block meta_description %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block head %}; {{ metatags }}; . {% for cssfile in css_files %}; ; {% endfor %}. {% for cssfile in extra_css_files %}; ; {% endfor %}. {%- block linktags %}; {%- if hasdoc('about') %}; ; {%- endif %}; {%- if hasdoc('genindex') %}; ; {%- endif %}; {%- if hasdoc('search') %}; ; {%- endif %}; {%- if hasdoc('copyright') %}; ; {%- endif %}; ; {%- if parents %}; ; {%- endif %}; {%- if next %}; ; {%- endif %}; {%- if prev %}; ; {%- endif %}; {%- endblock %}; {%- block extrahead %} {% endblock %}. {# Keep modernizr in head - http://modernizr.com/docs/#installing #}; . {% if pagename == ""annotationdb"" %}; ; {% endif %}. {% endblock %}. {% block content %}; . {# SIDE NAV, TOGGLES ON MOBILE #}; . {% block sidebartitle %}. {% include ""searchbox.html"" %}. {% endblock %}; . {% if not logo or not theme_logo_only %}; {{ project }} Docs; {% if theme_display_version %}; {%- set nav_version = version %}; {% if READTHEDOCS and current_version %}; {%- set nav_version = current_version %}; {% endif %}; {% if nav_version %}; ({{ nav_version }}); {% endif %}; {% endif %}; {% endif %}. {% if logo %}; {# Not strictly valid HTML, but it's the only way to display/scale it properly, without weird scripting or heaps of work #}; ; {% endif %}; . {% block menu %}; {% set toctree = toctree(maxdepth=4, collapse=theme_collapse_navigation, includehidden=True) %}; {% if toctree %}; {{ toctree }}; {% else %}; ; {{ toc }}; {% endif %}; {% endblock %}; . {# MOBILE NAV, TRIGGLES SIDE NAV ON TOGGLE #}; ; menu; {{ project }}. {# PAGE CONTENT #}; . {% include ""breadcrumb",MatchSource.DOCS,hail/python/hail/docs/_templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html:3,Modifiability,extend,extends,3,"{% extends ""dynamic-base.html"" %}. {# TEMPLATE VAR SETTINGS #}; {%- set url_root = pathto('', 1) %}; {%- if url_root == '#' %}{% set url_root = '' %}{% endif %}. {%- set script_files = script_files + ['_static/toggle.js'] %}; {%- set css_files = css_files + ['_static/rtd_modifications.css'] %}. {% if pagename == ""annotation_database_ui"" %}; {%- set css_files = css_files + [ '_static/annotationdb/annotationdb.css'] %}; {%- set script_files = script_files + ['_static/hail_version.js','_static/annotationdb/annotationdb.js'] %}; {% endif %}. {% if pagename == ""datasets"" %}; {%- set css_files = css_files + [ '_static/annotationdb/annotationdb.css'] %}; {%- set script_files = script_files + ['_static/hail_version.js','_static/datasets/datasets.js'] %}; {% endif %}. {% block title %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block meta_description %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block head %}; {{ metatags }}; . {% for cssfile in css_files %}; ; {% endfor %}. {% for cssfile in extra_css_files %}; ; {% endfor %}. {%- block linktags %}; {%- if hasdoc('about') %}; ; {%- endif %}; {%- if hasdoc('genindex') %}; ; {%- endif %}; {%- if hasdoc('search') %}; ; {%- endif %}; {%- if hasdoc('copyright') %}; ; {%- endif %}; ; {%- if parents %}; ; {%- endif %}; {%- if next %}; ; {%- endif %}; {%- if prev %}; ; {%- endif %}; {%- endblock %}; {%- block extrahead %} {% endblock %}. {# Keep modernizr in head - http://modernizr.com/docs/#installing #}; . {% if pagename == ""annotationdb"" %}; ; {% endif %}. {% endblock %}. {% block content %}; . {# SIDE NAV, TOGGLES ON MOBILE #}; . {% block sidebartitle %}. {% include ""searchbox.html"" %}. {% endblock %}; . {% if not logo or not theme_logo_only %}; {{ project }} Docs; {% if theme_display_version %}; {%- set nav_version = version %}; {% if READTHEDOCS and current_version %}; {%- set nav_version = current_version %}; {% endif %}; {% if nav_version %}; ({{ nav_version }}); {% endif %}; {% endif %}",MatchSource.DOCS,hail/python/hail/docs/_templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html:1719,Testability,log,logo,1719,"x }}; {% endblock %}. {% block meta_description %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block head %}; {{ metatags }}; . {% for cssfile in css_files %}; ; {% endfor %}. {% for cssfile in extra_css_files %}; ; {% endfor %}. {%- block linktags %}; {%- if hasdoc('about') %}; ; {%- endif %}; {%- if hasdoc('genindex') %}; ; {%- endif %}; {%- if hasdoc('search') %}; ; {%- endif %}; {%- if hasdoc('copyright') %}; ; {%- endif %}; ; {%- if parents %}; ; {%- endif %}; {%- if next %}; ; {%- endif %}; {%- if prev %}; ; {%- endif %}; {%- endblock %}; {%- block extrahead %} {% endblock %}. {# Keep modernizr in head - http://modernizr.com/docs/#installing #}; . {% if pagename == ""annotationdb"" %}; ; {% endif %}. {% endblock %}. {% block content %}; . {# SIDE NAV, TOGGLES ON MOBILE #}; . {% block sidebartitle %}. {% include ""searchbox.html"" %}. {% endblock %}; . {% if not logo or not theme_logo_only %}; {{ project }} Docs; {% if theme_display_version %}; {%- set nav_version = version %}; {% if READTHEDOCS and current_version %}; {%- set nav_version = current_version %}; {% endif %}; {% if nav_version %}; ({{ nav_version }}); {% endif %}; {% endif %}; {% endif %}. {% if logo %}; {# Not strictly valid HTML, but it's the only way to display/scale it properly, without weird scripting or heaps of work #}; ; {% endif %}; . {% block menu %}; {% set toctree = toctree(maxdepth=4, collapse=theme_collapse_navigation, includehidden=True) %}; {% if toctree %}; {{ toctree }}; {% else %}; ; {{ toc }}; {% endif %}; {% endblock %}; . {# MOBILE NAV, TRIGGLES SIDE NAV ON TOGGLE #}; ; menu; {{ project }}. {# PAGE CONTENT #}; . {% include ""breadcrumbs.html"" %}; . {% block body %}{% endblock %}; . {% include ""footer.html"" %}; . {% include ""versions.html"" %}. {% if not embedded %}; ; {%- for scriptfile in script_files %}; ; {%- endfor %}. {% endif %}. . {# STICKY NAVIGATION #}; {% if theme_sticky_navigation %}; ; {% endif %}. {%- block footer %} {% endblock %}. . {% endblock %}; ",MatchSource.DOCS,hail/python/hail/docs/_templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html:2022,Testability,log,logo,2022,"x }}; {% endblock %}. {% block meta_description %}; {{ title|striptags|e }}{{ titlesuffix }}; {% endblock %}. {% block head %}; {{ metatags }}; . {% for cssfile in css_files %}; ; {% endfor %}. {% for cssfile in extra_css_files %}; ; {% endfor %}. {%- block linktags %}; {%- if hasdoc('about') %}; ; {%- endif %}; {%- if hasdoc('genindex') %}; ; {%- endif %}; {%- if hasdoc('search') %}; ; {%- endif %}; {%- if hasdoc('copyright') %}; ; {%- endif %}; ; {%- if parents %}; ; {%- endif %}; {%- if next %}; ; {%- endif %}; {%- if prev %}; ; {%- endif %}; {%- endblock %}; {%- block extrahead %} {% endblock %}. {# Keep modernizr in head - http://modernizr.com/docs/#installing #}; . {% if pagename == ""annotationdb"" %}; ; {% endif %}. {% endblock %}. {% block content %}; . {# SIDE NAV, TOGGLES ON MOBILE #}; . {% block sidebartitle %}. {% include ""searchbox.html"" %}. {% endblock %}; . {% if not logo or not theme_logo_only %}; {{ project }} Docs; {% if theme_display_version %}; {%- set nav_version = version %}; {% if READTHEDOCS and current_version %}; {%- set nav_version = current_version %}; {% endif %}; {% if nav_version %}; ({{ nav_version }}); {% endif %}; {% endif %}; {% endif %}. {% if logo %}; {# Not strictly valid HTML, but it's the only way to display/scale it properly, without weird scripting or heaps of work #}; ; {% endif %}; . {% block menu %}; {% set toctree = toctree(maxdepth=4, collapse=theme_collapse_navigation, includehidden=True) %}; {% if toctree %}; {{ toctree }}; {% else %}; ; {{ toc }}; {% endif %}; {% endblock %}; . {# MOBILE NAV, TRIGGLES SIDE NAV ON TOGGLE #}; ; menu; {{ project }}. {# PAGE CONTENT #}; . {% include ""breadcrumbs.html"" %}; . {% block body %}{% endblock %}; . {% include ""footer.html"" %}; . {% include ""versions.html"" %}. {% if not embedded %}; ; {%- for scriptfile in script_files %}; ; {%- endfor %}. {% endif %}. . {# STICKY NAVIGATION #}; {% if theme_sticky_navigation %}; ; {% endif %}. {%- block footer %} {% endblock %}. . {% endblock %}; ",MatchSource.DOCS,hail/python/hail/docs/_templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/_templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/templates/billing.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Billing{% endblock %}; {% block head %}; . {% endblock %}; {% block content %}. Time Period:. Submit. Costs by Google Service. Service; Cost. {% for record in cost_by_service %}; ; {{ record['service'] }}; {{ record['cost'] }}. {% endfor %}; . Compute Engine Costs by Source. Source; Cost. {% for record in compute_cost_breakdown %}; ; {{ record['source'] }}; {{ record['cost'] }}. {% endfor %}; . Costs by Product. Service; Product; Source; Cost. {% for record in cost_by_sku_label %}; ; {{ record['service_description'] }}; {{ record['sku_description'] }}. {% if record['source'] is not none %}; {{ record['source'] }}; {% endif %}; ; {{ record['cost'] }}. {% endfor %}; . {% endblock %}; ",MatchSource.DOCS,monitoring/monitoring/templates/billing.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/templates/billing.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/404.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} 404 Not Found {% endblock %}; {% block meta_description %} 404 Not Found {% endblock %}; {% block content %}. 404; The page youre looking for does not exist!. {% endblock %}; ",MatchSource.DOCS,website/website/pages/404.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/404.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html:423,Deployability,integrat,integrated,423,"{% extends ""base.html"" %}; {% block title %} About {% endblock %}; {% block meta_description %} About Hail {% endblock %}; {% block content %}. About; Hail has been widely adopted in academia and industry, including as the; analysis platform for the genome; aggregation database and UK; Biobank rapid GWAS. Learn more about Hail-powered; science.; Hail is actively developed with new features and performance improvements; integrated weekly. See the changelog for; more information.; Hail Team; Hail is maintained by a team in the Neale; lab at the Stanley Center; for Psychiatric Research of; the Broad Institute of MIT and; Harvard and the Analytic and; Translational Genetics Unit; of Massachusetts General; Hospital.; Contact the Hail team; at hail@broadinstitute.org.; Follow us on Twitter. {% endblock %}; ",MatchSource.DOCS,website/website/pages/about.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html:329,Energy Efficiency,power,powered,329,"{% extends ""base.html"" %}; {% block title %} About {% endblock %}; {% block meta_description %} About Hail {% endblock %}; {% block content %}. About; Hail has been widely adopted in academia and industry, including as the; analysis platform for the genome; aggregation database and UK; Biobank rapid GWAS. Learn more about Hail-powered; science.; Hail is actively developed with new features and performance improvements; integrated weekly. See the changelog for; more information.; Hail Team; Hail is maintained by a team in the Neale; lab at the Stanley Center; for Psychiatric Research of; the Broad Institute of MIT and; Harvard and the Analytic and; Translational Genetics Unit; of Massachusetts General; Hospital.; Contact the Hail team; at hail@broadinstitute.org.; Follow us on Twitter. {% endblock %}; ",MatchSource.DOCS,website/website/pages/about.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html:423,Integrability,integrat,integrated,423,"{% extends ""base.html"" %}; {% block title %} About {% endblock %}; {% block meta_description %} About Hail {% endblock %}; {% block content %}. About; Hail has been widely adopted in academia and industry, including as the; analysis platform for the genome; aggregation database and UK; Biobank rapid GWAS. Learn more about Hail-powered; science.; Hail is actively developed with new features and performance improvements; integrated weekly. See the changelog for; more information.; Hail Team; Hail is maintained by a team in the Neale; lab at the Stanley Center; for Psychiatric Research of; the Broad Institute of MIT and; Harvard and the Analytic and; Translational Genetics Unit; of Massachusetts General; Hospital.; Contact the Hail team; at hail@broadinstitute.org.; Follow us on Twitter. {% endblock %}; ",MatchSource.DOCS,website/website/pages/about.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} About {% endblock %}; {% block meta_description %} About Hail {% endblock %}; {% block content %}. About; Hail has been widely adopted in academia and industry, including as the; analysis platform for the genome; aggregation database and UK; Biobank rapid GWAS. Learn more about Hail-powered; science.; Hail is actively developed with new features and performance improvements; integrated weekly. See the changelog for; more information.; Hail Team; Hail is maintained by a team in the Neale; lab at the Stanley Center; for Psychiatric Research of; the Broad Institute of MIT and; Harvard and the Analytic and; Translational Genetics Unit; of Massachusetts General; Hospital.; Contact the Hail team; at hail@broadinstitute.org.; Follow us on Twitter. {% endblock %}; ",MatchSource.DOCS,website/website/pages/about.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html:397,Performance,perform,performance,397,"{% extends ""base.html"" %}; {% block title %} About {% endblock %}; {% block meta_description %} About Hail {% endblock %}; {% block content %}. About; Hail has been widely adopted in academia and industry, including as the; analysis platform for the genome; aggregation database and UK; Biobank rapid GWAS. Learn more about Hail-powered; science.; Hail is actively developed with new features and performance improvements; integrated weekly. See the changelog for; more information.; Hail Team; Hail is maintained by a team in the Neale; lab at the Stanley Center; for Psychiatric Research of; the Broad Institute of MIT and; Harvard and the Analytic and; Translational Genetics Unit; of Massachusetts General; Hospital.; Contact the Hail team; at hail@broadinstitute.org.; Follow us on Twitter. {% endblock %}; ",MatchSource.DOCS,website/website/pages/about.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/about.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/cheatsheets.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} Cheat Sheets {% endblock %}; {% block meta_description %} Cheat Sheets {% endblock %}; {% block content %}. Cheat Sheets; Shortcuts to plinking through Hail. Hail has two cheat sheets, describing the two data structures in Hail: the Table and the MatrixTable.; . Tables; Tables are the Hail data structure for one-dimensional data. You can create a Table from TSVs, CSVs, sites VCFs, FAM files, and Pandas DataFrames. MatrixTables; MatrixTables are the Hail data structure for two-dimensional data. You can create a MatrixTable from VCF, BGEN, and PLINK files. {% endblock %}; ",MatchSource.DOCS,website/website/pages/cheatsheets.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/cheatsheets.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html:193,Energy Efficiency,efficient,efficient,193,"{% extends ""base.html"" %}; {% block title %} Get Help {% endblock %}; {% block meta_description %} Get Help {% endblock %}; {% block content %}. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. {% endblock %}; ",MatchSource.DOCS,website/website/pages/gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} Get Help {% endblock %}; {% block meta_description %} Get Help {% endblock %}; {% block content %}. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. {% endblock %}; ",MatchSource.DOCS,website/website/pages/gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html:264,Performance,load,loaded,264,"{% extends ""base.html"" %}; {% block title %} Get Help {% endblock %}; {% block meta_description %} Get Help {% endblock %}; {% block content %}. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. {% endblock %}; ",MatchSource.DOCS,website/website/pages/gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html:692,Performance,scalab,scalable,692,"{% extends ""base.html"" %}; {% block title %} Get Help {% endblock %}; {% block meta_description %} Get Help {% endblock %}; {% block content %}. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. {% endblock %}; ",MatchSource.DOCS,website/website/pages/gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/gethelp.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:828,Deployability,install,install,828,"{% extends ""base.html"" %}; {% block title %} Index {% endblock %}; {% block meta_description %} Hail Index Page {% endblock %}; {% block content %}. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abs",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:963,Deployability,install,installed,963,"{% extends ""base.html"" %}; {% block title %} Index {% endblock %}; {% block meta_description %} Hail Index Page {% endblock %}; {% block content %}. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abs",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2022,Deployability,integrat,integrated,2022,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:1050,Energy Efficiency,power,powerful,1050,"ck %}; {% block meta_description %} Hail Index Page {% endblock %}; {% block content %}. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis plat",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:1679,Energy Efficiency,power,powerful,1679,"mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources o",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2340,Energy Efficiency,efficient,efficient,2340," Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose str",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2022,Integrability,integrat,integrated,2022,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2539,Integrability,depend,dependencies,2539,"a combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous ",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} Index {% endblock %}; {% block meta_description %} Hail Index Page {% endblock %}; {% block content %}. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abs",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:1929,Performance,scalab,scalable,1929,"++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowled",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2037,Performance,scalab,scalable,2037,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2677,Performance,queue,queueing,2677,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html:2749,Performance,queue,queue,2749,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.DOCS,website/website/pages/index.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/index.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:344,Deployability,install,installed,344,"{% extends ""base.html"" %}; {% block title %} References {% endblock %}; {% block meta_description %} References {% endblock %}; {% block content %}. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leoska-Duniec, A., Borczyk, M., Korostyski, M. et al. Genetic variants in myostatin; 	 and its receptors promote ",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:749,Deployability,update,updated,749,"{% extends ""base.html"" %}; {% block title %} References {% endblock %}; {% block meta_description %} References {% endblock %}; {% block content %}. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leoska-Duniec, A., Borczyk, M., Korostyski, M. et al. Genetic variants in myostatin; 	 and its receptors promote ",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:9814,Deployability,integrat,integrate,9814,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al.Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:9434,Energy Efficiency,power,power,9434,"6-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 36993707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated population; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders wit",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:9814,Integrability,integrat,integrate,9814,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al.Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:12246,Integrability,depend,dependence,12246,"ctive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al.Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al.Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al.Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al.Leveraging genome-wide data to investigate differences between opioid use vs.opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al.Meta-analysis of Scandinavian Schizophrenia Exomes bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al.Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al.Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses. Hum Genet 138, 13311340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies. The Lancet Respiratory Medicine 7.6 (2019): 509-522. https",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:18194,Integrability,mediat,mediatum,18194,"ibutions to variation in human stature in prehistoric Europe. bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al.Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction. Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al.Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder. Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universitt Mnchen, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al.Whole exome sequencing study identifies novel rare and common Alzheimers-associated variants involved in immune response and transcriptional regulation. Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al.Common variant burden contributes to the familial aggregation of migraine in 1,589 families. Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al.Insights into the genetic epidemiology of Crohns and rare diseases in the Ashkenazi Jewish population. PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al.ASD and ADHD have a similar burden of rare protein-truncating variants. bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al.Deep coverage whole genome sequences and plasma lipoprotein",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} References {% endblock %}; {% block meta_description %} References {% endblock %}; {% block content %}. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leoska-Duniec, A., Borczyk, M., Korostyski, M. et al. Genetic variants in myostatin; 	 and its receptors promote ",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:1168,Safety,risk,risk,1168,"ock content %}. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leoska-Duniec, A., Borczyk, M., Korostyski, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/1",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:3025,Safety,risk,risk,3025," 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/10.1186/s12864-023-09869-2. 	 Chen, S., Francioli, L.C., Goodrich, J.K. et al. A genomic mutational constraint map; 	 using variation in 76,156 human genomes. Nature 625, 92100; 	 (2024). https://doi.org/10.1038/s41586-023-06045-0 https://www.nature.com/articles/s41586-023-06045-0. 	 Mosca, M.J., Cho, H. Reconstruction of private genomes through reference-based genotype; 	 imputation. Genome Biol 24, 271; 	 (2023). https://doi.org/10.1186/s13059-023-03105-6 https://link.springer.com/article/10.1186/s13059-023-03105-6. 	 Stberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntingtons disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 13711387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:3614,Safety,risk,risk,3614,"om/article/10.1186/s13059-023-03105-6. 	 Stberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntingtons disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 13711387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10.1177/08830738231186233. https://journals.sagepub.com/doi/full/10.1177/08830738231186233. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 13711387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Srensen, Jette Bork-Jensen, Hsn S Kizilkaya, Lrke S; 	 Gasbjerg, Alexander S Hauser, Jrgen Rungby, Henrik T Srensen, Allan Vaag, Jens S; 	 Nielsen, Oluf P",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:5451,Safety,risk,risk,5451,"sn S Kizilkaya, Lrke S; 	 Gasbjerg, Alexander S Hauser, Jrgen Rungby, Henrik T Srensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 28212833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, ., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329337; 	 (2023). https://doi.org/10.1038/s41431-022-01227-2 https://www.nature.com/articles/s41431-022-01227-2. 	 Steiner, H.E., Carrion, K.C., Giles, J.B., Lima, A.R., Yee, K., Sun, X., Cavallari,; 	 L.H., Perera, M.A., Duconge, J. and Karnes, J.H. (2023), Local Ancestry-Informed; 	 Candidate Pathway Ana",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:7561,Safety,predict,prediction,7561,"ino Populations. Clin Pharmacol; 	 Ther, 113:; 	 680-691. https://doi.org/10.1002/cpt.2787 https://ascpt.onlinelibrary.wiley.com/doi/full/10.1002/cpt.2787. 2022. 	 Huang, J., Tao, Q., Ang, T.F.A. et al. The impact of increasing levels of blood; 	 C-reactive protein on the inflammatory loci SPI1 and CD33 in Alzheimers disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 64366451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 18621871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis conv",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:10140,Safety,risk,risk,10140,"Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al.Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/article/doi/10.1093/jamia/ocaa068/5876972; Karaca M, Atceken N, Karaca , Civelek E, ekerel BE, Polimanti R. Phenotypic and Molecular Characterization of Risk Loci Associated With Asthma and Lung Function Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.o",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:12011,Safety,risk,risk,12011,"ciated With Asthma and Lung Function Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.org/DOIx.php?id=10.4168/aair.2020.12.5.806; Muniz Carvalho, C., Wendt, F.R., Maihofer, A.X. et al.Dissecting the genetic association of C-reactive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al.Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al.Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al.Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al.Leveraging genome-wide data to investigate differences between opioid use vs.opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al.Meta-analysis of Scandinavian Schizophrenia Exomes bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al.Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al.Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses. Hum G",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:13137,Safety,risk,risk,13137,"genome-wide data to investigate differences between opioid use vs.opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al.Meta-analysis of Scandinavian Schizophrenia Exomes bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al.Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al.Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses. Hum Genet 138, 13311340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies. The Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al.Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al.Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al.Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al.Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland. Nature Communications 10.1 (2019): 410. https://www",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:14292,Safety,risk,risk,14292,"-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al.Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al.Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al.Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al.Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland. Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al.Current clinical use of polygenic scores will risk exacerbating health disparities. bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al.Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals. American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al.The mutational constraint spectrum quantified from variation in 141,456 humans. bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al.Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease. bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al.Transcript expression-aware annotation improves rare variant discovery and interpretation. bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al.Landscape of multi-",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:14925,Safety,safe,safe,14925,"s. bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al.Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland. Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al.Current clinical use of polygenic scores will risk exacerbating health disparities. bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al.Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals. American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al.The mutational constraint spectrum quantified from variation in 141,456 humans. bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al.Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease. bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al.Transcript expression-aware annotation improves rare variant discovery and interpretation. bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al.Landscape of multi-nucleotide variants in 125,748 human exomes and 15,708 genomes. bioRxiv (2019): 573378. https://www.biorxiv.org/content/10.1101/573378v2; Minikel, Eric Vallabh, et al.Evaluating potential drug targets through human loss-of-function genetic variation. bioRxiv (2019): 530881. https://www.biorxiv.org/content/10.1101/530881v2; Collins, Ryan L., et al.An open resource of structural variation for medical and population genetics. bioRxiv (2019): 578674. https://www.biorxiv.org/content/10.1101/578674v1; Whiffin, Nicola, et al.Characterising the loss-of-function impact of 5untranslated region variants in whole genom",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:16954,Safety,risk,risk,16954,"biorxiv.org/content/10.1101/543504v1; Lacaze, Paul, et al.The Medical Genome Reference Bank: a whole-genome data resource of 4000 healthy elderly individuals. Rationale and cohort design. European Journal of Human Genetics 27.2 (2019): 308. https://www.nature.com/articles/s41431-018-0279-z; Cirulli, Elizabeth T., et al.Genome-wide rare variant analysis for thousands of phenotypes in 54,000 exomes. bioRxiv (2019): 692368. https://www.biorxiv.org/content/10.1101/692368v1.abstract; Kerminen, Sini, et al.Geographic Variation and Bias in the Polygenic Scores of Complex Diseases and Traits in Finland. American Journal of Human Genetics (2019). https://www.biorxiv.org/content/10.1101/485441v1.abstract; Jiang, Fan, Kyle Ferriter, and Claris Castillo. PIVOT: Cost-Aware Scheduling of Data-Intensive Applications in a Cloud-Agnostic System. https://renci.org/wp-content/uploads/2019/02/Cloud_19.pdf; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies. Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al.Genetic contributions to variation in human stature in prehistoric Europe. bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al.Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction. Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al.Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder. Science (2018): 1. https://science.sciencemag.org/content/362/6420/",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:17851,Safety,risk,risk,17851,"ads/2019/02/Cloud_19.pdf; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies. Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al.Genetic contributions to variation in human stature in prehistoric Europe. bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al.Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction. Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al.Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder. Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universitt Mnchen, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al.Whole exome sequencing study identifies novel rare and common Alzheimers-associated variants involved in immune response and transcriptional regulation. Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al.Common variant burden contributes to the familial aggregation of migraine in 1,589 families. Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al.Insights into the genetic epidemiology of Crohns and rare diseases in the Ashkenazi Jewish population. PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:19877,Safety,risk,risk,19877,"ournals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al.ASD and ADHD have a similar burden of rare protein-truncating variants. bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al.Deep coverage whole genome sequences and plasma lipoprotein (a) in individuals of European and African ancestries. Nature Communications 9.1 (2018): 2606. https://www.nature.com/articles/s41467-018-04668-w; Natarajan, Pradeep, et al.Deep-coverage whole genome sequences and blood lipids among 16,324 individuals. Nature Communications 9.1 (2018): 3391. https://www.nature.com/articles/s41467-018-04668-w; Ganna, Andrea, et al.Quantifying the impact of rare and ultra-rare coding variation across the phenotypic spectrum. American Journal of Human Genetics 102.6 (2018): 1204-1211. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992130/; Khera, Amit V., et al.Genome-wide polygenic scores for common diseases identify individuals with risk equivalent to monogenic mutations. Nature Genetics 50.9 (2018): 1219. https://www.nature.com/articles/s41588-018-0183-z?_ga=2.263293700.980063710.1543017600-1151073636.1543017600; Roselli, Carolina, et al.Multi-ethnic genome-wide association study for atrial fibrillation. Nature Genetics 50.9 (2018): 1225. https://www.nature.com/articles/s41588-018-0133-9; Arachchi, Harindra, et al.matchbox: An opensource tool for patient matching via the Matchmaker Exchange. Human Mutation 39.12 (2018): 1827-1834. https://onlinelibrary.wiley.com/doi/abs/10.1002/humu.23655; Laisk, Triin, et al.GWAS meta-analysis highlights the hypothalamic-pituitary-gonadal axis (HPG axis) in the genetic regulation of menstrual cycle length. bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al.Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channel",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:21607,Safety,risk,risk,21607,"2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al.Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channels. bioRxiv (2018): 246850. https://www.biorxiv.org/content/10.1101/246850v1.abstract; Haas, Mary E., et al.Genetic association of albuminuria with cardiometabolic disease and blood pressure. American Journal of Human Genetics 103.4 (2018): 461-473. https://www.cell.com/ajhg/pdf/S0002-9297(18)30270-2.pdf; Abel, Haley J., et al.Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes. bioRxiv (2018): 508515. https://www.biorxiv.org/content/10.1101/508515v1.abstract; Lane, Jacqueline M., et al.Biological and clinical insights from genetics of insomnia symptoms. bioRxiv (2018): 257956. https://www.biorxiv.org/content/10.1101/257956v1.abstract; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood onset and adult onset asthma. bioRxiv (2018): 427427. https://www.biorxiv.org/content/10.1101/427427v1.abstract. 2017. Lessard, Samuel, et al.Human genetic variation alters CRISPR-Cas9 on-and off-targeting specificity at therapeutically implicated loci. Proceedings of the National Academy of Sciences 114.52 (2017): E11257-E11266. https://www.pnas.org/content/114/52/E11257.long. 2016. Ganna, Andrea, et al.Ultra-rare disruptive and damaging mutations influence educational attainment in the general population. Nature Neuroscience 19.12 (2016): 1563. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127781/. Footnote In addition to software development, the Hail team engages in theoretical, algorithmic, and empirical research inspired by scientific collaboration. Examples include Loss landscapes of regularized linear autoencoders, Secure multi-party linear regression at plaintext speed, and A synthetic-diploid benchmark for accurate variant-calling evaluation. {% endblock ",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:7856,Testability,log,login,7856,"imers disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 64366451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 18621871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 36993707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated p",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:22557,Testability,benchmark,benchmark,22557,"): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al.Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channels. bioRxiv (2018): 246850. https://www.biorxiv.org/content/10.1101/246850v1.abstract; Haas, Mary E., et al.Genetic association of albuminuria with cardiometabolic disease and blood pressure. American Journal of Human Genetics 103.4 (2018): 461-473. https://www.cell.com/ajhg/pdf/S0002-9297(18)30270-2.pdf; Abel, Haley J., et al.Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes. bioRxiv (2018): 508515. https://www.biorxiv.org/content/10.1101/508515v1.abstract; Lane, Jacqueline M., et al.Biological and clinical insights from genetics of insomnia symptoms. bioRxiv (2018): 257956. https://www.biorxiv.org/content/10.1101/257956v1.abstract; Pividori, Milton, et al.Shared and distinct genetic risk factors for childhood onset and adult onset asthma. bioRxiv (2018): 427427. https://www.biorxiv.org/content/10.1101/427427v1.abstract. 2017. Lessard, Samuel, et al.Human genetic variation alters CRISPR-Cas9 on-and off-targeting specificity at therapeutically implicated loci. Proceedings of the National Academy of Sciences 114.52 (2017): E11257-E11266. https://www.pnas.org/content/114/52/E11257.long. 2016. Ganna, Andrea, et al.Ultra-rare disruptive and damaging mutations influence educational attainment in the general population. Nature Neuroscience 19.12 (2016): 1563. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127781/. Footnote In addition to software development, the Hail team engages in theoretical, algorithmic, and empirical research inspired by scientific collaboration. Examples include Loss landscapes of regularized linear autoencoders, Secure multi-party linear regression at plaintext speed, and A synthetic-diploid benchmark for accurate variant-calling evaluation. {% endblock %}; ",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html:5121,Usability,learn,learning-enabled,5121,"tem nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 13711387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Srensen, Jette Bork-Jensen, Hsn S Kizilkaya, Lrke S; 	 Gasbjerg, Alexander S Hauser, Jrgen Rungby, Henrik T Srensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 28212833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, ., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329",MatchSource.DOCS,website/website/pages/references.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/references.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/search.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} Search {% endblock %}; {% block meta_description %} Search {% endblock %}; {% block head %}. {% endblock %}; {% block content %}. {% endblock %}; ",MatchSource.DOCS,website/website/pages/search.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/search.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html:749,Integrability,interface,interface,749,"{% extends ""base.html"" %}; {% block title %} Tutorial {% endblock %}; {% block meta_description %} Tutorial {% endblock %}; {% block content %}. Import, prototype, scale; ; Perform analyses with distributed; dataframe-like; collections. import hail as hl; mt = hl.import_vcf('gs://bucket/path/myVCF.vcf.bgz'); mt.write('gs://bucket/path/dataset.mt', overwrite=True); # read matrix into env; mt = hl.read_matrix_table('gs://bucket/path/dataset.mt'); mt1 = hl.import_vcf('/path/to/my.vcf.bgz'); mt2 = hl.import_bgen('/path/to/my.bgen'); mt3 = hl.import_plink(bed='/path/to/my.bed',; bim='/path/to/my.bim',; fam='/path/to/my.fam'). Input Unification; ; Import formats such as bed, bgen, plink, or vcf, and manipulate them using a common dataframe-like interface. Genomic Dataframes; For large and dense structured matrices, like sequencing data, coordinate representations are; both; hard to work with and computationally inefficient. A core piece of Hail functionality is the; MatrixTable, a 2-dimensional generalization of Table. The MatrixTable makes it possible to; filter,; annotate, and aggregate symmetrically over rows and columns. # What is a MatrixTable?; mt.describe(widget=True). # filter to rare, loss-of-function variants; mt = mt.filter_rows(mt.variant_qc.AF[1] < 0.005); mt = mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's",MatchSource.DOCS,website/website/pages/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html:3,Modifiability,extend,extends,3,"{% extends ""base.html"" %}; {% block title %} Tutorial {% endblock %}; {% block meta_description %} Tutorial {% endblock %}; {% block content %}. Import, prototype, scale; ; Perform analyses with distributed; dataframe-like; collections. import hail as hl; mt = hl.import_vcf('gs://bucket/path/myVCF.vcf.bgz'); mt.write('gs://bucket/path/dataset.mt', overwrite=True); # read matrix into env; mt = hl.read_matrix_table('gs://bucket/path/dataset.mt'); mt1 = hl.import_vcf('/path/to/my.vcf.bgz'); mt2 = hl.import_bgen('/path/to/my.bgen'); mt3 = hl.import_plink(bed='/path/to/my.bed',; bim='/path/to/my.bim',; fam='/path/to/my.fam'). Input Unification; ; Import formats such as bed, bgen, plink, or vcf, and manipulate them using a common dataframe-like interface. Genomic Dataframes; For large and dense structured matrices, like sequencing data, coordinate representations are; both; hard to work with and computationally inefficient. A core piece of Hail functionality is the; MatrixTable, a 2-dimensional generalization of Table. The MatrixTable makes it possible to; filter,; annotate, and aggregate symmetrically over rows and columns. # What is a MatrixTable?; mt.describe(widget=True). # filter to rare, loss-of-function variants; mt = mt.filter_rows(mt.variant_qc.AF[1] < 0.005); mt = mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's",MatchSource.DOCS,website/website/pages/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html
https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html:2306,Safety,predict,predictor,2306," mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex = hl.impute_sex(mt.GT); mt = mt.annotate_cols(; sex_check = imputed_sex[mt.s].is_female == mt.reported_female; ). # must use Google cloud platform for this to work ; # annotation with vep; mt = hl.vep(mt). Variant Effect Predictor; Annotating variants with Variant effect predictor has never been easier. Rare-Variant Association Testing; Perform Gene Burden Tests on sequencing data with just a few lines of Python. gene_intervals = hl.read_table(""gs://my_bucket/gene_intervals.t""); mt = mt.annotate_rows(; gene = gene_intervals.index(mt.locus, all_matches=True).gene_name; ). mt = mt.explode_rows(mt.gene); mt = (mt.group_rows_by(mt.gene); .aggregate(burden = hl.agg.count_where(mt.GT.is_non_ref()))). result = hl.linear_regression_rows(y=mt.phenotype, x=mt.burden). # generate and save PC scores; eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(mt.GT, k=4). # run linear regression for the first 4 PCs; mt = mt.annotate_cols(scores = pca_scores[mt.sample_id].scores); results = hl.linear_regression_rows(; y=mt.phenotype,; x=mt.GT.n_alt_alleles(),; covariates=[; 1, mt.scores[0], mt.scores[1], mt.scores[2], mt.scores[3]]; ). Principal Component Analysis (PCA); Adjusting GWAS models with principal components as covariates has never been easier. {%",MatchSource.DOCS,website/website/pages/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/pages/tutorial.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/error.html:90,Availability,error,error,90,"{% extends ""layout.html"" %}; {% block title %}Error{% endblock %}; {% block content %}; {{error}}; {% endblock %}; ",MatchSource.DOCS,web_common/web_common/templates/error.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/error.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/error.html:3,Modifiability,extend,extends,3,"{% extends ""layout.html"" %}; {% block title %}Error{% endblock %}; {% block content %}; {{error}}; {% endblock %}; ",MatchSource.DOCS,web_common/web_common/templates/error.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/error.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/layout.html:244,Integrability,message,message,244,. Hail | {% block title %}{% endblock %}. {% if use_tailwind %}; . {% else %}; ; {% endif %}; {% block head %}{% endblock %}. {% if use_tailwind %}; {% include 'new_header.html' %}; {% else %}; {% include 'header.html' %}; {% endif %}; ; {% if message is defined %}; ; {{ message['text'] }}; ; {% endif %}; {% block content %}{% endblock %}; . ,MatchSource.DOCS,web_common/web_common/templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/layout.html:272,Integrability,message,message,272,. Hail | {% block title %}{% endblock %}. {% if use_tailwind %}; . {% else %}; ; {% endif %}; {% block head %}{% endblock %}. {% if use_tailwind %}; {% include 'new_header.html' %}; {% else %}; {% include 'header.html' %}; {% endif %}; ; {% if message is defined %}; ; {{ message['text'] }}; ; {% endif %}; {% block content %}{% endblock %}; . ,MatchSource.DOCS,web_common/web_common/templates/layout.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/layout.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/utils.html:358,Availability,failure,failure,358,"{% macro progress_spinner(color) %}. {% endmacro %}. {% macro success_check() %}. check. {% endmacro %}. {% macro fail_cross() %}. close. {% endmacro %}. {% macro cancel_circle() %}. block. {% endmacro %}. {% macro batch_state_indicator(batch) %}; {% if batch['n_jobs'] - batch['n_completed'] > 0 %}; {{ progress_spinner('text-red-600' if batch['state'] == 'failure' else 'text-sky-600') }}; {% elif batch['state'] == 'success' %}; {{ success_check() }}; {% elif batch['state'] == 'failure' %}; {{ fail_cross() }}; {% elif batch['state'] == 'cancelled' %}; {{ cancel_circle() }}; {% endif %}; {% endmacro %}. {% macro job_state_indicator(job) %}; {% if job['state'] == 'Running' %}; {{ progress_spinner('text-sky-600') }}; {% elif job['state'] == 'Success' %}; {{ success_check() }}; {% elif job['state'] == 'Failed' %}; {{ fail_cross() }}; {% elif job['state'] == 'Cancelled' %}; {{ cancel_circle() }}; {% endif %}; {% endmacro %}. {% macro danger_button(text) %}. {{ text }}. {% endmacro %}. {% macro submit_button(text) %}. {{ text }}. {% endmacro %}. {% macro link(href, text) %}. {{ text }}. {% endmacro %}. {% macro truncated_link(href, text) %}. {{ text }}. {% endmacro %}; ",MatchSource.DOCS,web_common/web_common/templates/utils.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/utils.html
https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/utils.html:482,Availability,failure,failure,482,"{% macro progress_spinner(color) %}. {% endmacro %}. {% macro success_check() %}. check. {% endmacro %}. {% macro fail_cross() %}. close. {% endmacro %}. {% macro cancel_circle() %}. block. {% endmacro %}. {% macro batch_state_indicator(batch) %}; {% if batch['n_jobs'] - batch['n_completed'] > 0 %}; {{ progress_spinner('text-red-600' if batch['state'] == 'failure' else 'text-sky-600') }}; {% elif batch['state'] == 'success' %}; {{ success_check() }}; {% elif batch['state'] == 'failure' %}; {{ fail_cross() }}; {% elif batch['state'] == 'cancelled' %}; {{ cancel_circle() }}; {% endif %}; {% endmacro %}. {% macro job_state_indicator(job) %}; {% if job['state'] == 'Running' %}; {{ progress_spinner('text-sky-600') }}; {% elif job['state'] == 'Success' %}; {{ success_check() }}; {% elif job['state'] == 'Failed' %}; {{ fail_cross() }}; {% elif job['state'] == 'Cancelled' %}; {{ cancel_circle() }}; {% endif %}; {% endmacro %}. {% macro danger_button(text) %}. {{ text }}. {% endmacro %}. {% macro submit_button(text) %}. {{ text }}. {% endmacro %}. {% macro link(href, text) %}. {{ text }}. {% endmacro %}. {% macro truncated_link(href, text) %}. {{ text }}. {% endmacro %}; ",MatchSource.DOCS,web_common/web_common/templates/utils.html,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/web_common/web_common/templates/utils.html
