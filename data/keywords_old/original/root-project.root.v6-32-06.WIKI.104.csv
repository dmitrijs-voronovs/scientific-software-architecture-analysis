id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://root.cern/doc/master/Integrator_8h_source.html:24762,Integrability,interface,interface,24762,"enFunction &f)evaluate the Integral of a function f over the infinite interval (-inf,+inf)Definition Integrator.h:258; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, unsigned int rule=0)Constructor of one dimensional Integrator, default type is adaptive.Definition Integrator.h:123; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(const IGenFunction &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:340; ROOT::Math::IntegratorOneDim::operator()double operator()(double x)define operator() for IntegralLowDefinition Integrator.h:386; ROOT::Math::IntegratorOneDim::SetRelTolerancevoid SetRelTolerance(double relTolerance)set the desired relative ErrorDefinition Integrator.h:435; ROOT::Math::IntegratorOneDim::fIntegratorVirtualIntegratorOneDim * fIntegratorpointer to integrator interface classDefinition Integrator.h:474; ROOT::Math::IntegratorOneDim::Namestd::string Name() constreturn name of integratorDefinition Integrator.h:459; ROOT::Math::IntegratorOneDim::GetIntegratorVirtualIntegratorOneDim * GetIntegrator()return a pointer to integrator objectDefinition Integrator.h:446; ROOT::Math::IntegratorOneDim::operator=IntegratorOneDim & operator=(const IntegratorOneDim &)Definition Integrator.h:182; ROOT::Math::IntegratorOneDim::IntegralLowdouble IntegralLow(double b)evaluate the Integral of a function f over the over the semi-infinite interval (-inf,...Definition Integrator.h:380; ROOT::Math::IntegratorOneDim::IntegralUpdouble IntegralUp(double a)evaluate the Integral of a function f over the semi-infinite interval (a,+inf) using the function pre...Definition Integrator.h:372; ROOT::Math::IntegratorOneDim::Statusint Status() constreturn the Error Status of the last Integral calculationDefinition Integrator.h:421; ROOT::Math::",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:24879,Integrability,integrat,integratorDefinition,24879,"h::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, unsigned int rule=0)Constructor of one dimensional Integrator, default type is adaptive.Definition Integrator.h:123; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(const IGenFunction &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:340; ROOT::Math::IntegratorOneDim::operator()double operator()(double x)define operator() for IntegralLowDefinition Integrator.h:386; ROOT::Math::IntegratorOneDim::SetRelTolerancevoid SetRelTolerance(double relTolerance)set the desired relative ErrorDefinition Integrator.h:435; ROOT::Math::IntegratorOneDim::fIntegratorVirtualIntegratorOneDim * fIntegratorpointer to integrator interface classDefinition Integrator.h:474; ROOT::Math::IntegratorOneDim::Namestd::string Name() constreturn name of integratorDefinition Integrator.h:459; ROOT::Math::IntegratorOneDim::GetIntegratorVirtualIntegratorOneDim * GetIntegrator()return a pointer to integrator objectDefinition Integrator.h:446; ROOT::Math::IntegratorOneDim::operator=IntegratorOneDim & operator=(const IntegratorOneDim &)Definition Integrator.h:182; ROOT::Math::IntegratorOneDim::IntegralLowdouble IntegralLow(double b)evaluate the Integral of a function f over the over the semi-infinite interval (-inf,...Definition Integrator.h:380; ROOT::Math::IntegratorOneDim::IntegralUpdouble IntegralUp(double a)evaluate the Integral of a function f over the semi-infinite interval (a,+inf) using the function pre...Definition Integrator.h:372; ROOT::Math::IntegratorOneDim::Statusint Status() constreturn the Error Status of the last Integral calculationDefinition Integrator.h:421; ROOT::Math::IntegratorOneDim::Resultdouble Result() constreturn the Result of the last Integral calculationDefinition Integrator.h:411; ROOT",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:25022,Integrability,integrat,integrator,25022,"ouble relTol=-1, unsigned int size=0, unsigned int rule=0)Constructor of one dimensional Integrator, default type is adaptive.Definition Integrator.h:123; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(const IGenFunction &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:340; ROOT::Math::IntegratorOneDim::operator()double operator()(double x)define operator() for IntegralLowDefinition Integrator.h:386; ROOT::Math::IntegratorOneDim::SetRelTolerancevoid SetRelTolerance(double relTolerance)set the desired relative ErrorDefinition Integrator.h:435; ROOT::Math::IntegratorOneDim::fIntegratorVirtualIntegratorOneDim * fIntegratorpointer to integrator interface classDefinition Integrator.h:474; ROOT::Math::IntegratorOneDim::Namestd::string Name() constreturn name of integratorDefinition Integrator.h:459; ROOT::Math::IntegratorOneDim::GetIntegratorVirtualIntegratorOneDim * GetIntegrator()return a pointer to integrator objectDefinition Integrator.h:446; ROOT::Math::IntegratorOneDim::operator=IntegratorOneDim & operator=(const IntegratorOneDim &)Definition Integrator.h:182; ROOT::Math::IntegratorOneDim::IntegralLowdouble IntegralLow(double b)evaluate the Integral of a function f over the over the semi-infinite interval (-inf,...Definition Integrator.h:380; ROOT::Math::IntegratorOneDim::IntegralUpdouble IntegralUp(double a)evaluate the Integral of a function f over the semi-infinite interval (a,+inf) using the function pre...Definition Integrator.h:372; ROOT::Math::IntegratorOneDim::Statusint Status() constreturn the Error Status of the last Integral calculationDefinition Integrator.h:421; ROOT::Math::IntegratorOneDim::Resultdouble Result() constreturn the Result of the last Integral calculationDefinition Integrator.h:411; ROOT::Math::IntegratorOneDim::~IntegratorOneDimvirtual ~IntegratorOneDim()destructor (will delete contained pointers)Definition Integra",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:26893,Integrability,integrat,integration,26893,"::IntegratorOneDim::~IntegratorOneDimvirtual ~IntegratorOneDim()destructor (will delete contained pointers)Definition Integrator.h:173; ROOT::Math::IntegratorOneDim::CreateIntegratorVirtualIntegratorOneDim * CreateIntegrator(IntegrationOneDim::Type type, double absTol, double relTol, unsigned int size, int rule)Definition Integrator.cxx:114; ROOT::Math::IntegratorOneDim::SetOptionsvoid SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the optionsDefinition Integrator.h:451; ROOT::Math::IntegratorOneDim::fFuncIGenFunction * fFuncpointer to owned functionDefinition Integrator.h:475; ROOT::Math::IntegratorOneDim::OptionsROOT::Math::IntegratorOneDimOptions Options() constretrieve the optionsDefinition Integrator.h:456; ROOT::Math::IntegratorOneDim::IntegralUpdouble IntegralUp(const IGenFunction &f, double a)evaluate the Integral of a function f over the semi-infinite interval (a,+inf)Definition Integrator.h:278; ROOT::Math::IntegratorOneDim::SetFunctionvoid SetFunction(Function &f)method to set the a generic integration functionDefinition Integrator.h:492; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const IGenFunction &f, double a, double b)evaluate the Integral of a function f over the defined interval (a,b)Definition Integrator.h:241; ROOT::Math::IntegratorOneDim::NEvalint NEval() constreturn number of function evaluations in calculating the integral (if integrator do not implement thi...Definition Integrator.h:427; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(Function &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:536; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:404; ROOT::Math::IntegratorOneDim::TypeIntegrationOneDim::Type TypeDefinition Integra",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:27258,Integrability,integrat,integrator,27258,"::Math::IntegratorOneDim::SetOptionsvoid SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the optionsDefinition Integrator.h:451; ROOT::Math::IntegratorOneDim::fFuncIGenFunction * fFuncpointer to owned functionDefinition Integrator.h:475; ROOT::Math::IntegratorOneDim::OptionsROOT::Math::IntegratorOneDimOptions Options() constretrieve the optionsDefinition Integrator.h:456; ROOT::Math::IntegratorOneDim::IntegralUpdouble IntegralUp(const IGenFunction &f, double a)evaluate the Integral of a function f over the semi-infinite interval (a,+inf)Definition Integrator.h:278; ROOT::Math::IntegratorOneDim::SetFunctionvoid SetFunction(Function &f)method to set the a generic integration functionDefinition Integrator.h:492; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const IGenFunction &f, double a, double b)evaluate the Integral of a function f over the defined interval (a,b)Definition Integrator.h:241; ROOT::Math::IntegratorOneDim::NEvalint NEval() constreturn number of function evaluations in calculating the integral (if integrator do not implement thi...Definition Integrator.h:427; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(Function &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:536; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:404; ROOT::Math::IntegratorOneDim::TypeIntegrationOneDim::Type TypeDefinition Integrator.h:102; ROOT::Math::IntegratorOneDim::Errordouble Error() constreturn the estimate of the absolute Error of the last Integral calculationDefinition Integrator.h:416; ROOT::Math::IntegratorOneDim::Integraldouble Integral(double a, double b)evaluate the Integral over the defined interval (a,b) using the function previously set with Integrat...De",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:28826,Integrability,interface,interface,28826," the integral of a function f over the defined interval (a,...Definition Integrator.h:404; ROOT::Math::IntegratorOneDim::TypeIntegrationOneDim::Type TypeDefinition Integrator.h:102; ROOT::Math::IntegratorOneDim::Errordouble Error() constreturn the estimate of the absolute Error of the last Integral calculationDefinition Integrator.h:416; ROOT::Math::IntegratorOneDim::Integraldouble Integral(double a, double b)evaluate the Integral over the defined interval (a,b) using the function previously set with Integrat...Definition Integrator.h:355; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(Function &f, IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, int rule=0)Template Constructor of one dimensional Integrator passing a generic function object.Definition Integrator.h:165; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(const IGenFunction &f, IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, int rule=0)Constructor of one dimensional Integrator passing a function interface.Definition Integrator.h:142; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const IGenFunction &f, const std::vector< double > &pts)evaluate the Integral of a function f with known singular points over the defined Integral (a,...Definition Integrator.h:316; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(const IntegratorOneDim &)Definition Integrator.h:181; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const std::vector< double > &pts)evaluate the Integral over the defined interval (a,b) using the function previously set with Integrat...Definition Integrator.h:396; ROOT::Math::IntegratorOneDim::IntegralLowdouble IntegralLow(const IGenFunction &f, double b)evaluate the Integral of a function f over the over the semi-infinite interval (-inf,...Definition Integrator.h:296; ROOT::Math::IntegratorOneDim::Integraldouble",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:30089,Integrability,integrat,integration,30089,"finition Integrator.h:316; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(const IntegratorOneDim &)Definition Integrator.h:181; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const std::vector< double > &pts)evaluate the Integral over the defined interval (a,b) using the function previously set with Integrat...Definition Integrator.h:396; ROOT::Math::IntegratorOneDim::IntegralLowdouble IntegralLow(const IGenFunction &f, double b)evaluate the Integral of a function f over the over the semi-infinite interval (-inf,...Definition Integrator.h:296; ROOT::Math::IntegratorOneDim::Integraldouble Integral()evaluate the Integral over the infinite interval (-inf,+inf) using the function previously set with I...Definition Integrator.h:364; ROOT::Math::IntegratorOneDim::GetTypestatic IntegrationOneDim::Type GetType(const char *name)static function to get the enumeration from a stringDefinition Integrator.cxx:53; ROOT::Math::VirtualIntegratorOneDimInterface (abstract) class for 1D numerical integration It must be implemented by the concrete Integr...Definition VirtualIntegrator.h:101; ROOT::Math::VirtualIntegratorOneDim::Optionsvirtual ROOT::Math::IntegratorOneDimOptions Options() const =0get the option used for the integration must be implemented by derived class; ROOT::Math::VirtualIntegratorOneDim::SetFunctionvirtual void SetFunction(const IGenFunction &)=0set integration function; ROOT::Math::VirtualIntegratorOneDim::IntegralCauchyvirtual double IntegralCauchy(double a, double b, double c)=0evaluate Cauchy integral; ROOT::Math::VirtualIntegratorOneDim::IntegralLowvirtual double IntegralLow(double b)=0evaluate integral over the (-inf, b); ROOT::Math::VirtualIntegratorOneDim::SetOptionsvirtual void SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the options (should be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(d",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:30319,Integrability,integrat,integration,30319,"mi-infinite interval (-inf,...Definition Integrator.h:296; ROOT::Math::IntegratorOneDim::Integraldouble Integral()evaluate the Integral over the infinite interval (-inf,+inf) using the function previously set with I...Definition Integrator.h:364; ROOT::Math::IntegratorOneDim::GetTypestatic IntegrationOneDim::Type GetType(const char *name)static function to get the enumeration from a stringDefinition Integrator.cxx:53; ROOT::Math::VirtualIntegratorOneDimInterface (abstract) class for 1D numerical integration It must be implemented by the concrete Integr...Definition VirtualIntegrator.h:101; ROOT::Math::VirtualIntegratorOneDim::Optionsvirtual ROOT::Math::IntegratorOneDimOptions Options() const =0get the option used for the integration must be implemented by derived class; ROOT::Math::VirtualIntegratorOneDim::SetFunctionvirtual void SetFunction(const IGenFunction &)=0set integration function; ROOT::Math::VirtualIntegratorOneDim::IntegralCauchyvirtual double IntegralCauchy(double a, double b, double c)=0evaluate Cauchy integral; ROOT::Math::VirtualIntegratorOneDim::IntegralLowvirtual double IntegralLow(double b)=0evaluate integral over the (-inf, b); ROOT::Math::VirtualIntegratorOneDim::SetOptionsvirtual void SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the options (should be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(double a, double b)=0evaluate integral; ROOT::Math::VirtualIntegratorOneDim::IntegralUpvirtual double IntegralUp(double a)=0evaluate integral over the (a, +inf); ROOT::Math::VirtualIntegrator::SetRelTolerancevirtual void SetRelTolerance(double)=0set the desired relative Error; ROOT::Math::VirtualIntegrator::SetAbsTolerancevirtual void SetAbsTolerance(double)=0set the desired absolute Error; ROOT::Math::VirtualIntegrator::Errorvirtual double Error() const =0return the estimate of the absolute Error of the last",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:30469,Integrability,integrat,integration,30469,"mi-infinite interval (-inf,...Definition Integrator.h:296; ROOT::Math::IntegratorOneDim::Integraldouble Integral()evaluate the Integral over the infinite interval (-inf,+inf) using the function previously set with I...Definition Integrator.h:364; ROOT::Math::IntegratorOneDim::GetTypestatic IntegrationOneDim::Type GetType(const char *name)static function to get the enumeration from a stringDefinition Integrator.cxx:53; ROOT::Math::VirtualIntegratorOneDimInterface (abstract) class for 1D numerical integration It must be implemented by the concrete Integr...Definition VirtualIntegrator.h:101; ROOT::Math::VirtualIntegratorOneDim::Optionsvirtual ROOT::Math::IntegratorOneDimOptions Options() const =0get the option used for the integration must be implemented by derived class; ROOT::Math::VirtualIntegratorOneDim::SetFunctionvirtual void SetFunction(const IGenFunction &)=0set integration function; ROOT::Math::VirtualIntegratorOneDim::IntegralCauchyvirtual double IntegralCauchy(double a, double b, double c)=0evaluate Cauchy integral; ROOT::Math::VirtualIntegratorOneDim::IntegralLowvirtual double IntegralLow(double b)=0evaluate integral over the (-inf, b); ROOT::Math::VirtualIntegratorOneDim::SetOptionsvirtual void SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the options (should be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(double a, double b)=0evaluate integral; ROOT::Math::VirtualIntegratorOneDim::IntegralUpvirtual double IntegralUp(double a)=0evaluate integral over the (a, +inf); ROOT::Math::VirtualIntegrator::SetRelTolerancevirtual void SetRelTolerance(double)=0set the desired relative Error; ROOT::Math::VirtualIntegrator::SetAbsTolerancevirtual void SetAbsTolerance(double)=0set the desired absolute Error; ROOT::Math::VirtualIntegrator::Errorvirtual double Error() const =0return the estimate of the absolute Error of the last",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:31742,Integrability,integrat,integrator,31742,"alIntegratorOneDim::SetFunctionvirtual void SetFunction(const IGenFunction &)=0set integration function; ROOT::Math::VirtualIntegratorOneDim::IntegralCauchyvirtual double IntegralCauchy(double a, double b, double c)=0evaluate Cauchy integral; ROOT::Math::VirtualIntegratorOneDim::IntegralLowvirtual double IntegralLow(double b)=0evaluate integral over the (-inf, b); ROOT::Math::VirtualIntegratorOneDim::SetOptionsvirtual void SetOptions(const ROOT::Math::IntegratorOneDimOptions &opt)set the options (should be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(double a, double b)=0evaluate integral; ROOT::Math::VirtualIntegratorOneDim::IntegralUpvirtual double IntegralUp(double a)=0evaluate integral over the (a, +inf); ROOT::Math::VirtualIntegrator::SetRelTolerancevirtual void SetRelTolerance(double)=0set the desired relative Error; ROOT::Math::VirtualIntegrator::SetAbsTolerancevirtual void SetAbsTolerance(double)=0set the desired absolute Error; ROOT::Math::VirtualIntegrator::Errorvirtual double Error() const =0return the estimate of the absolute Error of the last Integral calculation; ROOT::Math::VirtualIntegrator::NEvalvirtual int NEval() constreturn number of function evaluations in calculating the integral (if integrator do not implement thi...Definition VirtualIntegrator.h:84; ROOT::Math::VirtualIntegrator::Resultvirtual double Result() const =0return the Result of the last Integral calculation; ROOT::Math::VirtualIntegrator::Statusvirtual int Status() const =0return the Error Status of the last Integral calculation; ROOT::Math::WrappedFunctionTemplate class to wrap any C++ callable object which takes one argument i.e.Definition WrappedFunction.h:45; ROOT::Math::IntegrationOneDim::TypeTypeenumeration specifying the integration types.Definition AllIntegrationTypes.h:32; ROOT::Math::IntegrationOneDim::kDEFAULT@ kDEFAULTdefault type specified i",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:32101,Integrability,wrap,wrap,32101,"uld be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(double a, double b)=0evaluate integral; ROOT::Math::VirtualIntegratorOneDim::IntegralUpvirtual double IntegralUp(double a)=0evaluate integral over the (a, +inf); ROOT::Math::VirtualIntegrator::SetRelTolerancevirtual void SetRelTolerance(double)=0set the desired relative Error; ROOT::Math::VirtualIntegrator::SetAbsTolerancevirtual void SetAbsTolerance(double)=0set the desired absolute Error; ROOT::Math::VirtualIntegrator::Errorvirtual double Error() const =0return the estimate of the absolute Error of the last Integral calculation; ROOT::Math::VirtualIntegrator::NEvalvirtual int NEval() constreturn number of function evaluations in calculating the integral (if integrator do not implement thi...Definition VirtualIntegrator.h:84; ROOT::Math::VirtualIntegrator::Resultvirtual double Result() const =0return the Result of the last Integral calculation; ROOT::Math::VirtualIntegrator::Statusvirtual int Status() const =0return the Error Status of the last Integral calculation; ROOT::Math::WrappedFunctionTemplate class to wrap any C++ callable object which takes one argument i.e.Definition WrappedFunction.h:45; ROOT::Math::IntegrationOneDim::TypeTypeenumeration specifying the integration types.Definition AllIntegrationTypes.h:32; ROOT::Math::IntegrationOneDim::kDEFAULT@ kDEFAULTdefault type specified in the static optionsDefinition AllIntegrationTypes.h:33; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::IntegratorIntegratorOneDim IntegratorDefinition Integrator.h:480; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathIntegrator.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:32258,Integrability,integrat,integration,32258,"uld be re-implemented by derived classes -if more options than tolerance existDefinition VirtualIntegrator.h:140; ROOT::Math::VirtualIntegratorOneDim::Integralvirtual double Integral(double a, double b)=0evaluate integral; ROOT::Math::VirtualIntegratorOneDim::IntegralUpvirtual double IntegralUp(double a)=0evaluate integral over the (a, +inf); ROOT::Math::VirtualIntegrator::SetRelTolerancevirtual void SetRelTolerance(double)=0set the desired relative Error; ROOT::Math::VirtualIntegrator::SetAbsTolerancevirtual void SetAbsTolerance(double)=0set the desired absolute Error; ROOT::Math::VirtualIntegrator::Errorvirtual double Error() const =0return the estimate of the absolute Error of the last Integral calculation; ROOT::Math::VirtualIntegrator::NEvalvirtual int NEval() constreturn number of function evaluations in calculating the integral (if integrator do not implement thi...Definition VirtualIntegrator.h:84; ROOT::Math::VirtualIntegrator::Resultvirtual double Result() const =0return the Result of the last Integral calculation; ROOT::Math::VirtualIntegrator::Statusvirtual int Status() const =0return the Error Status of the last Integral calculation; ROOT::Math::WrappedFunctionTemplate class to wrap any C++ callable object which takes one argument i.e.Definition WrappedFunction.h:45; ROOT::Math::IntegrationOneDim::TypeTypeenumeration specifying the integration types.Definition AllIntegrationTypes.h:32; ROOT::Math::IntegrationOneDim::kDEFAULT@ kDEFAULTdefault type specified in the static optionsDefinition AllIntegrationTypes.h:33; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::IntegratorIntegratorOneDim IntegratorDefinition Integrator.h:480; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathIntegrator.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:1276,Modifiability,adapt,adaptive,1276,"***********************************; 5 * *; 6 * Copyright (c) 2007 ROOT Foundation, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// Header file for class Integrator; 12//; 13//; 14#ifndef ROOT_Math_Integrator; 15#define ROOT_Math_Integrator; 16 ; 17#include ""Math/AllIntegrationTypes.h""; 18 ; 19#include ""Math/IntegratorOptions.h""; 20 ; 21#include ""Math/IFunction.h""; 22 ; 23#include ""Math/VirtualIntegrator.h""; 24 ; 25#include <memory>; 26#include <vector>; 27#include <string>; 28 ; 29 ; 30/**; 31@defgroup NumAlgo Numerical Algorithms; 32 ; 33Numerical Algorithm classes from the \ref MathCore and \ref MathMore libraries.; 34 ; 35@ingroup MathCore; 36@ingroup MathMore; 37 ; 38*/; 39 ; 40 ; 41/**; 42 ; 43@defgroup Integration Numerical Integration; 44 ; 45Classes for numerical integration of functions.; 46These classes provide algorithms for integration of one-dimensional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerica",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:1293,Modifiability,adapt,adaptive,1293,"***********************************; 5 * *; 6 * Copyright (c) 2007 ROOT Foundation, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// Header file for class Integrator; 12//; 13//; 14#ifndef ROOT_Math_Integrator; 15#define ROOT_Math_Integrator; 16 ; 17#include ""Math/AllIntegrationTypes.h""; 18 ; 19#include ""Math/IntegratorOptions.h""; 20 ; 21#include ""Math/IFunction.h""; 22 ; 23#include ""Math/VirtualIntegrator.h""; 24 ; 25#include <memory>; 26#include <vector>; 27#include <string>; 28 ; 29 ; 30/**; 31@defgroup NumAlgo Numerical Algorithms; 32 ; 33Numerical Algorithm classes from the \ref MathCore and \ref MathMore libraries.; 34 ; 35@ingroup MathCore; 36@ingroup MathMore; 37 ; 38*/; 39 ; 40 ; 41/**; 42 ; 43@defgroup Integration Numerical Integration; 44 ; 45Classes for numerical integration of functions.; 46These classes provide algorithms for integration of one-dimensional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerica",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:1372,Modifiability,adapt,adaptive,1372,"***********************************; 5 * *; 6 * Copyright (c) 2007 ROOT Foundation, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// Header file for class Integrator; 12//; 13//; 14#ifndef ROOT_Math_Integrator; 15#define ROOT_Math_Integrator; 16 ; 17#include ""Math/AllIntegrationTypes.h""; 18 ; 19#include ""Math/IntegratorOptions.h""; 20 ; 21#include ""Math/IFunction.h""; 22 ; 23#include ""Math/VirtualIntegrator.h""; 24 ; 25#include <memory>; 26#include <vector>; 27#include <string>; 28 ; 29 ; 30/**; 31@defgroup NumAlgo Numerical Algorithms; 32 ; 33Numerical Algorithm classes from the \ref MathCore and \ref MathMore libraries.; 34 ; 35@ingroup MathCore; 36@ingroup MathMore; 37 ; 38*/; 39 ; 40 ; 41/**; 42 ; 43@defgroup Integration Numerical Integration; 44 ; 45Classes for numerical integration of functions.; 46These classes provide algorithms for integration of one-dimensional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerica",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:1670,Modifiability,config,configured,1670,"19#include ""Math/IntegratorOptions.h""; 20 ; 21#include ""Math/IFunction.h""; 22 ; 23#include ""Math/VirtualIntegrator.h""; 24 ; 25#include <memory>; 26#include <vector>; 27#include <string>; 28 ; 29 ; 30/**; 31@defgroup NumAlgo Numerical Algorithms; 32 ; 33Numerical Algorithm classes from the \ref MathCore and \ref MathMore libraries.; 34 ; 35@ingroup MathCore; 36@ingroup MathMore; 37 ; 38*/; 39 ; 40 ; 41/**; 42 ; 43@defgroup Integration Numerical Integration; 44 ; 45Classes for numerical integration of functions.; 46These classes provide algorithms for integration of one-dimensional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integr",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2161,Modifiability,plug-in,plug-in,2161,"ional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2373,Modifiability,adapt,adaptive,2373,"d or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2390,Modifiability,adapt,adaptive,2390,"d or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2651,Modifiability,adapt,adaptive,2651," with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on GSL integration algorithms see the; 90<A HREF=""http://www.gnu.org/software/gsl/manual/gsl-ref_16.html#SEC248"">GSL Manual</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:3817,Modifiability,adapt,adaptive,3817,"NGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on GSL integration algorithms see the; 90<A HREF=""http://www.gnu.org/software/gsl/manual/gsl-ref_16.html#SEC248"">GSL Manual</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; 104 // constructors; 105 ; 106 ; 107 /**; 108 Constructor of one dimensional Integrator, default type is adaptive; 109 ; 110 @param type integration type (adaptive, non-adaptive, etc..); 111 @param absTol desired absolute Error; 112 @param relTol desired relative Error; 113 @param size maximum number of sub-intervals; 114 @param rule Gauss-Kronrod integration rule (only for GSL kADAPTIVE type); 115 ; 116 Possible type values are : kGAUSS (simple Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(Integrati",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:3867,Modifiability,adapt,adaptive,3867,"NGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on GSL integration algorithms see the; 90<A HREF=""http://www.gnu.org/software/gsl/manual/gsl-ref_16.html#SEC248"">GSL Manual</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; 104 // constructors; 105 ; 106 ; 107 /**; 108 Constructor of one dimensional Integrator, default type is adaptive; 109 ; 110 @param type integration type (adaptive, non-adaptive, etc..); 111 @param absTol desired absolute Error; 112 @param relTol desired relative Error; 113 @param size maximum number of sub-intervals; 114 @param rule Gauss-Kronrod integration rule (only for GSL kADAPTIVE type); 115 ; 116 Possible type values are : kGAUSS (simple Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(Integrati",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:3881,Modifiability,adapt,adaptive,3881,"NGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on GSL integration algorithms see the; 90<A HREF=""http://www.gnu.org/software/gsl/manual/gsl-ref_16.html#SEC248"">GSL Manual</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; 104 // constructors; 105 ; 106 ; 107 /**; 108 Constructor of one dimensional Integrator, default type is adaptive; 109 ; 110 @param type integration type (adaptive, non-adaptive, etc..); 111 @param absTol desired absolute Error; 112 @param relTol desired relative Error; 113 @param size maximum number of sub-intervals; 114 @param rule Gauss-Kronrod integration rule (only for GSL kADAPTIVE type); 115 ; 116 Possible type values are : kGAUSS (simple Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(Integrati",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:5173,Modifiability,adapt,adaptive,5173,"le Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, unsigned int rule = 0) :; 124 fIntegrator(nullptr), fFunc(nullptr); 125 {; 126 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 127 }; 128 ; 129 /**; 130 Constructor of one dimensional Integrator passing a function interface; 131 ; 132 @param f integration function (1D interface). It is copied inside; 133 @param type integration type (adaptive, non-adaptive, etc..); 134 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 135 @param relTol desired relative tolerance; 136 @param size maximum number of sub-intervals; 137 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 138 ; 139 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 140 */; 141 explicit; 142 IntegratorOneDim(const IGenFunction &f, IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, int rule = 0) :; 143 fIntegrator(nullptr), fFunc(nullptr); 144 {; 145 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 146 SetFunction(f,true);; 147 }; 148 ; 149 /**; 150 Template Constructor of one dimensional Integrator passing a generic function object; 151 ; 152 @param f integration function (any C+",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:5187,Modifiability,adapt,adaptive,5187,"le Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, unsigned int rule = 0) :; 124 fIntegrator(nullptr), fFunc(nullptr); 125 {; 126 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 127 }; 128 ; 129 /**; 130 Constructor of one dimensional Integrator passing a function interface; 131 ; 132 @param f integration function (1D interface). It is copied inside; 133 @param type integration type (adaptive, non-adaptive, etc..); 134 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 135 @param relTol desired relative tolerance; 136 @param size maximum number of sub-intervals; 137 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 138 ; 139 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 140 */; 141 explicit; 142 IntegratorOneDim(const IGenFunction &f, IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, int rule = 0) :; 143 fIntegrator(nullptr), fFunc(nullptr); 144 {; 145 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 146 SetFunction(f,true);; 147 }; 148 ; 149 /**; 150 Template Constructor of one dimensional Integrator passing a generic function object; 151 ; 152 @param f integration function (any C+",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:6246,Modifiability,adapt,adaptive,6246," size = 0, unsigned int rule = 0) :; 124 fIntegrator(nullptr), fFunc(nullptr); 125 {; 126 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 127 }; 128 ; 129 /**; 130 Constructor of one dimensional Integrator passing a function interface; 131 ; 132 @param f integration function (1D interface). It is copied inside; 133 @param type integration type (adaptive, non-adaptive, etc..); 134 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 135 @param relTol desired relative tolerance; 136 @param size maximum number of sub-intervals; 137 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 138 ; 139 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 140 */; 141 explicit; 142 IntegratorOneDim(const IGenFunction &f, IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, int rule = 0) :; 143 fIntegrator(nullptr), fFunc(nullptr); 144 {; 145 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 146 SetFunction(f,true);; 147 }; 148 ; 149 /**; 150 Template Constructor of one dimensional Integrator passing a generic function object; 151 ; 152 @param f integration function (any C++ callable object implementing operator()(double x); 153 @param type integration type (adaptive, non-adaptive, etc..); 154 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 155 @param relTol desired relative tolerance; 156 @param size maximum number of sub-intervals; 157 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 158 ; 159 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 160 ; 161 */; 162 ; 163 template<class Function>; 164 explicit; 165 IntegratorOneDim(F",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:6260,Modifiability,adapt,adaptive,6260," size = 0, unsigned int rule = 0) :; 124 fIntegrator(nullptr), fFunc(nullptr); 125 {; 126 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 127 }; 128 ; 129 /**; 130 Constructor of one dimensional Integrator passing a function interface; 131 ; 132 @param f integration function (1D interface). It is copied inside; 133 @param type integration type (adaptive, non-adaptive, etc..); 134 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 135 @param relTol desired relative tolerance; 136 @param size maximum number of sub-intervals; 137 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 138 ; 139 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 140 */; 141 explicit; 142 IntegratorOneDim(const IGenFunction &f, IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, int rule = 0) :; 143 fIntegrator(nullptr), fFunc(nullptr); 144 {; 145 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 146 SetFunction(f,true);; 147 }; 148 ; 149 /**; 150 Template Constructor of one dimensional Integrator passing a generic function object; 151 ; 152 @param f integration function (any C++ callable object implementing operator()(double x); 153 @param type integration type (adaptive, non-adaptive, etc..); 154 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 155 @param relTol desired relative tolerance; 156 @param size maximum number of sub-intervals; 157 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE type); 158 ; 159 NOTE: When no values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 160 ; 161 */; 162 ; 163 template<class Function>; 164 explicit; 165 IntegratorOneDim(F",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:8567,Modifiability,variab,variables,8567,"gratorOneDim & operator=(const IntegratorOneDim &) { return *this; }; 183 ; 184public:; 185 ; 186 ; 187 // template methods for generic functors; 188 ; 189 /**; 190 method to set the a generic integration function; 191 @param f integration function. The function type must implement the assignment operator, <em> double operator() ( double x ) </em>; 192 ; 193 */; 194 ; 195 ; 196 template<class Function>; 197 inline void SetFunction(Function & f);; 198 ; 199 /**; 200 set one dimensional function for 1D integration; 201 */; 202 void SetFunction (const IGenFunction &f, bool copy = false) {; 203 if (!fIntegrator) return;; 204 if (copy) {; 205 if (fFunc) delete fFunc;; 206 fFunc = f.Clone();; 207 fIntegrator->SetFunction(*fFunc);; 208 return;; 209 }; 210 fIntegrator->SetFunction(f);; 211 }; 212 ; 213 ; 214 /**; 215 Set integration function from a multi-dim function type.; 216 Can be used in case of having 1D function implementing the generic interface; 217 @param f integration function; 218 @param icoord index of coordinate on which the integration is performed; 219 @param x array of the passed variables values. In case of dim=1 a 0 can be passed; 220 */; 221 void SetFunction(const IMultiGenFunction &f, unsigned int icoord , const double * x );; 222 ; 223 // integration methods using a function; 224 ; 225 /**; 226 evaluate the Integral of a function f over the defined interval (a,b); 227 @param f integration function. The function type must be a C++ callable object implementing operator()(double x); 228 @param a lower value of the integration interval; 229 @param b upper value of the integration interval; 230 */; 231 template<class Function>; 232 double Integral(Function & f, double a, double b);; 233 ; 234 ; 235 /**; 236 evaluate the Integral of a function f over the defined interval (a,b); 237 @param f integration function. The function type must implement the mathlib::IGenFunction interface; 238 @param a lower value of the integration interval; 239 @param b upper value",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:24103,Modifiability,adapt,adaptive,24103,"overridename of 1D integratorDefinition IntegratorOptions.cxx:203; ROOT::Math::IntegratorOneDimUser Class for performing numerical integration of a function in one dimension.Definition Integrator.h:98; ROOT::Math::IntegratorOneDim::SetAbsTolerancevoid SetAbsTolerance(double absTolerance)set the desired absolute ErrorDefinition Integrator.h:441; ROOT::Math::IntegratorOneDim::GetNamestatic std::string GetName(IntegrationOneDim::Type)static function to get a string from the enumerationDefinition Integrator.cxx:66; ROOT::Math::IntegratorOneDim::SetFunctionvoid SetFunction(const IGenFunction &f, bool copy=false)set one dimensional function for 1D integrationDefinition Integrator.h:202; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const IGenFunction &f)evaluate the Integral of a function f over the infinite interval (-inf,+inf)Definition Integrator.h:258; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, unsigned int rule=0)Constructor of one dimensional Integrator, default type is adaptive.Definition Integrator.h:123; ROOT::Math::IntegratorOneDim::IntegralCauchydouble IntegralCauchy(const IGenFunction &f, double a, double b, double c)evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,...Definition Integrator.h:340; ROOT::Math::IntegratorOneDim::operator()double operator()(double x)define operator() for IntegralLowDefinition Integrator.h:386; ROOT::Math::IntegratorOneDim::SetRelTolerancevoid SetRelTolerance(double relTolerance)set the desired relative ErrorDefinition Integrator.h:435; ROOT::Math::IntegratorOneDim::fIntegratorVirtualIntegratorOneDim * fIntegratorpointer to integrator interface classDefinition Integrator.h:474; ROOT::Math::IntegratorOneDim::Namestd::string Name() constreturn name of integratorDefinition Integrator.h:459; ROOT::Math::IntegratorOneDim::GetIntegratorVirtualIntegrat",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2081,Performance,perform,performing,2081,"ies.; 34 ; 35@ingroup MathCore; 36@ingroup MathMore; 37 ; 38*/; 39 ; 40 ; 41/**; 42 ; 43@defgroup Integration Numerical Integration; 44 ; 45Classes for numerical integration of functions.; 46These classes provide algorithms for integration of one-dimensional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and sem",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:2180,Performance,load,load,2180,"ional functions, with several adaptive and non-adaptive methods; 47and for integration of multi-dimensional function using an adaptive method or MonteCarlo Integration (GSLMCIntegrator).; 48The basic classes ROOT::Math::IntegratorOneDim provides a common interface for the one-dimensional methods while the class; 49ROOT::Math::IntegratorMultiDim provides the interface for the multi-dimensional ones.; 50The methods can be configured (e.g setting the default method with its default parameters) using the ROOT::Math::IntegratorOneDimOptions and; 51ROOT::Math::IntegratorMultiDimOptions classes.; 52 ; 53@ingroup NumAlgo; 54 ; 55*/; 56 ; 57 ; 58 ; 59namespace ROOT {; 60namespace Math {; 61 ; 62 ; 63 ; 64 ; 65//____________________________________________________________________________________________; 66/**; 67 ; 68User Class for performing numerical integration of a function in one dimension.; 69It uses the plug-in manager to load advanced numerical integration algorithms from GSL, which reimplements the; 70algorithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:3192,Performance,perform,performances,3192,"ithms used in the QUADPACK, a numerical integration package written in Fortran.; 71 ; 72Various types of adaptive and non-adaptive integration are supported. These include; 73integration over infinite and semi-infinite ranges and singular integrals.; 74 ; 75The integration type is selected using the Integration::type enumeration; 76in the class constructor.; 77The default type is adaptive integration with singularity; 78(ADAPTIVESINGULAR or QAGS in the QUADPACK convention) applying a Gauss-Kronrod 21-point integration rule.; 79In the case of ADAPTIVE type, the integration rule can also be specified via the; 80Integration::GKRule. The default rule is 31 points.; 81 ; 82In the case of integration over infinite and semi-infinite ranges, the type used is always; 83ADAPTIVESINGULAR applying a transformation from the original interval into (0,1).; 84 ; 85The ADAPTIVESINGULAR type is the most sophisticated type. When performances are; 86important, it is then recommended to use the NONADAPTIVE type in case of smooth functions or; 87 ADAPTIVE with a lower Gauss-Kronrod rule.; 88 ; 89For detailed description on GSL integration algorithms see the; 90<A HREF=""http://www.gnu.org/software/gsl/manual/gsl-ref_16.html#SEC248"">GSL Manual</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; 104 // constructors; 105 ; 106 ; 107 /**; 108 Constructor of one dimensional Integrator, default type is adaptive; 109 ; 110 @param type integration type (adaptive, non-adaptive, etc..); 111 @param absTol desired absolute Error; 112 @param relTol desired relative Error; 113 @param size maximum number of sub-intervals; 114 @param rule Gauss-Kronrod integration rule (only for GSL kADAPTIVE type); 115 ; 116 Possible type values are : kGAUSS (simple Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible r",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:8523,Performance,perform,performed,8523,"gratorOneDim & operator=(const IntegratorOneDim &) { return *this; }; 183 ; 184public:; 185 ; 186 ; 187 // template methods for generic functors; 188 ; 189 /**; 190 method to set the a generic integration function; 191 @param f integration function. The function type must implement the assignment operator, <em> double operator() ( double x ) </em>; 192 ; 193 */; 194 ; 195 ; 196 template<class Function>; 197 inline void SetFunction(Function & f);; 198 ; 199 /**; 200 set one dimensional function for 1D integration; 201 */; 202 void SetFunction (const IGenFunction &f, bool copy = false) {; 203 if (!fIntegrator) return;; 204 if (copy) {; 205 if (fFunc) delete fFunc;; 206 fFunc = f.Clone();; 207 fIntegrator->SetFunction(*fFunc);; 208 return;; 209 }; 210 fIntegrator->SetFunction(f);; 211 }; 212 ; 213 ; 214 /**; 215 Set integration function from a multi-dim function type.; 216 Can be used in case of having 1D function implementing the generic interface; 217 @param f integration function; 218 @param icoord index of coordinate on which the integration is performed; 219 @param x array of the passed variables values. In case of dim=1 a 0 can be passed; 220 */; 221 void SetFunction(const IMultiGenFunction &f, unsigned int icoord , const double * x );; 222 ; 223 // integration methods using a function; 224 ; 225 /**; 226 evaluate the Integral of a function f over the defined interval (a,b); 227 @param f integration function. The function type must be a C++ callable object implementing operator()(double x); 228 @param a lower value of the integration interval; 229 @param b upper value of the integration interval; 230 */; 231 template<class Function>; 232 double Integral(Function & f, double a, double b);; 233 ; 234 ; 235 /**; 236 evaluate the Integral of a function f over the defined interval (a,b); 237 @param f integration function. The function type must implement the mathlib::IGenFunction interface; 238 @param a lower value of the integration interval; 239 @param b upper value",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:14134,Performance,cache,cached,14134,"type must be a C++ callable object implementing operator()(double x); 324 @param a lower value of the integration interval; 325 @param b upper value of the integration interval; 326 @param c position of singularity; 327 ; 328 */; 329 template<class Function>; 330 double IntegralCauchy(Function & f, double a, double b, double c);; 331 ; 332 /**; 333 evaluate the Cauchy principal value of the integral of a function f over the defined interval (a,b) with a singularity at c; 334 @param f integration function. The function type must implement the mathlib::IGenFunction interface; 335 @param a lower value of the integration interval; 336 @param b upper value of the integration interval; 337 @param c position of singularity; 338 ; 339 */; 340 double IntegralCauchy(const IGenFunction & f, double a, double b, double c) {; 341 SetFunction(f,false);; 342 return IntegralCauchy(a,b,c);; 343 }; 344 ; 345 ; 346 ; 347 // integration method using cached function; 348 ; 349 /**; 350 evaluate the Integral over the defined interval (a,b) using the function previously set with Integrator::SetFunction method; 351 @param a lower value of the integration interval; 352 @param b upper value of the integration interval; 353 */; 354 ; 355 double Integral(double a, double b) {; 356 return !fIntegrator ? 0 : fIntegrator->Integral(a,b);; 357 }; 358 ; 359 ; 360 /**; 361 evaluate the Integral over the infinite interval (-inf,+inf) using the function previously set with Integrator::SetFunction method.; 362 */; 363 ; 364 double Integral( ) {; 365 return !fIntegrator ? 0 : fIntegrator->Integral();; 366 }; 367 ; 368 /**; 369 evaluate the Integral of a function f over the semi-infinite interval (a,+inf) using the function previously set with Integrator::SetFunction method.; 370 @param a lower value of the integration interval; 371 */; 372 double IntegralUp(double a ) {; 373 return !fIntegrator ? 0 : fIntegrator->IntegralUp(a);; 374 }; 375 ; 376 /**; 377 evaluate the Integral of a function f over the over ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:23085,Performance,perform,performing,23085,"ta h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; VirtualIntegrator.h; WrappedFunction.h; FunctionDouble_t(* Function)(Double_t)Definition Functor.C:4; ROOT::Math::IBaseFunctionMultiDimTemplDocumentation for the abstract class IBaseFunctionMultiDim.Definition IFunction.h:61; ROOT::Math::IBaseFunctionOneDimInterface (abstract class) for generic functions objects of one-dimension Provides a method to evalua...Definition IFunction.h:112; ROOT::Math::IBaseFunctionOneDim::Clonevirtual IBaseFunctionOneDim * Clone() const =0Clone a function.; ROOT::Math::IntegratorOneDimOptionsNumerical one dimensional integration options.Definition IntegratorOptions.h:113; ROOT::Math::IntegratorOneDimOptions::Integratorstd::string Integrator() const overridename of 1D integratorDefinition IntegratorOptions.cxx:203; ROOT::Math::IntegratorOneDimUser Class for performing numerical integration of a function in one dimension.Definition Integrator.h:98; ROOT::Math::IntegratorOneDim::SetAbsTolerancevoid SetAbsTolerance(double absTolerance)set the desired absolute ErrorDefinition Integrator.h:441; ROOT::Math::IntegratorOneDim::GetNamestatic std::string GetName(IntegrationOneDim::Type)static function to get a string from the enumerationDefinition Integrator.cxx:66; ROOT::Math::IntegratorOneDim::SetFunctionvoid SetFunction(const IGenFunction &f, bool copy=false)set one dimensional function for 1D integrationDefinition Integrator.h:202; ROOT::Math::IntegratorOneDim::Integraldouble Integral(const IGenFunction &f)evaluate the Integral of a function f over the infinite interval (-inf,+inf)Definition Integrator.h:258; ROOT::Math::IntegratorOneDim::IntegratorOneDimIntegratorOneDim(IntegrationOneDim::Type type=IntegrationOneDim::kDEFAULT, double absTol=-1, double relTol=-1, unsigned int size=0, unsigned int rule=0)Constructor of one dimensional Integrator, defa",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/Integrator_8h_source.html:4155,Usability,simpl,simple,4155,"l</A>.; 91 ; 92 ; 93@ingroup Integration; 94 ; 95*/; 96 ; 97 ; 98class IntegratorOneDim {; 99 ; 100public:; 101 ; 102 typedef IntegrationOneDim::Type Type; // for the enumerations defining the types; 103 ; 104 // constructors; 105 ; 106 ; 107 /**; 108 Constructor of one dimensional Integrator, default type is adaptive; 109 ; 110 @param type integration type (adaptive, non-adaptive, etc..); 111 @param absTol desired absolute Error; 112 @param relTol desired relative Error; 113 @param size maximum number of sub-intervals; 114 @param rule Gauss-Kronrod integration rule (only for GSL kADAPTIVE type); 115 ; 116 Possible type values are : kGAUSS (simple Gauss method), kADAPTIVE (from GSL), kADAPTIVESINGULAR (from GSL), kNONADAPTIVE (from GSL); 117 Possible rule values are kGAUS15 (rule = 1), kGAUS21( rule = 2), kGAUS31(rule =3), kGAUS41 (rule=4), kGAUS51 (rule =5), kGAUS61(rule =6); 118 lower rules are indicated for singular functions while higher for smooth functions to get better accuracies; 119 ; 120 NOTE: When the default values are passed, the values used are taken from the default defined in ROOT::Math::IntegratorOneDimOptions; 121 */; 122 explicit; 123 IntegratorOneDim(IntegrationOneDim::Type type = IntegrationOneDim::kDEFAULT, double absTol = -1, double relTol = -1, unsigned int size = 0, unsigned int rule = 0) :; 124 fIntegrator(nullptr), fFunc(nullptr); 125 {; 126 fIntegrator = CreateIntegrator(type, absTol, relTol, size, rule);; 127 }; 128 ; 129 /**; 130 Constructor of one dimensional Integrator passing a function interface; 131 ; 132 @param f integration function (1D interface). It is copied inside; 133 @param type integration type (adaptive, non-adaptive, etc..); 134 @param absTol desired absolute tolerance. The algorithm will stop when either the absolute OR the relative tolerance are satisfied.; 135 @param relTol desired relative tolerance; 136 @param size maximum number of sub-intervals; 137 @param rule Gauss-Kronrod integration rule (only for GSL ADAPTIVE ",MatchSource.WIKI,doc/master/Integrator_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Integrator_8h_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:2699,Availability,error,errorIgnoreLevel,2699,"deBase;; 50}; 51}; 52namespace RDF {; 53template <typename T>; 54class RResultPtr;; 55template<typename T, typename V>; 56class RInterface;; 57using RNode = RInterface<::ROOT::Detail::RDF::RNodeBase, void>;; 58class RDataSource;; 59} // namespace RDF; 60 ; 61} // namespace ROOT; 62 ; 63/// \cond HIDDEN_SYMBOLS; 64 ; 65namespace ROOT {; 66namespace Internal {; 67namespace RDF {; 68using namespace ROOT::Detail::RDF;; 69using namespace ROOT::RDF;; 70namespace TTraits = ROOT::TypeTraits;; 71 ; 72std::string DemangleTypeIdName(const std::type_info &typeInfo);; 73 ; 74ColumnNames_t; 75ConvertRegexToColumns(const ColumnNames_t &colNames, std::string_view columnNameRegexp, std::string_view callerName);; 76 ; 77/// An helper object that sets and resets gErrorIgnoreLevel via RAII.; 78class RIgnoreErrorLevelRAII {; 79private:; 80 int fCurIgnoreErrorLevel = gErrorIgnoreLevel;; 81 ; 82public:; 83 RIgnoreErrorLevelRAII(int errorIgnoreLevel) { gErrorIgnoreLevel = errorIgnoreLevel; }; 84 ~RIgnoreErrorLevelRAII() { gErrorIgnoreLevel = fCurIgnoreErrorLevel; }; 85};; 86 ; 87/****** BuildAction overloads *******/; 88 ; 89// clang-format off; 90/// This namespace defines types to be used for tag dispatching in RInterface.; 91namespace ActionTags {; 92struct Histo1D{};; 93struct Histo2D{};; 94struct Histo3D{};; 95struct HistoND{};; 96struct Graph{};; 97struct GraphAsymmErrors{};; 98struct Profile1D{};; 99struct Profile2D{};; 100struct Min{};; 101struct Max{};; 102struct Sum{};; 103struct Mean{};; 104struct Fill{};; 105struct StdDev{};; 106struct Display{};; 107struct Snapshot{};; 108struct Book{};; 109}; 110// clang-format on; 111 ; 112template <typename T, bool ISV6HISTO = std::is_base_of<TH1, std::decay_t<T>>::value>; 113struct HistoUtils {; 114 static void SetCanExtendAllAxes(T &h) { h.SetCanExtend(::TH1::kAllAxes); }; 115 static bool HasAxisLimits(T &h); 116 {; 117 auto xaxis = h.GetXaxis();; 118 return !(xaxis->GetXmin() == 0. && xaxis->GetXmax() == 0.);; 119 }; 120};; 121 ; 122templ",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:2739,Availability,error,errorIgnoreLevel,2739,"deBase;; 50}; 51}; 52namespace RDF {; 53template <typename T>; 54class RResultPtr;; 55template<typename T, typename V>; 56class RInterface;; 57using RNode = RInterface<::ROOT::Detail::RDF::RNodeBase, void>;; 58class RDataSource;; 59} // namespace RDF; 60 ; 61} // namespace ROOT; 62 ; 63/// \cond HIDDEN_SYMBOLS; 64 ; 65namespace ROOT {; 66namespace Internal {; 67namespace RDF {; 68using namespace ROOT::Detail::RDF;; 69using namespace ROOT::RDF;; 70namespace TTraits = ROOT::TypeTraits;; 71 ; 72std::string DemangleTypeIdName(const std::type_info &typeInfo);; 73 ; 74ColumnNames_t; 75ConvertRegexToColumns(const ColumnNames_t &colNames, std::string_view columnNameRegexp, std::string_view callerName);; 76 ; 77/// An helper object that sets and resets gErrorIgnoreLevel via RAII.; 78class RIgnoreErrorLevelRAII {; 79private:; 80 int fCurIgnoreErrorLevel = gErrorIgnoreLevel;; 81 ; 82public:; 83 RIgnoreErrorLevelRAII(int errorIgnoreLevel) { gErrorIgnoreLevel = errorIgnoreLevel; }; 84 ~RIgnoreErrorLevelRAII() { gErrorIgnoreLevel = fCurIgnoreErrorLevel; }; 85};; 86 ; 87/****** BuildAction overloads *******/; 88 ; 89// clang-format off; 90/// This namespace defines types to be used for tag dispatching in RInterface.; 91namespace ActionTags {; 92struct Histo1D{};; 93struct Histo2D{};; 94struct Histo3D{};; 95struct HistoND{};; 96struct Graph{};; 97struct GraphAsymmErrors{};; 98struct Profile1D{};; 99struct Profile2D{};; 100struct Min{};; 101struct Max{};; 102struct Sum{};; 103struct Mean{};; 104struct Fill{};; 105struct StdDev{};; 106struct Display{};; 107struct Snapshot{};; 108struct Book{};; 109}; 110// clang-format on; 111 ; 112template <typename T, bool ISV6HISTO = std::is_base_of<TH1, std::decay_t<T>>::value>; 113struct HistoUtils {; 114 static void SetCanExtendAllAxes(T &h) { h.SetCanExtend(::TH1::kAllAxes); }; 115 static bool HasAxisLimits(T &h); 116 {; 117 auto xaxis = h.GetXaxis();; 118 return !(xaxis->GetXmin() == 0. && xaxis->GetXmax() == 0.);; 119 }; 120};; 121 ; 122templ",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:15851,Availability,alive,alive,15851,"Names, std::string_view variationName,; 341 const std::vector<std::string> &variationTags, std::string_view expression, RLoopManager &lm,; 342 RDataSource *ds, const RColumnRegister &colRegister, const ColumnNames_t &branches,; 343 std::shared_ptr<RNodeBase> *upcastNodeOnHeap, bool isSingleColumn);; 344 ; 345std::string JitBuildAction(const ColumnNames_t &bl, std::shared_ptr<RDFDetail::RNodeBase> *prevNode,; 346 const std::type_info &art, const std::type_info &at, void *rOnHeap, TTree *tree,; 347 const unsigned int nSlots, const RColumnRegister &colRegister, RDataSource *ds,; 348 std::weak_ptr<RJittedAction> *jittedActionOnHeap, const bool vector2RVec = true);; 349 ; 350// Allocate a weak_ptr on the heap, return a pointer to it. The user is responsible for deleting this weak_ptr.; 351// This function is meant to be used by RInterface's methods that book code for jitting.; 352// The problem it solves is that we generate code to be lazily jitted with the addresses of certain objects in them,; 353// and we need to check those objects are still alive when the generated code is finally jitted and executed.; 354// So we pass addresses to weak_ptrs allocated on the heap to the jitted code, which is then responsible for; 355// the deletion of the weak_ptr object.; 356template <typename T>; 357std::weak_ptr<T> *MakeWeakOnHeap(const std::shared_ptr<T> &shPtr); 358{; 359 return new std::weak_ptr<T>(shPtr);; 360}; 361 ; 362// Same as MakeWeakOnHeap, but create a shared_ptr that makes sure the object is definitely kept alive.; 363template <typename T>; 364std::shared_ptr<T> *MakeSharedOnHeap(const std::shared_ptr<T> &shPtr); 365{; 366 return new std::shared_ptr<T>(shPtr);; 367}; 368 ; 369bool AtLeastOneEmptyString(const std::vector<std::string_view> strings);; 370 ; 371/// Take a shared_ptr<AnyNodeType> and return a shared_ptr<RNodeBase>.; 372/// This works for RLoopManager nodes as well as filters and ranges.; 373std::shared_ptr<RNodeBase> UpcastNode(std::shared_ptr<RNodeBase> p",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:16326,Availability,alive,alive,16326,"ode,; 346 const std::type_info &art, const std::type_info &at, void *rOnHeap, TTree *tree,; 347 const unsigned int nSlots, const RColumnRegister &colRegister, RDataSource *ds,; 348 std::weak_ptr<RJittedAction> *jittedActionOnHeap, const bool vector2RVec = true);; 349 ; 350// Allocate a weak_ptr on the heap, return a pointer to it. The user is responsible for deleting this weak_ptr.; 351// This function is meant to be used by RInterface's methods that book code for jitting.; 352// The problem it solves is that we generate code to be lazily jitted with the addresses of certain objects in them,; 353// and we need to check those objects are still alive when the generated code is finally jitted and executed.; 354// So we pass addresses to weak_ptrs allocated on the heap to the jitted code, which is then responsible for; 355// the deletion of the weak_ptr object.; 356template <typename T>; 357std::weak_ptr<T> *MakeWeakOnHeap(const std::shared_ptr<T> &shPtr); 358{; 359 return new std::weak_ptr<T>(shPtr);; 360}; 361 ; 362// Same as MakeWeakOnHeap, but create a shared_ptr that makes sure the object is definitely kept alive.; 363template <typename T>; 364std::shared_ptr<T> *MakeSharedOnHeap(const std::shared_ptr<T> &shPtr); 365{; 366 return new std::shared_ptr<T>(shPtr);; 367}; 368 ; 369bool AtLeastOneEmptyString(const std::vector<std::string_view> strings);; 370 ; 371/// Take a shared_ptr<AnyNodeType> and return a shared_ptr<RNodeBase>.; 372/// This works for RLoopManager nodes as well as filters and ranges.; 373std::shared_ptr<RNodeBase> UpcastNode(std::shared_ptr<RNodeBase> ptr);; 374 ; 375ColumnNames_t GetValidatedColumnNames(RLoopManager &lm, const unsigned int nColumns, const ColumnNames_t &columns,; 376 const RColumnRegister &validDefines, RDataSource *ds);; 377 ; 378std::vector<std::string> GetValidatedArgTypes(const ColumnNames_t &colNames, const RColumnRegister &colRegister,; 379 TTree *tree, RDataSource *ds, const std::string &context,; 380 bool vector2RVec);; 381 ;",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:26860,Availability,alive,alive,26860,"e> newVariation{new RVariation<std::decay_t<F>, IsSingleColumn>(; 569 std::move(variedColNames), variationName, std::forward<F>(f), std::move(tags), jittedVariation->GetTypeName(),; 570 *colRegister, *lm, inputColNames)};; 571 jittedVariation->SetVariation(std::move(newVariation));; 572 ; 573 doDeletes();; 574}; 575 ; 576/// Convenience function invoked by jitted code to build action nodes at runtime; 577template <typename ActionTag, typename... ColTypes, typename PrevNodeType, typename HelperArgType>; 578void CallBuildAction(std::shared_ptr<PrevNodeType> *prevNodeOnHeap, const char **colsPtr, std::size_t colsSize,; 579 const unsigned int nSlots, std::shared_ptr<HelperArgType> *helperArgOnHeap,; 580 std::weak_ptr<RJittedAction> *wkJittedActionOnHeap, RColumnRegister *colRegister) noexcept; 581{; 582 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 583 auto doDeletes = [&] {; 584 delete[] colsPtr;; 585 delete helperArgOnHeap;; 586 delete wkJittedActionOnHeap;; 587 // colRegister must be deleted before prevNodeOnHeap because their dtor needs the RLoopManager to be alive; 588 // and prevNodeOnHeap is what keeps it alive if the rest of the computation graph is already out of scope; 589 delete colRegister;; 590 delete prevNodeOnHeap;; 591 };; 592 ; 593 if (wkJittedActionOnHeap->expired()) {; 594 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 595 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 596 doDeletes();; 597 return;; 598 }; 599 ; 600 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 601 ; 602 auto jittedActionOnHeap = wkJittedActionOnHeap->lock();; 603 ; 604 // if we are here it means we are jitting, if we are jitting the loop manager must be alive; 605 auto &prevNodePtr = *prevNodeOnHeap;; 606 auto &loopManager = *prevNodePtr->GetLoopManagerUnchecked();; 607 using ColTypes_t = Type",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:26910,Availability,alive,alive,26910,"e> newVariation{new RVariation<std::decay_t<F>, IsSingleColumn>(; 569 std::move(variedColNames), variationName, std::forward<F>(f), std::move(tags), jittedVariation->GetTypeName(),; 570 *colRegister, *lm, inputColNames)};; 571 jittedVariation->SetVariation(std::move(newVariation));; 572 ; 573 doDeletes();; 574}; 575 ; 576/// Convenience function invoked by jitted code to build action nodes at runtime; 577template <typename ActionTag, typename... ColTypes, typename PrevNodeType, typename HelperArgType>; 578void CallBuildAction(std::shared_ptr<PrevNodeType> *prevNodeOnHeap, const char **colsPtr, std::size_t colsSize,; 579 const unsigned int nSlots, std::shared_ptr<HelperArgType> *helperArgOnHeap,; 580 std::weak_ptr<RJittedAction> *wkJittedActionOnHeap, RColumnRegister *colRegister) noexcept; 581{; 582 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 583 auto doDeletes = [&] {; 584 delete[] colsPtr;; 585 delete helperArgOnHeap;; 586 delete wkJittedActionOnHeap;; 587 // colRegister must be deleted before prevNodeOnHeap because their dtor needs the RLoopManager to be alive; 588 // and prevNodeOnHeap is what keeps it alive if the rest of the computation graph is already out of scope; 589 delete colRegister;; 590 delete prevNodeOnHeap;; 591 };; 592 ; 593 if (wkJittedActionOnHeap->expired()) {; 594 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 595 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 596 doDeletes();; 597 return;; 598 }; 599 ; 600 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 601 ; 602 auto jittedActionOnHeap = wkJittedActionOnHeap->lock();; 603 ; 604 // if we are here it means we are jitting, if we are jitting the loop manager must be alive; 605 auto &prevNodePtr = *prevNodeOnHeap;; 606 auto &loopManager = *prevNodePtr->GetLoopManagerUnchecked();; 607 using ColTypes_t = Type",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:27574,Availability,alive,alive,27574,"81{; 582 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 583 auto doDeletes = [&] {; 584 delete[] colsPtr;; 585 delete helperArgOnHeap;; 586 delete wkJittedActionOnHeap;; 587 // colRegister must be deleted before prevNodeOnHeap because their dtor needs the RLoopManager to be alive; 588 // and prevNodeOnHeap is what keeps it alive if the rest of the computation graph is already out of scope; 589 delete colRegister;; 590 delete prevNodeOnHeap;; 591 };; 592 ; 593 if (wkJittedActionOnHeap->expired()) {; 594 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 595 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 596 doDeletes();; 597 return;; 598 }; 599 ; 600 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 601 ; 602 auto jittedActionOnHeap = wkJittedActionOnHeap->lock();; 603 ; 604 // if we are here it means we are jitting, if we are jitting the loop manager must be alive; 605 auto &prevNodePtr = *prevNodeOnHeap;; 606 auto &loopManager = *prevNodePtr->GetLoopManagerUnchecked();; 607 using ColTypes_t = TypeList<ColTypes...>;; 608 constexpr auto nColumns = ColTypes_t::list_size;; 609 auto ds = loopManager.GetDataSource();; 610 if (ds != nullptr); 611 AddDSColumns(cols, loopManager, *ds, ColTypes_t(), *colRegister);; 612 ; 613 auto actionPtr = BuildAction<ColTypes...>(cols, std::move(*helperArgOnHeap), nSlots, std::move(prevNodePtr),; 614 ActionTag{}, *colRegister);; 615 jittedActionOnHeap->SetAction(std::move(actionPtr));; 616 ; 617 doDeletes();; 618}; 619 ; 620/// The contained `type` alias is `double` if `T == RInferredType`, `U` if `T == std::container<U>`, `T` otherwise.; 621template <typename T, bool Container = IsDataContainer<T>::value && !std::is_same<T, std::string>::value>; 622struct RMinReturnType {; 623 using type = T;; 624};; 625 ; 626template <>; 627struct RMinReturnType<RInferred",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:39792,Availability,error,errors,39792,"al bool HasColumn(std::string_view colName) const =0Checks if the dataset has a certain column.; ROOT::RDF::RDataSource::GetColumnReadersstd::vector< T ** > GetColumnReaders(std::string_view columnName)Called at most once per column by RDF.Definition RDataSource.hxx:154; ROOT::RDF::RInterfaceThe public interface to the RDataFrame federation of classes.Definition RInterface.hxx:113; ROOT::VecOps::RVecA ""std::vector""-like collection of values implementing handy operation to analyse them.Definition RVec.hxx:1529; R; TH1::kAllAxes@ kAllAxesDefinition TH1.h:76; TObjArrayAn array of TObjects.Definition TObjArray.h:31; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; double; int; ROOT::VecOps::SumT Sum(const RVec< T > &v, const T zero=T(0))Sum elements of an RVec.Definition RVec.hxx:1954; F#define F(x, y, z); ROOT::Detail::RDFDefinition RooAbsDataHelper.h:80; ROOT::Internal::RDF::SelectColumnsconst ColumnNames_t SelectColumns(unsigned int nRequiredNames, const ColumnNames_t &names, const ColumnNames_t &defaultNames)Choose between local column names or default column names, throw in case of errors.Definition RDFInterfaceUtils.cxx:586; ROOT::Internal::RDF::CheckForNoVariationsvoid CheckForNoVariations(const std::string &where, std::string_view definedColView, const RColumnRegister &colRegister)Throw if the column has systematic variations attached.Definition RDFInterfaceUtils.cxx:548; ROOT::Internal::RDF::ParseTreePathParsedTreePath ParseTreePath(std::string_view fullTreeName)Definition RDFInterfaceUtils.cxx:635; ROOT::Internal::RDF::CheckForRedefinitionvoid CheckForRedefinition(const std::string &where, std::string_view definedColView, const RColumnRegister &colRegister, const ColumnNames_t &treeColumns, const ColumnNames_t &dataSourceColumns)Throw if column definedColView is already there.Definition RDFInterfaceUtils.cxx:486; ROOT::Internal::RDF::CheckForDefinitionvoid CheckForDefinition(const std::string &where, std::string_view definedColView, const RCol",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:15954,Energy Efficiency,allocate,allocated,15954,"const ColumnNames_t &branches,; 343 std::shared_ptr<RNodeBase> *upcastNodeOnHeap, bool isSingleColumn);; 344 ; 345std::string JitBuildAction(const ColumnNames_t &bl, std::shared_ptr<RDFDetail::RNodeBase> *prevNode,; 346 const std::type_info &art, const std::type_info &at, void *rOnHeap, TTree *tree,; 347 const unsigned int nSlots, const RColumnRegister &colRegister, RDataSource *ds,; 348 std::weak_ptr<RJittedAction> *jittedActionOnHeap, const bool vector2RVec = true);; 349 ; 350// Allocate a weak_ptr on the heap, return a pointer to it. The user is responsible for deleting this weak_ptr.; 351// This function is meant to be used by RInterface's methods that book code for jitting.; 352// The problem it solves is that we generate code to be lazily jitted with the addresses of certain objects in them,; 353// and we need to check those objects are still alive when the generated code is finally jitted and executed.; 354// So we pass addresses to weak_ptrs allocated on the heap to the jitted code, which is then responsible for; 355// the deletion of the weak_ptr object.; 356template <typename T>; 357std::weak_ptr<T> *MakeWeakOnHeap(const std::shared_ptr<T> &shPtr); 358{; 359 return new std::weak_ptr<T>(shPtr);; 360}; 361 ; 362// Same as MakeWeakOnHeap, but create a shared_ptr that makes sure the object is definitely kept alive.; 363template <typename T>; 364std::shared_ptr<T> *MakeSharedOnHeap(const std::shared_ptr<T> &shPtr); 365{; 366 return new std::shared_ptr<T>(shPtr);; 367}; 368 ; 369bool AtLeastOneEmptyString(const std::vector<std::string_view> strings);; 370 ; 371/// Take a shared_ptr<AnyNodeType> and return a shared_ptr<RNodeBase>.; 372/// This works for RLoopManager nodes as well as filters and ranges.; 373std::shared_ptr<RNodeBase> UpcastNode(std::shared_ptr<RNodeBase> ptr);; 374 ; 375ColumnNames_t GetValidatedColumnNames(RLoopManager &lm, const unsigned int nColumns, const ColumnNames_t &columns,; 376 const RColumnRegister &validDefines, RDataSource *ds);; 377 ;",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:22509,Energy Efficiency,allocate,allocated,22509,"eDefineNode(DefineTypes::RDefinePerSampleTag, std::string_view name, std::string_view dummyType, F &&f,; 478 const ColumnNames_t &, RColumnRegister &, RLoopManager &lm); 479{; 480 return std::unique_ptr<RDefineBase>(; 481 new RDefinePerSample<std::decay_t<F>>(name, dummyType, std::forward<F>(f), lm));; 482}; 483 ; 484// Build a RDefine or a RDefinePerSample object and attach it to an existing RJittedDefine; 485// This function is meant to be called by jitted code right before starting the event loop.; 486// If colsPtr is null, build a RDefinePerSample (it has no input columns), otherwise a RDefine.; 487template <typename RDefineTypeTag, typename F>; 488void JitDefineHelper(F &&f, const char **colsPtr, std::size_t colsSize, std::string_view name, RLoopManager *lm,; 489 std::weak_ptr<RJittedDefine> *wkJittedDefine, RColumnRegister *colRegister,; 490 std::shared_ptr<RNodeBase> *prevNodeOnHeap) noexcept; 491{; 492 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 493 auto doDeletes = [&] {; 494 delete wkJittedDefine;; 495 delete colRegister;; 496 delete prevNodeOnHeap;; 497 delete[] colsPtr;; 498 };; 499 ; 500 if (wkJittedDefine->expired()) {; 501 // The branch of the computation graph that needed this jitted code went out of scope between the type; 502 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 503 doDeletes();; 504 return;; 505 }; 506 ; 507 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 508 ; 509 auto jittedDefine = wkJittedDefine->lock();; 510 ; 511 using Callable_t = std::decay_t<F>;; 512 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 513 ; 514 auto ds = lm->GetDataSource();; 515 if (ds != nullptr); 516 AddDSColumns(cols, *lm, *ds, ColTypes_t(), *colRegister);; 517 ; 518 // will never actually be used (trumped by jittedDefine->GetTypeName()), but we set it to something meaningful; 519 // to help devs debuggi",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:23674,Energy Efficiency,reduce,reduce,23674,"ng to do other than cleaning up.; 503 doDeletes();; 504 return;; 505 }; 506 ; 507 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 508 ; 509 auto jittedDefine = wkJittedDefine->lock();; 510 ; 511 using Callable_t = std::decay_t<F>;; 512 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 513 ; 514 auto ds = lm->GetDataSource();; 515 if (ds != nullptr); 516 AddDSColumns(cols, *lm, *ds, ColTypes_t(), *colRegister);; 517 ; 518 // will never actually be used (trumped by jittedDefine->GetTypeName()), but we set it to something meaningful; 519 // to help devs debugging; 520 const auto dummyType = ""jittedCol_t"";; 521 // use unique_ptr<RDefineBase> instead of make_unique<NewCol_t> to reduce jit/compile-times; 522 std::unique_ptr<RDefineBase> newCol{; 523 MakeDefineNode(RDefineTypeTag{}, name, dummyType, std::forward<F>(f), cols, *colRegister, *lm)};; 524 jittedDefine->SetDefine(std::move(newCol));; 525 ; 526 doDeletes();; 527}; 528 ; 529template <bool IsSingleColumn, typename F>; 530void JitVariationHelper(F &&f, const char **colsPtr, std::size_t colsSize, const char **variedCols,; 531 std::size_t variedColsSize, const char **variationTags, std::size_t variationTagsSize,; 532 std::string_view variationName, RLoopManager *lm,; 533 std::weak_ptr<RJittedVariation> *wkJittedVariation, RColumnRegister *colRegister,; 534 std::shared_ptr<RNodeBase> *prevNodeOnHeap) noexcept; 535{; 536 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 537 auto doDeletes = [&] {; 538 delete[] colsPtr;; 539 delete[] variedCols;; 540 delete[] variationTags;; 541 ; 542 delete wkJittedVariation;; 543 delete colRegister;; 544 delete prevNodeOnHeap;; 545 };; 546 ; 547 if (wkJittedVariation->expired()) {; 548 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 549 // jitting was booked and the time jitting actually happened. Nothing to do other than cleani",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:24412,Energy Efficiency,allocate,allocated,24412,"ng to do other than cleaning up.; 503 doDeletes();; 504 return;; 505 }; 506 ; 507 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 508 ; 509 auto jittedDefine = wkJittedDefine->lock();; 510 ; 511 using Callable_t = std::decay_t<F>;; 512 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 513 ; 514 auto ds = lm->GetDataSource();; 515 if (ds != nullptr); 516 AddDSColumns(cols, *lm, *ds, ColTypes_t(), *colRegister);; 517 ; 518 // will never actually be used (trumped by jittedDefine->GetTypeName()), but we set it to something meaningful; 519 // to help devs debugging; 520 const auto dummyType = ""jittedCol_t"";; 521 // use unique_ptr<RDefineBase> instead of make_unique<NewCol_t> to reduce jit/compile-times; 522 std::unique_ptr<RDefineBase> newCol{; 523 MakeDefineNode(RDefineTypeTag{}, name, dummyType, std::forward<F>(f), cols, *colRegister, *lm)};; 524 jittedDefine->SetDefine(std::move(newCol));; 525 ; 526 doDeletes();; 527}; 528 ; 529template <bool IsSingleColumn, typename F>; 530void JitVariationHelper(F &&f, const char **colsPtr, std::size_t colsSize, const char **variedCols,; 531 std::size_t variedColsSize, const char **variationTags, std::size_t variationTagsSize,; 532 std::string_view variationName, RLoopManager *lm,; 533 std::weak_ptr<RJittedVariation> *wkJittedVariation, RColumnRegister *colRegister,; 534 std::shared_ptr<RNodeBase> *prevNodeOnHeap) noexcept; 535{; 536 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 537 auto doDeletes = [&] {; 538 delete[] colsPtr;; 539 delete[] variedCols;; 540 delete[] variationTags;; 541 ; 542 delete wkJittedVariation;; 543 delete colRegister;; 544 delete prevNodeOnHeap;; 545 };; 546 ; 547 if (wkJittedVariation->expired()) {; 548 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 549 // jitting was booked and the time jitting actually happened. Nothing to do other than cleani",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:25656,Energy Efficiency,reduce,reduce,25656,"riedCols;; 540 delete[] variationTags;; 541 ; 542 delete wkJittedVariation;; 543 delete colRegister;; 544 delete prevNodeOnHeap;; 545 };; 546 ; 547 if (wkJittedVariation->expired()) {; 548 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 549 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 550 doDeletes();; 551 return;; 552 }; 553 ; 554 const ColumnNames_t inputColNames(colsPtr, colsPtr + colsSize);; 555 std::vector<std::string> variedColNames(variedCols, variedCols + variedColsSize);; 556 std::vector<std::string> tags(variationTags, variationTags + variationTagsSize);; 557 ; 558 auto jittedVariation = wkJittedVariation->lock();; 559 ; 560 using Callable_t = std::decay_t<F>;; 561 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 562 ; 563 auto ds = lm->GetDataSource();; 564 if (ds != nullptr); 565 AddDSColumns(inputColNames, *lm, *ds, ColTypes_t(), *colRegister);; 566 ; 567 // use unique_ptr<RDefineBase> instead of make_unique<NewCol_t> to reduce jit/compile-times; 568 std::unique_ptr<RVariationBase> newVariation{new RVariation<std::decay_t<F>, IsSingleColumn>(; 569 std::move(variedColNames), variationName, std::forward<F>(f), std::move(tags), jittedVariation->GetTypeName(),; 570 *colRegister, *lm, inputColNames)};; 571 jittedVariation->SetVariation(std::move(newVariation));; 572 ; 573 doDeletes();; 574}; 575 ; 576/// Convenience function invoked by jitted code to build action nodes at runtime; 577template <typename ActionTag, typename... ColTypes, typename PrevNodeType, typename HelperArgType>; 578void CallBuildAction(std::shared_ptr<PrevNodeType> *prevNodeOnHeap, const char **colsPtr, std::size_t colsSize,; 579 const unsigned int nSlots, std::shared_ptr<HelperArgType> *helperArgOnHeap,; 580 std::weak_ptr<RJittedAction> *wkJittedActionOnHeap, RColumnRegister *colRegister) noexcept; 581{; 582 // a helper to delete objects allocat",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:26556,Energy Efficiency,allocate,allocated,26556,"e> newVariation{new RVariation<std::decay_t<F>, IsSingleColumn>(; 569 std::move(variedColNames), variationName, std::forward<F>(f), std::move(tags), jittedVariation->GetTypeName(),; 570 *colRegister, *lm, inputColNames)};; 571 jittedVariation->SetVariation(std::move(newVariation));; 572 ; 573 doDeletes();; 574}; 575 ; 576/// Convenience function invoked by jitted code to build action nodes at runtime; 577template <typename ActionTag, typename... ColTypes, typename PrevNodeType, typename HelperArgType>; 578void CallBuildAction(std::shared_ptr<PrevNodeType> *prevNodeOnHeap, const char **colsPtr, std::size_t colsSize,; 579 const unsigned int nSlots, std::shared_ptr<HelperArgType> *helperArgOnHeap,; 580 std::weak_ptr<RJittedAction> *wkJittedActionOnHeap, RColumnRegister *colRegister) noexcept; 581{; 582 // a helper to delete objects allocated before jitting, so that the jitter can share data with lazily jitted code; 583 auto doDeletes = [&] {; 584 delete[] colsPtr;; 585 delete helperArgOnHeap;; 586 delete wkJittedActionOnHeap;; 587 // colRegister must be deleted before prevNodeOnHeap because their dtor needs the RLoopManager to be alive; 588 // and prevNodeOnHeap is what keeps it alive if the rest of the computation graph is already out of scope; 589 delete colRegister;; 590 delete prevNodeOnHeap;; 591 };; 592 ; 593 if (wkJittedActionOnHeap->expired()) {; 594 // The branch of the computation graph that needed this jitted variation went out of scope between the type; 595 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 596 doDeletes();; 597 return;; 598 }; 599 ; 600 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 601 ; 602 auto jittedActionOnHeap = wkJittedActionOnHeap->lock();; 603 ; 604 // if we are here it means we are jitting, if we are jitting the loop manager must be alive; 605 auto &prevNodePtr = *prevNodeOnHeap;; 606 auto &loopManager = *prevNodePtr->GetLoopManagerUnchecked();; 607 using ColTypes_t = Type",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:18127,Integrability,interface,interface,18127,"er,; 379 TTree *tree, RDataSource *ds, const std::string &context,; 380 bool vector2RVec);; 381 ; 382std::vector<bool> FindUndefinedDSColumns(const ColumnNames_t &requestedCols, const ColumnNames_t &definedDSCols);; 383 ; 384template <typename T>; 385void AddDSColumnsHelper(const std::string &colName, RLoopManager &lm, RDataSource &ds, RColumnRegister &colRegister); 386{; 387 if (colRegister.IsDefineOrAlias(colName) || !ds.HasColumn(colName) ||; 388 lm.HasDataSourceColumnReaders(colName, typeid(T))); 389 return;; 390 ; 391 const auto nSlots = lm.GetNSlots();; 392 std::vector<std::unique_ptr<RColumnReaderBase>> colReaders;; 393 colReaders.reserve(nSlots);; 394 ; 395 const auto valuePtrs = ds.GetColumnReaders<T>(colName);; 396 if (!valuePtrs.empty()) { // we are using the old GetColumnReaders mechanism in this RDataSource; 397 for (auto *ptr : valuePtrs); 398 colReaders.emplace_back(new RDSColumnReader<T>(ptr));; 399 ; 400 } else { // using the new GetColumnReaders mechanism; 401 // TODO consider changing the interface so we return all of these for all slots in one go; 402 for (auto slot = 0u; slot < lm.GetNSlots(); ++slot); 403 colReaders.emplace_back(ds.GetColumnReaders(slot, colName, typeid(T)));; 404 }; 405 ; 406 lm.AddDataSourceColumnReaders(colName, std::move(colReaders), typeid(T));; 407}; 408 ; 409/// Take list of column names that must be defined, current map of custom columns, current list of defined column names,; 410/// and return a new map of custom columns (with the new datasource columns added to it); 411template <typename... ColumnTypes>; 412void AddDSColumns(const std::vector<std::string> &requiredCols, RLoopManager &lm, RDataSource &ds,; 413 TTraits::TypeList<ColumnTypes...>, RColumnRegister &colRegister); 414{; 415 // hack to expand a template parameter pack without c++17 fold expressions.; 416 using expander = int[];; 417 int i = 0;; 418 (void)expander{(AddDSColumnsHelper<ColumnTypes>(requiredCols[i], lm, ds, colRegister), ++i)..., 0};; 419}; 420 ;",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:28714,Integrability,wrap,wrapper,28714,"anager must be alive; 605 auto &prevNodePtr = *prevNodeOnHeap;; 606 auto &loopManager = *prevNodePtr->GetLoopManagerUnchecked();; 607 using ColTypes_t = TypeList<ColTypes...>;; 608 constexpr auto nColumns = ColTypes_t::list_size;; 609 auto ds = loopManager.GetDataSource();; 610 if (ds != nullptr); 611 AddDSColumns(cols, loopManager, *ds, ColTypes_t(), *colRegister);; 612 ; 613 auto actionPtr = BuildAction<ColTypes...>(cols, std::move(*helperArgOnHeap), nSlots, std::move(prevNodePtr),; 614 ActionTag{}, *colRegister);; 615 jittedActionOnHeap->SetAction(std::move(actionPtr));; 616 ; 617 doDeletes();; 618}; 619 ; 620/// The contained `type` alias is `double` if `T == RInferredType`, `U` if `T == std::container<U>`, `T` otherwise.; 621template <typename T, bool Container = IsDataContainer<T>::value && !std::is_same<T, std::string>::value>; 622struct RMinReturnType {; 623 using type = T;; 624};; 625 ; 626template <>; 627struct RMinReturnType<RInferredType, false> {; 628 using type = double;; 629};; 630 ; 631template <typename T>; 632struct RMinReturnType<T, true> {; 633 using type = TTraits::TakeFirstParameter_t<T>;; 634};; 635 ; 636// return wrapper around f that prepends an `unsigned int slot` parameter; 637template <typename R, typename F, typename... Args>; 638std::function<R(unsigned int, Args...)> AddSlotParameter(F &f, TypeList<Args...>); 639{; 640 return [f](unsigned int, Args... a) mutable -> R { return f(a...); };; 641}; 642 ; 643template <typename ColType, typename... Rest>; 644struct RNeedJittingHelper {; 645 static constexpr bool value = RNeedJittingHelper<Rest...>::value;; 646};; 647 ; 648template <typename... Rest>; 649struct RNeedJittingHelper<RInferredType, Rest...> {; 650 static constexpr bool value = true;; 651};; 652 ; 653template <typename T>; 654struct RNeedJittingHelper<T> {; 655 static constexpr bool value = false;; 656};; 657 ; 658template <>; 659struct RNeedJittingHelper<RInferredType> {; 660 static constexpr bool value = true;; 661};; 662 ; 663te",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:35552,Integrability,rout,routines,35552,"namespace RDF; 781} // namespace Internal; 782 ; 783namespace Detail {; 784namespace RDF {; 785 ; 786/// The aliased type is `double` if `T == RInferredType`, `U` if `T == container<U>`, `T` otherwise.; 787template <typename T>; 788using MinReturnType_t = typename RDFInternal::RMinReturnType<T>::type;; 789 ; 790template <typename T>; 791using MaxReturnType_t = MinReturnType_t<T>;; 792 ; 793template <typename T>; 794using SumReturnType_t = MinReturnType_t<T>;; 795 ; 796} // namespace RDF; 797} // namespace Detail; 798} // namespace ROOT; 799 ; 800/// \endcond; 801 ; 802#endif; ActionHelpers.hxx; RAction.hxx; RColumnRegister.hxx; RDefinePerSample.hxx; RDefine.hxx; RFilter.hxx; RJittedAction.hxx; RJittedDefine.hxx; RJittedFilter.hxx; RJittedVariation.hxx; RLoopManager.hxx; f#define f(i)Definition RSha256.hxx:104; g#define g(i)Definition RSha256.hxx:105; a#define a(i)Definition RSha256.hxx:99; h#define h(i)Definition RSha256.hxx:106; RVariation.hxx; TError.h; gErrorIgnoreLevelInt_t gErrorIgnoreLevelError handling routines.Definition TError.cxx:31; filenameOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char filenameDefinition TGWin32VirtualXProxy.cxx:232; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFi",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:38979,Integrability,interface,interface,38979,"on RLoopManager.hxx:218; ROOT::Detail::RDF::RLoopManager::GetNSlotsunsigned int GetNSlots() constDefinition RLoopManager.hxx:230; ROOT::Detail::RDF::RLoopManager::GetLoopManagerUncheckedRLoopManager * GetLoopManagerUnchecked() finalDefinition RLoopManager.hxx:212; ROOT::Detail::RDF::RLoopManager::HasDataSourceColumnReadersbool HasDataSourceColumnReaders(const std::string &col, const std::type_info &ti) constReturn true if AddDataSourceColumnReaders was called for column name col.Definition RLoopManager.cxx:1103; ROOT::RDF::RDataSourceRDataSource defines an API that RDataFrame can use to read arbitrary data formats.Definition RDataSource.hxx:109; ROOT::RDF::RDataSource::HasColumnvirtual bool HasColumn(std::string_view colName) const =0Checks if the dataset has a certain column.; ROOT::RDF::RDataSource::GetColumnReadersstd::vector< T ** > GetColumnReaders(std::string_view columnName)Called at most once per column by RDF.Definition RDataSource.hxx:154; ROOT::RDF::RInterfaceThe public interface to the RDataFrame federation of classes.Definition RInterface.hxx:113; ROOT::VecOps::RVecA ""std::vector""-like collection of values implementing handy operation to analyse them.Definition RVec.hxx:1529; R; TH1::kAllAxes@ kAllAxesDefinition TH1.h:76; TObjArrayAn array of TObjects.Definition TObjArray.h:31; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; double; int; ROOT::VecOps::SumT Sum(const RVec< T > &v, const T zero=T(0))Sum elements of an RVec.Definition RVec.hxx:1954; F#define F(x, y, z); ROOT::Detail::RDFDefinition RooAbsDataHelper.h:80; ROOT::Internal::RDF::SelectColumnsconst ColumnNames_t SelectColumns(unsigned int nRequiredNames, const ColumnNames_t &names, const ColumnNames_t &defaultNames)Choose between local column names or default column names, throw in case of errors.Definition RDFInterfaceUtils.cxx:586; ROOT::Internal::RDF::CheckForNoVariationsvoid CheckForNoVariations(const std::string &where, std::string_view definedColView, const RColumnRegister",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:44803,Modifiability,variab,variable-sized,44803," &colRegister, const ColumnNames_t &branches, std::shared_ptr< RNodeBase > *upcastNodeOnHeap, bool isSingleColumn)Book the jitting of a Vary call.Definition RDFInterfaceUtils.cxx:782; ROOT::Internal::RDF::GetValidatedArgTypesstd::vector< std::string > GetValidatedArgTypes(const ColumnNames_t &colNames, const RColumnRegister &colRegister, TTree *tree, RDataSource *ds, const std::string &context, bool vector2RVec)Definition RDFInterfaceUtils.cxx:951; ROOT::Internal::RDF::CheckForDuplicateSnapshotColumnsvoid CheckForDuplicateSnapshotColumns(const ColumnNames_t &cols)Definition RDFInterfaceUtils.cxx:985; ROOT::Internal::RDF::ConvertRegexToColumnsColumnNames_t ConvertRegexToColumns(const ColumnNames_t &colNames, std::string_view columnNameRegexp, std::string_view callerName)Definition RDFInterfaceUtils.cxx:450; ROOT::Internal::RDF::AddSizeBranchesstd::pair< std::vector< std::string >, std::vector< std::string > > AddSizeBranches(const std::vector< std::string > &branches, TTree *tree, std::vector< std::string > &&colsWithoutAliases, std::vector< std::string > &&colsWithAliases)Return copies of colsWithoutAliases and colsWithAliases with size branches for variable-sized array b...Definition RDFInterfaceUtils.cxx:1001; ROOT::Internal::RDF::FindUndefinedDSColumnsstd::vector< bool > FindUndefinedDSColumns(const ColumnNames_t &requestedCols, const ColumnNames_t &definedCols)Return a bitset each element of which indicates whether the corresponding element in selectedColumns ...Definition RDFInterfaceUtils.cxx:976; ROOT::Internal::RDF::BookDefinePerSampleJitstd::shared_ptr< RJittedDefine > BookDefinePerSampleJit(std::string_view name, std::string_view expression, RLoopManager &lm, const RColumnRegister &colRegister, std::shared_ptr< RNodeBase > *upcastNodeOnHeap)Book the jitting of a DefinePerSample call.Definition RDFInterfaceUtils.cxx:750; ROOT::Internal::RDF::JitBuildActionstd::string JitBuildAction(const ColumnNames_t &cols, std::shared_ptr< RDFDetail::RNodeBase > *prevNode",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:11886,Performance,multi-thread,multi-thread,11886,"tHelperArgs> &snapHelperArgs,; 259 const unsigned int nSlots, std::shared_ptr<PrevNodeType> prevNode, ActionTags::Snapshot,; 260 const RColumnRegister &colRegister); 261{; 262 const auto &filename = snapHelperArgs->fFileName;; 263 const auto &dirname = snapHelperArgs->fDirName;; 264 const auto &treename = snapHelperArgs->fTreeName;; 265 const auto &outputColNames = snapHelperArgs->fOutputColNames;; 266 const auto &options = snapHelperArgs->fOptions;; 267 ; 268 auto sz = sizeof...(ColTypes);; 269 std::vector<bool> isDefine(sz);; 270 for (auto i = 0u; i < sz; ++i); 271 isDefine[i] = colRegister.IsDefineOrAlias(colNames[i]);; 272 ; 273 std::unique_ptr<RActionBase> actionPtr;; 274 if (!ROOT::IsImplicitMTEnabled()) {; 275 // single-thread snapshot; 276 using Helper_t = SnapshotHelper<ColTypes...>;; 277 using Action_t = RAction<Helper_t, PrevNodeType>;; 278 actionPtr.reset(; 279 new Action_t(Helper_t(filename, dirname, treename, colNames, outputColNames, options, std::move(isDefine)),; 280 colNames, prevNode, colRegister));; 281 } else {; 282 // multi-thread snapshot; 283 using Helper_t = SnapshotHelperMT<ColTypes...>;; 284 using Action_t = RAction<Helper_t, PrevNodeType>;; 285 actionPtr.reset(new Action_t(; 286 Helper_t(nSlots, filename, dirname, treename, colNames, outputColNames, options, std::move(isDefine)),; 287 colNames, prevNode, colRegister));; 288 }; 289 return actionPtr;; 290}; 291 ; 292// Book with custom helper type; 293template <typename... ColTypes, typename PrevNodeType, typename Helper_t>; 294std::unique_ptr<RActionBase>; 295BuildAction(const ColumnNames_t &bl, const std::shared_ptr<Helper_t> &h, const unsigned int /*nSlots*/,; 296 std::shared_ptr<PrevNodeType> prevNode, ActionTags::Book, const RColumnRegister &colRegister); 297{; 298 using Action_t = RAction<Helper_t, PrevNodeType, TTraits::TypeList<ColTypes...>>;; 299 return std::make_unique<Action_t>(Helper_t(std::move(*h)), bl, std::move(prevNode), colRegister);; 300}; 301 ; 302/****** end BuildAndBoo",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:46758,Performance,multi-thread,multi-threading,46758,"ree, const unsigned int nSlots, const RColumnRegister &colRegister, RDataSource *ds, std::weak_ptr< RJittedAction > *jittedActionOnHeap, const bool vector2RVec)Definition RDFInterfaceUtils.cxx:849; ROOT::Internal::RDF::FindUnknownColumnsColumnNames_t FindUnknownColumns(const ColumnNames_t &requiredCols, const ColumnNames_t &datasetColumns, const RColumnRegister &definedCols, const ColumnNames_t &dataSourceColumns)Definition RDFInterfaceUtils.cxx:611; ROOT::Internal::VecOps::voidvoid(off) SmallVectorTemplateBase< T; ROOT::Math::Chebyshev::Tdouble T(double x)Definition ChebyshevPol.h:34; ROOT::RDFDefinition RArrowDS.hxx:28; ROOT::RDF::ColumnNames_tstd::vector< std::string > ColumnNames_tDefinition RInterfaceBase.hxx:35; ROOT::TypeTraitsROOT type_traits extensions.Definition TypeTraits.hxx:21; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4; ROOT::IsImplicitMTEnabledBool_t IsImplicitMTEnabled()Returns true if the implicit multi-threading in ROOT is enabled.Definition TROOT.cxx:570; TMath::MaxShort_t Max(Short_t a, Short_t b)Returns the largest of a and b.Definition TMathBase.h:250; TMath::MinShort_t Min(Short_t a, Short_t b)Returns the smallest of a and b.Definition TMathBase.h:198; TMath::MeanDouble_t Mean(Long64_t n, const T *a, const Double_t *w=nullptr)Returns the weighted mean of an array a with length n.Definition TMath.h:1089; TMath::StdDevDouble_t StdDev(Long64_t n, const T *a, const Double_t *w=nullptr)Definition TMath.h:527; ROOT::Detail::RDF::ExtraArgsForDefine::NoneDefinition RDefine.hxx:39; ROOT::Detail::RDF::RInferredTypeDefinition Utils.hxx:59; ROOT::RDF::RSnapshotOptionsA collection of options to steer the creation of the dataset on file.Definition RSnapshotOptions.hxx:22; ROOT::TypeTraits::TypeListLightweight storage for a collection of types.Definition TypeTraits.hxx:25. treedataframeincROOTRDFInterfaceUtils.hxx. ROOT master - Reference Guide Generated on Tue",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:19997,Testability,mock,mock,19997,", 0};; 419}; 420 ; 421// this function is meant to be called by the jitted code generated by BookFilterJit; 422template <typename F, typename PrevNode>; 423void JitFilterHelper(F &&f, const char **colsPtr, std::size_t colsSize, std::string_view name,; 424 std::weak_ptr<RJittedFilter> *wkJittedFilter, std::shared_ptr<PrevNode> *prevNodeOnHeap,; 425 RColumnRegister *colRegister) noexcept; 426{; 427 if (wkJittedFilter->expired()) {; 428 // The branch of the computation graph that needed this jitted code went out of scope between the type; 429 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 430 delete wkJittedFilter;; 431 delete colRegister;; 432 delete prevNodeOnHeap;; 433 return;; 434 }; 435 ; 436 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 437 delete[] colsPtr;; 438 ; 439 const auto jittedFilter = wkJittedFilter->lock();; 440 ; 441 // mock Filter logic -- validity checks and Define-ition of RDataSource columns; 442 using Callable_t = std::decay_t<F>;; 443 using F_t = RFilter<Callable_t, PrevNode>;; 444 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 445 constexpr auto nColumns = ColTypes_t::list_size;; 446 CheckFilter(f);; 447 ; 448 auto &lm = *jittedFilter->GetLoopManagerUnchecked(); // RLoopManager must exist at this time; 449 auto ds = lm.GetDataSource();; 450 ; 451 if (ds != nullptr); 452 AddDSColumns(cols, lm, *ds, ColTypes_t(), *colRegister);; 453 ; 454 jittedFilter->SetFilter(; 455 std::unique_ptr<RFilterBase>(new F_t(std::forward<F>(f), cols, *prevNodeOnHeap, *colRegister, name)));; 456 // colRegister points to the columns structure in the heap, created before the jitted call so that the jitter can; 457 // share data after it has lazily compiled the code. Here the data has been used and the memory can be freed.; 458 delete colRegister;; 459 delete prevNodeOnHeap;; 460 delete wkJittedFilter;; 461}; 462 ; 463namespace DefineTypes {; 464struct RDefineTag {};; 465struct ",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/InterfaceUtils_8hxx_source.html:20009,Testability,log,logic,20009,", 0};; 419}; 420 ; 421// this function is meant to be called by the jitted code generated by BookFilterJit; 422template <typename F, typename PrevNode>; 423void JitFilterHelper(F &&f, const char **colsPtr, std::size_t colsSize, std::string_view name,; 424 std::weak_ptr<RJittedFilter> *wkJittedFilter, std::shared_ptr<PrevNode> *prevNodeOnHeap,; 425 RColumnRegister *colRegister) noexcept; 426{; 427 if (wkJittedFilter->expired()) {; 428 // The branch of the computation graph that needed this jitted code went out of scope between the type; 429 // jitting was booked and the time jitting actually happened. Nothing to do other than cleaning up.; 430 delete wkJittedFilter;; 431 delete colRegister;; 432 delete prevNodeOnHeap;; 433 return;; 434 }; 435 ; 436 const ColumnNames_t cols(colsPtr, colsPtr + colsSize);; 437 delete[] colsPtr;; 438 ; 439 const auto jittedFilter = wkJittedFilter->lock();; 440 ; 441 // mock Filter logic -- validity checks and Define-ition of RDataSource columns; 442 using Callable_t = std::decay_t<F>;; 443 using F_t = RFilter<Callable_t, PrevNode>;; 444 using ColTypes_t = typename TTraits::CallableTraits<Callable_t>::arg_types;; 445 constexpr auto nColumns = ColTypes_t::list_size;; 446 CheckFilter(f);; 447 ; 448 auto &lm = *jittedFilter->GetLoopManagerUnchecked(); // RLoopManager must exist at this time; 449 auto ds = lm.GetDataSource();; 450 ; 451 if (ds != nullptr); 452 AddDSColumns(cols, lm, *ds, ColTypes_t(), *colRegister);; 453 ; 454 jittedFilter->SetFilter(; 455 std::unique_ptr<RFilterBase>(new F_t(std::forward<F>(f), cols, *prevNodeOnHeap, *colRegister, name)));; 456 // colRegister points to the columns structure in the heap, created before the jitted call so that the jitter can; 457 // share data after it has lazily compiled the code. Here the data has been used and the memory can be freed.; 458 delete colRegister;; 459 delete prevNodeOnHeap;; 460 delete wkJittedFilter;; 461}; 462 ; 463namespace DefineTypes {; 464struct RDefineTag {};; 465struct ",MatchSource.WIKI,doc/master/InterfaceUtils_8hxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/InterfaceUtils_8hxx_source.html
https://root.cern/doc/master/IParamFunctionfwd_8h_source.html:1864,Integrability,interface,interface,1864,24 template<class T>; 25 class IParametricFunctionMultiDimTempl;; 26 using IParametricFunctionMultiDim = IParametricFunctionMultiDimTempl<double>;; 27 template<class T>; 28 class IParametricGradFunctionMultiDimTempl;; 29 using IParametricGradFunctionMultiDim = IParametricGradFunctionMultiDimTempl<double>;; 30 ; 31 typedef IParametricFunctionOneDim IParamFunction;; 32 typedef IParametricFunctionMultiDim IParamMultiFunction;; 33 template<class T>; 34 using IParamMultiFunctionTempl = IParametricFunctionMultiDimTempl<T>;; 35 ; 36 typedef IParametricGradFunctionOneDim IParamGradFunction;; 37 typedef IParametricGradFunctionMultiDim IParamMultiGradFunction;; 38 template<class T>; 39 using IParamMultiGradFunctionTempl = IParametricGradFunctionMultiDimTempl<T>;; 40 ; 41 ; 42 } // end namespace Math; 43 ; 44} // end namespace ROOT; 45 ; 46 ; 47#endif /* ROOT_Math_IParamFunctionfwd */; IFunctionfwd.h; ROOT::Math::IParametricFunctionMultiDimTempl< double >; ROOT::Math::IParametricFunctionOneDimSpecialized IParamFunction interface (abstract class) for one-dimensional parametric functions It is ...Definition IParamFunction.h:161; ROOT::Math::IParametricGradFunctionMultiDimTemplInterface (abstract class) for parametric gradient multi-dimensional functions providing in addition ...Definition IParamFunction.h:227; ROOT::Math::IParametricGradFunctionOneDimInterface (abstract class) for parametric one-dimensional gradient functions providing in addition to...Definition IParamFunction.h:330; MathNamespace for new Math classes and functions.; ROOT::Math::IParamFunctionIParametricFunctionOneDim IParamFunctionDefinition IParamFunctionfwd.h:31; ROOT::Math::IParamGradFunctionIParametricGradFunctionOneDim IParamGradFunctionDefinition IParamFunctionfwd.h:36; ROOT::Math::IParamMultiFunctionIParametricFunctionMultiDim IParamMultiFunctionDefinition IParamFunctionfwd.h:32; ROOT::Math::IParamMultiGradFunctionIParametricGradFunctionMultiDim IParamMultiGradFunctionDefinition IParamFunctionfwd.h:37; ,MatchSource.WIKI,doc/master/IParamFunctionfwd_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/IParamFunctionfwd_8h_source.html
https://root.cern/doc/master/labels1_8C.html:648,Testability,test,test,648,". ROOT: tutorials/graphs/labels1.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. labels1.C File ReferenceTutorials » Graphs tutorials. Detailed Description; Setting alphanumeric labels in a 1-d histogram. . ; void labels1(); {; Int_t i;; const Int_t nx = 20;; const char *people[nx] = {""Jean"",""Pierre"",""Marie"",""Odile"",; ""Sebastien"",""Fons"",""Rene"",""Nicolas"",""Xavier"",""Greg"",; ""Bjarne"",""Anton"",""Otto"",""Eddy"",""Peter"",""Pasha"",; ""Philippe"",""Suzanne"",""Jeff"",""Valery""};; TCanvas *c1 = new TCanvas(""c1"",""demo bin labels"",10,10,900,500);; c1->SetGrid();; c1->SetBottomMargin(0.15);; TH1F *h = new TH1F(""h"",""test"",nx,0,nx);; h->SetFillColor(38);; for (i=0;i<5000;i++) h->Fill(gRandom->Gaus(0.5*nx,0.2*nx));; h->SetStats(0);; for (i=1;i<=nx;i++) h->GetXaxis()->SetBinLabel(i,people[i-1]);; h->Draw();; TPaveText *pt = new TPaveText(0.6,0.7,0.98,0.98,""brNDC"");; pt->SetFillColor(18);; pt->SetTextAlign(12);; pt->AddText(""Use the axis Context Menu LabelsOption"");; pt->AddText("" \""a\"" to sort by alphabetic order"");; pt->AddText("" \"">\"" to sort by decreasing values"");; pt->AddText("" \""<\"" to sort by increasing values"");; pt->Draw();; }; h#define h(i)Definition RSha256.hxx:106; Int_tint Int_tDefinition RtypesCore.h:45; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttText::SetTextAlignvirtual void SetTextAlign(Short_t align=11)Set the text alignment.Definition TAttText.h:42; TCanvasThe Canvas class.Definition TCanvas.h:23; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TPaveTextA Pave (see TPave) with text, lines or/and boxes inside.Definition TPaveText.h:21; TPaveText::AddTextvirtual TText * AddText(Double_t x1, Double_t y1, const char *label)Add a new Text line to this pavetext at given coordinates.Definition TPaveText.cxx:191; TPaveText::Drawvoid Draw(Option_t *option="""") overrid",MatchSource.WIKI,doc/master/labels1_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/labels1_8C.html
https://root.cern/doc/master/labels2_8C.html:821,Testability,test,test,821,". ROOT: tutorials/graphs/labels2.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. labels2.C File ReferenceTutorials » Graphs tutorials. Detailed Description; Setting alphanumeric labels. . ; void labels2(); {; Int_t i;; const Int_t nx = 12;; const Int_t ny = 20;; const char *month[nx] = {""January"",""February"",""March"",""April"",; ""May"",""June"",""July"",""August"",""September"",""October"",; ""November"",""December""};; const char *people[ny] = {""Jean"",""Pierre"",""Marie"",""Odile"",; ""Sebastien"",""Fons"",""Rene"",""Nicolas"",""Xavier"",""Greg"",; ""Bjarne"",""Anton"",""Otto"",""Eddy"",""Peter"",""Pasha"",; ""Philippe"",""Suzanne"",""Jeff"",""Valery""};; TCanvas *c1 = new TCanvas(""c1"",""demo bin labels"",; 10,10,800,800);; c1->SetGrid();; c1->SetLeftMargin(0.15);; c1->SetBottomMargin(0.15);; TH2F *h = new TH2F(""h"",""test"",nx,0,nx,ny,0,ny);; for (i=0;i<5000;i++) {; h->Fill(gRandom->Gaus(0.5*nx,0.2*nx),; gRandom->Gaus(0.5*ny,0.2*ny));; }; h->SetStats(0);; for (i=1;i<=nx;i++) h->GetXaxis()->SetBinLabel(i,month[i-1]);; for (i=1;i<=ny;i++) h->GetYaxis()->SetBinLabel(i,people[i-1]);; h->Draw(""text"");; ; TPaveText *pt = new TPaveText(0.6,0.85,0.98,0.98,""brNDC"");; pt->SetFillColor(18);; pt->SetTextAlign(12);; pt->AddText(""Use the axis Context Menu LabelsOption"");; pt->AddText("" \""a\"" to sort by alphabetic order"");; pt->AddText("" \"">\"" to sort by decreasing values"");; pt->AddText("" \""<\"" to sort by increasing values"");; pt->Draw();; }; h#define h(i)Definition RSha256.hxx:106; Int_tint Int_tDefinition RtypesCore.h:45; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttText::SetTextAlignvirtual void SetTextAlign(Short_t align=11)Set the text alignment.Definition TAttText.h:42; TCanvasThe Canvas class.Definition TCanvas.h:23; TH2F2-D histogram with a float per channel (see TH1 documentation)Definition TH2.h:307; TPaveTextA Pave (see TPave) with text, lines or/and",MatchSource.WIKI,doc/master/labels2_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/labels2_8C.html
https://root.cern/doc/master/langaus_8C.html:3281,Availability,error,errors,3281,,MatchSource.WIKI,doc/master/langaus_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/langaus_8C.html
https://root.cern/doc/master/langaus_8C.html:3993,Availability,error,errors,3993,"r[0]) / par[0];; sum += fland * TMath::Gaus(x[0],xx,par[3]);; }; ; return (par[2] * step * sum * invsq2pi / par[3]);; }; ; ; ; TF1 *langaufit(TH1F *his, double *fitrange, double *startvalues, double *parlimitslo, double *parlimitshi, double *fitparams, double *fiterrors, double *ChiSqr, int *NDF); {; // Once again, here are the Landau * Gaussian parameters:; // par[0]=Width (scale) parameter of Landau density; // par[1]=Most Probable (MP, location) parameter of Landau density; // par[2]=Total area (integral -inf to inf, normalization constant); // par[3]=Width (sigma) of convoluted Gaussian function; //; // Variables for langaufit call:; // his histogram to fit; // fitrange[2] lo and hi boundaries of fit range; // startvalues[4] reasonable start values for the fit; // parlimitslo[4] lower parameter limits; // parlimitshi[4] upper parameter limits; // fitparams[4] returns the final fit parameters; // fiterrors[4] returns the final fit errors; // ChiSqr returns the chi square; // NDF returns ndf; ; int i;; char FunName[100];; ; sprintf(FunName,""Fitfcn_%s"",his->GetName());; ; TF1 *ffitold = (TF1*)gROOT->GetListOfFunctions()->FindObject(FunName);; if (ffitold) delete ffitold;; ; TF1 *ffit = new TF1(FunName,langaufun,fitrange[0],fitrange[1],4);; ffit->SetParameters(startvalues);; ffit->SetParNames(""Width"",""MP"",""Area"",""GSigma"");; ; for (i=0; i<4; i++) {; ffit->SetParLimits(i, parlimitslo[i], parlimitshi[i]);; }; ; his->Fit(FunName,""RB0""); // fit within specified range, use ParLimits, do not plot; ; ffit->GetParameters(fitparams); // obtain fit parameters; for (i=0; i<4; i++) {; fiterrors[i] = ffit->GetParError(i); // obtain fit parameter errors; }; ChiSqr[0] = ffit->GetChisquare(); // obtain chi^2; NDF[0] = ffit->GetNDF(); // obtain ndf; ; return (ffit); // return fit function; ; }; ; ; int langaupro(double *params, double &maxx, double &FWHM) {; ; // Searches for the location (x value) at the maximum of the; // Landau-Gaussian convolute and its full width at half-maximum.",MatchSource.WIKI,doc/master/langaus_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/langaus_8C.html
https://root.cern/doc/master/langaus_8C.html:4375,Energy Efficiency,efficient,efficient,4375,";; ; sprintf(FunName,""Fitfcn_%s"",his->GetName());; ; TF1 *ffitold = (TF1*)gROOT->GetListOfFunctions()->FindObject(FunName);; if (ffitold) delete ffitold;; ; TF1 *ffit = new TF1(FunName,langaufun,fitrange[0],fitrange[1],4);; ffit->SetParameters(startvalues);; ffit->SetParNames(""Width"",""MP"",""Area"",""GSigma"");; ; for (i=0; i<4; i++) {; ffit->SetParLimits(i, parlimitslo[i], parlimitshi[i]);; }; ; his->Fit(FunName,""RB0""); // fit within specified range, use ParLimits, do not plot; ; ffit->GetParameters(fitparams); // obtain fit parameters; for (i=0; i<4; i++) {; fiterrors[i] = ffit->GetParError(i); // obtain fit parameter errors; }; ChiSqr[0] = ffit->GetChisquare(); // obtain chi^2; NDF[0] = ffit->GetNDF(); // obtain ndf; ; return (ffit); // return fit function; ; }; ; ; int langaupro(double *params, double &maxx, double &FWHM) {; ; // Searches for the location (x value) at the maximum of the; // Landau-Gaussian convolute and its full width at half-maximum.; //; // The search is probably not very efficient, but it's a first try.; ; double p,x,fy,fxr,fxl;; double step;; double l,lold;; int i = 0;; int MAXCALLS = 10000;; ; ; // Search for maximum; ; p = params[1] - 0.1 * params[0];; step = 0.05 * params[0];; lold = -2.0;; l = -1.0;; ; ; while ( (l != lold) && (i < MAXCALLS) ) {; i++;; ; lold = l;; x = p + step;; l = langaufun(&x,params);; ; if (l < lold); step = -step/10;; ; p += step;; }; ; if (i == MAXCALLS); return (-1);; ; maxx = x;; ; fy = l/2;; ; ; // Search for right x location of fy; ; p = maxx + params[0];; step = params[0];; lold = -2.0;; l = -1e300;; i = 0;; ; ; while ( (l != lold) && (i < MAXCALLS) ) {; i++;; ; lold = l;; x = p + step;; l = TMath::Abs(langaufun(&x,params) - fy);; ; if (l > lold); step = -step/10;; ; p += step;; }; ; if (i == MAXCALLS); return (-2);; ; fxr = x;; ; ; // Search for left x location of fy; ; p = maxx - 0.5 * params[0];; step = -params[0];; lold = -2.0;; l = -1e300;; i = 0;; ; while ( (l != lold) && (i < MAXCALLS) ) {; i++;; ; lold = l;",MatchSource.WIKI,doc/master/langaus_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/langaus_8C.html
https://root.cern/doc/master/langaus_8C.html:1732,Modifiability,extend,extends,1732,"ted); Fitting done; Plotting results...; ; #include ""TH1.h""; #include ""TF1.h""; #include ""TROOT.h""; #include ""TStyle.h""; #include ""TMath.h""; ; double langaufun(double *x, double *par) {; ; //Fit parameters:; //par[0]=Width (scale) parameter of Landau density; //par[1]=Most Probable (MP, location) parameter of Landau density; //par[2]=Total area (integral -inf to inf, normalization constant); //par[3]=Width (sigma) of convoluted Gaussian function; //; //In the Landau distribution (represented by the CERNLIB approximation),; //the maximum is located at x=-0.22278298 with the location parameter=0.; //This shift is corrected within this function, so that the actual; //maximum is identical to the MP parameter.; ; // Numeric constants; double invsq2pi = 0.3989422804014; // (2 pi)^(-1/2); double mpshift = -0.22278298; // Landau maximum location; ; // Control constants; double np = 100.0; // number of convolution steps; double sc = 5.0; // convolution extends to +-sc Gaussian sigmas; ; // Variables; double xx;; double mpc;; double fland;; double sum = 0.0;; double xlow,xupp;; double step;; double i;; ; ; // MP shift correction; mpc = par[1] - mpshift * par[0];; ; // Range of convolution integral; xlow = x[0] - sc * par[3];; xupp = x[0] + sc * par[3];; ; step = (xupp-xlow) / np;; ; // Convolution integral of Landau and Gaussian by sum; for(i=1.0; i<=np/2; i++) {; xx = xlow + (i-.5) * step;; fland = TMath::Landau(xx,mpc,par[0]) / par[0];; sum += fland * TMath::Gaus(x[0],xx,par[3]);; ; xx = xupp - (i-.5) * step;; fland = TMath::Landau(xx,mpc,par[0]) / par[0];; sum += fland * TMath::Gaus(x[0],xx,par[3]);; }; ; return (par[2] * step * sum * invsq2pi / par[3]);; }; ; ; ; TF1 *langaufit(TH1F *his, double *fitrange, double *startvalues, double *parlimitslo, double *parlimitshi, double *fitparams, double *fiterrors, double *ChiSqr, int *NDF); {; // Once again, here are the Landau * Gaussian parameters:; // par[0]=Width (scale) parameter of Landau density; // par[1]=Most Probable (MP,",MatchSource.WIKI,doc/master/langaus_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/langaus_8C.html
https://root.cern/doc/master/latex_8C.html:355,Testability,test,test,355,". ROOT: tutorials/graphics/latex.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. latex.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro draws 5 Latex-style formula in a canvas and prints the canvas as a Postscript file. . ; void latex() {; TCanvas *c1 = new TCanvas(""c1"",""test"",600,700);; // write formulas; TLatex l;; l.SetTextAlign(12);; l.SetTextSize(0.04);; l.DrawLatex(0.1,0.9,""1) C(x) = d #sqrt{#frac{2}{#lambdaD}}\; #int^{x}_{0}cos(#frac{#pi}{2}t^{2})dt"");; l.DrawLatex(0.1,0.7,""2) C(x) = d #sqrt{#frac{2}{#lambdaD}}\; #int^{x}cos(#frac{#pi}{2}t^{2})dt"");; l.DrawLatex(0.1,0.5,""3) R = |A|^{2} = #frac{1}{2}#left(#[]{#frac{1}{2}+\; C(V)}^{2}+#[]{#frac{1}{2}+S(V)}^{2}#right)"");; l.DrawLatex(0.1,0.3,; ""4) F(t) = #sum_{i=-#infty}^{#infty}A(i)cos#[]{#frac{i}{t+i}}"");; l.DrawLatex(0.1,0.1,""5) {}_{3}^{7}Li"");; c1->Print(""latex.ps"");; }; TCanvasThe Canvas class.Definition TCanvas.h:23; TLatexTo draw Mathematical Formula.Definition TLatex.h:18; TLine::Printvoid Print(Option_t *option="""") const overrideDump this line with its attributes.Definition TLine.cxx:419; c1return c1Definition legend1.C:41; lTLine lDefinition textangle.C:4; AuthorRene Brun ; Definition in file latex.C. tutorialsgraphicslatex.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:29 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/latex_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/latex_8C.html
https://root.cern/doc/master/legend1_8C.html:292,Availability,error,error,292,". ROOT: graf2d/graf/doc/macros/legend1.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Functions |; Variables ; legend1.C File Reference. Functions; leg AddEntry (""f1"",""Function abs(#frac{sin(x)}{x})"",""l"");  ; leg AddEntry (""gr"",""Graph with error bars"",""lep"");  ; leg AddEntry (h1,""Histogram filled with random numbers"",""f"");  ; gr Draw (""P"");  ; f1 Draw (""same"");  ; h1 Draw ();  ; h1 FillRandom (""gaus"", 30000);  ;  for (Int_t i=0;i< n;i++);  ; h1 SetFillColor (kGreen);  ; h1 SetFillStyle (3003);  ; leg SetHeader (""The Legend Title"");  ; f1 SetLineColor (kBlue);  ; gr SetLineColor (kRed);  ; gr SetLineWidth (2);  ; f1 SetLineWidth (4);  ; gr SetMarkerColor (7);  ; gr SetMarkerSize (1.3);  ; gr SetMarkerStyle (21);  ; gr SetName (""gr"");  ; gStyle SetOptStat (0);  . Variables; return c1;  ; Double_t ex [n];  ; Double_t ey [n];  ; TF1 * f1 =new TF1(""f1"",""1000*TMath::Abs(sin(x)/x)"",-10,10);  ; TGraphErrors * gr = new TGraphErrors(n,x,y,ex,ey);  ; TH1F * h1 = new TH1F(""h1"",""TLegend Example"",200,-10,10);  ;  leg = new TLegend(0.1,0.7,0.48,0.9);  ; const Int_t n = 20;  ; Double_t x [n];  ; Double_t y [n];  . Function Documentation. ◆ AddEntry() [1/3]. leg AddEntry ; (; ""f1"" ; , . ""Function abs(#frac{sin(x)}{x})"" ; , . ""l"" ;  . ). ◆ AddEntry() [2/3]. leg AddEntry ; (; ""gr"" ; , . ""Graph with error bars"" ; , . ""lep"" ;  . ). ◆ AddEntry() [3/3]. leg AddEntry ; (; h1 ; , . ""Histogram filled with random numbers"" ; , . ""f"" ;  . ). ◆ Draw() [1/3]. gr Draw ; (; ""P"" ; ). ◆ Draw() [2/3]. f1 Draw ; (; ""same"" ; ). ◆ Draw() [3/3]. h1 Draw ; (; ). ◆ FillRandom(). h1 FillRandom ; (; ""gaus"" ; , . 30000 ;  . ). ◆ for(). for ; (; ). Definition at line 18 of file legend1.C. ◆ SetFillColor(). h1 SetFillColor ; (; kGreen ; ). ◆ SetFillStyle(). h1 SetFillStyle ; (; 3003 ; ). ◆ SetHeader(). leg SetHeader ; (; ""The Legend Title"" ; ). ◆ SetLineColor() [1/2]. f1 SetLineColor ; (; kBlue ; ). ◆ SetLineColor() [2/2]. gr SetLineColor ; (; kRed ; ). ◆ SetLin",MatchSource.WIKI,doc/master/legend1_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/legend1_8C.html
https://root.cern/doc/master/legend1_8C.html:1353,Availability,error,error,1353,"random numbers"",""f"");  ; gr Draw (""P"");  ; f1 Draw (""same"");  ; h1 Draw ();  ; h1 FillRandom (""gaus"", 30000);  ;  for (Int_t i=0;i< n;i++);  ; h1 SetFillColor (kGreen);  ; h1 SetFillStyle (3003);  ; leg SetHeader (""The Legend Title"");  ; f1 SetLineColor (kBlue);  ; gr SetLineColor (kRed);  ; gr SetLineWidth (2);  ; f1 SetLineWidth (4);  ; gr SetMarkerColor (7);  ; gr SetMarkerSize (1.3);  ; gr SetMarkerStyle (21);  ; gr SetName (""gr"");  ; gStyle SetOptStat (0);  . Variables; return c1;  ; Double_t ex [n];  ; Double_t ey [n];  ; TF1 * f1 =new TF1(""f1"",""1000*TMath::Abs(sin(x)/x)"",-10,10);  ; TGraphErrors * gr = new TGraphErrors(n,x,y,ex,ey);  ; TH1F * h1 = new TH1F(""h1"",""TLegend Example"",200,-10,10);  ;  leg = new TLegend(0.1,0.7,0.48,0.9);  ; const Int_t n = 20;  ; Double_t x [n];  ; Double_t y [n];  . Function Documentation. ◆ AddEntry() [1/3]. leg AddEntry ; (; ""f1"" ; , . ""Function abs(#frac{sin(x)}{x})"" ; , . ""l"" ;  . ). ◆ AddEntry() [2/3]. leg AddEntry ; (; ""gr"" ; , . ""Graph with error bars"" ; , . ""lep"" ;  . ). ◆ AddEntry() [3/3]. leg AddEntry ; (; h1 ; , . ""Histogram filled with random numbers"" ; , . ""f"" ;  . ). ◆ Draw() [1/3]. gr Draw ; (; ""P"" ; ). ◆ Draw() [2/3]. f1 Draw ; (; ""same"" ; ). ◆ Draw() [3/3]. h1 Draw ; (; ). ◆ FillRandom(). h1 FillRandom ; (; ""gaus"" ; , . 30000 ;  . ). ◆ for(). for ; (; ). Definition at line 18 of file legend1.C. ◆ SetFillColor(). h1 SetFillColor ; (; kGreen ; ). ◆ SetFillStyle(). h1 SetFillStyle ; (; 3003 ; ). ◆ SetHeader(). leg SetHeader ; (; ""The Legend Title"" ; ). ◆ SetLineColor() [1/2]. f1 SetLineColor ; (; kBlue ; ). ◆ SetLineColor() [2/2]. gr SetLineColor ; (; kRed ; ). ◆ SetLineWidth() [1/2]. gr SetLineWidth ; (; 2 ; ). ◆ SetLineWidth() [2/2]. f1 SetLineWidth ; (; 4 ; ). ◆ SetMarkerColor(). gr SetMarkerColor ; (; 7 ; ). ◆ SetMarkerSize(). gr SetMarkerSize ; (; 1. ; 3). ◆ SetMarkerStyle(). gr SetMarkerStyle ; (; 21 ; ). ◆ SetName(). gr SetName ; (; ""gr"" ; ). ◆ SetOptStat(). gStyle SetOptStat ; (; 0 ; ). Variable Documentation",MatchSource.WIKI,doc/master/legend1_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/legend1_8C.html
https://root.cern/doc/master/legend1_8C_source.html:1268,Availability,error,error,1268,"""c1"",""c1"",600,500);; 3 gStyle->SetOptStat(0);; 4 ; 5 TH1F *h1 = new TH1F(""h1"",""TLegend Example"",200,-10,10);; 6 h1->FillRandom(""gaus"",30000);; 7 h1->SetFillColor(kGreen);; 8 h1->SetFillStyle(3003);; 9 h1->Draw();; 10 ; 11 TF1 *f1=new TF1(""f1"",""1000*TMath::Abs(sin(x)/x)"",-10,10);; 12 f1->SetLineColor(kBlue);; 13 f1->SetLineWidth(4);; 14 f1->Draw(""same"");; 15 ; 16 const Int_t n = 20;; 17 Double_t x[n], y[n], ex[n], ey[n];; 18 for (Int_t i=0;i<n;i++) {; 19 x[i] = i*0.1;; 20 y[i] = 1000*sin(x[i]+0.2);; 21 x[i] = 17.8*x[i]-8.9;; 22 ex[i] = 1.0;; 23 ey[i] = 10.*i;; 24 }; 25 TGraphErrors *gr = new TGraphErrors(n,x,y,ex,ey);; 26 gr->SetName(""gr"");; 27 gr->SetLineColor(kRed);; 28 gr->SetLineWidth(2);; 29 gr->SetMarkerStyle(21);; 30 gr->SetMarkerSize(1.3);; 31 gr->SetMarkerColor(7);; 32 gr->Draw(""P"");; 33 ; 34 leg = new TLegend(0.1,0.7,0.48,0.9);; 35 leg->SetHeader(""The Legend Title"");; 36 leg->AddEntry(h1,""Histogram filled with random numbers"",""f"");; 37 leg->AddEntry(""f1"",""Function abs(#frac{sin(x)}{x})"",""l"");; 38 leg->AddEntry(""gr"",""Graph with error bars"",""lep"");; 39 leg->Draw();; 40 ; 41 return c1;; 42}; 43 ; Int_tint Int_tDefinition RtypesCore.h:45; Double_tdouble Double_tDefinition RtypesCore.h:59; kRed@ kRedDefinition Rtypes.h:66; kGreen@ kGreenDefinition Rtypes.h:66; kBlue@ kBlueDefinition Rtypes.h:66; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttFill::SetFillStylevirtual void SetFillStyle(Style_t fstyle)Set the fill area style.Definition TAttFill.h:39; TAttLine::SetLineWidthvirtual void SetLineWidth(Width_t lwidth)Set the line width.Definition TAttLine.h:43; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TAttMarker::SetMarkerColorvirtual void SetMarkerColor(Color_t mcolor=1)Set the marker color.Definition TAttMarker.h:38; TAttMarker::SetMarkerStylevirtual void SetMarkerStyle(Style_t ",MatchSource.WIKI,doc/master/legend1_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/legend1_8C_source.html
https://root.cern/doc/master/legend1_8C_source.html:2648,Availability,error,error,2648,"d SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttFill::SetFillStylevirtual void SetFillStyle(Style_t fstyle)Set the fill area style.Definition TAttFill.h:39; TAttLine::SetLineWidthvirtual void SetLineWidth(Width_t lwidth)Set the line width.Definition TAttLine.h:43; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TAttMarker::SetMarkerColorvirtual void SetMarkerColor(Color_t mcolor=1)Set the marker color.Definition TAttMarker.h:38; TAttMarker::SetMarkerStylevirtual void SetMarkerStyle(Style_t mstyle=1)Set the marker style.Definition TAttMarker.h:40; TAttMarker::SetMarkerSizevirtual void SetMarkerSize(Size_t msize=1)Set the marker size.Definition TAttMarker.h:45; TCanvasThe Canvas class.Definition TCanvas.h:23; TF11-Dim function classDefinition TF1.h:233; TF1::Drawvoid Draw(Option_t *option="""") overrideDraw this function with its current attributes.Definition TF1.cxx:1333; TGraphErrorsA TGraphErrors is a TGraph with error bars.Definition TGraphErrors.h:26; TGraph::SetNamevoid SetName(const char *name="""") overrideSet graph name.Definition TGraph.cxx:2381; TGraph::Drawvoid Draw(Option_t *chopt="""") overrideDraw this graph with its current attributes.Definition TGraph.cxx:831; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TH1::FillRandomvirtual void FillRandom(const char *fname, Int_t ntimes=5000, TRandom *rng=nullptr)Fill histogram following distribution in function fname.Definition TH1.cxx:3519; TH1::Drawvoid Draw(Option_t *option="""") overrideDraw this histogram with options.Definition TH1.cxx:3066; TLegendThis class displays a legend box (TPaveText) containing several legend entries.Definition TLegend.h:23; TStyle::SetOptStatvoid SetOptStat(Int_t stat=1)The type of information printed in the histogram statistics box can be selected via the parameter mod...Definition TStyle.cxx:1640; yDouble_t y[n]Definition legend1.C:17; c1return c1Defi",MatchSource.WIKI,doc/master/legend1_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/legend1_8C_source.html
https://root.cern/doc/master/legendautoplaced_8C.html:885,Energy Efficiency,green,green,885,". ROOT: tutorials/hist/legendautoplaced.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. legendautoplaced.C File ReferenceTutorials » Histograms tutorials. Detailed Description; The legend can be placed automatically in the current pad in an empty space found at painting time. ; The following example illustrate this facility. Only the width and height of the legend is specified in percentage of the pad size. ; void legendautoplaced(); {; auto c4 = new TCanvas(""c"", ""c"", 600,500);; auto hpx = new TH1D(""hpx"",""This is the hpx distribution"",100,-4.,4.);; hpx->FillRandom(""gaus"", 50000);; hpx->Draw(""E"");; hpx->GetYaxis()->SetTitle(""Y Axis title"");; hpx->GetYaxis()->SetTitleOffset(1.3); hpx->GetYaxis()->CenterTitle(true);; hpx->GetXaxis()->SetTitle(""X Axis title"");; hpx->GetXaxis()->CenterTitle(true);; ; auto h1 = new TH1D(""h1"",""A green histogram"",100,-2.,2.);; h1->FillRandom(""gaus"", 10000);; h1->SetLineColor(kGreen);; h1->Draw(""same"");; ; auto g = new TGraph();; g->SetPoint(0, -3.5, 100 );; g->SetPoint(1, -3.0, 300 );; g->SetPoint(2, -2.0, 1000 );; g->SetPoint(3, 1.0, 800 );; g->SetPoint(4, 0.0, 200 );; g->SetPoint(5, 3.0, 200 );; g->SetPoint(6, 3.0, 700 );; g->Draw(""L"");; g->SetTitle(""This is a TGraph"");; g->SetLineColor(kRed);; g->SetFillColor(0);; ; // TPad::BuildLegend() default placement values are such that they trigger; // the automatic placement.; c4->BuildLegend();; }; g#define g(i)Definition RSha256.hxx:105; kRed@ kRedDefinition Rtypes.h:66; kGreen@ kGreenDefinition Rtypes.h:66; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TCanvasThe Canvas class.Definition TCanvas.h:23; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1D1-D histogram with a double per channel (see TH1 documentation)Definition TH1.h:670; TH1::FillRandomvirtual void FillRandom(const char *fname, Int_t ntimes=5000, TRandom *rng=nullptr)Fil",MatchSource.WIKI,doc/master/legendautoplaced_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/legendautoplaced_8C.html
https://root.cern/doc/master/line3Dfit_8C.html:4653,Availability,error,error,4653,"lt = fitter.Result();; ; std::cout << ""Total final distance square "" << result.MinFcnValue() << std::endl;; result.Print(std::cout);; ; ; gr->Draw(""p0"");; ; // get fit parameters; const double * parFit = result.GetParams();; ; // draw the fitted line; int n = 1000;; double t0 = 0;; double dt = 10;; TPolyLine3D *l = new TPolyLine3D(n);; for (int i = 0; i <n;++i) {; double t = t0+ dt*i/n;; double x,y,z;; line(t,parFit,x,y,z);; l->SetPoint(i,x,y,z);; }; l->SetLineColor(kRed);; l->Draw(""same"");; ; // draw original line; TPolyLine3D *l0 = new TPolyLine3D(n);; for (int i = 0; i <n;++i) {; double t = t0+ dt*i/n;; double x,y,z;; line(t,p0,x,y,z);; l0->SetPoint(i,x,y,z);; }; l0->SetLineColor(kBlue);; l0->Draw(""same"");; return 0;; }; Fitter.h; Functor.h; d#define d(i)Definition RSha256.hxx:102; g#define g(i)Definition RSha256.hxx:105; kRed@ kRedDefinition Rtypes.h:66; kBlue@ kBlueDefinition Rtypes.h:66; TCanvas.h; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; TF2.h; N#define N; CrossTGLVector3 Cross(const TGLVector3 &v1, const TGLVector3 &v2)Definition TGLUtil.h:323; pwinID h TVirtualViewer3D TVirtualGLPainter pDefinition TGWin32VirtualGLProxy.cxx:51; resultOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t resultDefinition TGWin32VirtualXProxy.cxx:174; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TGraph2D.h; TH1.h; TMath.h; TPolyLine3D.h; operator()TRObject operator()(const T1 &t1) constDefinition TRFunctionImport__oprtr.h:14; TRandom2.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; Vector3D.h; ROOT::Fit::FitConfig::ParSettingsconst ParameterSettings & ParSettings",MatchSource.WIKI,doc/master/line3Dfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/line3Dfit_8C.html
https://root.cern/doc/master/line3Dfit_8C.html:6444,Deployability,configurat,configuration,6444," TRandom2.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; Vector3D.h; ROOT::Fit::FitConfig::ParSettingsconst ParameterSettings & ParSettings(unsigned int i) constget the parameter settings for the i-th parameter (const method)Definition FitConfig.h:76; ROOT::Fit::FitResultclass containing the result of the fit and all the related information (fitted parameter values,...Definition FitResult.h:47; ROOT::Fit::FitterFitter class, entry point for performing all type of fits.Definition Fitter.h:77; ROOT::Fit::Fitter::Resultconst FitResult & Result() constget fit resultDefinition Fitter.h:394; ROOT::Fit::Fitter::FitFCNbool FitFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Fit using the a generic FCN function as a C++ callable object implementing double () (const double *)...Definition Fitter.h:649; ROOT::Fit::Fitter::Configconst FitConfig & Config() constaccess to the fit configuration (const method)Definition Fitter.h:422; ROOT::Fit::Fitter::SetFCNbool SetFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Set a generic FCN function as a C++ callable object implementing double () (const double *) Note that...Definition Fitter.h:656; ROOT::Fit::ParameterSettings::SetStepSizevoid SetStepSize(double err)set the step sizeDefinition ParameterSettings.h:122; ROOT::Math::DisplacementVector3D< Cartesian3D< double >, DefaultCoordinateSystemTag >; ROOT::Math::FunctorDocumentation for class Functor class.Definition Functor.h:47; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TGraph2DGraphics object made of three arrays X, Y and Z with the same number of points each.Definition TGraph2D.h:41; TGraph2D::GetYDouble_t * GetY() constDefinition TGraph2D.h:124; TGraph2D::GetXDouble_t * GetX() constDefinition TGraph2D.h:123; TGraph2D::GetNI",MatchSource.WIKI,doc/master/line3Dfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/line3Dfit_8C.html
https://root.cern/doc/master/line3Dfit_8C.html:6444,Modifiability,config,configuration,6444," TRandom2.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; Vector3D.h; ROOT::Fit::FitConfig::ParSettingsconst ParameterSettings & ParSettings(unsigned int i) constget the parameter settings for the i-th parameter (const method)Definition FitConfig.h:76; ROOT::Fit::FitResultclass containing the result of the fit and all the related information (fitted parameter values,...Definition FitResult.h:47; ROOT::Fit::FitterFitter class, entry point for performing all type of fits.Definition Fitter.h:77; ROOT::Fit::Fitter::Resultconst FitResult & Result() constget fit resultDefinition Fitter.h:394; ROOT::Fit::Fitter::FitFCNbool FitFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Fit using the a generic FCN function as a C++ callable object implementing double () (const double *)...Definition Fitter.h:649; ROOT::Fit::Fitter::Configconst FitConfig & Config() constaccess to the fit configuration (const method)Definition Fitter.h:422; ROOT::Fit::Fitter::SetFCNbool SetFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Set a generic FCN function as a C++ callable object implementing double () (const double *) Note that...Definition Fitter.h:656; ROOT::Fit::ParameterSettings::SetStepSizevoid SetStepSize(double err)set the step sizeDefinition ParameterSettings.h:122; ROOT::Math::DisplacementVector3D< Cartesian3D< double >, DefaultCoordinateSystemTag >; ROOT::Math::FunctorDocumentation for class Functor class.Definition Functor.h:47; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TGraph2DGraphics object made of three arrays X, Y and Z with the same number of points each.Definition TGraph2D.h:41; TGraph2D::GetYDouble_t * GetY() constDefinition TGraph2D.h:124; TGraph2D::GetXDouble_t * GetX() constDefinition TGraph2D.h:123; TGraph2D::GetNI",MatchSource.WIKI,doc/master/line3Dfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/line3Dfit_8C.html
https://root.cern/doc/master/line3Dfit_8C.html:5952,Performance,perform,performing,5952," GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t resultDefinition TGWin32VirtualXProxy.cxx:174; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TGraph2D.h; TH1.h; TMath.h; TPolyLine3D.h; operator()TRObject operator()(const T1 &t1) constDefinition TRFunctionImport__oprtr.h:14; TRandom2.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; Vector3D.h; ROOT::Fit::FitConfig::ParSettingsconst ParameterSettings & ParSettings(unsigned int i) constget the parameter settings for the i-th parameter (const method)Definition FitConfig.h:76; ROOT::Fit::FitResultclass containing the result of the fit and all the related information (fitted parameter values,...Definition FitResult.h:47; ROOT::Fit::FitterFitter class, entry point for performing all type of fits.Definition Fitter.h:77; ROOT::Fit::Fitter::Resultconst FitResult & Result() constget fit resultDefinition Fitter.h:394; ROOT::Fit::Fitter::FitFCNbool FitFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Fit using the a generic FCN function as a C++ callable object implementing double () (const double *)...Definition Fitter.h:649; ROOT::Fit::Fitter::Configconst FitConfig & Config() constaccess to the fit configuration (const method)Definition Fitter.h:422; ROOT::Fit::Fitter::SetFCNbool SetFCN(unsigned int npar, Function &fcn, const double *params=nullptr, unsigned int dataSize=0, int fitType=0)Set a generic FCN function as a C++ callable object implementing double () (const double *) Note that...Definition Fitter.h:656; ROOT::Fit::ParameterSettings::SetStepSizevoid SetStepSize(double err)set the step sizeDefinition ParameterSettings.h:122; ROOT::Math::DisplacementVector3D< Cartesian3D< doubl",MatchSource.WIKI,doc/master/line3Dfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/line3Dfit_8C.html
https://root.cern/doc/master/line3Dfit_8C.html:2044,Testability,assert,assert,2044," // can choose z0 = 0 if line not parallel to x-y plane and z1 = 1;; x = p[0] + p[1]*t;; y = p[2] + p[3]*t;; z = t;; }; ; ; bool first = true;; ; // function Object to be minimized; struct SumDistance2 {; // the TGraph is a data member of the object; TGraph2D *fGraph;; ; SumDistance2(TGraph2D *g) : fGraph(g) {}; ; // calculate distance line-point; double distance2(double x,double y,double z, const double *p) {; // distance line point is D= | (xp-x0) cross ux |; // where ux is direction of line and x0 is a point in the line (like t = 0); XYZVector xp(x,y,z);; XYZVector x0(p[0], p[2], 0. );; XYZVector x1(p[0] + p[1], p[2] + p[3], 1. );; XYZVector u = (x1-x0).Unit();; double d2 = ((xp-x0).Cross(u)).Mag2();; return d2;; }; ; // implementation of the function to be minimized; double operator() (const double *par) {; assert(fGraph != nullptr);; double * x = fGraph->GetX();; double * y = fGraph->GetY();; double * z = fGraph->GetZ();; int npoints = fGraph->GetN();; double sum = 0;; for (int i = 0; i < npoints; ++i) {; double d = distance2(x[i],y[i],z[i],par);; sum += d;; }; if (first) {; std::cout << ""Total Initial distance square = "" << sum << std::endl;; }; first = false;; return sum;; }; ; };; ; int line3Dfit(); {; gStyle->SetOptStat(0);; gStyle->SetOptFit();; ; ; //double e = 0.1;; int nd = 10000;; ; ; // double xmin = 0; double ymin = 0;; // double xmax = 10; double ymax = 10;; ; TGraph2D * gr = new TGraph2D();; ; // Fill the 2D graph; double p0[4] = {10,20,1,2};; ; // generate graph with the 3d points; for (int N=0; N<nd; N++) {; double x,y,z = 0;; // Generate a random number; double t = gRandom->Uniform(0,10);; line(t,p0,x,y,z);; double err = 1;; // do a Gaussian smearing around the points in all coordinates; x += gRandom->Gaus(0,err);; y += gRandom->Gaus(0,err);; z += gRandom->Gaus(0,err);; gr->SetPoint(N,x,y,z);; //dt->SetPointError(N,0,0,err);; }; // fit the graph now; ; ROOT::Fit::Fitter fitter;; ; ; // make the functor objet; SumDistance2 sdist(gr);; ROOT::Math:",MatchSource.WIKI,doc/master/line3Dfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/line3Dfit_8C.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:4932,Availability,error,errors,4932,"nt NFitPoints() const { return fNEffPoints; }; 119 ; 120 /// i-th likelihood contribution and its gradient; 121 virtual double DataElement(const double * x, unsigned int i, double * g, double * h = nullptr, bool fullHessian = false) const {; 122 if (i==0) this->UpdateNCalls();; 123 return FitUtil::Evaluate<T>::EvalPdf(BaseFCN::ModelFunction(), BaseFCN::Data(), x, i, g, h, BaseFCN::IsAGradFCN(), fullHessian);; 124 }; 125 ; 126 // need to be virtual to be instantiated; 127 virtual void Gradient(const double *x, double *g) const {; 128 // evaluate the chi2 gradient; 129 FitUtil::Evaluate<typename BaseFCN::T>::EvalLogLGradient(BaseFCN::ModelFunction(), BaseFCN::Data(), x, g,; 130 fNEffPoints, fExecutionPolicy);; 131 }; 132 ; 133 /// get type of fit method function; 134 virtual typename BaseObjFunction::Type_t Type() const { return BaseObjFunction::kLogLikelihood; }; 135 ; 136 ; 137 // Use sum of the weight squared in evaluating the likelihood; 138 // (this is needed for calculating the errors); 139 void UseSumOfWeightSquare(bool on = true) {; 140 if (fWeight == 0) return; // do nothing if it was not weighted; 141 if (on) fWeight = 2;; 142 else fWeight = 1;; 143 }; 144 ; 145 ; 146 ; 147protected:; 148 ; 149 ; 150private:; 151 ; 152 /**; 153 Evaluation of the function (required by interface); 154 */; 155 virtual double DoEval (const double * x) const {; 156 this->UpdateNCalls();; 157 return FitUtil::Evaluate<T>::EvalLogL(BaseFCN::ModelFunction(), BaseFCN::Data(), x, fWeight, fIsExtended, fNEffPoints, fExecutionPolicy);; 158 }; 159 ; 160 // for derivatives; 161 virtual double DoDerivative(const double * x, unsigned int icoord ) const {; 162 Gradient(x, &fGrad[0]);; 163 return fGrad[icoord];; 164 }; 165 ; 166 ; 167 //data member; 168 bool fIsExtended; ///< flag for indicating if likelihood is extended; 169 int fWeight; ///< flag to indicate if needs to evaluate using weight or weight squared (default weight = 0); 170 ; 171 ; 172 mutable unsigned int fNEffPoints; ///< number ",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:5231,Integrability,interface,interface,5231,"aluate<T>::EvalPdf(BaseFCN::ModelFunction(), BaseFCN::Data(), x, i, g, h, BaseFCN::IsAGradFCN(), fullHessian);; 124 }; 125 ; 126 // need to be virtual to be instantiated; 127 virtual void Gradient(const double *x, double *g) const {; 128 // evaluate the chi2 gradient; 129 FitUtil::Evaluate<typename BaseFCN::T>::EvalLogLGradient(BaseFCN::ModelFunction(), BaseFCN::Data(), x, g,; 130 fNEffPoints, fExecutionPolicy);; 131 }; 132 ; 133 /// get type of fit method function; 134 virtual typename BaseObjFunction::Type_t Type() const { return BaseObjFunction::kLogLikelihood; }; 135 ; 136 ; 137 // Use sum of the weight squared in evaluating the likelihood; 138 // (this is needed for calculating the errors); 139 void UseSumOfWeightSquare(bool on = true) {; 140 if (fWeight == 0) return; // do nothing if it was not weighted; 141 if (on) fWeight = 2;; 142 else fWeight = 1;; 143 }; 144 ; 145 ; 146 ; 147protected:; 148 ; 149 ; 150private:; 151 ; 152 /**; 153 Evaluation of the function (required by interface); 154 */; 155 virtual double DoEval (const double * x) const {; 156 this->UpdateNCalls();; 157 return FitUtil::Evaluate<T>::EvalLogL(BaseFCN::ModelFunction(), BaseFCN::Data(), x, fWeight, fIsExtended, fNEffPoints, fExecutionPolicy);; 158 }; 159 ; 160 // for derivatives; 161 virtual double DoDerivative(const double * x, unsigned int icoord ) const {; 162 Gradient(x, &fGrad[0]);; 163 return fGrad[icoord];; 164 }; 165 ; 166 ; 167 //data member; 168 bool fIsExtended; ///< flag for indicating if likelihood is extended; 169 int fWeight; ///< flag to indicate if needs to evaluate using weight or weight squared (default weight = 0); 170 ; 171 ; 172 mutable unsigned int fNEffPoints; ///< number of effective points used in the fit; 173 ; 174 mutable std::vector<double> fGrad; ///< for derivatives; 175 ; 176 ::ROOT::EExecutionPolicy fExecutionPolicy; ///< Execution policy; 177};; 178 // define useful typedef's; 179 // using LogLikelihoodFunction_v = LogLikelihoodFCN<ROOT::Math::IMultiGenFunc",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:12066,Integrability,interface,interface,12066,"ned int NFitPoints() constDefinition LogLikelihoodFCN.h:118; ROOT::Fit::LogLikelihoodFCN::fGradstd::vector< double > fGradfor derivativesDefinition LogLikelihoodFCN.h:174; ROOT::Fit::LogLikelihoodFCN::Type_tBaseObjFunction::Type_t Type_tDefinition LogLikelihoodFCN.h:51; ROOT::Fit::LogLikelihoodFCN::fIsExtendedbool fIsExtendedflag for indicating if likelihood is extendedDefinition LogLikelihoodFCN.h:168; ROOT::Fit::LogLikelihoodFCN::fExecutionPolicy::ROOT::EExecutionPolicy fExecutionPolicyExecution policy.Definition LogLikelihoodFCN.h:176; ROOT::Fit::LogLikelihoodFCN::fNEffPointsunsigned int fNEffPointsnumber of effective points used in the fitDefinition LogLikelihoodFCN.h:172; ROOT::Fit::LogLikelihoodFCN::DataElementvirtual double DataElement(const double *x, unsigned int i, double *g, double *h=nullptr, bool fullHessian=false) consti-th likelihood contribution and its gradientDefinition LogLikelihoodFCN.h:121; ROOT::Fit::LogLikelihoodFCN::DoEvalvirtual double DoEval(const double *x) constEvaluation of the function (required by interface)Definition LogLikelihoodFCN.h:155; ROOT::Fit::UnBinDataClass describing the un-binned data sets (just x coordinates values) of any dimensions.Definition UnBinData.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::Type_tType_tenumeration specifying the possible fit method typesDefinition FitMethodFunction.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::kLogLikelihood@ kLogLikelihoodDefinition FitMethodFunction.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::IsAGradFCNstatic bool IsAGradFCN()Static function to indicate if a function is supporting gradient.Definition FitMethodFunction.h:135; ROOT::Math::BasicFitMethodFunction< DerivFunType >::BaseFunctionFunctionType::BaseFunc BaseFunctionDefinition FitMethodFunction.h:43; ROOT::Math::BasicFitMethodFunction< DerivFunType >::UpdateNCallsvirtual void UpdateNCalls() constupdate number of callsDefinition FitMethodFunction.h:124; ROOT::Math::IParametricFunctionMu",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:13048,Integrability,interface,interface,13048,"ired by interface)Definition LogLikelihoodFCN.h:155; ROOT::Fit::UnBinDataClass describing the un-binned data sets (just x coordinates values) of any dimensions.Definition UnBinData.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::Type_tType_tenumeration specifying the possible fit method typesDefinition FitMethodFunction.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::kLogLikelihood@ kLogLikelihoodDefinition FitMethodFunction.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::IsAGradFCNstatic bool IsAGradFCN()Static function to indicate if a function is supporting gradient.Definition FitMethodFunction.h:135; ROOT::Math::BasicFitMethodFunction< DerivFunType >::BaseFunctionFunctionType::BaseFunc BaseFunctionDefinition FitMethodFunction.h:43; ROOT::Math::BasicFitMethodFunction< DerivFunType >::UpdateNCallsvirtual void UpdateNCalls() constupdate number of callsDefinition FitMethodFunction.h:124; ROOT::Math::IParametricFunctionMultiDimTemplIParamFunction interface (abstract class) describing multi-dimensional parametric functions It is a d...Definition IParamFunction.h:108; double; xDouble_t x[n]Definition legend1.C:17; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; ROOT::Fit::LogLikelihoodFunctionLogLikelihoodFCN< ROOT::Math::IMultiGenFunction, ROOT::Math::IParamMultiFunction > LogLikelihoodFunctionDefinition LogLikelihoodFCN.h:180; ROOT::Fit::LogLikelihoodGradFunctionLogLikelihoodFCN< ROOT::Math::IMultiGradFunction, ROOT::Math::IParamMultiFunction > LogLikelihoodGradFunctionDefinition LogLikelihoodFCN.h:181; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4; ROOT::EExecutionPolicyEExecutionPolicyDefinition EExecutionPolicy.hxx:5; ROOT::EExecutionPolicy::kSequential@ kSequential; ROOT::Fit::FitUtil::Evaluate::Ev",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:1967,Modifiability,extend,extended,1967,"de ""Fit/UnBinData.h""; 20#include ""Math/IParamFunction.h""; 21 ; 22#include <memory>; 23#include <vector>; 24 ; 25namespace ROOT {; 26 ; 27 namespace Fit {; 28 ; 29 ; 30//___________________________________________________________________________________; 31/**; 32 LogLikelihoodFCN class; 33 for likelihood fits; 34 ; 35 it is template to distinguish gradient and non-gradient case; 36 ; 37 @ingroup FitMethodFunc; 38*/; 39template<class DerivFunType,class ModelFunType = ROOT::Math::IParamMultiFunction>; 40class LogLikelihoodFCN : public BasicFCN<DerivFunType,ModelFunType,UnBinData> {; 41 ; 42public:; 43 ; 44 typedef typename ModelFunType::BackendType T;; 45 typedef BasicFCN<DerivFunType,ModelFunType,UnBinData> BaseFCN;; 46 ; 47 typedef ::ROOT::Math::BasicFitMethodFunction<DerivFunType> BaseObjFunction;; 48 typedef typename BaseObjFunction::BaseFunction BaseFunction;; 49 ; 50 typedef ::ROOT::Math::IParamMultiFunctionTempl<T> IModelFunction;; 51 typedef typename BaseObjFunction::Type_t Type_t;; 52 ; 53 ; 54 /**; 55 Constructor from unbin data set and model function (pdf); 56 */; 57 LogLikelihoodFCN (const std::shared_ptr<UnBinData> & data, const std::shared_ptr<IModelFunction> & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 58 BaseFCN( data, func),; 59 fIsExtended(extended),; 60 fWeight(weight),; 61 fNEffPoints(0),; 62 fGrad ( std::vector<double> ( func->NPar() ) ),; 63 fExecutionPolicy(executionPolicy); 64 {}; 65 ; 66 /**; 67 Constructor from unbin data set and model function (pdf) for object managed by users; 68 */; 69 LogLikelihoodFCN (const UnBinData & data, const IModelFunction & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 70 BaseFCN(std::make_shared<UnBinData>(data), std::shared_ptr<IModelFunction>(dynamic_cast<IModelFunction*>(func.Clone() ) ) ),; 71 fIsExtended(extended),; 72 fWeight(",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:2118,Modifiability,extend,extended,2118,"de ""Fit/UnBinData.h""; 20#include ""Math/IParamFunction.h""; 21 ; 22#include <memory>; 23#include <vector>; 24 ; 25namespace ROOT {; 26 ; 27 namespace Fit {; 28 ; 29 ; 30//___________________________________________________________________________________; 31/**; 32 LogLikelihoodFCN class; 33 for likelihood fits; 34 ; 35 it is template to distinguish gradient and non-gradient case; 36 ; 37 @ingroup FitMethodFunc; 38*/; 39template<class DerivFunType,class ModelFunType = ROOT::Math::IParamMultiFunction>; 40class LogLikelihoodFCN : public BasicFCN<DerivFunType,ModelFunType,UnBinData> {; 41 ; 42public:; 43 ; 44 typedef typename ModelFunType::BackendType T;; 45 typedef BasicFCN<DerivFunType,ModelFunType,UnBinData> BaseFCN;; 46 ; 47 typedef ::ROOT::Math::BasicFitMethodFunction<DerivFunType> BaseObjFunction;; 48 typedef typename BaseObjFunction::BaseFunction BaseFunction;; 49 ; 50 typedef ::ROOT::Math::IParamMultiFunctionTempl<T> IModelFunction;; 51 typedef typename BaseObjFunction::Type_t Type_t;; 52 ; 53 ; 54 /**; 55 Constructor from unbin data set and model function (pdf); 56 */; 57 LogLikelihoodFCN (const std::shared_ptr<UnBinData> & data, const std::shared_ptr<IModelFunction> & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 58 BaseFCN( data, func),; 59 fIsExtended(extended),; 60 fWeight(weight),; 61 fNEffPoints(0),; 62 fGrad ( std::vector<double> ( func->NPar() ) ),; 63 fExecutionPolicy(executionPolicy); 64 {}; 65 ; 66 /**; 67 Constructor from unbin data set and model function (pdf) for object managed by users; 68 */; 69 LogLikelihoodFCN (const UnBinData & data, const IModelFunction & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 70 BaseFCN(std::make_shared<UnBinData>(data), std::shared_ptr<IModelFunction>(dynamic_cast<IModelFunction*>(func.Clone() ) ) ),; 71 fIsExtended(extended),; 72 fWeight(",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:2472,Modifiability,extend,extended,2472,"de ""Fit/UnBinData.h""; 20#include ""Math/IParamFunction.h""; 21 ; 22#include <memory>; 23#include <vector>; 24 ; 25namespace ROOT {; 26 ; 27 namespace Fit {; 28 ; 29 ; 30//___________________________________________________________________________________; 31/**; 32 LogLikelihoodFCN class; 33 for likelihood fits; 34 ; 35 it is template to distinguish gradient and non-gradient case; 36 ; 37 @ingroup FitMethodFunc; 38*/; 39template<class DerivFunType,class ModelFunType = ROOT::Math::IParamMultiFunction>; 40class LogLikelihoodFCN : public BasicFCN<DerivFunType,ModelFunType,UnBinData> {; 41 ; 42public:; 43 ; 44 typedef typename ModelFunType::BackendType T;; 45 typedef BasicFCN<DerivFunType,ModelFunType,UnBinData> BaseFCN;; 46 ; 47 typedef ::ROOT::Math::BasicFitMethodFunction<DerivFunType> BaseObjFunction;; 48 typedef typename BaseObjFunction::BaseFunction BaseFunction;; 49 ; 50 typedef ::ROOT::Math::IParamMultiFunctionTempl<T> IModelFunction;; 51 typedef typename BaseObjFunction::Type_t Type_t;; 52 ; 53 ; 54 /**; 55 Constructor from unbin data set and model function (pdf); 56 */; 57 LogLikelihoodFCN (const std::shared_ptr<UnBinData> & data, const std::shared_ptr<IModelFunction> & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 58 BaseFCN( data, func),; 59 fIsExtended(extended),; 60 fWeight(weight),; 61 fNEffPoints(0),; 62 fGrad ( std::vector<double> ( func->NPar() ) ),; 63 fExecutionPolicy(executionPolicy); 64 {}; 65 ; 66 /**; 67 Constructor from unbin data set and model function (pdf) for object managed by users; 68 */; 69 LogLikelihoodFCN (const UnBinData & data, const IModelFunction & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 70 BaseFCN(std::make_shared<UnBinData>(data), std::shared_ptr<IModelFunction>(dynamic_cast<IModelFunction*>(func.Clone() ) ) ),; 71 fIsExtended(extended),; 72 fWeight(",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:2726,Modifiability,extend,extended,2726," 53 ; 54 /**; 55 Constructor from unbin data set and model function (pdf); 56 */; 57 LogLikelihoodFCN (const std::shared_ptr<UnBinData> & data, const std::shared_ptr<IModelFunction> & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 58 BaseFCN( data, func),; 59 fIsExtended(extended),; 60 fWeight(weight),; 61 fNEffPoints(0),; 62 fGrad ( std::vector<double> ( func->NPar() ) ),; 63 fExecutionPolicy(executionPolicy); 64 {}; 65 ; 66 /**; 67 Constructor from unbin data set and model function (pdf) for object managed by users; 68 */; 69 LogLikelihoodFCN (const UnBinData & data, const IModelFunction & func, int weight = 0, bool extended = false, const ::ROOT::EExecutionPolicy &executionPolicy = ::ROOT::EExecutionPolicy::kSequential) :; 70 BaseFCN(std::make_shared<UnBinData>(data), std::shared_ptr<IModelFunction>(dynamic_cast<IModelFunction*>(func.Clone() ) ) ),; 71 fIsExtended(extended),; 72 fWeight(weight),; 73 fNEffPoints(0),; 74 fGrad ( std::vector<double> ( func.NPar() ) ),; 75 fExecutionPolicy(executionPolicy); 76 {}; 77 ; 78 /**; 79 Destructor (no operations); 80 */; 81 virtual ~LogLikelihoodFCN () {}; 82 ; 83 /**; 84 Copy constructor; 85 */; 86 LogLikelihoodFCN(const LogLikelihoodFCN & f) :; 87 BaseFCN(f.DataPtr(), f.ModelFunctionPtr() ),; 88 fIsExtended(f.fIsExtended ),; 89 fWeight( f.fWeight ),; 90 fNEffPoints( f.fNEffPoints ),; 91 fGrad( f.fGrad),; 92 fExecutionPolicy(f.fExecutionPolicy); 93 { }; 94 ; 95 ; 96 /**; 97 Assignment operator; 98 */; 99 LogLikelihoodFCN & operator = (const LogLikelihoodFCN & rhs) {; 100 SetData(rhs.DataPtr() );; 101 SetModelFunction(rhs.ModelFunctionPtr() );; 102 fNEffPoints = rhs.fNEffPoints;; 103 fGrad = rhs.fGrad;; 104 fIsExtended = rhs.fIsExtended;; 105 fWeight = rhs.fWeight;; 106 fExecutionPolicy = rhs.fExecutionPolicy;; 107 return *this;; 108 }; 109 ; 110 ; 111 /// clone the function (need to return Base for Windows); 112 virtual BaseFunction *",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:5751,Modifiability,extend,extended,5751," ; 133 /// get type of fit method function; 134 virtual typename BaseObjFunction::Type_t Type() const { return BaseObjFunction::kLogLikelihood; }; 135 ; 136 ; 137 // Use sum of the weight squared in evaluating the likelihood; 138 // (this is needed for calculating the errors); 139 void UseSumOfWeightSquare(bool on = true) {; 140 if (fWeight == 0) return; // do nothing if it was not weighted; 141 if (on) fWeight = 2;; 142 else fWeight = 1;; 143 }; 144 ; 145 ; 146 ; 147protected:; 148 ; 149 ; 150private:; 151 ; 152 /**; 153 Evaluation of the function (required by interface); 154 */; 155 virtual double DoEval (const double * x) const {; 156 this->UpdateNCalls();; 157 return FitUtil::Evaluate<T>::EvalLogL(BaseFCN::ModelFunction(), BaseFCN::Data(), x, fWeight, fIsExtended, fNEffPoints, fExecutionPolicy);; 158 }; 159 ; 160 // for derivatives; 161 virtual double DoDerivative(const double * x, unsigned int icoord ) const {; 162 Gradient(x, &fGrad[0]);; 163 return fGrad[icoord];; 164 }; 165 ; 166 ; 167 //data member; 168 bool fIsExtended; ///< flag for indicating if likelihood is extended; 169 int fWeight; ///< flag to indicate if needs to evaluate using weight or weight squared (default weight = 0); 170 ; 171 ; 172 mutable unsigned int fNEffPoints; ///< number of effective points used in the fit; 173 ; 174 mutable std::vector<double> fGrad; ///< for derivatives; 175 ; 176 ::ROOT::EExecutionPolicy fExecutionPolicy; ///< Execution policy; 177};; 178 // define useful typedef's; 179 // using LogLikelihoodFunction_v = LogLikelihoodFCN<ROOT::Math::IMultiGenFunction, ROOT::Math::IParametricFunctionMultiDimTempl<T>>;; 180 typedef LogLikelihoodFCN<ROOT::Math::IMultiGenFunction, ROOT::Math::IParamMultiFunction> LogLikelihoodFunction;; 181 typedef LogLikelihoodFCN<ROOT::Math::IMultiGradFunction, ROOT::Math::IParamMultiFunction> LogLikelihoodGradFunction;; 182 ; 183 } // end namespace Fit; 184 ; 185} // end namespace ROOT; 186 ; 187 ; 188#endif /* ROOT_Fit_LogLikelihoodFCN */; BasicFCN.",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:8599,Modifiability,extend,extended,8599,"Ptr() constaccess to function pointerDefinition BasicFCN.h:81; ROOT::Fit::BasicFCN::SetModelFunctionvoid SetModelFunction(const std::shared_ptr< IModelFunction > &func)Set the function pointer.Definition BasicFCN.h:101; ROOT::Fit::BasicFCN::Datavirtual const DataType & Data() constaccess to const reference to the dataDefinition BasicFCN.h:72; ROOT::Fit::BasicFCN::DataPtrstd::shared_ptr< DataType > DataPtr() constaccess to data pointerDefinition BasicFCN.h:75; ROOT::Fit::BasicFCN::ModelFunctionvirtual const IModelFunction & ModelFunction() constaccess to const reference to the model functionDefinition BasicFCN.h:78; ROOT::Fit::LogLikelihoodFCNLogLikelihoodFCN class for likelihood fits.Definition LogLikelihoodFCN.h:40; ROOT::Fit::LogLikelihoodFCN::BaseFunctionBaseObjFunction::BaseFunction BaseFunctionDefinition LogLikelihoodFCN.h:48; ROOT::Fit::LogLikelihoodFCN::LogLikelihoodFCNLogLikelihoodFCN(const UnBinData &data, const IModelFunction &func, int weight=0, bool extended=false, const ::ROOT::EExecutionPolicy &executionPolicy=::ROOT::EExecutionPolicy::kSequential)Constructor from unbin data set and model function (pdf) for object managed by users.Definition LogLikelihoodFCN.h:69; ROOT::Fit::LogLikelihoodFCN::Typevirtual BaseObjFunction::Type_t Type() constget type of fit method functionDefinition LogLikelihoodFCN.h:134; ROOT::Fit::LogLikelihoodFCN::operator=LogLikelihoodFCN & operator=(const LogLikelihoodFCN &rhs)Assignment operator.Definition LogLikelihoodFCN.h:99; ROOT::Fit::LogLikelihoodFCN::DoDerivativevirtual double DoDerivative(const double *x, unsigned int icoord) constDefinition LogLikelihoodFCN.h:161; ROOT::Fit::LogLikelihoodFCN::IModelFunction::ROOT::Math::IParamMultiFunctionTempl< T > IModelFunctionDefinition LogLikelihoodFCN.h:50; ROOT::Fit::LogLikelihoodFCN::fWeightint fWeightflag to indicate if needs to evaluate using weight or weight squared (default weight = 0)Definition LogLikelihoodFCN.h:169; ROOT::Fit::LogLikelihoodFCN::LogLikelihoodFCNLogLikelihoodF",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:10092,Modifiability,extend,extended,10092,"ihoodFCN.h:99; ROOT::Fit::LogLikelihoodFCN::DoDerivativevirtual double DoDerivative(const double *x, unsigned int icoord) constDefinition LogLikelihoodFCN.h:161; ROOT::Fit::LogLikelihoodFCN::IModelFunction::ROOT::Math::IParamMultiFunctionTempl< T > IModelFunctionDefinition LogLikelihoodFCN.h:50; ROOT::Fit::LogLikelihoodFCN::fWeightint fWeightflag to indicate if needs to evaluate using weight or weight squared (default weight = 0)Definition LogLikelihoodFCN.h:169; ROOT::Fit::LogLikelihoodFCN::LogLikelihoodFCNLogLikelihoodFCN(const LogLikelihoodFCN &f)Copy constructor.Definition LogLikelihoodFCN.h:86; ROOT::Fit::LogLikelihoodFCN::BaseFCNBasicFCN< DerivFunType, ModelFunType, UnBinData > BaseFCNDefinition LogLikelihoodFCN.h:45; ROOT::Fit::LogLikelihoodFCN::TModelFunType::BackendType TDefinition LogLikelihoodFCN.h:44; ROOT::Fit::LogLikelihoodFCN::LogLikelihoodFCNLogLikelihoodFCN(const std::shared_ptr< UnBinData > &data, const std::shared_ptr< IModelFunction > &func, int weight=0, bool extended=false, const ::ROOT::EExecutionPolicy &executionPolicy=::ROOT::EExecutionPolicy::kSequential)Constructor from unbin data set and model function (pdf)Definition LogLikelihoodFCN.h:57; ROOT::Fit::LogLikelihoodFCN::UseSumOfWeightSquarevoid UseSumOfWeightSquare(bool on=true)Definition LogLikelihoodFCN.h:139; ROOT::Fit::LogLikelihoodFCN::Gradientvirtual void Gradient(const double *x, double *g) constDefinition LogLikelihoodFCN.h:127; ROOT::Fit::LogLikelihoodFCN::BaseObjFunction::ROOT::Math::BasicFitMethodFunction< DerivFunType > BaseObjFunctionDefinition LogLikelihoodFCN.h:47; ROOT::Fit::LogLikelihoodFCN::Clonevirtual BaseFunction * Clone() constclone the function (need to return Base for Windows)Definition LogLikelihoodFCN.h:112; ROOT::Fit::LogLikelihoodFCN::~LogLikelihoodFCNvirtual ~LogLikelihoodFCN()Destructor (no operations)Definition LogLikelihoodFCN.h:81; ROOT::Fit::LogLikelihoodFCN::NFitPointsvirtual unsigned int NFitPoints() constDefinition LogLikelihoodFCN.h:118; ROOT::Fit::Log",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:11386,Modifiability,extend,extendedDefinition,11386,"uare(bool on=true)Definition LogLikelihoodFCN.h:139; ROOT::Fit::LogLikelihoodFCN::Gradientvirtual void Gradient(const double *x, double *g) constDefinition LogLikelihoodFCN.h:127; ROOT::Fit::LogLikelihoodFCN::BaseObjFunction::ROOT::Math::BasicFitMethodFunction< DerivFunType > BaseObjFunctionDefinition LogLikelihoodFCN.h:47; ROOT::Fit::LogLikelihoodFCN::Clonevirtual BaseFunction * Clone() constclone the function (need to return Base for Windows)Definition LogLikelihoodFCN.h:112; ROOT::Fit::LogLikelihoodFCN::~LogLikelihoodFCNvirtual ~LogLikelihoodFCN()Destructor (no operations)Definition LogLikelihoodFCN.h:81; ROOT::Fit::LogLikelihoodFCN::NFitPointsvirtual unsigned int NFitPoints() constDefinition LogLikelihoodFCN.h:118; ROOT::Fit::LogLikelihoodFCN::fGradstd::vector< double > fGradfor derivativesDefinition LogLikelihoodFCN.h:174; ROOT::Fit::LogLikelihoodFCN::Type_tBaseObjFunction::Type_t Type_tDefinition LogLikelihoodFCN.h:51; ROOT::Fit::LogLikelihoodFCN::fIsExtendedbool fIsExtendedflag for indicating if likelihood is extendedDefinition LogLikelihoodFCN.h:168; ROOT::Fit::LogLikelihoodFCN::fExecutionPolicy::ROOT::EExecutionPolicy fExecutionPolicyExecution policy.Definition LogLikelihoodFCN.h:176; ROOT::Fit::LogLikelihoodFCN::fNEffPointsunsigned int fNEffPointsnumber of effective points used in the fitDefinition LogLikelihoodFCN.h:172; ROOT::Fit::LogLikelihoodFCN::DataElementvirtual double DataElement(const double *x, unsigned int i, double *g, double *h=nullptr, bool fullHessian=false) consti-th likelihood contribution and its gradientDefinition LogLikelihoodFCN.h:121; ROOT::Fit::LogLikelihoodFCN::DoEvalvirtual double DoEval(const double *x) constEvaluation of the function (required by interface)Definition LogLikelihoodFCN.h:155; ROOT::Fit::UnBinDataClass describing the un-binned data sets (just x coordinates values) of any dimensions.Definition UnBinData.h:46; ROOT::Math::BasicFitMethodFunction< DerivFunType >::Type_tType_tenumeration specifying the possible fit method",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html:14189,Modifiability,extend,extended,14189,"tricFunctionMultiDimTemplIParamFunction interface (abstract class) describing multi-dimensional parametric functions It is a d...Definition IParamFunction.h:108; double; xDouble_t x[n]Definition legend1.C:17; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; ROOT::Fit::LogLikelihoodFunctionLogLikelihoodFCN< ROOT::Math::IMultiGenFunction, ROOT::Math::IParamMultiFunction > LogLikelihoodFunctionDefinition LogLikelihoodFCN.h:180; ROOT::Fit::LogLikelihoodGradFunctionLogLikelihoodFCN< ROOT::Math::IMultiGradFunction, ROOT::Math::IParamMultiFunction > LogLikelihoodGradFunctionDefinition LogLikelihoodFCN.h:181; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4; ROOT::EExecutionPolicyEExecutionPolicyDefinition EExecutionPolicy.hxx:5; ROOT::EExecutionPolicy::kSequential@ kSequential; ROOT::Fit::FitUtil::Evaluate::EvalLogLstatic double EvalLogL(const IModelFunctionTempl< double > &func, const UnBinData &data, const double *p, int iWeight, bool extended, unsigned int &nPoints, ::ROOT::EExecutionPolicy executionPolicy, unsigned nChunks=0)Definition FitUtil.h:1413; ROOT::Fit::FitUtil::Evaluate::EvalLogLGradientstatic void EvalLogLGradient(const IModelFunctionTempl< double > &func, const UnBinData &data, const double *p, double *g, unsigned int &nPoints, ::ROOT::EExecutionPolicy executionPolicy=::ROOT::EExecutionPolicy::kSequential, unsigned nChunks=0)Definition FitUtil.h:1464; ROOT::Fit::FitUtil::Evaluate::EvalPdfstatic double EvalPdf(const IModelFunctionTempl< double > &func, const UnBinData &data, const double *p, unsigned int i, double *g, double *h, bool hasGrad, bool fullHessian)Definition FitUtil.h:1451. mathmathcoreincFitLogLikelihoodFCN.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:39 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/LogLikelihoodFCN_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LogLikelihoodFCN_8h_source.html
https://root.cern/doc/master/logscales_8C.html:23,Testability,log,logscales,23,". ROOT: tutorials/hist/logscales.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. logscales.C File ReferenceTutorials » Histograms tutorials. Detailed Description; Draw parametric functions with log scales. . ; void logscales() {; TCanvas *c1 = new TCanvas(""c1"", ""Various options on LOG scales plots"",0,0,700,900);; c1->SetFillColor(30);; ; TPad *pad1 = new TPad(""pad1"",""pad1"",0.03,0.62,0.50,0.92,32);; TPad *pad2 = new TPad(""pad2"",""pad2"",0.51,0.62,0.98,0.92,33);; TPad *pad3 = new TPad(""pad3"",""pad3"",0.03,0.02,0.97,0.535,38);; pad1->Draw(); pad2->Draw(); pad3->Draw();; ; TPaveLabel *title = new TPaveLabel(0.1,0.94,0.9,0.98, ""Various options on LOG scales plots"");; title->SetFillColor(16);; title->SetTextFont(42);; title->Draw();; ; TPaveText *pave = new TPaveText(0.1,0.55,0.9,0.61);; pave->SetFillColor(42);; pave->SetTextAlign(12);; pave->SetTextFont(42);; pave->AddText(""When more Log labels are requested, the overlapping labels are removed"");; pave->Draw();; ; pad1->cd();; pad1->SetLogy();; pad1->SetGridy();; TF1 *f1 = new TF1(""f1"",""x*sin(x)*exp(-0.1*x)+15"",-10.,10.);; TF1 *f2 = new TF1(""f2"",""(sin(x)+cos(x))**5+15"",-10.,10.);; TF1 *f3 = new TF1(""f3"",""(sin(x)/(x)-x*cos(x))+15"",-10.,10.);; f1->SetLineWidth(1); f1->SetLineColor(2);; f2->SetLineWidth(1); f2->SetLineColor(3);; f3->SetLineWidth(1); f3->SetLineColor(4);; f1->Draw();; f2->Draw(""same"");; f3->Draw(""same"");; f1->GetYaxis()->SetMoreLogLabels();; TPaveText *pave1 = new TPaveText(-6,2,6,6);; pave1->SetFillColor(42);; pave1->SetTextAlign(12);; pave1->SetTextFont(42);; pave1->AddText(""Log scale along Y axis."");; pave1->AddText(""More Log labels requested."");; pave1->Draw();; ; pad2->cd();; double x[10] = { 200, 300, 400, 500, 600, 650, 700, 710, 900,1000 };; double y[10] = { 200, 1000, 900, 400, 500, 250, 800, 150, 201, 220 };; TGraph *g_2 = new TGraph(10,x,y);; g_2->Draw(""AL*"");; g_2->SetMarkerColor(2);; g_2->GetYaxis()->SetMoreLogLabels();; g_2->GetYaxis()->SetNoExponen",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/logscales_8C.html:132,Testability,log,logscales,132,". ROOT: tutorials/hist/logscales.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. logscales.C File ReferenceTutorials » Histograms tutorials. Detailed Description; Draw parametric functions with log scales. . ; void logscales() {; TCanvas *c1 = new TCanvas(""c1"", ""Various options on LOG scales plots"",0,0,700,900);; c1->SetFillColor(30);; ; TPad *pad1 = new TPad(""pad1"",""pad1"",0.03,0.62,0.50,0.92,32);; TPad *pad2 = new TPad(""pad2"",""pad2"",0.51,0.62,0.98,0.92,33);; TPad *pad3 = new TPad(""pad3"",""pad3"",0.03,0.02,0.97,0.535,38);; pad1->Draw(); pad2->Draw(); pad3->Draw();; ; TPaveLabel *title = new TPaveLabel(0.1,0.94,0.9,0.98, ""Various options on LOG scales plots"");; title->SetFillColor(16);; title->SetTextFont(42);; title->Draw();; ; TPaveText *pave = new TPaveText(0.1,0.55,0.9,0.61);; pave->SetFillColor(42);; pave->SetTextAlign(12);; pave->SetTextFont(42);; pave->AddText(""When more Log labels are requested, the overlapping labels are removed"");; pave->Draw();; ; pad1->cd();; pad1->SetLogy();; pad1->SetGridy();; TF1 *f1 = new TF1(""f1"",""x*sin(x)*exp(-0.1*x)+15"",-10.,10.);; TF1 *f2 = new TF1(""f2"",""(sin(x)+cos(x))**5+15"",-10.,10.);; TF1 *f3 = new TF1(""f3"",""(sin(x)/(x)-x*cos(x))+15"",-10.,10.);; f1->SetLineWidth(1); f1->SetLineColor(2);; f2->SetLineWidth(1); f2->SetLineColor(3);; f3->SetLineWidth(1); f3->SetLineColor(4);; f1->Draw();; f2->Draw(""same"");; f3->Draw(""same"");; f1->GetYaxis()->SetMoreLogLabels();; TPaveText *pave1 = new TPaveText(-6,2,6,6);; pave1->SetFillColor(42);; pave1->SetTextAlign(12);; pave1->SetTextFont(42);; pave1->AddText(""Log scale along Y axis."");; pave1->AddText(""More Log labels requested."");; pave1->Draw();; ; pad2->cd();; double x[10] = { 200, 300, 400, 500, 600, 650, 700, 710, 900,1000 };; double y[10] = { 200, 1000, 900, 400, 500, 250, 800, 150, 201, 220 };; TGraph *g_2 = new TGraph(10,x,y);; g_2->Draw(""AL*"");; g_2->SetMarkerColor(2);; g_2->GetYaxis()->SetMoreLogLabels();; g_2->GetYaxis()->SetNoExponen",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/logscales_8C.html:245,Testability,log,log,245,". ROOT: tutorials/hist/logscales.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. logscales.C File ReferenceTutorials » Histograms tutorials. Detailed Description; Draw parametric functions with log scales. . ; void logscales() {; TCanvas *c1 = new TCanvas(""c1"", ""Various options on LOG scales plots"",0,0,700,900);; c1->SetFillColor(30);; ; TPad *pad1 = new TPad(""pad1"",""pad1"",0.03,0.62,0.50,0.92,32);; TPad *pad2 = new TPad(""pad2"",""pad2"",0.51,0.62,0.98,0.92,33);; TPad *pad3 = new TPad(""pad3"",""pad3"",0.03,0.02,0.97,0.535,38);; pad1->Draw(); pad2->Draw(); pad3->Draw();; ; TPaveLabel *title = new TPaveLabel(0.1,0.94,0.9,0.98, ""Various options on LOG scales plots"");; title->SetFillColor(16);; title->SetTextFont(42);; title->Draw();; ; TPaveText *pave = new TPaveText(0.1,0.55,0.9,0.61);; pave->SetFillColor(42);; pave->SetTextAlign(12);; pave->SetTextFont(42);; pave->AddText(""When more Log labels are requested, the overlapping labels are removed"");; pave->Draw();; ; pad1->cd();; pad1->SetLogy();; pad1->SetGridy();; TF1 *f1 = new TF1(""f1"",""x*sin(x)*exp(-0.1*x)+15"",-10.,10.);; TF1 *f2 = new TF1(""f2"",""(sin(x)+cos(x))**5+15"",-10.,10.);; TF1 *f3 = new TF1(""f3"",""(sin(x)/(x)-x*cos(x))+15"",-10.,10.);; f1->SetLineWidth(1); f1->SetLineColor(2);; f2->SetLineWidth(1); f2->SetLineColor(3);; f3->SetLineWidth(1); f3->SetLineColor(4);; f1->Draw();; f2->Draw(""same"");; f3->Draw(""same"");; f1->GetYaxis()->SetMoreLogLabels();; TPaveText *pave1 = new TPaveText(-6,2,6,6);; pave1->SetFillColor(42);; pave1->SetTextAlign(12);; pave1->SetTextFont(42);; pave1->AddText(""Log scale along Y axis."");; pave1->AddText(""More Log labels requested."");; pave1->Draw();; ; pad2->cd();; double x[10] = { 200, 300, 400, 500, 600, 650, 700, 710, 900,1000 };; double y[10] = { 200, 1000, 900, 400, 500, 250, 800, 150, 201, 220 };; TGraph *g_2 = new TGraph(10,x,y);; g_2->Draw(""AL*"");; g_2->SetMarkerColor(2);; g_2->GetYaxis()->SetMoreLogLabels();; g_2->GetYaxis()->SetNoExponen",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/logscales_8C.html:266,Testability,log,logscales,266,". ROOT: tutorials/hist/logscales.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. logscales.C File ReferenceTutorials » Histograms tutorials. Detailed Description; Draw parametric functions with log scales. . ; void logscales() {; TCanvas *c1 = new TCanvas(""c1"", ""Various options on LOG scales plots"",0,0,700,900);; c1->SetFillColor(30);; ; TPad *pad1 = new TPad(""pad1"",""pad1"",0.03,0.62,0.50,0.92,32);; TPad *pad2 = new TPad(""pad2"",""pad2"",0.51,0.62,0.98,0.92,33);; TPad *pad3 = new TPad(""pad3"",""pad3"",0.03,0.02,0.97,0.535,38);; pad1->Draw(); pad2->Draw(); pad3->Draw();; ; TPaveLabel *title = new TPaveLabel(0.1,0.94,0.9,0.98, ""Various options on LOG scales plots"");; title->SetFillColor(16);; title->SetTextFont(42);; title->Draw();; ; TPaveText *pave = new TPaveText(0.1,0.55,0.9,0.61);; pave->SetFillColor(42);; pave->SetTextAlign(12);; pave->SetTextFont(42);; pave->AddText(""When more Log labels are requested, the overlapping labels are removed"");; pave->Draw();; ; pad1->cd();; pad1->SetLogy();; pad1->SetGridy();; TF1 *f1 = new TF1(""f1"",""x*sin(x)*exp(-0.1*x)+15"",-10.,10.);; TF1 *f2 = new TF1(""f2"",""(sin(x)+cos(x))**5+15"",-10.,10.);; TF1 *f3 = new TF1(""f3"",""(sin(x)/(x)-x*cos(x))+15"",-10.,10.);; f1->SetLineWidth(1); f1->SetLineColor(2);; f2->SetLineWidth(1); f2->SetLineColor(3);; f3->SetLineWidth(1); f3->SetLineColor(4);; f1->Draw();; f2->Draw(""same"");; f3->Draw(""same"");; f1->GetYaxis()->SetMoreLogLabels();; TPaveText *pave1 = new TPaveText(-6,2,6,6);; pave1->SetFillColor(42);; pave1->SetTextAlign(12);; pave1->SetTextFont(42);; pave1->AddText(""Log scale along Y axis."");; pave1->AddText(""More Log labels requested."");; pave1->Draw();; ; pad2->cd();; double x[10] = { 200, 300, 400, 500, 600, 650, 700, 710, 900,1000 };; double y[10] = { 200, 1000, 900, 400, 500, 250, 800, 150, 201, 220 };; TGraph *g_2 = new TGraph(10,x,y);; g_2->Draw(""AL*"");; g_2->SetMarkerColor(2);; g_2->GetYaxis()->SetMoreLogLabels();; g_2->GetYaxis()->SetNoExponen",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/logscales_8C.html:4534,Testability,log,log,4534,"exponents (they would be 0 or 1)"");; pave3->Draw();; }; a#define a(i)Definition RSha256.hxx:99; Int_tint Int_tDefinition RtypesCore.h:45; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttLine::SetLineWidthvirtual void SetLineWidth(Width_t lwidth)Set the line width.Definition TAttLine.h:43; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TAttMarker::SetMarkerColorvirtual void SetMarkerColor(Color_t mcolor=1)Set the marker color.Definition TAttMarker.h:38; TAttText::SetTextAlignvirtual void SetTextAlign(Short_t align=11)Set the text alignment.Definition TAttText.h:42; TAttText::SetTextFontvirtual void SetTextFont(Font_t tfont=62)Set the text font.Definition TAttText.h:46; TAxis::SetMoreLogLabelsvoid SetMoreLogLabels(Bool_t more=kTRUE)Set the kMoreLogLabels bit flag When this option is selected more labels are drawn when in log scale ...Definition TAxis.h:223; TAxis::SetNoExponentvoid SetNoExponent(Bool_t noExponent=kTRUE)Set the NoExponent flag By default, an exponent of the form 10^N is used when the label value are eit...Definition TAxis.h:233; TCanvasThe Canvas class.Definition TCanvas.h:23; TF11-Dim function classDefinition TF1.h:233; TF1::GetYaxisTAxis * GetYaxis() constGet y axis of the function.Definition TF1.cxx:2411; TF1::SetNpxvirtual void SetNpx(Int_t npx=100)Set the number of points used to draw the function.Definition TF1.cxx:3433; TF1::Drawvoid Draw(Option_t *option="""") overrideDraw this function with its current attributes.Definition TF1.cxx:1333; TF1::SetParametervirtual void SetParameter(Int_t param, Double_t value)Definition TF1.h:667; TF1::GetXaxisTAxis * GetXaxis() constGet x axis of the function.Definition TF1.cxx:2400; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/logscales_8C.html:7229,Testability,log,logscales,7229,"phA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TGraph::Drawvoid Draw(Option_t *chopt="""") overrideDraw this graph with its current attributes.Definition TGraph.cxx:831; TGraph::GetXaxisTAxis * GetXaxis() constGet x axis of the graph.Definition TGraph.cxx:1566; TGraph::GetYaxisTAxis * GetYaxis() constGet y axis of the graph.Definition TGraph.cxx:1575; TPadThe most important graphics class in the ROOT system.Definition TPad.h:28; TPad::SetGridxvoid SetGridx(Int_t value=1) overrideDefinition TPad.h:336; TPad::SetLogyvoid SetLogy(Int_t value=1) overrideSet Lin/Log scale for Y.Definition TPad.cxx:6100; TPad::SetGridyvoid SetGridy(Int_t value=1) overrideDefinition TPad.h:337; TPad::cdTVirtualPad * cd(Int_t subpadnumber=0) overrideSet Current pad.Definition TPad.cxx:693; TPad::Drawvoid Draw(Option_t *option="""") overrideDraw Pad in Current pad (re-parent pad if necessary).Definition TPad.cxx:1364; TPad::SetLogxvoid SetLogx(Int_t value=1) overrideSet Lin/Log scale for X.Definition TPad.cxx:6086; TPaveLabelA Pave (see TPave) with a text centered in the Pave.Definition TPaveLabel.h:20; TPaveLabel::Drawvoid Draw(Option_t *option="""") overrideDraw this pavelabel with its current attributes.Definition TPaveLabel.cxx:88; TPaveTextA Pave (see TPave) with text, lines or/and boxes inside.Definition TPaveText.h:21; TPaveText::AddTextvirtual TText * AddText(Double_t x1, Double_t y1, const char *label)Add a new Text line to this pavetext at given coordinates.Definition TPaveText.cxx:191; TPaveText::Drawvoid Draw(Option_t *option="""") overrideDraw this pavetext with its current attributes.Definition TPaveText.cxx:242; yDouble_t y[n]Definition legend1.C:17; c1return c1Definition legend1.C:41; xDouble_t x[n]Definition legend1.C:17; f1TF1 * f1Definition legend1.C:11; AuthorOlivier Couet ; Definition in file logscales.C. tutorialshistlogscales.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:29 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/logscales_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/logscales_8C.html
https://root.cern/doc/master/LorentzRotation_8cxx_source.html:4397,Availability,error,error,4397," = 1.0;; 99}; 100 ; 101LorentzRotation::LorentzRotation(RotationY const & r) {; 102 // construct from RotationY; 103 Scalar s = r.SinAngle();; 104 Scalar c = r.CosAngle();; 105 fM[kXX] = c ; fM[kXY] = 0.0; fM[kXZ] = s ; fM[kXT] = 0.0;; 106 fM[kYX] = 0.0; fM[kYY] = 1.0; fM[kYZ] = 0.0; fM[kYT] = 0.0;; 107 fM[kZX] = -s ; fM[kZY] = 0.0; fM[kZZ] = c ; fM[kZT] = 0.0;; 108 fM[kTX] = 0.0; fM[kTY] = 0.0; fM[kTZ] = 0.0; fM[kTT] = 1.0;; 109}; 110 ; 111LorentzRotation::LorentzRotation(RotationZ const & r) {; 112 // construct from RotationX; 113 Scalar s = r.SinAngle();; 114 Scalar c = r.CosAngle();; 115 fM[kXX] = c ; fM[kXY] = -s ; fM[kXZ] = 0.0; fM[kXT] = 0.0;; 116 fM[kYX] = s ; fM[kYY] = c ; fM[kYZ] = 0.0; fM[kYT] = 0.0;; 117 fM[kZX] = 0.0; fM[kZY] = 0.0; fM[kZZ] = 1.0; fM[kZT] = 0.0;; 118 fM[kTX] = 0.0; fM[kTY] = 0.0; fM[kTZ] = 0.0; fM[kTT] = 1.0;; 119}; 120 ; 121void; 122LorentzRotation::Rectify() {; 123 // Assuming the representation of this is close to a true Lorentz Rotation,; 124 // but may have drifted due to round-off error from many operations,; 125 // this forms an ""exact"" orthosymplectic matrix for the Lorentz Rotation; 126 // again.; 127 ; 128 typedef LorentzVector< PxPyPzE4D<Scalar> > FourVector;; 129 if (fM[kTT] <= 0) {; 130 GenVector::Throw (; 131 ""LorentzRotation:Rectify(): Non-positive TT component - cannot rectify"");; 132 return;; 133 }; 134 FourVector t ( fM[kTX], fM[kTY], fM[kTZ], fM[kTT] );; 135 Scalar m2 = t.M2();; 136 if ( m2 <= 0 ) {; 137 GenVector::Throw (; 138 ""LorentzRotation:Rectify(): Non-timelike time row - cannot rectify"");; 139 return;; 140 }; 141 t /= std::sqrt(m2);; 142 FourVector z ( fM[kZX], fM[kZY], fM[kZZ], fM[kZT] );; 143 z = z - z.Dot(t)*t;; 144 m2 = z.M2();; 145 if ( m2 >= 0 ) {; 146 GenVector::Throw (; 147 ""LorentzRotation:Rectify(): Non-spacelike Z row projection - ""; 148 ""cannot rectify"");; 149 return;; 150 }; 151 z /= std::sqrt(-m2);; 152 FourVector y ( fM[kYX], fM[kYY], fM[kYZ], fM[kYT] );; 153 y = y - y.Dot(t)*t - y.Dot(z)*z;; 15",MatchSource.WIKI,doc/master/LorentzRotation_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/LorentzRotation_8cxx_source.html
https://root.cern/doc/master/mandelbrot_8C.html:4628,Availability,error,errors,4628,".h; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; TVirtualPad.h; gPad#define gPadDefinition TVirtualPad.h:308; TAxis::GetBinCentervirtual Double_t GetBinCenter(Int_t bin) constReturn center of bin.Definition TAxis.cxx:478; TAxis::GetXmaxDouble_t GetXmax() constDefinition TAxis.h:140; TAxis::GetXminDouble_t GetXmin() constDefinition TAxis.h:139; TCanvasThe Canvas class.Definition TCanvas.h:23; TComplexDefinition TComplex.h:29; TComplex::RhoDouble_t Rho() constDefinition TComplex.h:48; TH1::GetNbinsYvirtual Int_t GetNbinsY() constDefinition TH1.h:298; TH1::GetXaxisTAxis * GetXaxis()Definition TH1.h:324; TH1::GetNbinsXvirtual Int_t GetNbinsX() constDefinition TH1.h:297; TH1::GetYaxisTAxis * GetYaxis()Definition TH1.h:325; TH1::Drawvoid Draw(Option_t *option="""") overrideDraw this histogram with options.Definition TH1.cxx:3066; TH1::SetContourvirtual void SetContour(Int_t nlevels, const Double_t *levels=nullptr)Set the number and values of contour levels.Definition TH1.cxx:8483; TH1::SetBinsvirtual void SetBins(Int_t nx, Double_t xmin, Double_t xmax)Redefine x axis parameters.Definition TH1.cxx:8767; TH1::SetStatsvirtual void SetStats(Bool_t stats=kTRUE)Set statistics option on/off.Definition TH1.cxx:8990; TH2F2-D histogram with a float per channel (see TH1 documentation)Definition TH2.h:307; TH2F::Resetvoid Reset(Option_t *option="""") overrideReset this histogram: contents, errors, etc.Definition TH2.cxx:3972; TH2::FillInt_t Fill(Double_t) overrideInvalid Fill method.Definition TH2.cxx:393; TStyle::SetPadGridXvoid SetPadGridX(Bool_t gridx)Definition TStyle.h:362; TStyle::SetPadGridYvoid SetPadGridY(Bool_t gridy)Definition TStyle.h:363; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; AuthorLuigi Bardelli barde.nosp@m.lli@.nosp@m.fi.in.nosp@m.fn.i.nosp@m.t ; Definition in file mandelbrot.C. tutorialsgraphicsmandelbrot.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:29 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/mandelbrot_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mandelbrot_8C.html
https://root.cern/doc/master/mandelbrot_8C.html:1494,Energy Efficiency,allocate,allocate,1494,"rot set. ; Pressing the keys 'z' and 'u' will zoom and unzoom the picture near the mouse location, 'r' will reset to the default view.; Try it (in compiled mode!) with: root mandelbrot.C+. Details; when a mouse event occurs the myexec() function is called (by using AddExec). Depending on the pressed key, the mygenerate() function is called, with the proper arguments. Note the last_x and last_y variables that are used in myexec() to store the last pointer coordinates (px is not a pointer position in kKeyPress events).; ; #include <TStyle.h>; #include <TROOT.h>; #include <TH2.h>; #include <TComplex.h>; #include <TVirtualPad.h>; #include <TCanvas.h>; ; TH2F *last_histo = nullptr;; ; void mygenerate(double factor, double cen_x, double cen_y); {; printf(""Regenerating...\n"");; // resize histo:; if(factor>0); {; double dx=last_histo->GetXaxis()->GetXmax()-last_histo->GetXaxis()->GetXmin();; double dy=last_histo->GetYaxis()->GetXmax()-last_histo->GetYaxis()->GetXmin();; last_histo->SetBins(; last_histo->GetNbinsX(),; cen_x-factor*dx/2,; cen_x+factor*dx/2,; last_histo->GetNbinsY(),; cen_y-factor*dy/2,; cen_y+factor*dy/2; );; last_histo->Reset();; }; else; {; if(last_histo) delete last_histo;; // allocate first view...; last_histo = new TH2F(""h2"",; ""Mandelbrot [move mouse and press z to zoom, u to unzoom, r to reset]"",; 200,-2,2,200,-2,2);; last_histo->SetStats(false);; }; const int max_iter=50;; for(int bx=1;bx<=last_histo->GetNbinsX();bx++); for(int by=1;by<=last_histo->GetNbinsY();by++); {; double x=last_histo->GetXaxis()->GetBinCenter(bx);; double y=last_histo->GetYaxis()->GetBinCenter(by);; TComplex point( x,y);; TComplex z=point;; int iter=0;; while (z.Rho()<2){; z=z*z+point;; last_histo->Fill(x,y);; iter++;; if(iter>max_iter) break;; }; }; last_histo->SetContour(99);; last_histo->Draw(""colz"");; gPad->Modified();; gPad->Update();; printf(""Done.\n"");; }; ; void myexec(); {; // get event information; int event = gPad->GetEvent();; int px = gPad->GetEventX();; int py = gPad",MatchSource.WIKI,doc/master/mandelbrot_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mandelbrot_8C.html
https://root.cern/doc/master/mandelbrot_8C.html:685,Modifiability,variab,variables,685,". ROOT: tutorials/graphics/mandelbrot.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mandelbrot.C File ReferenceTutorials » Graphics tutorials. Detailed Description; Using TExec to handle keyboard events and TComplex to draw the Mandelbrot set. ; Pressing the keys 'z' and 'u' will zoom and unzoom the picture near the mouse location, 'r' will reset to the default view.; Try it (in compiled mode!) with: root mandelbrot.C+. Details; when a mouse event occurs the myexec() function is called (by using AddExec). Depending on the pressed key, the mygenerate() function is called, with the proper arguments. Note the last_x and last_y variables that are used in myexec() to store the last pointer coordinates (px is not a pointer position in kKeyPress events).; ; #include <TStyle.h>; #include <TROOT.h>; #include <TH2.h>; #include <TComplex.h>; #include <TVirtualPad.h>; #include <TCanvas.h>; ; TH2F *last_histo = nullptr;; ; void mygenerate(double factor, double cen_x, double cen_y); {; printf(""Regenerating...\n"");; // resize histo:; if(factor>0); {; double dx=last_histo->GetXaxis()->GetXmax()-last_histo->GetXaxis()->GetXmin();; double dy=last_histo->GetYaxis()->GetXmax()-last_histo->GetYaxis()->GetXmin();; last_histo->SetBins(; last_histo->GetNbinsX(),; cen_x-factor*dx/2,; cen_x+factor*dx/2,; last_histo->GetNbinsY(),; cen_y-factor*dy/2,; cen_y+factor*dy/2; );; last_histo->Reset();; }; else; {; if(last_histo) delete last_histo;; // allocate first view...; last_histo = new TH2F(""h2"",; ""Mandelbrot [move mouse and press z to zoom, u to unzoom, r to reset]"",; 200,-2,2,200,-2,2);; last_histo->SetStats(false);; }; const int max_iter=50;; for(int bx=1;bx<=last_histo->GetNbinsX();bx++); for(int by=1;by<=last_histo->GetNbinsY();by++); {; double x=last_histo->GetXaxis()->GetBinCenter(bx);; double y=last_histo->GetYaxis()->GetBinCenter(by);; TComplex point( x,y);; TComplex z=point;; int iter=0;; while (z.Rho()<2){; z=z*z+point;; last_histo-",MatchSource.WIKI,doc/master/mandelbrot_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mandelbrot_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:715,Availability,avail,available,715,". ROOT: tutorials/graphics/mass_spectrum.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mass_spectrum.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro makes use of some basic graphics primitives such as line, arrow and text. ; It has been written using the TCanvas ToolBar to produce a first draft and was then modified for fine adjustments. Note also the use of C functions. They allow to simplify the macro reading and editing by avoiding code repetition or defining some graphics attributes in one single place. This technique to generate drawings may appear not very user friendly compare to all the ""wysiwyg"" graphics editors available. In some cases it can be more powerful than a GUI interface because it allows to generate very precise drawing and using computation to generate them. ; void hline (Double_t x, Double_t y); {; Double_t dx = 0.1;; TLine *l = new TLine(x,y,x+dx,y);; l->Draw();; l->SetLineWidth(4);; }; ; void DrawArrow (Double_t x1, Double_t y1, Double_t x2, Double_t y2, Int_t ls); {; TArrow *arr = new TArrow(x1,y1,x2,y2,0.025,""|>"");; arr->SetFillColor(1);; arr->SetFillStyle(1001);; arr->SetLineStyle(ls);; arr->SetAngle(19);; arr->Draw();; }; ; void mass_spectrum(); {; TCanvas *C = new TCanvas(""C"",""C"",800,500);; ; hline (0.10,0.25);; hline (0.10,0.80);; hline (0.30,0.90);; hline (0.30,0.35);; hline (0.45,0.60);; hline (0.58,0.68);; hline (0.73,0.70);; hline (0.89,0.75);; ; DrawArrow(0.32, 0.90, 0.32, 0.35, 1);; DrawArrow(0.34, 0.90, 0.34, 0.35, 1);; DrawArrow(0.36, 0.90, 0.36, 0.60, 1);; DrawArrow(0.38, 0.90, 0.38, 0.70, 1);; ; DrawArrow(0.30, 0.90, 0.18, 0.25, 1);; DrawArrow(0.30, 0.35, 0.19, 0.25, 1);; DrawArrow(0.40, 0.90, 0.47, 0.61, 1);; ; DrawArrow(0.15, 0.25, 0.15, 0.19, 1);; DrawArrow(0.15, 0.80, 0.15, 0.74, 1);; ; DrawArrow(0.50, 0.60, 0.50, 0.54, 1);; DrawArrow(0.60, 0.68, 0.60, 0.62, 1);; DrawArrow(0.94, 0.75, 0.94, 0.69, 1);; ; DrawArrow(0.32, 0.35, 0.32, 0.19, 1)",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:755,Energy Efficiency,power,powerful,755,". ROOT: tutorials/graphics/mass_spectrum.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mass_spectrum.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro makes use of some basic graphics primitives such as line, arrow and text. ; It has been written using the TCanvas ToolBar to produce a first draft and was then modified for fine adjustments. Note also the use of C functions. They allow to simplify the macro reading and editing by avoiding code repetition or defining some graphics attributes in one single place. This technique to generate drawings may appear not very user friendly compare to all the ""wysiwyg"" graphics editors available. In some cases it can be more powerful than a GUI interface because it allows to generate very precise drawing and using computation to generate them. ; void hline (Double_t x, Double_t y); {; Double_t dx = 0.1;; TLine *l = new TLine(x,y,x+dx,y);; l->Draw();; l->SetLineWidth(4);; }; ; void DrawArrow (Double_t x1, Double_t y1, Double_t x2, Double_t y2, Int_t ls); {; TArrow *arr = new TArrow(x1,y1,x2,y2,0.025,""|>"");; arr->SetFillColor(1);; arr->SetFillStyle(1001);; arr->SetLineStyle(ls);; arr->SetAngle(19);; arr->Draw();; }; ; void mass_spectrum(); {; TCanvas *C = new TCanvas(""C"",""C"",800,500);; ; hline (0.10,0.25);; hline (0.10,0.80);; hline (0.30,0.90);; hline (0.30,0.35);; hline (0.45,0.60);; hline (0.58,0.68);; hline (0.73,0.70);; hline (0.89,0.75);; ; DrawArrow(0.32, 0.90, 0.32, 0.35, 1);; DrawArrow(0.34, 0.90, 0.34, 0.35, 1);; DrawArrow(0.36, 0.90, 0.36, 0.60, 1);; DrawArrow(0.38, 0.90, 0.38, 0.70, 1);; ; DrawArrow(0.30, 0.90, 0.18, 0.25, 1);; DrawArrow(0.30, 0.35, 0.19, 0.25, 1);; DrawArrow(0.40, 0.90, 0.47, 0.61, 1);; ; DrawArrow(0.15, 0.25, 0.15, 0.19, 1);; DrawArrow(0.15, 0.80, 0.15, 0.74, 1);; ; DrawArrow(0.50, 0.60, 0.50, 0.54, 1);; DrawArrow(0.60, 0.68, 0.60, 0.62, 1);; DrawArrow(0.94, 0.75, 0.94, 0.69, 1);; ; DrawArrow(0.32, 0.35, 0.32, 0.19, 1)",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:775,Integrability,interface,interface,775,". ROOT: tutorials/graphics/mass_spectrum.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mass_spectrum.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro makes use of some basic graphics primitives such as line, arrow and text. ; It has been written using the TCanvas ToolBar to produce a first draft and was then modified for fine adjustments. Note also the use of C functions. They allow to simplify the macro reading and editing by avoiding code repetition or defining some graphics attributes in one single place. This technique to generate drawings may appear not very user friendly compare to all the ""wysiwyg"" graphics editors available. In some cases it can be more powerful than a GUI interface because it allows to generate very precise drawing and using computation to generate them. ; void hline (Double_t x, Double_t y); {; Double_t dx = 0.1;; TLine *l = new TLine(x,y,x+dx,y);; l->Draw();; l->SetLineWidth(4);; }; ; void DrawArrow (Double_t x1, Double_t y1, Double_t x2, Double_t y2, Int_t ls); {; TArrow *arr = new TArrow(x1,y1,x2,y2,0.025,""|>"");; arr->SetFillColor(1);; arr->SetFillStyle(1001);; arr->SetLineStyle(ls);; arr->SetAngle(19);; arr->Draw();; }; ; void mass_spectrum(); {; TCanvas *C = new TCanvas(""C"",""C"",800,500);; ; hline (0.10,0.25);; hline (0.10,0.80);; hline (0.30,0.90);; hline (0.30,0.35);; hline (0.45,0.60);; hline (0.58,0.68);; hline (0.73,0.70);; hline (0.89,0.75);; ; DrawArrow(0.32, 0.90, 0.32, 0.35, 1);; DrawArrow(0.34, 0.90, 0.34, 0.35, 1);; DrawArrow(0.36, 0.90, 0.36, 0.60, 1);; DrawArrow(0.38, 0.90, 0.38, 0.70, 1);; ; DrawArrow(0.30, 0.90, 0.18, 0.25, 1);; DrawArrow(0.30, 0.35, 0.19, 0.25, 1);; DrawArrow(0.40, 0.90, 0.47, 0.61, 1);; ; DrawArrow(0.15, 0.25, 0.15, 0.19, 1);; DrawArrow(0.15, 0.80, 0.15, 0.74, 1);; ; DrawArrow(0.50, 0.60, 0.50, 0.54, 1);; DrawArrow(0.60, 0.68, 0.60, 0.62, 1);; DrawArrow(0.94, 0.75, 0.94, 0.69, 1);; ; DrawArrow(0.32, 0.35, 0.32, 0.19, 1)",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:516,Safety,avoid,avoiding,516,". ROOT: tutorials/graphics/mass_spectrum.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mass_spectrum.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro makes use of some basic graphics primitives such as line, arrow and text. ; It has been written using the TCanvas ToolBar to produce a first draft and was then modified for fine adjustments. Note also the use of C functions. They allow to simplify the macro reading and editing by avoiding code repetition or defining some graphics attributes in one single place. This technique to generate drawings may appear not very user friendly compare to all the ""wysiwyg"" graphics editors available. In some cases it can be more powerful than a GUI interface because it allows to generate very precise drawing and using computation to generate them. ; void hline (Double_t x, Double_t y); {; Double_t dx = 0.1;; TLine *l = new TLine(x,y,x+dx,y);; l->Draw();; l->SetLineWidth(4);; }; ; void DrawArrow (Double_t x1, Double_t y1, Double_t x2, Double_t y2, Int_t ls); {; TArrow *arr = new TArrow(x1,y1,x2,y2,0.025,""|>"");; arr->SetFillColor(1);; arr->SetFillStyle(1001);; arr->SetLineStyle(ls);; arr->SetAngle(19);; arr->Draw();; }; ; void mass_spectrum(); {; TCanvas *C = new TCanvas(""C"",""C"",800,500);; ; hline (0.10,0.25);; hline (0.10,0.80);; hline (0.30,0.90);; hline (0.30,0.35);; hline (0.45,0.60);; hline (0.58,0.68);; hline (0.73,0.70);; hline (0.89,0.75);; ; DrawArrow(0.32, 0.90, 0.32, 0.35, 1);; DrawArrow(0.34, 0.90, 0.34, 0.35, 1);; DrawArrow(0.36, 0.90, 0.36, 0.60, 1);; DrawArrow(0.38, 0.90, 0.38, 0.70, 1);; ; DrawArrow(0.30, 0.90, 0.18, 0.25, 1);; DrawArrow(0.30, 0.35, 0.19, 0.25, 1);; DrawArrow(0.40, 0.90, 0.47, 0.61, 1);; ; DrawArrow(0.15, 0.25, 0.15, 0.19, 1);; DrawArrow(0.15, 0.80, 0.15, 0.74, 1);; ; DrawArrow(0.50, 0.60, 0.50, 0.54, 1);; DrawArrow(0.60, 0.68, 0.60, 0.62, 1);; DrawArrow(0.94, 0.75, 0.94, 0.69, 1);; ; DrawArrow(0.32, 0.35, 0.32, 0.19, 1)",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:474,Usability,simpl,simplify,474,". ROOT: tutorials/graphics/mass_spectrum.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mass_spectrum.C File ReferenceTutorials » Graphics tutorials. Detailed Description; This macro makes use of some basic graphics primitives such as line, arrow and text. ; It has been written using the TCanvas ToolBar to produce a first draft and was then modified for fine adjustments. Note also the use of C functions. They allow to simplify the macro reading and editing by avoiding code repetition or defining some graphics attributes in one single place. This technique to generate drawings may appear not very user friendly compare to all the ""wysiwyg"" graphics editors available. In some cases it can be more powerful than a GUI interface because it allows to generate very precise drawing and using computation to generate them. ; void hline (Double_t x, Double_t y); {; Double_t dx = 0.1;; TLine *l = new TLine(x,y,x+dx,y);; l->Draw();; l->SetLineWidth(4);; }; ; void DrawArrow (Double_t x1, Double_t y1, Double_t x2, Double_t y2, Int_t ls); {; TArrow *arr = new TArrow(x1,y1,x2,y2,0.025,""|>"");; arr->SetFillColor(1);; arr->SetFillStyle(1001);; arr->SetLineStyle(ls);; arr->SetAngle(19);; arr->Draw();; }; ; void mass_spectrum(); {; TCanvas *C = new TCanvas(""C"",""C"",800,500);; ; hline (0.10,0.25);; hline (0.10,0.80);; hline (0.30,0.90);; hline (0.30,0.35);; hline (0.45,0.60);; hline (0.58,0.68);; hline (0.73,0.70);; hline (0.89,0.75);; ; DrawArrow(0.32, 0.90, 0.32, 0.35, 1);; DrawArrow(0.34, 0.90, 0.34, 0.35, 1);; DrawArrow(0.36, 0.90, 0.36, 0.60, 1);; DrawArrow(0.38, 0.90, 0.38, 0.70, 1);; ; DrawArrow(0.30, 0.90, 0.18, 0.25, 1);; DrawArrow(0.30, 0.35, 0.19, 0.25, 1);; DrawArrow(0.40, 0.90, 0.47, 0.61, 1);; ; DrawArrow(0.15, 0.25, 0.15, 0.19, 1);; DrawArrow(0.15, 0.80, 0.15, 0.74, 1);; ; DrawArrow(0.50, 0.60, 0.50, 0.54, 1);; DrawArrow(0.60, 0.68, 0.60, 0.62, 1);; DrawArrow(0.94, 0.75, 0.94, 0.69, 1);; ; DrawArrow(0.32, 0.35, 0.32, 0.19, 1)",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/mass__spectrum_8C.html:5858,Usability,simpl,simple,5858,"irtualXProxy.cxx:70; y1Option_t Option_t TPoint TPoint const char y1Definition TGWin32VirtualXProxy.cxx:70; TArrowDraw all kinds of Arrows.Definition TArrow.h:29; TArrow::Drawvoid Draw(Option_t *option="""") overrideDraw this arrow with its current attributes.Definition TArrow.cxx:120; TArrow::SetAnglevirtual void SetAngle(Float_t angle=60)Definition TArrow.h:60; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttFill::SetFillStylevirtual void SetFillStyle(Style_t fstyle)Set the fill area style.Definition TAttFill.h:39; TAttLine::SetLineStylevirtual void SetLineStyle(Style_t lstyle)Set the line style.Definition TAttLine.h:42; TAttLine::SetLineWidthvirtual void SetLineWidth(Width_t lwidth)Set the line width.Definition TAttLine.h:43; TAttText::SetTextAlignvirtual void SetTextAlign(Short_t align=11)Set the text alignment.Definition TAttText.h:42; TAttText::SetTextFontvirtual void SetTextFont(Font_t tfont=62)Set the text font.Definition TAttText.h:46; TAttText::SetTextSizevirtual void SetTextSize(Float_t tsize=1)Set the text size.Definition TAttText.h:47; TCanvasThe Canvas class.Definition TCanvas.h:23; TLatexTo draw Mathematical Formula.Definition TLatex.h:18; TLatex::DrawLatexTLatex * DrawLatex(Double_t x, Double_t y, const char *text)Make a copy of this object with the new parameters And copy object attributes.Definition TLatex.cxx:1943; TLineUse the TLine constructor to create a simple line.Definition TLine.h:22; TObject::Drawvirtual void Draw(Option_t *option="""")Default Draw method for all objects.Definition TObject.cxx:280; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; TMath::Cconstexpr Double_t C()Velocity of light in .Definition TMath.h:114; lTLine lDefinition textangle.C:4; AuthorOlivier Couet ; Definition in file mass_spectrum.C. tutorialsgraphicsmass_spectrum.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:29 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/mass__spectrum_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mass__spectrum_8C.html
https://root.cern/doc/master/MathFuncs_8h_source.html:16325,Availability,avail,available,16325,"lynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax); 482{; 483 int denom = lowestOrder + nCoeffs;; 484 double min = coeffs[nCoeffs - 1] / double(denom);; 485 double max = coeffs[nCoeffs - 1] / double(denom);; 486 ; 487 for (int i = nCoeffs - 2; i >= 0; i--) {; 488 denom--;; 489 min = (coeffs[i] / double(denom)) + xMin * min;; 490 max = (coeffs[i] / double(denom)) + xMax * max;; 491 }; 492 ; 493 max = max * std::pow(xMax, 1 + lowestOrder);; 494 min = min * std::pow(xMin, 1 + lowestOrder);; 495 ; 496 return max - min + (pdfMode && lowestOrder > 0.0 ? xMax - xMin : 0.0);; 497}; 498 ; 499/// use fast FMA if available, fall back to normal arithmetic if not; 500inline double fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by h",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:28029,Availability,avail,available,28029,"ssIntegraldouble bifurGaussIntegral(double xMin, double xMax, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:453; RooFit::Detail::MathFuncs::cbShapedouble cbShape(double m, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:382; RooFit::Detail::MathFuncs::polynomialdouble polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:130; RooFit::Detail::MathFuncs::recursiveFractiondouble recursiveFraction(double *a, unsigned int n)Definition MathFuncs.h:371; RooFit::Detail::MathFuncs::constraintSumdouble constraintSum(double const *comp, unsigned int compSize)Definition MathFuncs.h:163; RooFit::Detail::MathFuncs::cbShapeIntegraldouble cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:649; RooFit::Detail::MathFuncs::fast_fmadouble fast_fma(double x, double y, double z) noexceptuse fast FMA if available, fall back to normal arithmetic if notDefinition MathFuncs.h:500; RooFit::Detail::MathFuncs::logNormalIntegralStandarddouble logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma)Definition MathFuncs.h:638; RooFit::Detail::MathFuncs::landaudouble landau(double x, double mu, double sigma)Definition MathFuncs.h:331; RooFit::Detail::MathFuncs::gaussiandouble gaussian(double x, double mean, double sigma)Function to evaluate an un-normalized RooGaussian.Definition MathFuncs.h:86; RooFit::Detail::MathFuncs::productdouble product(double const *factors, std::size_t nFactors)Definition MathFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coef",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:31927,Availability,error,error,31927,"le logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of PDFs (or othe...Definition JSONIO.h:26; TMath::ErfDouble_t Erf(Double_t x)Computation of the error function erf(x).Definition TMath.cxx:190; TMath::QuietNaNDouble_t QuietNaN()Returns a quiet NaN as defined by IEEE 754.Definition TMath.h:902; TMath::Sqrt2constexpr Double_t Sqrt2()Definition TMath.h:86; TMath::ErfcDouble_t Erfc(Double_t x)Computes the complementary error function erfc(x).Definition TMath.cxx:199; TMath::LnGammaDouble_t LnGamma(Double_t z)Computation of ln[gamma(z)] for all z.Definition TMath.cxx:509; TMath::SignalingNaNDouble_t SignalingNaN()Returns a signaling NaN as defined by IEEE 754](http://en.wikipedia.org/wiki/NaN#Signaling_NaN).Definition TMath.h:910; TMath::TwoPiconstexpr Double_t TwoPi()Definition TMath.h:44; mTMarker mDefinition textangle.C:8; sumstatic uint64_t sum(uint64_t i)Definition Factory.cxx:2345. roofitroofitcoreincRooFitDetailMathFuncs.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:51 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:32200,Availability,error,error,32200,"le logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of PDFs (or othe...Definition JSONIO.h:26; TMath::ErfDouble_t Erf(Double_t x)Computation of the error function erf(x).Definition TMath.cxx:190; TMath::QuietNaNDouble_t QuietNaN()Returns a quiet NaN as defined by IEEE 754.Definition TMath.h:902; TMath::Sqrt2constexpr Double_t Sqrt2()Definition TMath.h:86; TMath::ErfcDouble_t Erfc(Double_t x)Computes the complementary error function erfc(x).Definition TMath.cxx:199; TMath::LnGammaDouble_t LnGamma(Double_t z)Computation of ln[gamma(z)] for all z.Definition TMath.cxx:509; TMath::SignalingNaNDouble_t SignalingNaN()Returns a signaling NaN as defined by IEEE 754](http://en.wikipedia.org/wiki/NaN#Signaling_NaN).Definition TMath.h:910; TMath::TwoPiconstexpr Double_t TwoPi()Definition TMath.h:44; mTMarker mDefinition textangle.C:8; sumstatic uint64_t sum(uint64_t i)Definition Factory.cxx:2345. roofitroofitcoreincRooFitDetailMathFuncs.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:51 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11330,Deployability,update,update,11330,"low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 r",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12831,Deployability,integrat,integrate,12831,"75 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highes",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12898,Deployability,integrat,integrate,12898,";; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highest precision there.; 434 // Therefore, the different cases for rang",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17375,Deployability,integrat,integrate,17375,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17543,Deployability,integrat,integrate,17543,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17650,Deployability,integrat,integrate,17650,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:18202,Deployability,integrat,integrate,18202," is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 559 acurr = newval;; 560 ; 561 newval = btwox * bcurr - blast;; 562 blast = bcurr;; 563 bcurr = newval;; 564 ; 565 ++nminus1;; 566 const double term1 = (bcurr - acurr) / (nminus1 + 1.);; 567 const double intTn = 0.5 * (term1 - term2);; 568 sum = fast_fma(intTn, c, sum);; 569 }; 570 }; 571 }; 572 ; 573 // take care to multiply with the right factor to account for the mapping to; 574 // normalised range [-1, 1]; 575 return halfrange * sum;; 576}; 577 ; 578// The last param should be of type bool but it is not as that causes some issues with Cling for some reason...; 579inline double; 580poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative); 581{; 582 if (protectNegative && mu < 0.0) {; 583 return std::exp(-2.0 * mu); // make it fall quickly; 584 }; 585 ; 586 if (code == 1) {; 587 // Implement integral over x as summation. Add special handling in case; 588 // range boundaries are not on integer ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:23748,Energy Efficiency,power,power,23748,"/ absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 686 double b = n / absAlpha - absAlpha;; 687 ; 688 double term1 = 0.0;; 689 if (useLog) {; 690 term1 = a * sig * (std::log(b - tmin) - std::log(n / absAlpha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double xhiScaled = (xhi - xmin) / (xmax - xmin);; 709 ; 710 int degree = nCoefs - 1; // n+1 polys of degree n; 711 double norm = 0.;; 712 ; 713 for (int i = 0; i <= degree; ++i) {; 714 // for each of the i Bernstein basis polynomials; 715 // represent it in the 'power basis' (the naive polynomial basis); 716 // where the integral is straight forward.; 717 double temp = 0.;; 718 for (int j = i; j <= degree; ++j) { // power basis≈ß; 719 double binCoefs = binomial(degree, j) * binomial(j, i);; 720 double oneOverJPlusOne = 1. / (j + 1.);; 721 double powDiff = std::pow(xhiScaled, j + 1.) - std::pow(xloScaled, j + 1.);; 722 temp += std::pow(-1., j - i) * binCoefs * powDiff * oneOverJPlusOne;; 723 }; 724 temp *= coefs[i]; // include coeff; 725 norm += temp; // add this basis's contribution to total; 726 }; 727 ; 728 return norm * (xmax - xmin);; 729}; 730 ; 731} // namespace MathFuncs; 732 ; 733} // namespace Detail; 734 ; 735} // namespace RooFit; 736 ; 737#endif; PdfFuncMathCore.h; ProbFuncMathCore.h; d#define d(i)Definition RSha256.hxx:102; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; S0#define S0(x)Definition RSha256.hxx:88; S1#define S1(x)Definition RSha256.hxx:89; c#define c(i)Definition",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:23905,Energy Efficiency,power,power,23905,"pha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double xhiScaled = (xhi - xmin) / (xmax - xmin);; 709 ; 710 int degree = nCoefs - 1; // n+1 polys of degree n; 711 double norm = 0.;; 712 ; 713 for (int i = 0; i <= degree; ++i) {; 714 // for each of the i Bernstein basis polynomials; 715 // represent it in the 'power basis' (the naive polynomial basis); 716 // where the integral is straight forward.; 717 double temp = 0.;; 718 for (int j = i; j <= degree; ++j) { // power basis≈ß; 719 double binCoefs = binomial(degree, j) * binomial(j, i);; 720 double oneOverJPlusOne = 1. / (j + 1.);; 721 double powDiff = std::pow(xhiScaled, j + 1.) - std::pow(xloScaled, j + 1.);; 722 temp += std::pow(-1., j - i) * binCoefs * powDiff * oneOverJPlusOne;; 723 }; 724 temp *= coefs[i]; // include coeff; 725 norm += temp; // add this basis's contribution to total; 726 }; 727 ; 728 return norm * (xmax - xmin);; 729}; 730 ; 731} // namespace MathFuncs; 732 ; 733} // namespace Detail; 734 ; 735} // namespace RooFit; 736 ; 737#endif; PdfFuncMathCore.h; ProbFuncMathCore.h; d#define d(i)Definition RSha256.hxx:102; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; S0#define S0(x)Definition RSha256.hxx:88; S1#define S1(x)Definition RSha256.hxx:89; c#define c(i)Definition RSha256.hxx:101; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition RSha256.hxx:103; totalstatic unsigned int totalDefinition TGWin32ProxyDefs.h:40; resultOption_t Option_t TPoint TPoint const ch",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12831,Integrability,integrat,integrate,12831,"75 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highes",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12898,Integrability,integrat,integrate,12898,";; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highest precision there.; 434 // Therefore, the different cases for rang",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17375,Integrability,integrat,integrate,17375,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17543,Integrability,integrat,integrate,17543,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:17650,Integrability,integrat,integrate,17650,"ble fast_fma(double x, double y, double z) noexcept; 501{; 502#if defined(FP_FAST_FMA) // check if std::fma has fast hardware implementation; 503 return std::fma(x, y, z);; 504#else // defined(FP_FAST_FMA); 505 // std::fma might be slow, so use a more pedestrian implementation; 506#if defined(__clang__); 507#pragma STDC FP_CONTRACT ON // hint clang that using an FMA is okay here; 508#endif // defined(__clang__); 509 return (x * y) + z;; 510#endif // defined(FP_FAST_FMA); 511}; 512 ; 513inline double chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull,; 514 double xMaxFull); 515{; 516 const double halfrange = .5 * (xMax - xMin);; 517 const double mid = .5 * (xMax + xMin);; 518 ; 519 // the full range of the function is mapped to the normalised [-1, 1] range; 520 const double b = (xMaxFull - mid) / halfrange;; 521 const double a = (xMinFull - mid) / halfrange;; 522 ; 523 // coefficient for integral(T_0(x)) is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 5",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:18202,Integrability,integrat,integrate,18202," is 1 (implicit), integrate by hand; 524 // T_0(x) and T_1(x), and use for n > 1: integral(T_n(x) dx) =; 525 // (T_n+1(x) / (n + 1) - T_n-1(x) / (n - 1)) / 2; 526 double sum = b - a; // integrate T_0(x) by hand; 527 ; 528 const unsigned int iend = nCoeffs;; 529 if (iend > 0) {; 530 {; 531 // integrate T_1(x) by hand...; 532 const double c = coeffs[0];; 533 sum = fast_fma(0.5 * (b + a) * (b - a), c, sum);; 534 }; 535 if (1 < iend) {; 536 double bcurr = b;; 537 double btwox = 2 * b;; 538 double blast = 1;; 539 ; 540 double acurr = a;; 541 double atwox = 2 * a;; 542 double alast = 1;; 543 ; 544 double newval = atwox * acurr - alast;; 545 alast = acurr;; 546 acurr = newval;; 547 ; 548 newval = btwox * bcurr - blast;; 549 blast = bcurr;; 550 bcurr = newval;; 551 double nminus1 = 1.;; 552 for (unsigned int i = 1; iend != i; ++i) {; 553 // integrate using recursion relation; 554 const double c = coeffs[i];; 555 const double term2 = (blast - alast) / nminus1;; 556 ; 557 newval = atwox * acurr - alast;; 558 alast = acurr;; 559 acurr = newval;; 560 ; 561 newval = btwox * bcurr - blast;; 562 blast = bcurr;; 563 bcurr = newval;; 564 ; 565 ++nminus1;; 566 const double term1 = (bcurr - acurr) / (nminus1 + 1.);; 567 const double intTn = 0.5 * (term1 - term2);; 568 sum = fast_fma(intTn, c, sum);; 569 }; 570 }; 571 }; 572 ; 573 // take care to multiply with the right factor to account for the mapping to; 574 // normalised range [-1, 1]; 575 return halfrange * sum;; 576}; 577 ; 578// The last param should be of type bool but it is not as that causes some issues with Cling for some reason...; 579inline double; 580poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative); 581{; 582 if (protectNegative && mu < 0.0) {; 583 return std::exp(-2.0 * mu); // make it fall quickly; 584 }; 585 ; 586 if (code == 1) {; 587 // Implement integral over x as summation. Add special handling in case; 588 // range boundaries are not on integer ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:6282,Modifiability,flexible,flexibleInterpSingle,6282,"terpolation; 184 double central = low + (idx + 0.5) * binWidth;; 185 if (val > low + 0.5 * binWidth && val < high - 0.5 * binWidth) {; 186 double slope;; 187 if (val < central) {; 188 slope = vals[idx] - vals[idx - 1];; 189 } else {; 190 slope = vals[idx + 1] - vals[idx];; 191 }; 192 return vals[idx] + slope * (val - central) / binWidth;; 193 }; 194 ; 195 return vals[idx];; 196}; 197 ; 198inline double poisson(double x, double par); 199{; 200 if (par < 0); 201 return TMath::QuietNaN();; 202 ; 203 if (x < 0) {; 204 return 0;; 205 } else if (x == 0.0) {; 206 return std::exp(-par);; 207 } else {; 208 double out = x * std::log(par) - TMath::LnGamma(x + 1.) - par;; 209 return std::exp(out);; 210 }; 211}; 212 ; 213inline double flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal,; 214 double paramVal, double res); 215{; 216 if (code == 0) {; 217 // piece-wise linear; 218 if (paramVal > 0) {; 219 return paramVal * (high - nominal);; 220 } else {; 221 return paramVal * (nominal - low);; 222 }; 223 } else if (code == 1) {; 224 // piece-wise log; 225 if (paramVal >= 0) {; 226 return res * (std::pow(high / nominal, +paramVal) - 1);; 227 } else {; 228 return res * (std::pow(low / nominal, -paramVal) - 1);; 229 }; 230 } else if (code == 2) {; 231 // parabolic with linear; 232 double a = 0.5 * (high + low) - nominal;; 233 double b = 0.5 * (high - low);; 234 double c = 0;; 235 if (paramVal > 1) {; 236 return (2 * a + b) * (paramVal - 1) + high - nominal;; 237 } else if (paramVal < -1) {; 238 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 239 } else {; 240 return a * std::pow(paramVal, 2) + b * paramVal + c;; 241 }; 242 } else if (code == 3) {; 243 // parabolic version of log-normal; 244 double a = 0.5 * (high + low) - nominal;; 245 double b = 0.5 * (high - low);; 246 double c = 0;; 247 if (paramVal > 1) {; 248 return (2 * a + b) * (paramVal - 1) + high - nominal;; 249 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) *",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10071,Modifiability,flexible,flexibleInterp,10071,"/ (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 * A1 + x0 * x0 * S2);; 309 ; 310 // evaluate the 6-th degree polynomial using Horner's method; 311 double value = 1. + x * (a + x * (b + x * (c + x * (d + x * (e + x * f)))));; 312 mod = value;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10326,Modifiability,flexible,flexibleInterpSingle,10326,"/ (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 * A1 + x0 * x0 * S2);; 309 ; 310 // evaluate the 6-th degree polynomial using Horner's method; 311 double value = 1. + x * (a + x * (b + x * (c + x * (d + x * (e + x * f)))));; 312 mod = value;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12819,Modifiability,variab,variable,12819,"75 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highes",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:12886,Modifiability,variab,variable,12886,";; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized RooGaussian over x. To calculate the integral over; 412/// mean, just interchange the respective values of x and mean.; 413/// @param xMin Minimum value of variable to integrate wrt.; 414/// @param xMax Maximum value of of variable to integrate wrt.; 415/// @param mean Mean.; 416/// @param sigma Sigma.; 417/// @return The integral of an un-normalized RooGaussian over the value in x.; 418inline double gaussianIntegral(double xMin, double xMax, double mean, double sigma); 419{; 420 // The normalisation constant 1./sqrt(2*pi*sigma^2) is left out in evaluate().; 421 // Therefore, the integral is scaled up by that amount to make RooFit normalise; 422 // correctly.; 423 double resultScale = 0.5 * std::sqrt(TMath::TwoPi()) * sigma;; 424 ; 425 // Here everything is scaled and shifted into a standard normal distribution:; 426 double xscale = TMath::Sqrt2() * sigma;; 427 double scaledMin = 0.;; 428 double scaledMax = 0.;; 429 scaledMin = (xMin - mean) / xscale;; 430 scaledMax = (xMax - mean) / xscale;; 431 ; 432 // Here we go for maximum precision: We compute all integrals in the UPPER; 433 // tail of the Gaussian, because erfc has the highest precision there.; 434 // Therefore, the different cases for rang",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:29240,Modifiability,flexible,flexibleInterpSingledouble,29240,"ndau(double x, double mu, double sigma)Definition MathFuncs.h:331; RooFit::Detail::MathFuncs::gaussiandouble gaussian(double x, double mean, double sigma)Function to evaluate an un-normalized RooGaussian.Definition MathFuncs.h:86; RooFit::Detail::MathFuncs::productdouble product(double const *factors, std::size_t nFactors)Definition MathFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int numBins)Definition MathFuncs.h:172; RooFit::Detail::MathFuncs::flexibleInterpSingledouble flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal, double paramVal, double res)Definition MathFuncs.h:213; RooFit::Detail::MathFuncs::interpolate1ddouble interpolate1d(double low, double high, double val, unsigned int numBins, double const *vals)Definition MathFuncs.h:178; RooFit::Detail::MathFuncs::logNormalStandarddouble logNormalStandard(double x, double sigma, double mu)Definition MathFuncs.h:343; RooFit::Detail::MathFuncs::bifurGaussdouble bifurGauss(double x, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, doub",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:29267,Modifiability,flexible,flexibleInterpSingle,29267,"ndau(double x, double mu, double sigma)Definition MathFuncs.h:331; RooFit::Detail::MathFuncs::gaussiandouble gaussian(double x, double mean, double sigma)Function to evaluate an un-normalized RooGaussian.Definition MathFuncs.h:86; RooFit::Detail::MathFuncs::productdouble product(double const *factors, std::size_t nFactors)Definition MathFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int numBins)Definition MathFuncs.h:172; RooFit::Detail::MathFuncs::flexibleInterpSingledouble flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal, double paramVal, double res)Definition MathFuncs.h:213; RooFit::Detail::MathFuncs::interpolate1ddouble interpolate1d(double low, double high, double val, unsigned int numBins, double const *vals)Definition MathFuncs.h:178; RooFit::Detail::MathFuncs::logNormalStandarddouble logNormalStandard(double x, double sigma, double mu)Definition MathFuncs.h:343; RooFit::Detail::MathFuncs::bifurGaussdouble bifurGauss(double x, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, doub",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:31391,Modifiability,flexible,flexibleInterpdouble,31391,"MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative)Definition MathFuncs.h:580; RooFit::Detail::MathFuncs::logNormaldouble logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of PDFs (or othe...Definition JSONIO.h:26; TMath::ErfDouble_t Erf(Double_t x)Computation of the error function erf(x).Definition TMath.cxx:190; TMath::QuietNaNDouble_t QuietNaN()Returns a quiet NaN as defined by IEEE 754.Definition TMath.h:902; TMath::Sqrt2constexpr Double_t Sqrt2()Definition TMath.h:86; TMath::ErfcDouble_t Erfc(Double_t x)Computes the complementary error function erfc(x).Definition TMath.cxx:199; TMath::LnGammaDouble_t LnGamma(Double_t z)Computation of ln[gamma(z)] for all z.Definition TMath.cxx:509; TMath::SignalingNaNDouble_t SignalingNaN()Returns a signaling NaN as defined by IEEE 754](http://en.wikipedia.org/wiki/",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:31412,Modifiability,flexible,flexibleInterp,31412,"MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative)Definition MathFuncs.h:580; RooFit::Detail::MathFuncs::logNormaldouble logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of PDFs (or othe...Definition JSONIO.h:26; TMath::ErfDouble_t Erf(Double_t x)Computation of the error function erf(x).Definition TMath.cxx:190; TMath::QuietNaNDouble_t QuietNaN()Returns a quiet NaN as defined by IEEE 754.Definition TMath.h:902; TMath::Sqrt2constexpr Double_t Sqrt2()Definition TMath.h:86; TMath::ErfcDouble_t Erfc(Double_t x)Computes the complementary error function erfc(x).Definition TMath.cxx:199; TMath::LnGammaDouble_t LnGamma(Double_t z)Computation of ln[gamma(z)] for all z.Definition TMath.cxx:509; TMath::SignalingNaNDouble_t SignalingNaN()Returns a signaling NaN as defined by IEEE 754](http://en.wikipedia.org/wiki/",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:5003,Testability,log,log,5003,";; 135 retVal = retVal * std::pow(x, lowestOrder);; 136 return retVal + (pdfMode && lowestOrder > 0 ? 1.0 : 0.0);; 137}; 138 ; 139inline double chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax); 140{; 141 // transform to range [-1, +1]; 142 const double xPrime = (x_in - 0.5 * (xMax + xMin)) / (0.5 * (xMax - xMin));; 143 ; 144 // extract current values of coefficients; 145 double sum = 1.;; 146 if (nCoeffs > 0) {; 147 double curr = xPrime;; 148 double twox = 2 * xPrime;; 149 double last = 1;; 150 double newval = twox * curr - last;; 151 last = curr;; 152 curr = newval;; 153 for (unsigned int i = 0; nCoeffs != i; ++i) {; 154 sum += last * coeffs[i];; 155 newval = twox * curr - last;; 156 last = curr;; 157 curr = newval;; 158 }; 159 }; 160 return sum;; 161}; 162 ; 163inline double constraintSum(double const *comp, unsigned int compSize); 164{; 165 double sum = 0;; 166 for (unsigned int i = 0; i < compSize; i++) {; 167 sum -= std::log(comp[i]);; 168 }; 169 return sum;; 170}; 171 ; 172inline unsigned int getUniformBinning(double low, double high, double val, unsigned int numBins); 173{; 174 double binWidth = (high - low) / numBins;; 175 return val >= high ? numBins - 1 : std::abs((val - low) / binWidth);; 176}; 177 ; 178inline double interpolate1d(double low, double high, double val, unsigned int numBins, double const* vals); 179{; 180 double binWidth = (high - low) / numBins;; 181 int idx = val >= high ? numBins - 1 : std::abs((val - low) / binWidth);; 182 ; 183 // interpolation; 184 double central = low + (idx + 0.5) * binWidth;; 185 if (val > low + 0.5 * binWidth && val < high - 0.5 * binWidth) {; 186 double slope;; 187 if (val < central) {; 188 slope = vals[idx] - vals[idx - 1];; 189 } else {; 190 slope = vals[idx + 1] - vals[idx];; 191 }; 192 return vals[idx] + slope * (val - central) / binWidth;; 193 }; 194 ; 195 return vals[idx];; 196}; 197 ; 198inline double poisson(double x, double par); 199{; 200 if (par < 0); 201 return TMat",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:6177,Testability,log,log,6177,"73{; 174 double binWidth = (high - low) / numBins;; 175 return val >= high ? numBins - 1 : std::abs((val - low) / binWidth);; 176}; 177 ; 178inline double interpolate1d(double low, double high, double val, unsigned int numBins, double const* vals); 179{; 180 double binWidth = (high - low) / numBins;; 181 int idx = val >= high ? numBins - 1 : std::abs((val - low) / binWidth);; 182 ; 183 // interpolation; 184 double central = low + (idx + 0.5) * binWidth;; 185 if (val > low + 0.5 * binWidth && val < high - 0.5 * binWidth) {; 186 double slope;; 187 if (val < central) {; 188 slope = vals[idx] - vals[idx - 1];; 189 } else {; 190 slope = vals[idx + 1] - vals[idx];; 191 }; 192 return vals[idx] + slope * (val - central) / binWidth;; 193 }; 194 ; 195 return vals[idx];; 196}; 197 ; 198inline double poisson(double x, double par); 199{; 200 if (par < 0); 201 return TMath::QuietNaN();; 202 ; 203 if (x < 0) {; 204 return 0;; 205 } else if (x == 0.0) {; 206 return std::exp(-par);; 207 } else {; 208 double out = x * std::log(par) - TMath::LnGamma(x + 1.) - par;; 209 return std::exp(out);; 210 }; 211}; 212 ; 213inline double flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal,; 214 double paramVal, double res); 215{; 216 if (code == 0) {; 217 // piece-wise linear; 218 if (paramVal > 0) {; 219 return paramVal * (high - nominal);; 220 } else {; 221 return paramVal * (nominal - low);; 222 }; 223 } else if (code == 1) {; 224 // piece-wise log; 225 if (paramVal >= 0) {; 226 return res * (std::pow(high / nominal, +paramVal) - 1);; 227 } else {; 228 return res * (std::pow(low / nominal, -paramVal) - 1);; 229 }; 230 } else if (code == 2) {; 231 // parabolic with linear; 232 double a = 0.5 * (high + low) - nominal;; 233 double b = 0.5 * (high - low);; 234 double c = 0;; 235 if (paramVal > 1) {; 236 return (2 * a + b) * (paramVal - 1) + high - nominal;; 237 } else if (paramVal < -1) {; 238 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:6643,Testability,log,log,6643,"terpolation; 184 double central = low + (idx + 0.5) * binWidth;; 185 if (val > low + 0.5 * binWidth && val < high - 0.5 * binWidth) {; 186 double slope;; 187 if (val < central) {; 188 slope = vals[idx] - vals[idx - 1];; 189 } else {; 190 slope = vals[idx + 1] - vals[idx];; 191 }; 192 return vals[idx] + slope * (val - central) / binWidth;; 193 }; 194 ; 195 return vals[idx];; 196}; 197 ; 198inline double poisson(double x, double par); 199{; 200 if (par < 0); 201 return TMath::QuietNaN();; 202 ; 203 if (x < 0) {; 204 return 0;; 205 } else if (x == 0.0) {; 206 return std::exp(-par);; 207 } else {; 208 double out = x * std::log(par) - TMath::LnGamma(x + 1.) - par;; 209 return std::exp(out);; 210 }; 211}; 212 ; 213inline double flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal,; 214 double paramVal, double res); 215{; 216 if (code == 0) {; 217 // piece-wise linear; 218 if (paramVal > 0) {; 219 return paramVal * (high - nominal);; 220 } else {; 221 return paramVal * (nominal - low);; 222 }; 223 } else if (code == 1) {; 224 // piece-wise log; 225 if (paramVal >= 0) {; 226 return res * (std::pow(high / nominal, +paramVal) - 1);; 227 } else {; 228 return res * (std::pow(low / nominal, -paramVal) - 1);; 229 }; 230 } else if (code == 2) {; 231 // parabolic with linear; 232 double a = 0.5 * (high + low) - nominal;; 233 double b = 0.5 * (high - low);; 234 double c = 0;; 235 if (paramVal > 1) {; 236 return (2 * a + b) * (paramVal - 1) + high - nominal;; 237 } else if (paramVal < -1) {; 238 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 239 } else {; 240 return a * std::pow(paramVal, 2) + b * paramVal + c;; 241 }; 242 } else if (code == 3) {; 243 // parabolic version of log-normal; 244 double a = 0.5 * (high + low) - nominal;; 245 double b = 0.5 * (high - low);; 246 double c = 0;; 247 if (paramVal > 1) {; 248 return (2 * a + b) * (paramVal - 1) + high - nominal;; 249 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) *",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:7292,Testability,log,log-normal,7292,"-par);; 207 } else {; 208 double out = x * std::log(par) - TMath::LnGamma(x + 1.) - par;; 209 return std::exp(out);; 210 }; 211}; 212 ; 213inline double flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal,; 214 double paramVal, double res); 215{; 216 if (code == 0) {; 217 // piece-wise linear; 218 if (paramVal > 0) {; 219 return paramVal * (high - nominal);; 220 } else {; 221 return paramVal * (nominal - low);; 222 }; 223 } else if (code == 1) {; 224 // piece-wise log; 225 if (paramVal >= 0) {; 226 return res * (std::pow(high / nominal, +paramVal) - 1);; 227 } else {; 228 return res * (std::pow(low / nominal, -paramVal) - 1);; 229 }; 230 } else if (code == 2) {; 231 // parabolic with linear; 232 double a = 0.5 * (high + low) - nominal;; 233 double b = 0.5 * (high - low);; 234 double c = 0;; 235 if (paramVal > 1) {; 236 return (2 * a + b) * (paramVal - 1) + high - nominal;; 237 } else if (paramVal < -1) {; 238 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 239 } else {; 240 return a * std::pow(paramVal, 2) + b * paramVal + c;; 241 }; 242 } else if (code == 3) {; 243 // parabolic version of log-normal; 244 double a = 0.5 * (high + low) - nominal;; 245 double b = 0.5 * (high - low);; 246 double c = 0;; 247 if (paramVal > 1) {; 248 return (2 * a + b) * (paramVal - 1) + high - nominal;; 249 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 251 } else {; 252 return a * std::pow(paramVal, 2) + b * paramVal + c;; 253 }; 254 } else if (code == 4) {; 255 double x = paramVal;; 256 if (x >= boundary) {; 257 return x * (high - nominal);; 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8654,Testability,log,logHi,8654,"9 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 251 } else {; 252 return a * std::pow(paramVal, 2) + b * paramVal + c;; 253 }; 254 } else if (code == 4) {; 255 double x = paramVal;; 256 if (x >= boundary) {; 257 return x * (high - nominal);; 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8667,Testability,log,log,8667,"9 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 251 } else {; 252 return a * std::pow(paramVal, 2) + b * paramVal + c;; 253 }; 254 } else if (code == 4) {; 255 double x = paramVal;; 256 if (x >= boundary) {; 257 return x * (high - nominal);; 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8690,Testability,log,logLo,8690,"9 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 251 } else {; 252 return a * std::pow(paramVal, 2) + b * paramVal + c;; 253 }; 254 } else if (code == 4) {; 255 double x = paramVal;; 256 if (x >= boundary) {; 257 return x * (high - nominal);; 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8703,Testability,log,log,8703,"9 } else if (paramVal < -1) {; 250 return -1 * (2 * a - b) * (paramVal + 1) + low - nominal;; 251 } else {; 252 return a * std::pow(paramVal, 2) + b * paramVal + c;; 253 }; 254 } else if (code == 4) {; 255 double x = paramVal;; 256 if (x >= boundary) {; 257 return x * (high - nominal);; 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8764,Testability,log,logHi,8764," 258 } else if (x <= -boundary) {; 259 return x * (nominal - low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8826,Testability,log,logLo,8826,"low);; 260 }; 261 ; 262 // interpolate 6th degree; 263 double t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 * A1 + x0 * x0 * S2);; 309 ; 310 // evaluate the 6-th degree",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8888,Testability,log,logHi,8888,"t = x / boundary;; 264 double eps_plus = high - nominal;; 265 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 * A1 + x0 * x0 * S2);; 309 ; 310 // evaluate the 6-th degree polynomial using Horner's method; 311 double value = 1. + x * ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:8954,Testability,log,logLo,8954,"5 double eps_minus = nominal - low;; 266 double S = 0.5 * (eps_plus + eps_minus);; 267 double A = 0.0625 * (eps_plus - eps_minus);; 268 ; 269 return x * (S + t * A * (15 + t * t * (-10 + t * t * 3)));; 270 } else if (code == 5) {; 271 double x = paramVal;; 272 double mod = 1.0;; 273 if (x >= boundary) {; 274 mod = std::pow(high / nominal, +paramVal);; 275 } else if (x <= -boundary) {; 276 mod = std::pow(low / nominal, -paramVal);; 277 } else {; 278 // interpolate 6th degree exp; 279 double x0 = boundary;; 280 ; 281 high /= nominal;; 282 low /= nominal;; 283 ; 284 // GHL: Swagato's suggestions; 285 double powUp = std::pow(high, x0);; 286 double powDown = std::pow(low, x0);; 287 double logHi = std::log(high);; 288 double logLo = std::log(low);; 289 double powUpLog = high <= 0.0 ? 0.0 : powUp * logHi;; 290 double powDownLog = low <= 0.0 ? 0.0 : -powDown * logLo;; 291 double powUpLog2 = high <= 0.0 ? 0.0 : powUpLog * logHi;; 292 double powDownLog2 = low <= 0.0 ? 0.0 : -powDownLog * logLo;; 293 ; 294 double S0 = 0.5 * (powUp + powDown);; 295 double A0 = 0.5 * (powUp - powDown);; 296 double S1 = 0.5 * (powUpLog + powDownLog);; 297 double A1 = 0.5 * (powUpLog - powDownLog);; 298 double S2 = 0.5 * (powUpLog2 + powDownLog2);; 299 double A2 = 0.5 * (powUpLog2 - powDownLog2);; 300 ; 301 // fcns+der+2nd_der are eq at bd; 302 ; 303 double a = 1. / (8 * x0) * (15 * A0 - 7 * x0 * S1 + x0 * x0 * A2);; 304 double b = 1. / (8 * x0 * x0) * (-24 + 24 * S0 - 9 * x0 * A1 + x0 * x0 * S2);; 305 double c = 1. / (4 * std::pow(x0, 3)) * (-5 * A0 + 5 * x0 * S1 - x0 * x0 * A2);; 306 double d = 1. / (4 * std::pow(x0, 4)) * (12 - 12 * S0 + 7 * x0 * A1 - x0 * x0 * S2);; 307 double e = 1. / (8 * std::pow(x0, 5)) * (+3 * A0 - 3 * x0 * S1 + x0 * x0 * A2);; 308 double f = 1. / (8 * std::pow(x0, 6)) * (-8 + 8 * S0 - 5 * x0 * A1 + x0 * x0 * S2);; 309 ; 310 // evaluate the 6-th degree polynomial using Horner's method; 311 double value = 1. + x * (a + x * (b + x * (c + x * (d + x * (e + x * f)))));; 312 m",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10696,Testability,log,logNormal,10696,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10789,Testability,log,log,10789,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10812,Testability,log,log,10812,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:10852,Testability,log,logNormalStandard,10852,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11231,Testability,log,log,11231,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11293,Testability,log,log-formula,11293,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11316,Testability,log,log,11316,"lue;; 313 }; 314 return res * (mod - 1.0);; 315 }; 316 ; 317 return 0.0;; 318}; 319 ; 320inline double flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low,; 321 double const *high, double boundary, double nominal, int doCutoff); 322{; 323 double total = nominal;; 324 for (std::size_t i = 0; i < n; ++i) {; 325 total += flexibleInterpSingle(code, low[i], high[i], boundary, nominal, params[i], total);; 326 }; 327 ; 328 return doCutoff && total <= 0 ? TMath::Limits<double>::Min() : total;; 329}; 330 ; 331inline double landau(double x, double mu, double sigma); 332{; 333 if (sigma <= 0.); 334 return 0.;; 335 return ROOT::Math::landau_pdf((x - mu) / sigma);; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11533,Testability,log,log,11533,");; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized R",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11549,Testability,log,log,11549,");; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized R",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11602,Testability,log,log,11602,");; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized R",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:11682,Testability,log,log,11682,");; 336}; 337 ; 338inline double logNormal(double x, double k, double m0); 339{; 340 return ROOT::Math::lognormal_pdf(x, std::log(m0), std::abs(std::log(k)));; 341}; 342 ; 343inline double logNormalStandard(double x, double sigma, double mu); 344{; 345 return ROOT::Math::lognormal_pdf(x, mu, std::abs(sigma));; 346}; 347 ; 348inline double effProd(double eff, double pdf); 349{; 350 return eff * pdf;; 351}; 352 ; 353inline double nll(double pdf, double weight, int binnedL, int doBinOffset); 354{; 355 if (binnedL) {; 356 // Special handling of this case since std::log(Poisson(0,0)=0 but can't be; 357 // calculated with usual log-formula since std::log(mu)=0. No update of result; 358 // is required since term=0.; 359 if (std::abs(pdf) < 1e-10 && std::abs(weight) < 1e-10) {; 360 return 0.0;; 361 }; 362 if (doBinOffset) {; 363 return pdf - weight - weight * (std::log(pdf) - std::log(weight));; 364 }; 365 return pdf - weight * std::log(pdf) + TMath::LnGamma(weight + 1);; 366 } else {; 367 return -weight * std::log(pdf);; 368 }; 369}; 370 ; 371inline double recursiveFraction(double *a, unsigned int n); 372{; 373 double prod = a[0];; 374 ; 375 for (unsigned int i = 1; i < n; ++i) {; 376 prod *= 1.0 - a[i];; 377 }; 378 ; 379 return prod;; 380}; 381 ; 382inline double cbShape(double m, double m0, double sigma, double alpha, double n); 383{; 384 double t = (m - m0) / sigma;; 385 if (alpha < 0); 386 t = -t;; 387 ; 388 double absAlpha = std::abs((double)alpha);; 389 ; 390 if (t >= -absAlpha) {; 391 return std::exp(-0.5 * t * t);; 392 } else {; 393 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 394 double b = n / absAlpha - absAlpha;; 395 ; 396 return a / std::pow(b - t, n);; 397 }; 398}; 399 ; 400// For RooCBShape; 401inline double approxErf(double arg); 402{; 403 if (arg > 5.0); 404 return 1.0;; 405 if (arg < -5.0); 406 return -1.0;; 407 ; 408 return TMath::Erf(arg);; 409}; 410 ; 411/// @brief Function to calculate the integral of an un-normalized R",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:20903,Testability,log,logNormalIntegral,20903,"as integers. ixMin is included, ixMax outside.; 603 const unsigned int ixMin = integrandMin;; 604 const unsigned int ixMax = std::min(integrandMax + 1, (double)std::numeric_limits<unsigned int>::max());; 605 ; 606 // Sum from 0 to just before the bin outside of the range.; 607 if (ixMin == 0) {; 608 return ROOT::Math::inc_gamma_c(ixMax, mu);; 609 } else {; 610 // If necessary, subtract from 0 to the beginning of the range; 611 if (ixMin <= mu) {; 612 return ROOT::Math::inc_gamma_c(ixMax, mu) - ROOT::Math::inc_gamma_c(ixMin, mu);; 613 } else {; 614 // Avoid catastrophic cancellation in the high tails:; 615 return ROOT::Math::inc_gamma(ixMin, mu) - ROOT::Math::inc_gamma(ixMax, mu);; 616 }; 617 }; 618 }; 619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0)",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21054,Testability,log,log,21054,"mits<unsigned int>::max());; 605 ; 606 // Sum from 0 to just before the bin outside of the range.; 607 if (ixMin == 0) {; 608 return ROOT::Math::inc_gamma_c(ixMax, mu);; 609 } else {; 610 // If necessary, subtract from 0 to the beginning of the range; 611 if (ixMin <= mu) {; 612 return ROOT::Math::inc_gamma_c(ixMax, mu) - ROOT::Math::inc_gamma_c(ixMin, mu);; 613 } else {; 614 // Avoid catastrophic cancellation in the high tails:; 615 return ROOT::Math::inc_gamma(ixMin, mu) - ROOT::Math::inc_gamma(ixMax, mu);; 616 }; 617 }; 618 }; 619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alph",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21109,Testability,log,log,21109,"se {; 610 // If necessary, subtract from 0 to the beginning of the range; 611 if (ixMin <= mu) {; 612 return ROOT::Math::inc_gamma_c(ixMax, mu) - ROOT::Math::inc_gamma_c(ixMin, mu);; 613 } else {; 614 // Avoid catastrophic cancellation in the high tails:; 615 return ROOT::Math::inc_gamma(ixMin, mu) - ROOT::Math::inc_gamma(ixMax, mu);; 616 }; 617 }; 618 }; 619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig *",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21160,Testability,log,log,21160,"se {; 610 // If necessary, subtract from 0 to the beginning of the range; 611 if (ixMin <= mu) {; 612 return ROOT::Math::inc_gamma_c(ixMax, mu) - ROOT::Math::inc_gamma_c(ixMin, mu);; 613 } else {; 614 // Avoid catastrophic cancellation in the high tails:; 615 return ROOT::Math::inc_gamma(ixMin, mu) - ROOT::Math::inc_gamma(ixMax, mu);; 616 }; 617 }; 618 }; 619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig *",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21248,Testability,log,logNormalIntegralStandard,21248,"se {; 610 // If necessary, subtract from 0 to the beginning of the range; 611 if (ixMin <= mu) {; 612 return ROOT::Math::inc_gamma_c(ixMax, mu) - ROOT::Math::inc_gamma_c(ixMin, mu);; 613 } else {; 614 // Avoid catastrophic cancellation in the high tails:; 615 return ROOT::Math::inc_gamma(ixMin, mu) - ROOT::Math::inc_gamma(ixMax, mu);; 616 }; 617 }; 618 }; 619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig *",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21461,Testability,log,log,21461,"619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:21514,Testability,log,log,21514,"619 ; 620 // the integral with respect to the mean is the integral of a gamma distribution; 621 // negative ix does not need protection (gamma returns 0.0); 622 const double ix = 1 + x;; 623 ; 624 return ROOT::Math::inc_gamma(ix, integrandMax) - ROOT::Math::inc_gamma(ix, integrandMin);; 625}; 626 ; 627inline double logNormalIntegral(double xMin, double xMax, double m0, double k); 628{; 629 const double root2 = std::sqrt(2.);; 630 ; 631 double ln_k = std::abs(std::log(k));; 632 double ret =; 633 0.5 * (TMath::Erf(std::log(xMax / m0) / (root2 * ln_k)) - TMath::Erf(std::log(xMin / m0) / (root2 * ln_k)));; 634 ; 635 return ret;; 636}; 637 ; 638inline double logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma); 639{; 640 const double root2 = std::sqrt(2.);; 641 ; 642 double ln_k = std::abs(sigma);; 643 double ret =; 644 0.5 * (TMath::Erf((std::log(xMax) - mu) / (root2 * ln_k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:22513,Testability,log,log,22513,"k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(b - tmax, n - 1.0)));; 683 }; 684 } else {; 685 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 686 double b = n / absAlpha - absAlpha;; 687 ; 688 double term1 = 0.0;; 689 if (useLog) {; 690 term1 = a * sig * (std::log(b - tmin) - std::log(n / absAlpha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:22534,Testability,log,log,22534,"k)) - TMath::Erf((std::log(xMin) - mu) / (root2 * ln_k)));; 645 ; 646 return ret;; 647}; 648 ; 649inline double cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n); 650{; 651 const double sqrtPiOver2 = 1.2533141373;; 652 const double sqrt2 = 1.4142135624;; 653 ; 654 double result = 0.0;; 655 bool useLog = false;; 656 ; 657 if (std::abs(n - 1.0) < 1.0e-05); 658 useLog = true;; 659 ; 660 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(b - tmax, n - 1.0)));; 683 }; 684 } else {; 685 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 686 double b = n / absAlpha - absAlpha;; 687 ; 688 double term1 = 0.0;; 689 if (useLog) {; 690 term1 = a * sig * (std::log(b - tmin) - std::log(n / absAlpha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:22901,Testability,log,log,22901,"60 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(b - tmax, n - 1.0)));; 683 }; 684 } else {; 685 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 686 double b = n / absAlpha - absAlpha;; 687 ; 688 double term1 = 0.0;; 689 if (useLog) {; 690 term1 = a * sig * (std::log(b - tmin) - std::log(n / absAlpha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double xhiScaled = (xhi - xmin) / (xmax - xmin);; 709 ; 710 int degree = nCoefs - 1; // n+1 polys of degree n; 711 double norm = 0.;; 712 ; 713 for (int i = 0; i <= degree; ++i) {; 714 // for each of the i Bernstein basis polynomials; 715 // represent it in the 'power basis' (the naive polynomial basis); 716 // where the integral is straight forward.; 717 double temp = 0.;; 718 for (int j = i; j <= degree; ++j) { // power basis",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:22922,Testability,log,log,22922,"60 double sig = std::abs(sigma);; 661 ; 662 double tmin = (mMin - m0) / sig;; 663 double tmax = (mMax - m0) / sig;; 664 ; 665 if (alpha < 0) {; 666 double tmp = tmin;; 667 tmin = -tmax;; 668 tmax = -tmp;; 669 }; 670 ; 671 double absAlpha = std::abs(alpha);; 672 ; 673 if (tmin >= -absAlpha) {; 674 result += sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(tmin / sqrt2));; 675 } else if (tmax <= -absAlpha) {; 676 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 677 double b = n / absAlpha - absAlpha;; 678 ; 679 if (useLog) {; 680 result += a * sig * (std::log(b - tmin) - std::log(b - tmax));; 681 } else {; 682 result += a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(b - tmax, n - 1.0)));; 683 }; 684 } else {; 685 double a = std::pow(n / absAlpha, n) * std::exp(-0.5 * absAlpha * absAlpha);; 686 double b = n / absAlpha - absAlpha;; 687 ; 688 double term1 = 0.0;; 689 if (useLog) {; 690 term1 = a * sig * (std::log(b - tmin) - std::log(n / absAlpha));; 691 } else {; 692 term1 = a * sig / (1.0 - n) * (1.0 / (std::pow(b - tmin, n - 1.0)) - 1.0 / (std::pow(n / absAlpha, n - 1.0)));; 693 }; 694 ; 695 double term2 = sig * sqrtPiOver2 * (approxErf(tmax / sqrt2) - approxErf(-absAlpha / sqrt2));; 696 ; 697 result += term1 + term2;; 698 }; 699 ; 700 if (result == 0); 701 return 1.E-300;; 702 return result;; 703}; 704 ; 705inline double bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs); 706{; 707 double xloScaled = (xlo - xmin) / (xmax - xmin);; 708 double xhiScaled = (xhi - xmin) / (xmax - xmin);; 709 ; 710 int degree = nCoefs - 1; // n+1 polys of degree n; 711 double norm = 0.;; 712 ; 713 for (int i = 0; i <= degree; ++i) {; 714 // for each of the i Bernstein basis polynomials; 715 // represent it in the 'power basis' (the naive polynomial basis); 716 // where the integral is straight forward.; 717 double temp = 0.;; 718 for (int j = i; j <= degree; ++j) { // power basis",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:25643,Testability,log,lognormal,25643," f(i)Definition RSha256.hxx:104; S0#define S0(x)Definition RSha256.hxx:88; S1#define S1(x)Definition RSha256.hxx:89; c#define c(i)Definition RSha256.hxx:101; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition RSha256.hxx:103; totalstatic unsigned int totalDefinition TGWin32ProxyDefs.h:40; resultOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t resultDefinition TGWin32VirtualXProxy.cxx:174; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; TMath.h; double; ROOT::Math::lognormal_pdfdouble lognormal_pdf(double x, double m, double s, double x0=0)Probability density function of the lognormal distribution.Definition PdfFuncMathCore.h:483; ROOT::Math::landau_pdfdouble landau_pdf(double x, double xi=1, double x0=0)Probability density function of the Landau distribution:Definition PdfFuncMathCore.cxx:21; ROOT::Math::inc_gamma_cdouble inc_gamma_c(double a, double x)Calculates the normalized (regularized) upper incomplete gamma function (upper integral)Definition SpecFuncMathCore.cxx:103; ROOT::Math::inc_gammadouble inc_gamma(double a, double x)Calculates the normalized (regularized) lower incomplete gamma function (lower integral)Definition SpecFuncMathCore.cxx:99; sigmaconst Double_t sigmaDefinition h1analysisProxy.h:11; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; RooFit::Detail::MathFuncs::logNormalIntegraldouble logNormalIntegral(double xMin, double xMax, double m0, double k)Definition MathFuncs.h:627; RooFit::Detail::MathFuncs::gaussianI",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:26435,Testability,log,logNormalIntegraldouble,26435,"efinition THbookFile.cxx:95; TMath.h; double; ROOT::Math::lognormal_pdfdouble lognormal_pdf(double x, double m, double s, double x0=0)Probability density function of the lognormal distribution.Definition PdfFuncMathCore.h:483; ROOT::Math::landau_pdfdouble landau_pdf(double x, double xi=1, double x0=0)Probability density function of the Landau distribution:Definition PdfFuncMathCore.cxx:21; ROOT::Math::inc_gamma_cdouble inc_gamma_c(double a, double x)Calculates the normalized (regularized) upper incomplete gamma function (upper integral)Definition SpecFuncMathCore.cxx:103; ROOT::Math::inc_gammadouble inc_gamma(double a, double x)Calculates the normalized (regularized) lower incomplete gamma function (lower integral)Definition SpecFuncMathCore.cxx:99; sigmaconst Double_t sigmaDefinition h1analysisProxy.h:11; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; RooFit::Detail::MathFuncs::logNormalIntegraldouble logNormalIntegral(double xMin, double xMax, double m0, double k)Definition MathFuncs.h:627; RooFit::Detail::MathFuncs::gaussianIntegraldouble gaussianIntegral(double xMin, double xMax, double mean, double sigma)Function to calculate the integral of an un-normalized RooGaussian over x.Definition MathFuncs.h:418; RooFit::Detail::MathFuncs::chebychevIntegraldouble chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull, double xMaxFull)Definition MathFuncs.h:513; RooFit::Detail::MathFuncs::bifurGaussIntegraldouble bifurGaussIntegral(double xMin, double xMax, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:453; RooFit::Detail::MathFuncs::cbShapedouble cbShape(double m, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:382; RooFit::Detail::MathFuncs::polynomialdouble polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:26459,Testability,log,logNormalIntegral,26459,"efinition THbookFile.cxx:95; TMath.h; double; ROOT::Math::lognormal_pdfdouble lognormal_pdf(double x, double m, double s, double x0=0)Probability density function of the lognormal distribution.Definition PdfFuncMathCore.h:483; ROOT::Math::landau_pdfdouble landau_pdf(double x, double xi=1, double x0=0)Probability density function of the Landau distribution:Definition PdfFuncMathCore.cxx:21; ROOT::Math::inc_gamma_cdouble inc_gamma_c(double a, double x)Calculates the normalized (regularized) upper incomplete gamma function (upper integral)Definition SpecFuncMathCore.cxx:103; ROOT::Math::inc_gammadouble inc_gamma(double a, double x)Calculates the normalized (regularized) lower incomplete gamma function (lower integral)Definition SpecFuncMathCore.cxx:99; sigmaconst Double_t sigmaDefinition h1analysisProxy.h:11; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; RooFit::Detail::MathFuncs::logNormalIntegraldouble logNormalIntegral(double xMin, double xMax, double m0, double k)Definition MathFuncs.h:627; RooFit::Detail::MathFuncs::gaussianIntegraldouble gaussianIntegral(double xMin, double xMax, double mean, double sigma)Function to calculate the integral of an un-normalized RooGaussian over x.Definition MathFuncs.h:418; RooFit::Detail::MathFuncs::chebychevIntegraldouble chebychevIntegral(double const *coeffs, unsigned int nCoeffs, double xMin, double xMax, double xMinFull, double xMaxFull)Definition MathFuncs.h:513; RooFit::Detail::MathFuncs::bifurGaussIntegraldouble bifurGaussIntegral(double xMin, double xMax, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:453; RooFit::Detail::MathFuncs::cbShapedouble cbShape(double m, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:382; RooFit::Detail::MathFuncs::polynomialdouble polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:28132,Testability,log,logNormalIntegralStandarddouble,28132," cbShape(double m, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:382; RooFit::Detail::MathFuncs::polynomialdouble polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:130; RooFit::Detail::MathFuncs::recursiveFractiondouble recursiveFraction(double *a, unsigned int n)Definition MathFuncs.h:371; RooFit::Detail::MathFuncs::constraintSumdouble constraintSum(double const *comp, unsigned int compSize)Definition MathFuncs.h:163; RooFit::Detail::MathFuncs::cbShapeIntegraldouble cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:649; RooFit::Detail::MathFuncs::fast_fmadouble fast_fma(double x, double y, double z) noexceptuse fast FMA if available, fall back to normal arithmetic if notDefinition MathFuncs.h:500; RooFit::Detail::MathFuncs::logNormalIntegralStandarddouble logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma)Definition MathFuncs.h:638; RooFit::Detail::MathFuncs::landaudouble landau(double x, double mu, double sigma)Definition MathFuncs.h:331; RooFit::Detail::MathFuncs::gaussiandouble gaussian(double x, double mean, double sigma)Function to evaluate an un-normalized RooGaussian.Definition MathFuncs.h:86; RooFit::Detail::MathFuncs::productdouble product(double const *factors, std::size_t nFactors)Definition MathFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int num",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:28164,Testability,log,logNormalIntegralStandard,28164," cbShape(double m, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:382; RooFit::Detail::MathFuncs::polynomialdouble polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:130; RooFit::Detail::MathFuncs::recursiveFractiondouble recursiveFraction(double *a, unsigned int n)Definition MathFuncs.h:371; RooFit::Detail::MathFuncs::constraintSumdouble constraintSum(double const *comp, unsigned int compSize)Definition MathFuncs.h:163; RooFit::Detail::MathFuncs::cbShapeIntegraldouble cbShapeIntegral(double mMin, double mMax, double m0, double sigma, double alpha, double n)Definition MathFuncs.h:649; RooFit::Detail::MathFuncs::fast_fmadouble fast_fma(double x, double y, double z) noexceptuse fast FMA if available, fall back to normal arithmetic if notDefinition MathFuncs.h:500; RooFit::Detail::MathFuncs::logNormalIntegralStandarddouble logNormalIntegralStandard(double xMin, double xMax, double mu, double sigma)Definition MathFuncs.h:638; RooFit::Detail::MathFuncs::landaudouble landau(double x, double mu, double sigma)Definition MathFuncs.h:331; RooFit::Detail::MathFuncs::gaussiandouble gaussian(double x, double mean, double sigma)Function to evaluate an un-normalized RooGaussian.Definition MathFuncs.h:86; RooFit::Detail::MathFuncs::productdouble product(double const *factors, std::size_t nFactors)Definition MathFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int num",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:29615,Testability,log,logNormalStandarddouble,29615,"athFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int numBins)Definition MathFuncs.h:172; RooFit::Detail::MathFuncs::flexibleInterpSingledouble flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal, double paramVal, double res)Definition MathFuncs.h:213; RooFit::Detail::MathFuncs::interpolate1ddouble interpolate1d(double low, double high, double val, unsigned int numBins, double const *vals)Definition MathFuncs.h:178; RooFit::Detail::MathFuncs::logNormalStandarddouble logNormalStandard(double x, double sigma, double mu)Definition MathFuncs.h:343; RooFit::Detail::MathFuncs::bifurGaussdouble bifurGauss(double x, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs)Definition MathFuncs.h:705; RooFit::Detail::MathFuncs::approxErfdouble approxErf(double arg)Definition MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonI",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:29639,Testability,log,logNormalStandard,29639,"athFuncs.h:93; RooFit::Detail::MathFuncs::chebychevdouble chebychev(double *coeffs, unsigned int nCoeffs, double x_in, double xMin, double xMax)Definition MathFuncs.h:139; RooFit::Detail::MathFuncs::poissondouble poisson(double x, double par)Definition MathFuncs.h:198; RooFit::Detail::MathFuncs::binomialdouble binomial(int n, int k)Calculates the binomial coefficient n over k.Definition MathFuncs.h:31; RooFit::Detail::MathFuncs::getUniformBinningunsigned int getUniformBinning(double low, double high, double val, unsigned int numBins)Definition MathFuncs.h:172; RooFit::Detail::MathFuncs::flexibleInterpSingledouble flexibleInterpSingle(unsigned int code, double low, double high, double boundary, double nominal, double paramVal, double res)Definition MathFuncs.h:213; RooFit::Detail::MathFuncs::interpolate1ddouble interpolate1d(double low, double high, double val, unsigned int numBins, double const *vals)Definition MathFuncs.h:178; RooFit::Detail::MathFuncs::logNormalStandarddouble logNormalStandard(double x, double sigma, double mu)Definition MathFuncs.h:343; RooFit::Detail::MathFuncs::bifurGaussdouble bifurGauss(double x, double mean, double sigmaL, double sigmaR)Definition MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs)Definition MathFuncs.h:705; RooFit::Detail::MathFuncs::approxErfdouble approxErf(double arg)Definition MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonI",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:30812,Testability,log,logNormaldouble,30812,"ion MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs)Definition MathFuncs.h:705; RooFit::Detail::MathFuncs::approxErfdouble approxErf(double arg)Definition MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative)Definition MathFuncs.h:580; RooFit::Detail::MathFuncs::logNormaldouble logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:30828,Testability,log,logNormal,30828,"ion MathFuncs.h:108; RooFit::Detail::MathFuncs::ratiodouble ratio(double numerator, double denominator)Definition MathFuncs.h:103; RooFit::Detail::MathFuncs::polynomialIntegraldouble polynomialIntegral(double const *coeffs, int nCoeffs, int lowestOrder, double xMin, double xMax)In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.Definition MathFuncs.h:481; RooFit::Detail::MathFuncs::bernsteinIntegraldouble bernsteinIntegral(double xlo, double xhi, double xmin, double xmax, double *coefs, int nCoefs)Definition MathFuncs.h:705; RooFit::Detail::MathFuncs::approxErfdouble approxErf(double arg)Definition MathFuncs.h:401; RooFit::Detail::MathFuncs::effProddouble effProd(double eff, double pdf)Definition MathFuncs.h:348; RooFit::Detail::MathFuncs::poissonIntegraldouble poissonIntegral(int code, double mu, double x, double integrandMin, double integrandMax, unsigned int protectNegative)Definition MathFuncs.h:580; RooFit::Detail::MathFuncs::logNormaldouble logNormal(double x, double k, double m0)Definition MathFuncs.h:338; RooFit::Detail::MathFuncs::nlldouble nll(double pdf, double weight, int binnedL, int doBinOffset)Definition MathFuncs.h:353; RooFit::Detail::MathFuncs::bernsteindouble bernstein(double x, double xmin, double xmax, double *coefs, int nCoefs)The caller needs to make sure that there is at least one coefficient.Definition MathFuncs.h:48; RooFit::Detail::MathFuncs::efficiencydouble efficiency(double effFuncVal, int catIndex, int sigCatIndex)Definition MathFuncs.h:117; RooFit::Detail::MathFuncs::flexibleInterpdouble flexibleInterp(unsigned int code, double const *params, unsigned int n, double const *low, double const *high, double boundary, double nominal, int doCutoff)Definition MathFuncs.h:320; RooFit::Detail::MathFuncs::exponentialIntegraldouble exponentialIntegral(double xMin, double xMax, double constant)Definition MathFuncs.h:470; RooFitThe namespace RooFit contains mostly switches that change the behaviour of functions of ",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/MathFuncs_8h_source.html:3119,Usability,simpl,simplification,3119,"a0; // c0 - 2 * c1 + c2; 69 return (a2 * xScaled + a1) * xScaled + a0;; 70 }; 71 ; 72 double t = xScaled;; 73 double s = 1. - xScaled;; 74 ; 75 double result = coefs[0] * s;; 76 for (int i = 1; i < degree; i++) {; 77 result = (result + t * binomial(degree, i) * coefs[i]) * s;; 78 t *= xScaled;; 79 }; 80 result += t * coefs[degree];; 81 ; 82 return result;; 83}; 84 ; 85/// @brief Function to evaluate an un-normalized RooGaussian.; 86inline double gaussian(double x, double mean, double sigma); 87{; 88 const double arg = x - mean;; 89 const double sig = sigma;; 90 return std::exp(-0.5 * arg * arg / (sig * sig));; 91}; 92 ; 93inline double product(double const *factors, std::size_t nFactors); 94{; 95 double out = 1.0;; 96 for (std::size_t i = 0; i < nFactors; ++i) {; 97 out *= factors[i];; 98 }; 99 return out;; 100}; 101 ; 102// RooRatio evaluate function.; 103inline double ratio(double numerator, double denominator); 104{; 105 return numerator / denominator;; 106}; 107 ; 108inline double bifurGauss(double x, double mean, double sigmaL, double sigmaR); 109{; 110 // Note: this simplification does not work with Clad as of v1.1!; 111 // return gaussian(x, mean, x < mean ? sigmaL : sigmaR);; 112 if (x < mean); 113 return gaussian(x, mean, sigmaL);; 114 return gaussian(x, mean, sigmaR);; 115}; 116 ; 117inline double efficiency(double effFuncVal, int catIndex, int sigCatIndex); 118{; 119 // Truncate efficiency function in range 0.0-1.0; 120 effFuncVal = std::clamp(effFuncVal, 0.0, 1.0);; 121 ; 122 if (catIndex == sigCatIndex); 123 return effFuncVal; // Accept case; 124 else; 125 return 1 - effFuncVal; // Reject case; 126}; 127 ; 128/// In pdfMode, a coefficient for the constant term of 1.0 is implied if lowestOrder > 0.; 129template <bool pdfMode = false>; 130inline double polynomial(double const *coeffs, int nCoeffs, int lowestOrder, double x); 131{; 132 double retVal = coeffs[nCoeffs - 1];; 133 for (int i = nCoeffs - 2; i >= 0; i--); 134 retVal = coeffs[i] + x * retVal;; 135",MatchSource.WIKI,doc/master/MathFuncs_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MathFuncs_8h_source.html
https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html:15431,Integrability,wrap,wrapper,15431,"=================; 406// unit: returns a unit vector (worse performance); 407//==============================================================================; 408template <class A, class T, unsigned int D>; 409inline VecExpr<BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T>, T, D>; 410 unit(const VecExpr<A,T,D>& lhs) {; 411 typedef BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T> DivOpBinOp;; 412 return VecExpr<DivOpBinOp,T,D>(DivOpBinOp(DivOp<T>(),lhs,Constant<T>(mag(lhs))));; 413}; 414#endif; 415 ; 416 ; 417 } // namespace Math; 418 ; 419} // namespace ROOT; 420 ; 421 ; 422 ; 423#endif /* ROOT_Math_Functions */; Expression.h; ROOT::Math::SVectorSVector: a generic fixed size Vector class.Definition SVector.h:75; ROOT::Math::SVector::UnitSVector< T, D > & Unit()transform vector into a vector of length 1Definition SVector.icc:477; ROOT::Math::SVector::applyT apply(unsigned int i) constaccess the parse tree. Index starts from zeroDefinition SVector.icc:537; ROOT::Math::VecExprExpression wrapper class for Vector objects.Definition Expression.h:64; ROOT::Math::VecExpr::applyT apply(unsigned int i) constDefinition Expression.h:77; ROOT::Math::Squareconst T Square(const T &x)square Template function to compute , for any type T returning a type TDefinition Functions.h:74; ROOT::Math::Minimumconst T Minimum(const T &lhs, const T &rhs)minimum.Definition Functions.h:100; ROOT::Math::Signint Sign(const T &x)sign.Definition Functions.h:128; ROOT::Math::Roundint Round(const T &x)round.Definition Functions.h:113; ROOT::Math::Maximumconst T Maximum(const T &lhs, const T &rhs)maximum.Definition Functions.h:86; ROOT::Math::UnitSVector< T, D > Unit(const SVector< T, D > &rhs)Unit.Definition Functions.h:382; ROOT::Math::CrossSVector< T, 3 > Cross(const SVector< T, 3 > &lhs, const SVector< T, 3 > &rhs)Vector Cross Product (only for 3-dim vectors) .Definition Functions.h:323; ROOT::Math::Lmag2T Lmag2(const SVector< T, 4 > &rhs)Lmag2: Square of Minkowski Lorentz-Vector norm (only for ",MatchSource.WIKI,doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html
https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html:13939,Performance,perform,performance,13939,"();; 384}; 385 ; 386//==============================================================================; 387// unit: returns a unit vector; 388//==============================================================================; 389template <class A, class T, unsigned int D>; 390inline SVector<T,D> Unit(const VecExpr<A,T,D>& rhs) {; 391 return SVector<T,D>(rhs).Unit();; 392}; 393 ; 394#ifdef XXX; 395//==============================================================================; 396// unit: returns a unit vector (worse performance); 397//==============================================================================; 398template <class T, unsigned int D>; 399inline VecExpr<BinaryOp<DivOp<T>, SVector<T,D>, Constant<T>, T>, T, D>; 400 unit(const SVector<T,D>& lhs) {; 401 typedef BinaryOp<DivOp<T>, SVector<T,D>, Constant<T>, T> DivOpBinOp;; 402 return VecExpr<DivOpBinOp,T,D>(DivOpBinOp(DivOp<T>(),lhs,Constant<T>(mag(lhs))));; 403}; 404 ; 405//==============================================================================; 406// unit: returns a unit vector (worse performance); 407//==============================================================================; 408template <class A, class T, unsigned int D>; 409inline VecExpr<BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T>, T, D>; 410 unit(const VecExpr<A,T,D>& lhs) {; 411 typedef BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T> DivOpBinOp;; 412 return VecExpr<DivOpBinOp,T,D>(DivOpBinOp(DivOp<T>(),lhs,Constant<T>(mag(lhs))));; 413}; 414#endif; 415 ; 416 ; 417 } // namespace Math; 418 ; 419} // namespace ROOT; 420 ; 421 ; 422 ; 423#endif /* ROOT_Math_Functions */; Expression.h; ROOT::Math::SVectorSVector: a generic fixed size Vector class.Definition SVector.h:75; ROOT::Math::SVector::UnitSVector< T, D > & Unit()transform vector into a vector of length 1Definition SVector.icc:477; ROOT::Math::SVector::applyT apply(unsigned int i) constaccess the parse tree. Index starts from zeroDefinition SVector.icc:537; ROOT::Math::VecExprE",MatchSource.WIKI,doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html
https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html:14488,Performance,perform,performance,14488,"();; 384}; 385 ; 386//==============================================================================; 387// unit: returns a unit vector; 388//==============================================================================; 389template <class A, class T, unsigned int D>; 390inline SVector<T,D> Unit(const VecExpr<A,T,D>& rhs) {; 391 return SVector<T,D>(rhs).Unit();; 392}; 393 ; 394#ifdef XXX; 395//==============================================================================; 396// unit: returns a unit vector (worse performance); 397//==============================================================================; 398template <class T, unsigned int D>; 399inline VecExpr<BinaryOp<DivOp<T>, SVector<T,D>, Constant<T>, T>, T, D>; 400 unit(const SVector<T,D>& lhs) {; 401 typedef BinaryOp<DivOp<T>, SVector<T,D>, Constant<T>, T> DivOpBinOp;; 402 return VecExpr<DivOpBinOp,T,D>(DivOpBinOp(DivOp<T>(),lhs,Constant<T>(mag(lhs))));; 403}; 404 ; 405//==============================================================================; 406// unit: returns a unit vector (worse performance); 407//==============================================================================; 408template <class A, class T, unsigned int D>; 409inline VecExpr<BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T>, T, D>; 410 unit(const VecExpr<A,T,D>& lhs) {; 411 typedef BinaryOp<DivOp<T>, VecExpr<A,T,D>, Constant<T>, T> DivOpBinOp;; 412 return VecExpr<DivOpBinOp,T,D>(DivOpBinOp(DivOp<T>(),lhs,Constant<T>(mag(lhs))));; 413}; 414#endif; 415 ; 416 ; 417 } // namespace Math; 418 ; 419} // namespace ROOT; 420 ; 421 ; 422 ; 423#endif /* ROOT_Math_Functions */; Expression.h; ROOT::Math::SVectorSVector: a generic fixed size Vector class.Definition SVector.h:75; ROOT::Math::SVector::UnitSVector< T, D > & Unit()transform vector into a vector of length 1Definition SVector.icc:477; ROOT::Math::SVector::applyT apply(unsigned int i) constaccess the parse tree. Index starts from zeroDefinition SVector.icc:537; ROOT::Math::VecExprE",MatchSource.WIKI,doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/math_2smatrix_2inc_2Math_2Functions_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:2745,Availability,error,error,2745," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:3012,Availability,error,errors,3012," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:3083,Availability,error,error,3083," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:4346,Availability,error,error,4346,"with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math; 140 ; 141} // end namespace ROOT; 142 ; 143 ; 144 ; 145 ; 146 ; 147#endif /* ROOT_Math_Math */; M_PI#define M_PIDefinition Rotated.cxx:105; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::log1pdouble log1p(double x)declarations for functions which are not implemented by some compilersDefinition Math.h:98; ROOT::Math::Pidouble Pi()Mathematical constants.Definition Math.h:88; ROOT::Math::expm1double expm1(double x)exp(x) -1 with error cancellation when x is smallDefinition Math.h:110; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4; sumstatic uint64_t sum(uint64_t i)Definition Factory.cxx:2345. mathmathcoreincMathMath.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:1929,Integrability,depend,dependent,1929,"e <limits>; 31#endif; 32 ; 33 ; 34#ifndef M_PI; 35 ; 36#define M_PI 3.14159265358979323846264338328 // Pi; 37#endif; 38 ; 39#ifndef M_PI_2; 40#define M_PI_2 1.57079632679489661923132169164 // Pi/2; 41#endif; 42 ; 43#ifndef M_PI_4; 44#define M_PI_4 0.78539816339744830961566084582 // Pi/4; 45#endif; 46 ; 47/**; 48 \namespace ROOT; 49 Namespace for new ROOT classes and functions; 50 */; 51 ; 52namespace ROOT {; 53 ; 54/**; 55\namespace Math; 56Namespace for new Math classes and functions.; 57See the \ref Math ""Math Libraries"" page for a detailed description.; 58*/; 59 ; 60namespace Math {; 61// Enable Vc/VecCore template instantiations to replace std math functions.; 62//; 63// Vc declares `std::sqrt(Vc-type)`. To use this for Vc-`SCALAR`s, the call; 64// to `sqrt()` must only be resolved at the template instantiation time, when; 65// the Vc headers are guaranteed to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 10",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:2426,Testability,log,log,2426," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:2731,Testability,log,log,2731," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/Math_8h_source.html:2978,Testability,log,log,2978," to be included, and thus its `sqrt()`; 66// overloads have been declared.; 67// The trick is to keep sqrt() dependent (on its argument type) by making it; 68// an unqualified name. The `std::` of `std::sqrt()` makes it a qualified; 69// name, so the code here has to use `sqrt()`, not `std::sqrt()`. To still; 70// find `std::sqrt()` we pull `std::sqrt()` into the surrounding namespace.; 71//; 72// We don't want to use 'using namespace std' because it would pollute the including headers.; 73using std::atan2;; 74using std::cos;; 75using std::cosh;; 76using std::exp;; 77using std::floor;; 78using std::log;; 79using std::pow;; 80using std::sin;; 81using std::sinh;; 82using std::sqrt;; 83using std::tan;; 84 ; 85/**; 86 Mathematical constants; 87*/; 88inline double Pi(); 89{; 90 return M_PI;; 91 }; 92 ; 93 /**; 94 declarations for functions which are not implemented by some compilers; 95 */; 96 ; 97 /// log(1+x) with error cancelation when x is small; 98 inline double log1p(double x); 99 {; 100#ifndef HAVE_NO_LOG1P; 101 return ::log1p(x);; 102#else; 103 // if log1p is not in c math library; 104 volatile double y;; 105 y = 1 + x;; 106 return std::log(y) - ((y-1)-x)/y ; /* cancels errors with IEEE arithmetic */; 107#endif; 108}; 109/// exp(x) -1 with error cancellation when x is small; 110inline double expm1( double x) {; 111#ifndef HAVE_NO_EXPM1; 112 return ::expm1(x);; 113#else; 114 // compute using taylor expansion until difference is less than epsilon; 115 // use for values smaller than 0.5 (for larger (exp(x)-1 is fine; 116 if (std::abs(x) < 0.5); 117 {; 118 // taylor series S = x + (1/2!) x^2 + (1/3!) x^3 + ...; 119 ; 120 double i = 1.0;; 121 double sum = x;; 122 double term = x / 1.0;; 123 do {; 124 i++ ;; 125 term *= x/i;; 126 sum += term;; 127 }; 128 while (std::abs(term) > std::abs(sum) * std::numeric_limits<double>::epsilon() ) ;; 129 ; 130 return sum ;; 131 }; 132 else; 133 {; 134 return std::exp(x) - 1;; 135 }; 136#endif; 137}; 138 ; 139 } // end namespace Math;",MatchSource.WIKI,doc/master/Math_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Math_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:43193,Integrability,rout,routines,43193,"e user wants to pass const objects need to copy the matrices; 983// It will work only for symmetric matrices; 984template <class T, unsigned int D>; 985bool SolveChol( SMatrix<T, D, D, MatRepSym<T, D> > & mat, SVector<T, D> & vec ) {; 986 CholeskyDecomp<T, D> decomp(mat);; 987 return decomp.Solve(vec);; 988}; 989 ; 990/// same function as before but not overwriting the matrix and returning a copy of the vector; 991/// (this is the slow version); 992template <class T, unsigned int D>; 993SVector<T,D> SolveChol( const SMatrix<T, D, D, MatRepSym<T, D> > & mat, const SVector<T, D> & vec, int & ifail ) {; 994 SMatrix<T, D, D, MatRepSym<T, D> > atmp(mat);; 995 SVector<T,D> vret(vec);; 996 bool ok = SolveChol( atmp, vret);; 997 ifail = (ok) ? 0 : -1;; 998 return vret;; 999}; 1000 ; 1001 ; 1002 ; 1003 } // namespace Math; 1004 ; 1005} // namespace ROOT; 1006 ; 1007 ; 1008#endif /* ROOT_Math_MatrixFunctions */; BinaryOpPolicy.h; CholeskyDecomp.hheader file containing the templated implementation of matrix inversion routines for use with ROOT's ...; Expression.h; HelperOps.h; Productstatic Double_t Product(const Double_t *x, const Float_t *y)Product.Definition TCTUB.cxx:101; pwinID h TVirtualViewer3D TVirtualGLPainter pDefinition TGWin32VirtualGLProxy.cxx:51; offsetOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h offsetDefinition TGWin32V",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:44526,Integrability,wrap,wrapper,44526,"LineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h offsetDefinition TGWin32VirtualXProxy.cxx:245; ROOT::Math::CholeskyDecompclass to compute the Cholesky decomposition of a matrixDefinition CholeskyDecomp.h:77; ROOT::Math::CholeskyDecomp::Solvebool Solve(V &rhs) constsolves a linear system for the given right hand sideDefinition CholeskyDecomp.h:136; ROOT::Math::ExprDefinition Expression.h:138; ROOT::Math::MatRepStdExpression wrapper class for Matrix objects.Definition MatrixRepresentationsStatic.h:54; ROOT::Math::MatRepSymMatRepSym Matrix storage representation for a symmetric matrix of dimension NxN This class is a templ...Definition MatrixRepresentationsStatic.h:213; ROOT::Math::MatrixMulOpClass for Matrix-Matrix multiplication.Definition MatrixFunctions.h:348; ROOT::Math::MatrixMulOp::rhs_const MatrixB & rhs_Definition MatrixFunctions.h:373; ROOT::Math::MatrixMulOp::IsInUsebool IsInUse(const T *p) constDefinition MatrixFunctions.h:366; ROOT::Math::MatrixMulOp::MatrixMulOpMatrixMulOp(const MatrixA &lhs, const MatrixB &rhs)Definition MatrixFunctions.h:351; ROOT::Math::MatrixMulOp::~MatrixMulOp~MatrixMulOp()Definition MatrixFunctions.h:355; ROOT::Math::MatrixMulOp::operator()T operator()(unsigned int i, unsigned j) constDefinition MatrixFunctions.h:362; ROOT::Math::MatrixMulOp::applyT apply(unsigned int i) constcalcDefinition MatrixFunctions.h:358; ROOT::Math::MatrixMulOp::lhs_const MatrixA & lhs_Defini",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:47361,Integrability,wrap,wrapper,47361,"840; ROOT::Math::TensorMulOp::applyVector1::value_type apply(unsigned int i) constVector2::kSize is the number of columns in the resulting matrix.Definition MatrixFunctions.h:848; ROOT::Math::TensorMulOp::operator()Vector1::value_type operator()(unsigned int i, unsigned j) constDefinition MatrixFunctions.h:851; ROOT::Math::TransposeOpClass for Transpose Operations.Definition MatrixFunctions.h:503; ROOT::Math::TransposeOp::operator()T operator()(unsigned int i, unsigned j) constDefinition MatrixFunctions.h:516; ROOT::Math::TransposeOp::applyT apply(unsigned int i) constDefinition MatrixFunctions.h:513; ROOT::Math::TransposeOp::~TransposeOp~TransposeOp()Definition MatrixFunctions.h:510; ROOT::Math::TransposeOp::IsInUsebool IsInUse(const T *p) constDefinition MatrixFunctions.h:520; ROOT::Math::TransposeOp::TransposeOpTransposeOp(const Matrix &rhs)Definition MatrixFunctions.h:506; ROOT::Math::TransposeOp::rhs_const Matrix & rhs_Definition MatrixFunctions.h:525; ROOT::Math::VecExprExpression wrapper class for Vector objects.Definition Expression.h:64; ROOT::Math::VectorMatrixColOpClass for Vector-Matrix multiplication.Definition MatrixFunctions.h:174; ROOT::Math::VectorMatrixColOp::lhs_const Vector & lhs_Definition MatrixFunctions.h:198; ROOT::Math::VectorMatrixColOp::TVector::value_type TDefinition MatrixFunctions.h:177; ROOT::Math::VectorMatrixColOp::~VectorMatrixColOp~VectorMatrixColOp()Definition MatrixFunctions.h:183; ROOT::Math::VectorMatrixColOp::VectorMatrixColOpVectorMatrixColOp(const Vector &lhs, const Matrix &rhs)Definition MatrixFunctions.h:179; ROOT::Math::VectorMatrixColOp::rhs_const Matrix & rhs_Definition MatrixFunctions.h:199; ROOT::Math::VectorMatrixColOp::IsInUsebool IsInUse(const T *p) constDefinition MatrixFunctions.h:192; ROOT::Math::VectorMatrixColOp::applyMatrix::value_type apply(unsigned int i) constcalcDefinition MatrixFunctions.h:186; ROOT::Math::VectorMatrixRowOpDefinition MatrixFunctions.h:109; ROOT::Math::VectorMatrixRowOp::VectorMatrixRowOp",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:2499,Performance,perform,performances,2499,"2 namespace Math {; 53 ; 54 template <class T, unsigned int D>; 55 class SVector;; 56 ; 57#ifdef XXX; 58//==============================================================================; 59// SMatrix * SVector; 60//==============================================================================; 61template <class T, unsigned int D1, unsigned int D2, class R>; 62SVector<T,D1> operator*(const SMatrix<T,D1,D2,R>& rhs, const SVector<T,D2>& lhs); 63{; 64 SVector<T,D1> tmp;; 65 for(unsigned int i=0; i<D1; ++i) {; 66 const unsigned int rpos = i*D2;; 67 for(unsigned int j=0; j<D2; ++j) {; 68 tmp[i] += rhs.apply(rpos+j) * lhs.apply(j);; 69 }; 70 }; 71 return tmp;; 72}; 73#endif; 74 ; 75 ; 76// matrix-vector product:; 77// use apply(i) function for matrices. Tested (11/05/06) with using (i,j) but; 78// performances are slightly worse (not clear why); 79 ; 80//==============================================================================; 81// meta_row_dot; 82//==============================================================================; 83template <unsigned int I>; 84struct meta_row_dot {; 85 template <class A, class B>; 86 static inline typename A::value_type f(const A& lhs, const B& rhs,; 87 const unsigned int offset) {; 88 return lhs.apply(offset+I) * rhs.apply(I) + meta_row_dot<I-1>::f(lhs,rhs,offset);; 89 }; 90};; 91 ; 92 ; 93//==============================================================================; 94// meta_row_dot<0>; 95//==============================================================================; 96template <>; 97struct meta_row_dot<0> {; 98 template <class A, class B>; 99 static inline typename A::value_type f(const A& lhs, const B& rhs,; 100 const unsigned int offset) {; 101 return lhs.apply(offset) * rhs.apply(0);; 102 }; 103};; 104 ; 105//==============================================================================; 106// VectorMatrixRowOp; 107//==============================================================================; 108template <class Matrix, cla",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:2536,Usability,clear,clear,2536,"2 namespace Math {; 53 ; 54 template <class T, unsigned int D>; 55 class SVector;; 56 ; 57#ifdef XXX; 58//==============================================================================; 59// SMatrix * SVector; 60//==============================================================================; 61template <class T, unsigned int D1, unsigned int D2, class R>; 62SVector<T,D1> operator*(const SMatrix<T,D1,D2,R>& rhs, const SVector<T,D2>& lhs); 63{; 64 SVector<T,D1> tmp;; 65 for(unsigned int i=0; i<D1; ++i) {; 66 const unsigned int rpos = i*D2;; 67 for(unsigned int j=0; j<D2; ++j) {; 68 tmp[i] += rhs.apply(rpos+j) * lhs.apply(j);; 69 }; 70 }; 71 return tmp;; 72}; 73#endif; 74 ; 75 ; 76// matrix-vector product:; 77// use apply(i) function for matrices. Tested (11/05/06) with using (i,j) but; 78// performances are slightly worse (not clear why); 79 ; 80//==============================================================================; 81// meta_row_dot; 82//==============================================================================; 83template <unsigned int I>; 84struct meta_row_dot {; 85 template <class A, class B>; 86 static inline typename A::value_type f(const A& lhs, const B& rhs,; 87 const unsigned int offset) {; 88 return lhs.apply(offset+I) * rhs.apply(I) + meta_row_dot<I-1>::f(lhs,rhs,offset);; 89 }; 90};; 91 ; 92 ; 93//==============================================================================; 94// meta_row_dot<0>; 95//==============================================================================; 96template <>; 97struct meta_row_dot<0> {; 98 template <class A, class B>; 99 static inline typename A::value_type f(const A& lhs, const B& rhs,; 100 const unsigned int offset) {; 101 return lhs.apply(offset) * rhs.apply(0);; 102 }; 103};; 104 ; 105//==============================================================================; 106// VectorMatrixRowOp; 107//==============================================================================; 108template <class Matrix, cla",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/MatrixFunctions_8h_source.html:22835,Usability,clear,clear,22835,"p<SMatrix<T,D1,D2,R>,T,D1,D2>, T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>; 540 Transpose(const SMatrix<T,D1,D2, R>& rhs) {; 541 typedef TransposeOp<SMatrix<T,D1,D2,R>,T,D1,D2> MatTrOp;; 542 ; 543 return Expr<MatTrOp, T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>(MatTrOp(rhs));; 544}; 545 ; 546//==============================================================================; 547// transpose; 548//==============================================================================; 549template <class A, class T, unsigned int D1, unsigned int D2, class R>; 550inline Expr<TransposeOp<Expr<A,T,D1,D2,R>,T,D1,D2>, T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>; 551 Transpose(const Expr<A,T,D1,D2,R>& rhs) {; 552 typedef TransposeOp<Expr<A,T,D1,D2,R>,T,D1,D2> MatTrOp;; 553 ; 554 return Expr<MatTrOp, T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>(MatTrOp(rhs));; 555}; 556 ; 557 ; 558#ifdef ENABLE_TEMPORARIES_TRANSPOSE; 559// sometimes is faster to create a temp, not clear why; 560 ; 561//==============================================================================; 562// transpose; 563//==============================================================================; 564template <class T, unsigned int D1, unsigned int D2, class R>; 565inline SMatrix< T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>; 566 Transpose(const SMatrix<T,D1,D2, R>& rhs) {; 567 typedef TransposeOp<SMatrix<T,D1,D2,R>,T,D1,D2> MatTrOp;; 568 ; 569 return SMatrix< T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>; 570 ( Expr<MatTrOp, T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>(MatTrOp(rhs)) );; 571}; 572 ; 573//==============================================================================; 574// transpose; 575//==============================================================================; 576template <class A, class T, unsigned int D1, unsigned int D2, class R>; 577inline SMatrix< T, D2, D1, typename TranspPolicy<T,D1,D2,R>::RepType>; 578 Transpose(const Expr<A,",MatchSource.WIKI,doc/master/MatrixFunctions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MatrixFunctions_8h_source.html
https://root.cern/doc/master/md5_8inl.html:250,Integrability,depend,dependency,250,". ROOT: net/http/civetweb/md5.inl File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Macros |; Typedefs |; Functions ; md5.inl File Reference. #include <stdint.h>; #include <string.h>. Include dependency graph for md5.inl:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; struct  md5_state_s;  . Macros; #define BYTE_ORDER   (0);  ; #define F(x, y, z)   (((x) & (y)) | (~(x) & (z)));  ; #define G(x, y, z)   (((x) & (z)) | ((y) & ~(z)));  ; #define H(x, y, z)   ((x) ^ (y) ^ (z));  ; #define I(x, y, z)   ((y) ^ ((x) | ~(z)));  ; #define md5_INCLUDED;  ; #define ROTATE_LEFT(x, n)   (((x) << (n)) | ((x) >> (32 - (n))));  ; #define SET(a, b, c, d, k, s, Ti);  ; #define SET(a, b, c, d, k, s, Ti);  ; #define SET(a, b, c, d, k, s, Ti);  ; #define SET(a, b, c, d, k, s, Ti);  ; #define T1   /* 0xd76aa478 */ (T_MASK ^ 0x28955b87);  ; #define T10   /* 0x8b44f7af */ (T_MASK ^ 0x74bb0850);  ; #define T11   /* 0xffff5bb1 */ (T_MASK ^ 0x0000a44e);  ; #define T12   /* 0x895cd7be */ (T_MASK ^ 0x76a32841);  ; #define T13   (0x6b901122);  ; #define T14   /* 0xfd987193 */ (T_MASK ^ 0x02678e6c);  ; #define T15   /* 0xa679438e */ (T_MASK ^ 0x5986bc71);  ; #define T16   (0x49b40821);  ; #define T17   /* 0xf61e2562 */ (T_MASK ^ 0x09e1da9d);  ; #define T18   /* 0xc040b340 */ (T_MASK ^ 0x3fbf4cbf);  ; #define T19   (0x265e5a51);  ; #define T2   /* 0xe8c7b756 */ (T_MASK ^ 0x173848a9);  ; #define T20   /* 0xe9b6c7aa */ (T_MASK ^ 0x16493855);  ; #define T21   /* 0xd62f105d */ (T_MASK ^ 0x29d0efa2);  ; #define T22   (0x02441453);  ; #define T23   /* 0xd8a1e681 */ (T_MASK ^ 0x275e197e);  ; #define T24   /* 0xe7d3fbc8 */ (T_MASK ^ 0x182c0437);  ; #define T25   (0x21e1cde6);  ; #define T26   /* 0xc33707d6 */ (T_MASK ^ 0x3cc8f829);  ; #define T27   ",MatchSource.WIKI,doc/master/md5_8inl.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl.html
https://root.cern/doc/master/md5_8inl_source.html:639,Availability,avail,available,639,". ROOT: net/http/civetweb/md5.inl Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. md5.inl. Go to the documentation of this file. 1/*; 2 * This an amalgamation of md5.c and md5.h into a single file; 3 * with all static declaration to reduce linker conflicts; 4 * in Civetweb.; 5 *; 6 * The MD5_STATIC declaration was added to facilitate static; 7 * inclusion.; 8 * No Face Press, LLC; 9 */; 10 ; 11/* $Id: md5.h,v 1.4 2002/04/13 19:20:28 lpd Exp $ */; 12/*; 13 Independent implementation of MD5 (RFC 1321).; 14 ; 15 This code implements the MD5 Algorithm defined in RFC 1321, whose; 16 text is available at; 17 http://www.ietf.org/rfc/rfc1321.txt; 18 The code is derived from the text of the RFC, including the test suite; 19 (section A.5) but excluding the rest of Appendix A. It does not include; 20 any code or documentation that is identified in the RFC as being; 21 copyrighted.; 22 ; 23 The original and principal author of md5.h is L. Peter Deutsch; 24 <ghost@aladdin.com>. Other authors are noted in the change history; 25 that follows (in reverse chronological order):; 26 ; 27 2002-04-13 lpd Removed support for non-ANSI compilers; removed; 28 references to Ghostscript; clarified derivation from RFC 1321;; 29 now handles byte order either statically or dynamically.; 30 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 31 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5);; 32 added conditionalization for C++ compilation from Martin; 33 Purschke <purschke@bnl.gov>.; 34 1999-05-03 lpd Original version.; 35 */; 36 ; 37#if !defined(md5_INCLUDED); 38#define md5_INCLUDED; 39 ; 40/*; 41 * This package supports both compile-time and run-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; ",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:4334,Availability,avail,available,4334,"e authors be held liable for any damages; 85 arising from the use of this software.; 86 ; 87 Permission is granted to anyone to use this software for any purpose,; 88 including commercial applications, and to alter it and redistribute it; 89 freely, subject to the following restrictions:; 90 ; 91 1. The origin of this software must not be misrepresented; you must not; 92 claim that you wrote the original software. If you use this software; 93 in a product, an acknowledgment in the product documentation would be; 94 appreciated but is not required.; 95 2. Altered source versions must be plainly marked as such, and must not be; 96 misrepresented as being the original software.; 97 3. This notice may not be removed or altered from any source distribution.; 98 ; 99 L. Peter Deutsch; 100 ghost@aladdin.com; 101 ; 102 */; 103/* $Id: md5.c,v 1.6 2002/04/13 19:20:28 lpd Exp $ */; 104/*; 105 Independent implementation of MD5 (RFC 1321).; 106 ; 107 This code implements the MD5 Algorithm defined in RFC 1321, whose; 108 text is available at; 109 http://www.ietf.org/rfc/rfc1321.txt; 110 The code is derived from the text of the RFC, including the test suite; 111 (section A.5) but excluding the rest of Appendix A. It does not include; 112 any code or documentation that is identified in the RFC as being; 113 copyrighted.; 114 ; 115 The original and principal author of md5.c is L. Peter Deutsch; 116 <ghost@aladdin.com>. Other authors are noted in the change history; 117 that follows (in reverse chronological order):; 118 ; 119 2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order; 120 either statically or dynamically; added missing #include <string.h>; 121 in library.; 122 2002-03-11 lpd Corrected argument list for main(), and added int return; 123 type, in test program and T value program.; 124 2002-02-21 lpd Added missing #include <stdio.h> in test program.; 125 2000-07-03 lpd Patched to eliminate warnings about ""constant is; 126 unsigned in ANSI C, signed in tr",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:279,Energy Efficiency,reduce,reduce,279,". ROOT: net/http/civetweb/md5.inl Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. md5.inl. Go to the documentation of this file. 1/*; 2 * This an amalgamation of md5.c and md5.h into a single file; 3 * with all static declaration to reduce linker conflicts; 4 * in Civetweb.; 5 *; 6 * The MD5_STATIC declaration was added to facilitate static; 7 * inclusion.; 8 * No Face Press, LLC; 9 */; 10 ; 11/* $Id: md5.h,v 1.4 2002/04/13 19:20:28 lpd Exp $ */; 12/*; 13 Independent implementation of MD5 (RFC 1321).; 14 ; 15 This code implements the MD5 Algorithm defined in RFC 1321, whose; 16 text is available at; 17 http://www.ietf.org/rfc/rfc1321.txt; 18 The code is derived from the text of the RFC, including the test suite; 19 (section A.5) but excluding the rest of Appendix A. It does not include; 20 any code or documentation that is identified in the RFC as being; 21 copyrighted.; 22 ; 23 The original and principal author of md5.h is L. Peter Deutsch; 24 <ghost@aladdin.com>. Other authors are noted in the change history; 25 that follows (in reverse chronological order):; 26 ; 27 2002-04-13 lpd Removed support for non-ANSI compilers; removed; 28 references to Ghostscript; clarified derivation from RFC 1321;; 29 now handles byte order either statically or dynamically.; 30 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 31 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5);; 32 added conditionalization for C++ compilation from Martin; 33 Purschke <purschke@bnl.gov>.; 34 1999-05-03 lpd Original version.; 35 */; 36 ; 37#if !defined(md5_INCLUDED); 38#define md5_INCLUDED; 39 ; 40/*; 41 * This package supports both compile-time and run-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; ",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:2149,Energy Efficiency,efficient,efficiently,2149,"utsch; 24 <ghost@aladdin.com>. Other authors are noted in the change history; 25 that follows (in reverse chronological order):; 26 ; 27 2002-04-13 lpd Removed support for non-ANSI compilers; removed; 28 references to Ghostscript; clarified derivation from RFC 1321;; 29 now handles byte order either statically or dynamically.; 30 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 31 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5);; 32 added conditionalization for C++ compilation from Martin; 33 Purschke <purschke@bnl.gov>.; 34 1999-05-03 lpd Original version.; 35 */; 36 ; 37#if !defined(md5_INCLUDED); 38#define md5_INCLUDED; 39 ; 40/*; 41 * This package supports both compile-time and run-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; if ARCH_IS_BIG_ENDIAN is not defined, the code will be compiled to; 46 * run on either big- or little-endian CPUs, but will run slightly less; 47 * efficiently on either one than if ARCH_IS_BIG_ENDIAN is defined.; 48 */; 49 ; 50typedef unsigned char md5_byte_t; /* 8-bit byte */; 51typedef unsigned int md5_word_t; /* 32-bit word */; 52 ; 53/* Define the state of the MD5 Algorithm. */; 54typedef struct md5_state_s {; 55 md5_word_t count[2]; /* message length in bits, lsw first */; 56 md5_word_t abcd[4]; /* digest buffer */; 57 md5_byte_t buf[64]; /* accumulate block */; 58} md5_state_t;; 59 ; 60#if defined(__cplusplus); 61extern ""C"" {; 62#endif; 63 ; 64/* Initialize the algorithm. */; 65MD5_STATIC void md5_init(md5_state_t *pms);; 66 ; 67/* Append a string to the message. */; 68MD5_STATIC void; 69md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes);; 70 ; 71/* Finish the message and return the digest. */; 72MD5_STATIC void md5_finish(md5_state_t *pms, md5_byte_t diges",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:9502,Energy Efficiency,efficient,efficient,9502,"K ^ 0x70f3336d); 200#define T55 /* 0xffeff47d */ (T_MASK ^ 0x00100b82); 201#define T56 /* 0x85845dd1 */ (T_MASK ^ 0x7a7ba22e); 202#define T57 (0x6fa87e4f); 203#define T58 /* 0xfe2ce6e0 */ (T_MASK ^ 0x01d3191f); 204#define T59 /* 0xa3014314 */ (T_MASK ^ 0x5cfebceb); 205#define T60 (0x4e0811a1); 206#define T61 /* 0xf7537e82 */ (T_MASK ^ 0x08ac817d); 207#define T62 /* 0xbd3af235 */ (T_MASK ^ 0x42c50dca); 208#define T63 (0x2ad7d2bb); 209#define T64 /* 0xeb86d391 */ (T_MASK ^ 0x14792c6e); 210 ; 211static void; 212md5_process(md5_state_t *pms, const md5_byte_t *data /*[64]*/); 213{; 214 md5_word_t a = pms->abcd[0], b = pms->abcd[1], c = pms->abcd[2],; 215 d = pms->abcd[3];; 216 md5_word_t t;; 217#if BYTE_ORDER > 0; 218 /* Define storage only for big-endian CPUs. */; 219 md5_word_t X[16];; 220#else; 221 /* Define storage for little-endian or both types of CPUs. */; 222 md5_word_t xbuf[16];; 223 const md5_word_t *X;; 224#endif; 225 ; 226 {; 227#if BYTE_ORDER == 0; 228 /*; 229 * Determine dynamically whether this is a big-endian or; 230 * little-endian machine, since we can use a more efficient; 231 * algorithm on the latter.; 232 */; 233 static const int w = 1;; 234 ; 235 if (*((const md5_byte_t *)&w)) /* dynamic little-endian */; 236#endif; 237#if BYTE_ORDER <= 0 /* little-endian */; 238 {; 239 /*; 240 * On little-endian machines, we can process properly aligned; 241 * data without copying it.; 242 */; 243 if (!(((uintptr_t) data) & 3)) {; 244 /* data are properly aligned, a direct assignment is possible */; 245 /* cast through a (void *) should avoid a compiler warning,; 246 see; 247 https://github.com/bel2125/civetweb/issues/94#issuecomment-98112861; 248 */; 249 X = (const md5_word_t *)(const void *)data;; 250 } else {; 251 /* not aligned */; 252 memcpy(xbuf, data, 64);; 253 X = xbuf;; 254 }; 255 }; 256#endif; 257#if BYTE_ORDER == 0; 258 else /* dynamic big-endian */; 259#endif; 260#if BYTE_ORDER >= 0 /* big-endian */; 261 {; 262 /*; 263 * On big-endian machines, we must",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:2447,Integrability,message,message,2447,"33 Purschke <purschke@bnl.gov>.; 34 1999-05-03 lpd Original version.; 35 */; 36 ; 37#if !defined(md5_INCLUDED); 38#define md5_INCLUDED; 39 ; 40/*; 41 * This package supports both compile-time and run-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; if ARCH_IS_BIG_ENDIAN is not defined, the code will be compiled to; 46 * run on either big- or little-endian CPUs, but will run slightly less; 47 * efficiently on either one than if ARCH_IS_BIG_ENDIAN is defined.; 48 */; 49 ; 50typedef unsigned char md5_byte_t; /* 8-bit byte */; 51typedef unsigned int md5_word_t; /* 32-bit word */; 52 ; 53/* Define the state of the MD5 Algorithm. */; 54typedef struct md5_state_s {; 55 md5_word_t count[2]; /* message length in bits, lsw first */; 56 md5_word_t abcd[4]; /* digest buffer */; 57 md5_byte_t buf[64]; /* accumulate block */; 58} md5_state_t;; 59 ; 60#if defined(__cplusplus); 61extern ""C"" {; 62#endif; 63 ; 64/* Initialize the algorithm. */; 65MD5_STATIC void md5_init(md5_state_t *pms);; 66 ; 67/* Append a string to the message. */; 68MD5_STATIC void; 69md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes);; 70 ; 71/* Finish the message and return the digest. */; 72MD5_STATIC void md5_finish(md5_state_t *pms, md5_byte_t digest[16]);; 73 ; 74#if defined(__cplusplus); 75} /* end extern ""C"" */; 76#endif; 77 ; 78#endif /* md5_INCLUDED */; 79 ; 80/*; 81 Copyright (C) 1999, 2000, 2002 Aladdin Enterprises. All rights reserved.; 82 ; 83 This software is provided 'as-is', without any express or implied; 84 warranty. In no event will the authors be held liable for any damages; 85 arising from the use of this software.; 86 ; 87 Permission is granted to anyone to use this software for any purpose,; 88 including commercial applications, and to alter it and redistribut",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:2773,Integrability,message,message,2773,"-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; if ARCH_IS_BIG_ENDIAN is not defined, the code will be compiled to; 46 * run on either big- or little-endian CPUs, but will run slightly less; 47 * efficiently on either one than if ARCH_IS_BIG_ENDIAN is defined.; 48 */; 49 ; 50typedef unsigned char md5_byte_t; /* 8-bit byte */; 51typedef unsigned int md5_word_t; /* 32-bit word */; 52 ; 53/* Define the state of the MD5 Algorithm. */; 54typedef struct md5_state_s {; 55 md5_word_t count[2]; /* message length in bits, lsw first */; 56 md5_word_t abcd[4]; /* digest buffer */; 57 md5_byte_t buf[64]; /* accumulate block */; 58} md5_state_t;; 59 ; 60#if defined(__cplusplus); 61extern ""C"" {; 62#endif; 63 ; 64/* Initialize the algorithm. */; 65MD5_STATIC void md5_init(md5_state_t *pms);; 66 ; 67/* Append a string to the message. */; 68MD5_STATIC void; 69md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes);; 70 ; 71/* Finish the message and return the digest. */; 72MD5_STATIC void md5_finish(md5_state_t *pms, md5_byte_t digest[16]);; 73 ; 74#if defined(__cplusplus); 75} /* end extern ""C"" */; 76#endif; 77 ; 78#endif /* md5_INCLUDED */; 79 ; 80/*; 81 Copyright (C) 1999, 2000, 2002 Aladdin Enterprises. All rights reserved.; 82 ; 83 This software is provided 'as-is', without any express or implied; 84 warranty. In no event will the authors be held liable for any damages; 85 arising from the use of this software.; 86 ; 87 Permission is granted to anyone to use this software for any purpose,; 88 including commercial applications, and to alter it and redistribute it; 89 freely, subject to the following restrictions:; 90 ; 91 1. The origin of this software must not be misrepresented; you must not; 92 claim that you wrote the original software. If you use thi",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:2898,Integrability,message,message,2898,"un only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; if ARCH_IS_BIG_ENDIAN is not defined, the code will be compiled to; 46 * run on either big- or little-endian CPUs, but will run slightly less; 47 * efficiently on either one than if ARCH_IS_BIG_ENDIAN is defined.; 48 */; 49 ; 50typedef unsigned char md5_byte_t; /* 8-bit byte */; 51typedef unsigned int md5_word_t; /* 32-bit word */; 52 ; 53/* Define the state of the MD5 Algorithm. */; 54typedef struct md5_state_s {; 55 md5_word_t count[2]; /* message length in bits, lsw first */; 56 md5_word_t abcd[4]; /* digest buffer */; 57 md5_byte_t buf[64]; /* accumulate block */; 58} md5_state_t;; 59 ; 60#if defined(__cplusplus); 61extern ""C"" {; 62#endif; 63 ; 64/* Initialize the algorithm. */; 65MD5_STATIC void md5_init(md5_state_t *pms);; 66 ; 67/* Append a string to the message. */; 68MD5_STATIC void; 69md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes);; 70 ; 71/* Finish the message and return the digest. */; 72MD5_STATIC void md5_finish(md5_state_t *pms, md5_byte_t digest[16]);; 73 ; 74#if defined(__cplusplus); 75} /* end extern ""C"" */; 76#endif; 77 ; 78#endif /* md5_INCLUDED */; 79 ; 80/*; 81 Copyright (C) 1999, 2000, 2002 Aladdin Enterprises. All rights reserved.; 82 ; 83 This software is provided 'as-is', without any express or implied; 84 warranty. In no event will the authors be held liable for any damages; 85 arising from the use of this software.; 86 ; 87 Permission is granted to anyone to use this software for any purpose,; 88 including commercial applications, and to alter it and redistribute it; 89 freely, subject to the following restrictions:; 90 ; 91 1. The origin of this software must not be misrepresented; you must not; 92 claim that you wrote the original software. If you use this software; 93 in a product, an acknowledgment in the product documentation would be; 94 appreciated but is not required",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:15362,Integrability,message,message,15362," 14, 15, T51);; 377 SET(b, c, d, a, 5, 21, T52);; 378 SET(a, b, c, d, 12, 6, T53);; 379 SET(d, a, b, c, 3, 10, T54);; 380 SET(c, d, a, b, 10, 15, T55);; 381 SET(b, c, d, a, 1, 21, T56);; 382 SET(a, b, c, d, 8, 6, T57);; 383 SET(d, a, b, c, 15, 10, T58);; 384 SET(c, d, a, b, 6, 15, T59);; 385 SET(b, c, d, a, 13, 21, T60);; 386 SET(a, b, c, d, 4, 6, T61);; 387 SET(d, a, b, c, 11, 10, T62);; 388 SET(c, d, a, b, 2, 15, T63);; 389 SET(b, c, d, a, 9, 21, T64);; 390#undef SET; 391 ; 392 /* Then perform the following additions. (That is increment each; 393 of the four registers by the value it had before this block; 394 was started.) */; 395 pms->abcd[0] += a;; 396 pms->abcd[1] += b;; 397 pms->abcd[2] += c;; 398 pms->abcd[3] += d;; 399}; 400 ; 401MD5_STATIC void; 402md5_init(md5_state_t *pms); 403{; 404 pms->count[0] = pms->count[1] = 0;; 405 pms->abcd[0] = 0x67452301;; 406 pms->abcd[1] = /*0xefcdab89*/ T_MASK ^ 0x10325476;; 407 pms->abcd[2] = /*0x98badcfe*/ T_MASK ^ 0x67452301;; 408 pms->abcd[3] = 0x10325476;; 409}; 410 ; 411MD5_STATIC void; 412md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes); 413{; 414 const md5_byte_t *p = data;; 415 size_t left = nbytes;; 416 size_t offset = (pms->count[0] >> 3) & 63;; 417 md5_word_t nbits = (md5_word_t)(nbytes << 3);; 418 ; 419 if (nbytes <= 0); 420 return;; 421 ; 422 /* Update the message length. */; 423 pms->count[1] += (md5_word_t)(nbytes >> 29);; 424 pms->count[0] += nbits;; 425 if (pms->count[0] < nbits); 426 pms->count[1]++;; 427 ; 428 /* Process an initial partial block. */; 429 if (offset) {; 430 size_t copy = (offset + nbytes > 64 ? 64 - offset : nbytes);; 431 ; 432 memcpy(pms->buf + offset, p, copy);; 433 if (offset + copy < 64); 434 return;; 435 p += copy;; 436 left -= copy;; 437 md5_process(pms, pms->buf);; 438 }; 439 ; 440 /* Process full blocks. */; 441 for (; left >= 64; p += 64, left -= 64); 442 md5_process(pms, p);; 443 ; 444 /* Process a final partial block. */; 445 if (left); 446 memcpy(pms->buf, p",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:14502,Performance,perform,perform,14502,"8);; 353 SET(c, d, a, b, 7, 16, T39);; 354 SET(b, c, d, a, 10, 23, T40);; 355 SET(a, b, c, d, 13, 4, T41);; 356 SET(d, a, b, c, 0, 11, T42);; 357 SET(c, d, a, b, 3, 16, T43);; 358 SET(b, c, d, a, 6, 23, T44);; 359 SET(a, b, c, d, 9, 4, T45);; 360 SET(d, a, b, c, 12, 11, T46);; 361 SET(c, d, a, b, 15, 16, T47);; 362 SET(b, c, d, a, 2, 23, T48);; 363#undef SET; 364 ; 365/* Round 4. */; 366/* Let [abcd k s t] denote the operation; 367 a = b + ((a + I(b,c,d) + X[k] + T[i]) <<< s). */; 368#define I(x, y, z) ((y) ^ ((x) | ~(z))); 369#define SET(a, b, c, d, k, s, Ti) \; 370 t = (a) + I(b, c, d) + X[k] + (Ti); \; 371 (a) = ROTATE_LEFT(t, s) + (b); 372 ; 373 /* Do the following 16 operations. */; 374 SET(a, b, c, d, 0, 6, T49);; 375 SET(d, a, b, c, 7, 10, T50);; 376 SET(c, d, a, b, 14, 15, T51);; 377 SET(b, c, d, a, 5, 21, T52);; 378 SET(a, b, c, d, 12, 6, T53);; 379 SET(d, a, b, c, 3, 10, T54);; 380 SET(c, d, a, b, 10, 15, T55);; 381 SET(b, c, d, a, 1, 21, T56);; 382 SET(a, b, c, d, 8, 6, T57);; 383 SET(d, a, b, c, 15, 10, T58);; 384 SET(c, d, a, b, 6, 15, T59);; 385 SET(b, c, d, a, 13, 21, T60);; 386 SET(a, b, c, d, 4, 6, T61);; 387 SET(d, a, b, c, 11, 10, T62);; 388 SET(c, d, a, b, 2, 15, T63);; 389 SET(b, c, d, a, 9, 21, T64);; 390#undef SET; 391 ; 392 /* Then perform the following additions. (That is increment each; 393 of the four registers by the value it had before this block; 394 was started.) */; 395 pms->abcd[0] += a;; 396 pms->abcd[1] += b;; 397 pms->abcd[2] += c;; 398 pms->abcd[3] += d;; 399}; 400 ; 401MD5_STATIC void; 402md5_init(md5_state_t *pms); 403{; 404 pms->count[0] = pms->count[1] = 0;; 405 pms->abcd[0] = 0x67452301;; 406 pms->abcd[1] = /*0xefcdab89*/ T_MASK ^ 0x10325476;; 407 pms->abcd[2] = /*0x98badcfe*/ T_MASK ^ 0x67452301;; 408 pms->abcd[3] = 0x10325476;; 409}; 410 ; 411MD5_STATIC void; 412md5_append(md5_state_t *pms, const md5_byte_t *data, size_t nbytes); 413{; 414 const md5_byte_t *p = data;; 415 size_t left = nbytes;; 416 size_t offset = (pms->co",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:9974,Safety,avoid,avoid,9974,"md5_process(md5_state_t *pms, const md5_byte_t *data /*[64]*/); 213{; 214 md5_word_t a = pms->abcd[0], b = pms->abcd[1], c = pms->abcd[2],; 215 d = pms->abcd[3];; 216 md5_word_t t;; 217#if BYTE_ORDER > 0; 218 /* Define storage only for big-endian CPUs. */; 219 md5_word_t X[16];; 220#else; 221 /* Define storage for little-endian or both types of CPUs. */; 222 md5_word_t xbuf[16];; 223 const md5_word_t *X;; 224#endif; 225 ; 226 {; 227#if BYTE_ORDER == 0; 228 /*; 229 * Determine dynamically whether this is a big-endian or; 230 * little-endian machine, since we can use a more efficient; 231 * algorithm on the latter.; 232 */; 233 static const int w = 1;; 234 ; 235 if (*((const md5_byte_t *)&w)) /* dynamic little-endian */; 236#endif; 237#if BYTE_ORDER <= 0 /* little-endian */; 238 {; 239 /*; 240 * On little-endian machines, we can process properly aligned; 241 * data without copying it.; 242 */; 243 if (!(((uintptr_t) data) & 3)) {; 244 /* data are properly aligned, a direct assignment is possible */; 245 /* cast through a (void *) should avoid a compiler warning,; 246 see; 247 https://github.com/bel2125/civetweb/issues/94#issuecomment-98112861; 248 */; 249 X = (const md5_word_t *)(const void *)data;; 250 } else {; 251 /* not aligned */; 252 memcpy(xbuf, data, 64);; 253 X = xbuf;; 254 }; 255 }; 256#endif; 257#if BYTE_ORDER == 0; 258 else /* dynamic big-endian */; 259#endif; 260#if BYTE_ORDER >= 0 /* big-endian */; 261 {; 262 /*; 263 * On big-endian machines, we must arrange the bytes in the; 264 * right order.; 265 */; 266 const md5_byte_t *xp = data;; 267 int i;; 268 ; 269#if BYTE_ORDER == 0; 270 X = xbuf; /* (dynamic only) */; 271#else; 272#define xbuf X /* (static only) */; 273#endif; 274 for (i = 0; i < 16; ++i, xp += 4); 275 xbuf[i] = (md5_word_t)(xp[0]) + (md5_word_t)(xp[1] << 8); 276 + (md5_word_t)(xp[2] << 16); 277 + (md5_word_t)(xp[3] << 24);; 278 }; 279#endif; 280 }; 281 ; 282#define ROTATE_LEFT(x, n) (((x) << (n)) | ((x) >> (32 - (n)))); 283 ; 284/* Round 1. *",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:756,Testability,test,test,756,". ROOT: net/http/civetweb/md5.inl Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. md5.inl. Go to the documentation of this file. 1/*; 2 * This an amalgamation of md5.c and md5.h into a single file; 3 * with all static declaration to reduce linker conflicts; 4 * in Civetweb.; 5 *; 6 * The MD5_STATIC declaration was added to facilitate static; 7 * inclusion.; 8 * No Face Press, LLC; 9 */; 10 ; 11/* $Id: md5.h,v 1.4 2002/04/13 19:20:28 lpd Exp $ */; 12/*; 13 Independent implementation of MD5 (RFC 1321).; 14 ; 15 This code implements the MD5 Algorithm defined in RFC 1321, whose; 16 text is available at; 17 http://www.ietf.org/rfc/rfc1321.txt; 18 The code is derived from the text of the RFC, including the test suite; 19 (section A.5) but excluding the rest of Appendix A. It does not include; 20 any code or documentation that is identified in the RFC as being; 21 copyrighted.; 22 ; 23 The original and principal author of md5.h is L. Peter Deutsch; 24 <ghost@aladdin.com>. Other authors are noted in the change history; 25 that follows (in reverse chronological order):; 26 ; 27 2002-04-13 lpd Removed support for non-ANSI compilers; removed; 28 references to Ghostscript; clarified derivation from RFC 1321;; 29 now handles byte order either statically or dynamically.; 30 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 31 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5);; 32 added conditionalization for C++ compilation from Martin; 33 Purschke <purschke@bnl.gov>.; 34 1999-05-03 lpd Original version.; 35 */; 36 ; 37#if !defined(md5_INCLUDED); 38#define md5_INCLUDED; 39 ; 40/*; 41 * This package supports both compile-time and run-time determination of CPU; 42 * byte order. If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be; 43 * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is; 44 * defined as non-zero, the code will be compiled to run only on big-endian; 45 * CPUs; ",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:4453,Testability,test,test,4453,"se this software for any purpose,; 88 including commercial applications, and to alter it and redistribute it; 89 freely, subject to the following restrictions:; 90 ; 91 1. The origin of this software must not be misrepresented; you must not; 92 claim that you wrote the original software. If you use this software; 93 in a product, an acknowledgment in the product documentation would be; 94 appreciated but is not required.; 95 2. Altered source versions must be plainly marked as such, and must not be; 96 misrepresented as being the original software.; 97 3. This notice may not be removed or altered from any source distribution.; 98 ; 99 L. Peter Deutsch; 100 ghost@aladdin.com; 101 ; 102 */; 103/* $Id: md5.c,v 1.6 2002/04/13 19:20:28 lpd Exp $ */; 104/*; 105 Independent implementation of MD5 (RFC 1321).; 106 ; 107 This code implements the MD5 Algorithm defined in RFC 1321, whose; 108 text is available at; 109 http://www.ietf.org/rfc/rfc1321.txt; 110 The code is derived from the text of the RFC, including the test suite; 111 (section A.5) but excluding the rest of Appendix A. It does not include; 112 any code or documentation that is identified in the RFC as being; 113 copyrighted.; 114 ; 115 The original and principal author of md5.c is L. Peter Deutsch; 116 <ghost@aladdin.com>. Other authors are noted in the change history; 117 that follows (in reverse chronological order):; 118 ; 119 2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order; 120 either statically or dynamically; added missing #include <string.h>; 121 in library.; 122 2002-03-11 lpd Corrected argument list for main(), and added int return; 123 type, in test program and T value program.; 124 2002-02-21 lpd Added missing #include <stdio.h> in test program.; 125 2000-07-03 lpd Patched to eliminate warnings about ""constant is; 126 unsigned in ANSI C, signed in traditional""; made test program; 127 self-checking.; 128 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 129",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:5093,Testability,test,test,5093,"n.; 98 ; 99 L. Peter Deutsch; 100 ghost@aladdin.com; 101 ; 102 */; 103/* $Id: md5.c,v 1.6 2002/04/13 19:20:28 lpd Exp $ */; 104/*; 105 Independent implementation of MD5 (RFC 1321).; 106 ; 107 This code implements the MD5 Algorithm defined in RFC 1321, whose; 108 text is available at; 109 http://www.ietf.org/rfc/rfc1321.txt; 110 The code is derived from the text of the RFC, including the test suite; 111 (section A.5) but excluding the rest of Appendix A. It does not include; 112 any code or documentation that is identified in the RFC as being; 113 copyrighted.; 114 ; 115 The original and principal author of md5.c is L. Peter Deutsch; 116 <ghost@aladdin.com>. Other authors are noted in the change history; 117 that follows (in reverse chronological order):; 118 ; 119 2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order; 120 either statically or dynamically; added missing #include <string.h>; 121 in library.; 122 2002-03-11 lpd Corrected argument list for main(), and added int return; 123 type, in test program and T value program.; 124 2002-02-21 lpd Added missing #include <stdio.h> in test program.; 125 2000-07-03 lpd Patched to eliminate warnings about ""constant is; 126 unsigned in ANSI C, signed in traditional""; made test program; 127 self-checking.; 128 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 129 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5).; 130 1999-05-03 lpd Original version.; 131 */; 132 ; 133#if !defined(MD5_STATIC); 134#include <stdint.h>; 135#include <string.h>; 136#endif; 137 ; 138#undef BYTE_ORDER /* 1 = big-endian, -1 = little-endian, 0 = unknown */; 139#if defined(ARCH_IS_BIG_ENDIAN); 140#define BYTE_ORDER (ARCH_IS_BIG_ENDIAN ? 1 : -1); 141#else; 142#define BYTE_ORDER (0); 143#endif; 144 ; 145#define T_MASK ((md5_word_t)~0); 146#define T1 /* 0xd76aa478 */ (T_MASK ^ 0x28955b87); 147#define T2 /* 0xe8c7b756 */ (T_MASK ^ 0x173848a9); 148#define T3 (0x242070db); 149#define T4 /* 0xc1",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:5183,Testability,test,test,5183," 104/*; 105 Independent implementation of MD5 (RFC 1321).; 106 ; 107 This code implements the MD5 Algorithm defined in RFC 1321, whose; 108 text is available at; 109 http://www.ietf.org/rfc/rfc1321.txt; 110 The code is derived from the text of the RFC, including the test suite; 111 (section A.5) but excluding the rest of Appendix A. It does not include; 112 any code or documentation that is identified in the RFC as being; 113 copyrighted.; 114 ; 115 The original and principal author of md5.c is L. Peter Deutsch; 116 <ghost@aladdin.com>. Other authors are noted in the change history; 117 that follows (in reverse chronological order):; 118 ; 119 2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order; 120 either statically or dynamically; added missing #include <string.h>; 121 in library.; 122 2002-03-11 lpd Corrected argument list for main(), and added int return; 123 type, in test program and T value program.; 124 2002-02-21 lpd Added missing #include <stdio.h> in test program.; 125 2000-07-03 lpd Patched to eliminate warnings about ""constant is; 126 unsigned in ANSI C, signed in traditional""; made test program; 127 self-checking.; 128 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 129 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5).; 130 1999-05-03 lpd Original version.; 131 */; 132 ; 133#if !defined(MD5_STATIC); 134#include <stdint.h>; 135#include <string.h>; 136#endif; 137 ; 138#undef BYTE_ORDER /* 1 = big-endian, -1 = little-endian, 0 = unknown */; 139#if defined(ARCH_IS_BIG_ENDIAN); 140#define BYTE_ORDER (ARCH_IS_BIG_ENDIAN ? 1 : -1); 141#else; 142#define BYTE_ORDER (0); 143#endif; 144 ; 145#define T_MASK ((md5_word_t)~0); 146#define T1 /* 0xd76aa478 */ (T_MASK ^ 0x28955b87); 147#define T2 /* 0xe8c7b756 */ (T_MASK ^ 0x173848a9); 148#define T3 (0x242070db); 149#define T4 /* 0xc1bdceee */ (T_MASK ^ 0x3e423111); 150#define T5 /* 0xf57c0faf */ (T_MASK ^ 0x0a83f050); 151#define T6 (0x4787c62a); 152#defi",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md5_8inl_source.html:5320,Testability,test,test,5320,"s the MD5 Algorithm defined in RFC 1321, whose; 108 text is available at; 109 http://www.ietf.org/rfc/rfc1321.txt; 110 The code is derived from the text of the RFC, including the test suite; 111 (section A.5) but excluding the rest of Appendix A. It does not include; 112 any code or documentation that is identified in the RFC as being; 113 copyrighted.; 114 ; 115 The original and principal author of md5.c is L. Peter Deutsch; 116 <ghost@aladdin.com>. Other authors are noted in the change history; 117 that follows (in reverse chronological order):; 118 ; 119 2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order; 120 either statically or dynamically; added missing #include <string.h>; 121 in library.; 122 2002-03-11 lpd Corrected argument list for main(), and added int return; 123 type, in test program and T value program.; 124 2002-02-21 lpd Added missing #include <stdio.h> in test program.; 125 2000-07-03 lpd Patched to eliminate warnings about ""constant is; 126 unsigned in ANSI C, signed in traditional""; made test program; 127 self-checking.; 128 1999-11-04 lpd Edited comments slightly for automatic TOC extraction.; 129 1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5).; 130 1999-05-03 lpd Original version.; 131 */; 132 ; 133#if !defined(MD5_STATIC); 134#include <stdint.h>; 135#include <string.h>; 136#endif; 137 ; 138#undef BYTE_ORDER /* 1 = big-endian, -1 = little-endian, 0 = unknown */; 139#if defined(ARCH_IS_BIG_ENDIAN); 140#define BYTE_ORDER (ARCH_IS_BIG_ENDIAN ? 1 : -1); 141#else; 142#define BYTE_ORDER (0); 143#endif; 144 ; 145#define T_MASK ((md5_word_t)~0); 146#define T1 /* 0xd76aa478 */ (T_MASK ^ 0x28955b87); 147#define T2 /* 0xe8c7b756 */ (T_MASK ^ 0x173848a9); 148#define T3 (0x242070db); 149#define T4 /* 0xc1bdceee */ (T_MASK ^ 0x3e423111); 150#define T5 /* 0xf57c0faf */ (T_MASK ^ 0x0a83f050); 151#define T6 (0x4787c62a); 152#define T7 /* 0xa8304613 */ (T_MASK ^ 0x57cfb9ec); 153#define T8 /* 0xfd469501 */ (T_MASK ^ ",MatchSource.WIKI,doc/master/md5_8inl_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md5_8inl_source.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:288,Availability,robust,robust,288,". ROOT: RNTuple Introduction. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. RNTuple Introduction. RNTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an as",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1299,Availability,robust,robust,1299,"he use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of dif",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:257,Integrability,interface,interfaces,257,". ROOT: RNTuple Introduction. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. RNTuple Introduction. RNTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an as",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:319,Integrability,interface,interfaces,319,". ROOT: RNTuple Introduction. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. RNTuple Introduction. RNTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an as",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1282,Integrability,interface,interfaces,1282,"NTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are th",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1306,Integrability,interface,interfaces,1306,"he use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of dif",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:2941,Integrability,interface,interface,2941,"ode to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of different bit sizes. A C++ type may be mapped to multiple columns. For instance, an std::vector<float> maps to two columns, an offset column indicating the size of the vector per entry, and a payload column with the float data.; Columns are partitioned into pages (roughly: TTree baskets) of a few kB – a few tens of kB each. The physical layer (only) needs to provide the means to store and retrieve pages. The physical layer is decoupled from the high-level C++ logic. The physical layer implements an abstract page storage interface, so that dedicated implementations for key-value stores and other storage systems are conceivable. At this point, the only provided backend stores the pages in ROOT files.; RNTuples are further grouped into clusters, which are, like TTree clusters, self-contained blocks of consecutive entries. Clusters provide a unit of writing and will provide the means for parallel writing of data in a future version of RNTuple. Related classes. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:32 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1374,Modifiability,layers,layers,1374,"he use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of dif",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1089,Performance,optimiz,optimized,1089,"NTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are th",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:339,Safety,safe,safe,339,". ROOT: RNTuple Introduction. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. RNTuple Introduction. RNTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an as",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1260,Safety,safe,safe,1260,"NTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are th",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1336,Safety,safe,safety,1336,"he use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of dif",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1382,Testability,log,logical,1382,"he use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of dif",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1560,Testability,log,logical,1560,"cal layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of different bit sizes. A C++ type may be mapped to multiple columns. For instance, an std::vector<float> maps to two columns, an offset column indicating the size of the vecto",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:2879,Testability,log,logic,2879,"ode to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of different bit sizes. A C++ type may be mapped to multiple columns. For instance, an std::vector<float> maps to two columns, an offset column indicating the size of the vector per entry, and a payload column with the float data.; Columns are partitioned into pages (roughly: TTree baskets) of a few kB – a few tens of kB each. The physical layer (only) needs to provide the means to store and retrieve pages. The physical layer is decoupled from the high-level C++ logic. The physical layer implements an abstract page storage interface, so that dedicated implementations for key-value stores and other storage systems are conceivable. At this point, the only provided backend stores the pages in ROOT files.; RNTuples are further grouped into clusters, which are, like TTree clusters, self-contained blocks of consecutive entries. Clusters provide a unit of writing and will provide the means for parallel writing of data in a future version of RNTuple. Related classes. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:32 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:735,Usability,clear,clear,735,". ROOT: RNTuple Introduction. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. RNTuple Introduction. RNTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an as",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:1119,Usability,simpl,simple,1119,"NTuple (for n-tuple and nested tuple) is the experimental evolution of TTree columnar data storage. RNTuple introduces new interfaces that aim to be more robust. In particular, the new interfaces are type-safe through the use of templates, and the ownership is well-defined through the use of smart pointers. For instance tree->Branch(""px"", &Category, ""px/F"");; becomes auto px = model->MakeField<float>(""px"");; // px is std::shared_ptr<float>; The physical layout changes slightly from big endian to little endian so that it matches the in-memory layout on most modern architectures. Combined with a clear separation of offset/index data and payload data for collections, uncompressed RNTuple data can be directly mapped to memory without further copies. Goals; RNTuple shall investigate improvements of the TTree I/O in the following ways. More speed; Improve mapping to vectorized and parallel hardware; For types known at compile / JIT time: generate optimized code; Optimized for simple types (float, int, and vectors of them); Better memory control: work with a fixed budget of pre-defined I/O buffers; Naturally thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are th",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html:2271,Usability,simpl,simple,2271,"ly thread-safe and asynchronous interfaces. More robust interfaces; Compile-time type safety by default; Decomposition into layers: logical layer, primitives layer, storage layer; Separation of data model and live data; Self-contained I/O code to support creation of a standalone I/O library. Concepts; At the logical layer, the user defines a data model using the RNTupleModel class. The data model is a collection of serializable C++ types with associated names, similar to branches in a TTree. The data model can contain (nested) collections, e.g., a type can be std::vector<std::vector<float>>.; Each serializable type is represented by a field, concretely by a templated version of RField, e.g. RField<double>. A field can generate or adopt an associated value, which represents a memory location storing a value of the given C++ type. These distinguished memory locations are the destinations and sources for the deserialization and serialization.; The (de-)serialization is a mapping from the C++ type to the more simple column type system. A column contains an arbitrary number of fixed-sized elements of a well-defined set of types: integers and floats of different bit sizes. A C++ type may be mapped to multiple columns. For instance, an std::vector<float> maps to two columns, an offset column indicating the size of the vector per entry, and a payload column with the float data.; Columns are partitioned into pages (roughly: TTree baskets) of a few kB – a few tens of kB each. The physical layer (only) needs to provide the means to store and retrieve pages. The physical layer is decoupled from the high-level C++ logic. The physical layer implements an abstract page storage interface, so that dedicated implementations for key-value stores and other storage systems are conceivable. At this point, the only provided backend stores the pages in ROOT files.; RNTuples are further grouped into clusters, which are, like TTree clusters, self-contained blocks of consecutive entries. Clus",MatchSource.WIKI,doc/master/md_tree_2ntuple_2v7_2doc_2README.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/md_tree_2ntuple_2v7_2doc_2README.html
https://root.cern/doc/master/mesh_8c.html:285,Integrability,depend,dependency,285,". ROOT: graf3d/eve7/glu/mesh.c File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Macros |; Functions ; mesh.c File Reference. #include ""gluos.h""; #include <stddef.h>; #include <assert.h>; #include ""mesh.h""; #include ""memalloc.h"". Include dependency graph for mesh.c:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Macros; #define FALSE   0;  ; #define TRUE   1;  . Functions; GLUhalfEdge * __gl_meshAddEdgeVertex (GLUhalfEdge *eOrg);  ; void __gl_meshCheckMesh (GLUmesh *mesh);  ; GLUhalfEdge * __gl_meshConnect (GLUhalfEdge *eOrg, GLUhalfEdge *eDst);  ; int __gl_meshDelete (GLUhalfEdge *eDel);  ; void __gl_meshDeleteMesh (GLUmesh *mesh);  ; GLUhalfEdge * __gl_meshMakeEdge (GLUmesh *mesh);  ; GLUmesh * __gl_meshNewMesh (void);  ; int __gl_meshSplice (GLUhalfEdge *eOrg, GLUhalfEdge *eDst);  ; GLUhalfEdge * __gl_meshSplitEdge (GLUhalfEdge *eOrg);  ; GLUmesh * __gl_meshUnion (GLUmesh *mesh1, GLUmesh *mesh2);  ; void __gl_meshZapFace (GLUface *fZap);  ; static GLUface * allocFace ();  ; static GLUvertex * allocVertex ();  ; static void KillEdge (GLUhalfEdge *eDel);  ; static void KillFace (GLUface *fDel, GLUface *newLface);  ; static void KillVertex (GLUvertex *vDel, GLUvertex *newOrg);  ; static GLUhalfEdge * MakeEdge (GLUhalfEdge *eNext);  ; static void MakeFace (GLUface *newFace, GLUhalfEdge *eOrig, GLUface *fNext);  ; static void MakeVertex (GLUvertex *newVertex, GLUhalfEdge *eOrig, GLUvertex *vNext);  ; static void Splice (GLUhalfEdge *a, GLUhalfEdge *b);  . Macro Definition Documentation. ◆ FALSE. #define FALSE   0. Definition at line 45 of file mesh.c. ◆ TRUE. #define TRUE   1. Definition at line 42 of file mesh.c. Function Documentation. ◆ __gl_meshAddEdgeVertex(). GLUhalfEdge * __gl_meshAddEdgeVertex ; (; GLUhalfEdge * ; eOrg). Definition at line 441 of file mesh.c. ◆ __gl_meshCheckMesh(). void __gl_meshCheckMesh ; (; GLUmesh * ; mesh). Definition at line 737 of file mesh.c. ◆ __gl_m",MatchSource.WIKI,doc/master/mesh_8c.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c.html
https://root.cern/doc/master/mesh_8c.html:224,Testability,assert,assert,224,". ROOT: graf3d/eve7/glu/mesh.c File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Macros |; Functions ; mesh.c File Reference. #include ""gluos.h""; #include <stddef.h>; #include <assert.h>; #include ""mesh.h""; #include ""memalloc.h"". Include dependency graph for mesh.c:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Macros; #define FALSE   0;  ; #define TRUE   1;  . Functions; GLUhalfEdge * __gl_meshAddEdgeVertex (GLUhalfEdge *eOrg);  ; void __gl_meshCheckMesh (GLUmesh *mesh);  ; GLUhalfEdge * __gl_meshConnect (GLUhalfEdge *eOrg, GLUhalfEdge *eDst);  ; int __gl_meshDelete (GLUhalfEdge *eDel);  ; void __gl_meshDeleteMesh (GLUmesh *mesh);  ; GLUhalfEdge * __gl_meshMakeEdge (GLUmesh *mesh);  ; GLUmesh * __gl_meshNewMesh (void);  ; int __gl_meshSplice (GLUhalfEdge *eOrg, GLUhalfEdge *eDst);  ; GLUhalfEdge * __gl_meshSplitEdge (GLUhalfEdge *eOrg);  ; GLUmesh * __gl_meshUnion (GLUmesh *mesh1, GLUmesh *mesh2);  ; void __gl_meshZapFace (GLUface *fZap);  ; static GLUface * allocFace ();  ; static GLUvertex * allocVertex ();  ; static void KillEdge (GLUhalfEdge *eDel);  ; static void KillFace (GLUface *fDel, GLUface *newLface);  ; static void KillVertex (GLUvertex *vDel, GLUvertex *newOrg);  ; static GLUhalfEdge * MakeEdge (GLUhalfEdge *eNext);  ; static void MakeFace (GLUface *newFace, GLUhalfEdge *eOrig, GLUface *fNext);  ; static void MakeVertex (GLUvertex *newVertex, GLUhalfEdge *eOrig, GLUvertex *vNext);  ; static void Splice (GLUhalfEdge *a, GLUhalfEdge *b);  . Macro Definition Documentation. ◆ FALSE. #define FALSE   0. Definition at line 45 of file mesh.c. ◆ TRUE. #define TRUE   1. Definition at line 42 of file mesh.c. Function Documentation. ◆ __gl_meshAddEdgeVertex(). GLUhalfEdge * __gl_meshAddEdgeVertex ; (; GLUhalfEdge * ; eOrg). Definition at line 441 of file mesh.c. ◆ __gl_meshCheckMesh(). void __gl_meshCheckMesh ; (; GLUmesh * ; mesh). Definition at line 737 of file mesh.c. ◆ __gl_m",MatchSource.WIKI,doc/master/mesh_8c.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c.html
https://root.cern/doc/master/mesh_8c_source.html:7134,Deployability,update,updates,7134,"ked ""inside"" if the old one was. This is a; 184 * convenience for the common case where a face has been split in two.; 185 */; 186 fNew->inside = fNext->inside;; 187 ; 188 /* fix other edges on this face loop */; 189 e = eOrig;; 190 do {; 191 e->Lface = fNew;; 192 e = e->Lnext;; 193 } while( e != eOrig );; 194}; 195 ; 196/* KillEdge( eDel ) destroys an edge (the half-edges eDel and eDel->Sym),; 197 * and removes from the global edge list.; 198 */; 199static void KillEdge( GLUhalfEdge *eDel ); 200{; 201 GLUhalfEdge *ePrev, *eNext;; 202 ; 203 /* Half-edges are allocated in pairs, see EdgePair above */; 204 if( eDel->Sym < eDel ) { eDel = eDel->Sym; }; 205 ; 206 /* delete from circular doubly-linked list */; 207 eNext = eDel->next;; 208 ePrev = eDel->Sym->next;; 209 eNext->Sym->next = ePrev;; 210 ePrev->Sym->next = eNext;; 211 ; 212 memFree( eDel );; 213}; 214 ; 215 ; 216/* KillVertex( vDel ) destroys a vertex and removes it from the global; 217 * vertex list. It updates the vertex loop to point to a given new vertex.; 218 */; 219static void KillVertex( GLUvertex *vDel, GLUvertex *newOrg ); 220{; 221 GLUhalfEdge *e, *eStart = vDel->anEdge;; 222 GLUvertex *vPrev, *vNext;; 223 ; 224 /* change the origin of all affected edges */; 225 e = eStart;; 226 do {; 227 e->Org = newOrg;; 228 e = e->Onext;; 229 } while( e != eStart );; 230 ; 231 /* delete from circular doubly-linked list */; 232 vPrev = vDel->prev;; 233 vNext = vDel->next;; 234 vNext->prev = vPrev;; 235 vPrev->next = vNext;; 236 ; 237 memFree( vDel );; 238}; 239 ; 240/* KillFace( fDel ) destroys a face and removes it from the global face; 241 * list. It updates the face loop to point to a given new face.; 242 */; 243static void KillFace( GLUface *fDel, GLUface *newLface ); 244{; 245 GLUhalfEdge *e, *eStart = fDel->anEdge;; 246 GLUface *fPrev, *fNext;; 247 ; 248 /* change the left face of all affected edges */; 249 e = eStart;; 250 do {; 251 e->Lface = newLface;; 252 e = e->Lnext;; 253 } while( e != eStart );; 254 ; 2",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:7790,Deployability,update,updates,7790," }; 205 ; 206 /* delete from circular doubly-linked list */; 207 eNext = eDel->next;; 208 ePrev = eDel->Sym->next;; 209 eNext->Sym->next = ePrev;; 210 ePrev->Sym->next = eNext;; 211 ; 212 memFree( eDel );; 213}; 214 ; 215 ; 216/* KillVertex( vDel ) destroys a vertex and removes it from the global; 217 * vertex list. It updates the vertex loop to point to a given new vertex.; 218 */; 219static void KillVertex( GLUvertex *vDel, GLUvertex *newOrg ); 220{; 221 GLUhalfEdge *e, *eStart = vDel->anEdge;; 222 GLUvertex *vPrev, *vNext;; 223 ; 224 /* change the origin of all affected edges */; 225 e = eStart;; 226 do {; 227 e->Org = newOrg;; 228 e = e->Onext;; 229 } while( e != eStart );; 230 ; 231 /* delete from circular doubly-linked list */; 232 vPrev = vDel->prev;; 233 vNext = vDel->next;; 234 vNext->prev = vPrev;; 235 vPrev->next = vNext;; 236 ; 237 memFree( vDel );; 238}; 239 ; 240/* KillFace( fDel ) destroys a face and removes it from the global face; 241 * list. It updates the face loop to point to a given new face.; 242 */; 243static void KillFace( GLUface *fDel, GLUface *newLface ); 244{; 245 GLUhalfEdge *e, *eStart = fDel->anEdge;; 246 GLUface *fPrev, *fNext;; 247 ; 248 /* change the left face of all affected edges */; 249 e = eStart;; 250 do {; 251 e->Lface = newLface;; 252 e = e->Lnext;; 253 } while( e != eStart );; 254 ; 255 /* delete from circular doubly-linked list */; 256 fPrev = fDel->prev;; 257 fNext = fDel->next;; 258 fNext->prev = fPrev;; 259 fPrev->next = fNext;; 260 ; 261 memFree( fDel );; 262}; 263 ; 264 ; 265/****************** Basic Edge Operations **********************/; 266 ; 267/* __gl_meshMakeEdge creates one edge, two vertices, and a loop (face).; 268 * The loop consists of the two new half-edges.; 269 */; 270GLUhalfEdge *__gl_meshMakeEdge( GLUmesh *mesh ); 271{; 272 GLUvertex *newVertex1= allocVertex();; 273 GLUvertex *newVertex2= allocVertex();; 274 GLUface *newFace= allocFace();; 275 GLUhalfEdge *e;; 276 ; 277 /* if any one is null then all ge",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:359,Energy Efficiency,charge,charge,359,". ROOT: graf3d/eve7/glu/mesh.c Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. mesh.c. Go to the documentation of this file. 1/*; 2 * SGI FREE SOFTWARE LICENSE B (Version 2.0, Sept. 18, 2008); 3 * Copyright (C) 1991-2000 Silicon Graphics, Inc. All Rights Reserved.; 4 *; 5 * Permission is hereby granted, free of charge, to any person obtaining a; 6 * copy of this software and associated documentation files (the ""Software""),; 7 * to deal in the Software without restriction, including without limitation; 8 * the rights to use, copy, modify, merge, publish, distribute, sublicense,; 9 * and/or sell copies of the Software, and to permit persons to whom the; 10 * Software is furnished to do so, subject to the following conditions:; 11 *; 12 * The above copyright notice including the dates of first publication and; 13 * either this permission notice or a reference to; 14 * http://oss.sgi.com/projects/FreeB/; 15 * shall be included in all copies or substantial portions of the Software.; 16 *; 17 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS; 18 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,; 19 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL; 20 * SILICON GRAPHICS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,; 21 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF; 22 * OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE; 23 * SOFTWARE.; 24 *; 25 * Except as contained in this notice, the name of Silicon Graphics, Inc.; 26 * shall not be used in advertising or otherwise to promote the sale, use or; 27 * other dealings in this Software without prior written authorization from; 28 * Silicon Graphics, Inc.; 29 */; 30/*; 31** Author: Eric Veach, July 1994.; 32**; 33*/; 34 ; 35#include ""gluos.h""; 36#include <stddef.h>; 37#include <assert.h>; 38#include ""mesh.h""; 39#include ""memalloc.h""; 40 ; 41#ifn",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:2484,Energy Efficiency,allocate,allocated,2484,"INGS IN THE; 23 * SOFTWARE.; 24 *; 25 * Except as contained in this notice, the name of Silicon Graphics, Inc.; 26 * shall not be used in advertising or otherwise to promote the sale, use or; 27 * other dealings in this Software without prior written authorization from; 28 * Silicon Graphics, Inc.; 29 */; 30/*; 31** Author: Eric Veach, July 1994.; 32**; 33*/; 34 ; 35#include ""gluos.h""; 36#include <stddef.h>; 37#include <assert.h>; 38#include ""mesh.h""; 39#include ""memalloc.h""; 40 ; 41#ifndef TRUE; 42#define TRUE 1; 43#endif; 44#ifndef FALSE; 45#define FALSE 0; 46#endif; 47 ; 48static GLUvertex *allocVertex(); 49{; 50 return (GLUvertex *)memAlloc( sizeof( GLUvertex ));; 51}; 52 ; 53static GLUface *allocFace(); 54{; 55 return (GLUface *)memAlloc( sizeof( GLUface ));; 56}; 57 ; 58/************************ Utility Routines ************************/; 59 ; 60/* MakeEdge creates a new pair of half-edges which form their own loop.; 61 * No vertex or face structures are allocated, but these must be assigned; 62 * before the current edge operation is completed.; 63 */; 64static GLUhalfEdge *MakeEdge( GLUhalfEdge *eNext ); 65{; 66 GLUhalfEdge *e;; 67 GLUhalfEdge *eSym;; 68 GLUhalfEdge *ePrev;; 69 EdgePair *pair = (EdgePair *)memAlloc( sizeof( EdgePair ));; 70 if (pair == NULL) return NULL;; 71 ; 72 e = &pair->e;; 73 eSym = &pair->eSym;; 74 ; 75 /* Make sure eNext points to the first edge of the edge pair */; 76 if( eNext->Sym < eNext ) { eNext = eNext->Sym; }; 77 ; 78 /* Insert in circular doubly-linked list before eNext.; 79 * Note that the prev pointer is stored in Sym->next.; 80 */; 81 ePrev = eNext->Sym->next;; 82 eSym->next = ePrev;; 83 ePrev->Sym->next = e;; 84 e->next = eNext;; 85 eNext->Sym->next = eSym;; 86 ; 87 e->Sym = eSym;; 88 e->Onext = e;; 89 e->Lnext = eSym;; 90 e->Org = NULL;; 91 e->Lface = NULL;; 92 e->winding = 0;; 93 e->activeRegion = NULL;; 94 ; 95 eSym->Sym = e;; 96 eSym->Onext = eSym;; 97 eSym->Lnext = e;; 98 eSym->Org = NULL;; 99 eSym->Lface = NULL;; 100 ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:6724,Energy Efficiency,allocate,allocated,6724,"nked list before fNext */; 172 fPrev = fNext->prev;; 173 fNew->prev = fPrev;; 174 fPrev->next = fNew;; 175 fNew->next = fNext;; 176 fNext->prev = fNew;; 177 ; 178 fNew->anEdge = eOrig;; 179 fNew->data = NULL;; 180 fNew->trail = NULL;; 181 fNew->marked = FALSE;; 182 ; 183 /* The new face is marked ""inside"" if the old one was. This is a; 184 * convenience for the common case where a face has been split in two.; 185 */; 186 fNew->inside = fNext->inside;; 187 ; 188 /* fix other edges on this face loop */; 189 e = eOrig;; 190 do {; 191 e->Lface = fNew;; 192 e = e->Lnext;; 193 } while( e != eOrig );; 194}; 195 ; 196/* KillEdge( eDel ) destroys an edge (the half-edges eDel and eDel->Sym),; 197 * and removes from the global edge list.; 198 */; 199static void KillEdge( GLUhalfEdge *eDel ); 200{; 201 GLUhalfEdge *ePrev, *eNext;; 202 ; 203 /* Half-edges are allocated in pairs, see EdgePair above */; 204 if( eDel->Sym < eDel ) { eDel = eDel->Sym; }; 205 ; 206 /* delete from circular doubly-linked list */; 207 eNext = eDel->next;; 208 ePrev = eDel->Sym->next;; 209 eNext->Sym->next = ePrev;; 210 ePrev->Sym->next = eNext;; 211 ; 212 memFree( eDel );; 213}; 214 ; 215 ; 216/* KillVertex( vDel ) destroys a vertex and removes it from the global; 217 * vertex list. It updates the vertex loop to point to a given new vertex.; 218 */; 219static void KillVertex( GLUvertex *vDel, GLUvertex *newOrg ); 220{; 221 GLUhalfEdge *e, *eStart = vDel->anEdge;; 222 GLUvertex *vPrev, *vNext;; 223 ; 224 /* change the origin of all affected edges */; 225 e = eStart;; 226 do {; 227 e->Org = newOrg;; 228 e = e->Onext;; 229 } while( e != eStart );; 230 ; 231 /* delete from circular doubly-linked list */; 232 vPrev = vDel->prev;; 233 vNext = vDel->next;; 234 vNext->prev = vPrev;; 235 vPrev->next = vNext;; 236 ; 237 memFree( vDel );; 238}; 239 ; 240/* KillFace( fDel ) destroys a face and removes it from the global face; 241 * list. It updates the face loop to point to a given new face.; 242 */; 243static void ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:12497,Energy Efficiency,allocate,allocate,12497,"Dst->Org.; 349 * Make sure the old vertex points to a valid half-edge.; 350 */; 351 MakeVertex( newVertex, eDst, eOrg->Org );; 352 eOrg->Org->anEdge = eOrg;; 353 }; 354 if( ! joiningLoops ) {; 355 GLUface *newFace= allocFace();; 356 if (newFace == NULL) return 0;; 357 ; 358 /* We split one loop into two -- the new loop is eDst->Lface.; 359 * Make sure the old face points to a valid half-edge.; 360 */; 361 MakeFace( newFace, eDst, eOrg->Lface );; 362 eOrg->Lface->anEdge = eOrg;; 363 }; 364 ; 365 return 1;; 366}; 367 ; 368 ; 369/* __gl_meshDelete( eDel ) removes the edge eDel. There are several cases:; 370 * if (eDel->Lface != eDel->Rface), we join two loops into one; the loop; 371 * eDel->Lface is deleted. Otherwise, we are splitting one loop into two;; 372 * the newly created loop will contain eDel->Dst. If the deletion of eDel; 373 * would create isolated vertices, those are deleted as well.; 374 *; 375 * This function could be implemented as two calls to __gl_meshSplice; 376 * plus a few calls to memFree, but this would allocate and delete; 377 * unnecessary vertices and faces.; 378 */; 379int __gl_meshDelete( GLUhalfEdge *eDel ); 380{; 381 GLUhalfEdge *eDelSym = eDel->Sym;; 382 int joiningLoops = FALSE;; 383 ; 384 /* First step: disconnect the origin vertex eDel->Org. We make all; 385 * changes to get a consistent mesh in this ""intermediate"" state.; 386 */; 387 if( eDel->Lface != eDel->Rface ) {; 388 /* We are joining two loops into one -- remove the left face */; 389 joiningLoops = TRUE;; 390 KillFace( eDel->Lface, eDel->Rface );; 391 }; 392 ; 393 if( eDel->Onext == eDel ) {; 394 KillVertex( eDel->Org, NULL );; 395 } else {; 396 /* Make sure that eDel->Org and eDel->Rface point to valid half-edges */; 397 eDel->Rface->anEdge = eDel->Oprev;; 398 eDel->Org->anEdge = eDel->Onext;; 399 ; 400 Splice( eDel, eDel->Oprev );; 401 if( ! joiningLoops ) {; 402 GLUface *newFace= allocFace();; 403 if (newFace == NULL) return 0;; 404 ; 405 /* We are splitting one loop into two ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:16653,Energy Efficiency,reduce,reduced,16653,"LL;; 475 ; 476 eNew = tempHalfEdge->Sym;; 477 ; 478 /* Disconnect eOrg from eOrg->Dst and connect it to eNew->Org */; 479 Splice( eOrg->Sym, eOrg->Sym->Oprev );; 480 Splice( eOrg->Sym, eNew );; 481 ; 482 /* Set the vertex and face information */; 483 eOrg->Dst = eNew->Org;; 484 eNew->Dst->anEdge = eNew->Sym; /* may have pointed to eOrg->Sym */; 485 eNew->Rface = eOrg->Rface;; 486 eNew->winding = eOrg->winding; /* copy old winding information */; 487 eNew->Sym->winding = eOrg->Sym->winding;; 488 ; 489 return eNew;; 490}; 491 ; 492 ; 493/* __gl_meshConnect( eOrg, eDst ) creates a new edge from eOrg->Dst; 494 * to eDst->Org, and returns the corresponding half-edge eNew.; 495 * If eOrg->Lface == eDst->Lface, this splits one loop into two,; 496 * and the newly created loop is eNew->Lface. Otherwise, two disjoint; 497 * loops are merged into one, and the loop eDst->Lface is destroyed.; 498 *; 499 * If (eOrg == eDst), the new face will have only two edges.; 500 * If (eOrg->Lnext == eDst), the old face is reduced to a single edge.; 501 * If (eOrg->Lnext->Lnext == eDst), the old face is reduced to two edges.; 502 */; 503GLUhalfEdge *__gl_meshConnect( GLUhalfEdge *eOrg, GLUhalfEdge *eDst ); 504{; 505 GLUhalfEdge *eNewSym;; 506 int joiningLoops = FALSE;; 507 GLUhalfEdge *eNew = MakeEdge( eOrg );; 508 if (eNew == NULL) return NULL;; 509 ; 510 eNewSym = eNew->Sym;; 511 ; 512 if( eDst->Lface != eOrg->Lface ) {; 513 /* We are connecting two disjoint loops -- destroy eDst->Lface */; 514 joiningLoops = TRUE;; 515 KillFace( eDst->Lface, eOrg->Lface );; 516 }; 517 ; 518 /* Connect the new edge appropriately */; 519 Splice( eNew, eOrg->Lnext );; 520 Splice( eNewSym, eDst );; 521 ; 522 /* Set the vertex and face information */; 523 eNew->Org = eOrg->Dst;; 524 eNewSym->Org = eDst->Org;; 525 eNew->Lface = eNewSym->Lface = eOrg->Lface;; 526 ; 527 /* Make sure the old face points to a valid half-edge */; 528 eOrg->Lface->anEdge = eNewSym;; 529 ; 530 if( ! joiningLoops ) {; 531 GLUface *newFa",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:16735,Energy Efficiency,reduce,reduced,16735,"Org->Dst and connect it to eNew->Org */; 479 Splice( eOrg->Sym, eOrg->Sym->Oprev );; 480 Splice( eOrg->Sym, eNew );; 481 ; 482 /* Set the vertex and face information */; 483 eOrg->Dst = eNew->Org;; 484 eNew->Dst->anEdge = eNew->Sym; /* may have pointed to eOrg->Sym */; 485 eNew->Rface = eOrg->Rface;; 486 eNew->winding = eOrg->winding; /* copy old winding information */; 487 eNew->Sym->winding = eOrg->Sym->winding;; 488 ; 489 return eNew;; 490}; 491 ; 492 ; 493/* __gl_meshConnect( eOrg, eDst ) creates a new edge from eOrg->Dst; 494 * to eDst->Org, and returns the corresponding half-edge eNew.; 495 * If eOrg->Lface == eDst->Lface, this splits one loop into two,; 496 * and the newly created loop is eNew->Lface. Otherwise, two disjoint; 497 * loops are merged into one, and the loop eDst->Lface is destroyed.; 498 *; 499 * If (eOrg == eDst), the new face will have only two edges.; 500 * If (eOrg->Lnext == eDst), the old face is reduced to a single edge.; 501 * If (eOrg->Lnext->Lnext == eDst), the old face is reduced to two edges.; 502 */; 503GLUhalfEdge *__gl_meshConnect( GLUhalfEdge *eOrg, GLUhalfEdge *eDst ); 504{; 505 GLUhalfEdge *eNewSym;; 506 int joiningLoops = FALSE;; 507 GLUhalfEdge *eNew = MakeEdge( eOrg );; 508 if (eNew == NULL) return NULL;; 509 ; 510 eNewSym = eNew->Sym;; 511 ; 512 if( eDst->Lface != eOrg->Lface ) {; 513 /* We are connecting two disjoint loops -- destroy eDst->Lface */; 514 joiningLoops = TRUE;; 515 KillFace( eDst->Lface, eOrg->Lface );; 516 }; 517 ; 518 /* Connect the new edge appropriately */; 519 Splice( eNew, eOrg->Lnext );; 520 Splice( eNewSym, eDst );; 521 ; 522 /* Set the vertex and face information */; 523 eNew->Org = eOrg->Dst;; 524 eNewSym->Org = eDst->Org;; 525 eNew->Lface = eNewSym->Lface = eOrg->Lface;; 526 ; 527 /* Make sure the old face points to a valid half-edge */; 528 eOrg->Lface->anEdge = eNewSym;; 529 ; 530 if( ! joiningLoops ) {; 531 GLUface *newFace= allocFace();; 532 if (newFace == NULL) return NULL;; 533 ; 534 /* We spl",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:3822,Integrability,depend,depending,3822,"m = &pair->eSym;; 74 ; 75 /* Make sure eNext points to the first edge of the edge pair */; 76 if( eNext->Sym < eNext ) { eNext = eNext->Sym; }; 77 ; 78 /* Insert in circular doubly-linked list before eNext.; 79 * Note that the prev pointer is stored in Sym->next.; 80 */; 81 ePrev = eNext->Sym->next;; 82 eSym->next = ePrev;; 83 ePrev->Sym->next = e;; 84 e->next = eNext;; 85 eNext->Sym->next = eSym;; 86 ; 87 e->Sym = eSym;; 88 e->Onext = e;; 89 e->Lnext = eSym;; 90 e->Org = NULL;; 91 e->Lface = NULL;; 92 e->winding = 0;; 93 e->activeRegion = NULL;; 94 ; 95 eSym->Sym = e;; 96 eSym->Onext = eSym;; 97 eSym->Lnext = e;; 98 eSym->Org = NULL;; 99 eSym->Lface = NULL;; 100 eSym->winding = 0;; 101 eSym->activeRegion = NULL;; 102 ; 103 return e;; 104}; 105 ; 106/* Splice( a, b ) is best described by the Guibas/Stolfi paper or the; 107 * CS348a notes (see mesh.h). Basically it modifies the mesh so that; 108 * a->Onext and b->Onext are exchanged. This can have various effects; 109 * depending on whether a and b belong to different face or vertex rings.; 110 * For more explanation see __gl_meshSplice() below.; 111 */; 112static void Splice( GLUhalfEdge *a, GLUhalfEdge *b ); 113{; 114 GLUhalfEdge *aOnext = a->Onext;; 115 GLUhalfEdge *bOnext = b->Onext;; 116 ; 117 aOnext->Sym->Lnext = b;; 118 bOnext->Sym->Lnext = a;; 119 a->Onext = bOnext;; 120 b->Onext = aOnext;; 121}; 122 ; 123/* MakeVertex( newVertex, eOrig, vNext ) attaches a new vertex and makes it the; 124 * origin of all edges in the vertex loop to which eOrig belongs. ""vNext"" gives; 125 * a place to insert the new vertex in the global vertex list. We insert; 126 * the new vertex *before* vNext so that algorithms which walk the vertex; 127 * list will not see the newly created vertices.; 128 */; 129static void MakeVertex( GLUvertex *newVertex,; 130 GLUhalfEdge *eOrig, GLUvertex *vNext ); 131{; 132 GLUhalfEdge *e;; 133 GLUvertex *vPrev;; 134 GLUvertex *vNew = newVertex;; 135 ; 136 assert(vNew != NULL);; 137 ; 138 /* insert in ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:14275,Integrability,rout,routines,14275,"v;; 398 eDel->Org->anEdge = eDel->Onext;; 399 ; 400 Splice( eDel, eDel->Oprev );; 401 if( ! joiningLoops ) {; 402 GLUface *newFace= allocFace();; 403 if (newFace == NULL) return 0;; 404 ; 405 /* We are splitting one loop into two -- create a new loop for eDel. */; 406 MakeFace( newFace, eDel, eDel->Lface );; 407 }; 408 }; 409 ; 410 /* Claim: the mesh is now in a consistent state, except that eDel->Org; 411 * may have been deleted. Now we disconnect eDel->Dst.; 412 */; 413 if( eDelSym->Onext == eDelSym ) {; 414 KillVertex( eDelSym->Org, NULL );; 415 KillFace( eDelSym->Lface, NULL );; 416 } else {; 417 /* Make sure that eDel->Dst and eDel->Lface point to valid half-edges */; 418 eDel->Lface->anEdge = eDelSym->Oprev;; 419 eDelSym->Org->anEdge = eDelSym->Onext;; 420 Splice( eDelSym, eDelSym->Oprev );; 421 }; 422 ; 423 /* Any isolated vertices or faces have already been freed. */; 424 KillEdge( eDel );; 425 ; 426 return 1;; 427}; 428 ; 429 ; 430/******************** Other Edge Operations **********************/; 431 ; 432/* All these routines can be implemented with the basic edge; 433 * operations above. They are provided for convenience and efficiency.; 434 */; 435 ; 436 ; 437/* __gl_meshAddEdgeVertex( eOrg ) creates a new edge eNew such that; 438 * eNew == eOrg->Lnext, and eNew->Dst is a newly created vertex.; 439 * eOrg and eNew will have the same left face.; 440 */; 441GLUhalfEdge *__gl_meshAddEdgeVertex( GLUhalfEdge *eOrg ); 442{; 443 GLUhalfEdge *eNewSym;; 444 GLUhalfEdge *eNew = MakeEdge( eOrg );; 445 if (eNew == NULL) return NULL;; 446 ; 447 eNewSym = eNew->Sym;; 448 ; 449 /* Connect the new edge appropriately */; 450 Splice( eNew, eOrg->Lnext );; 451 ; 452 /* Set the vertex and face information */; 453 eNew->Org = eOrg->Dst;; 454 {; 455 GLUvertex *newVertex= allocVertex();; 456 if (newVertex == NULL) return NULL;; 457 ; 458 MakeVertex( newVertex, eNewSym, eNew->Org );; 459 }; 460 eNew->Lface = eNewSym->Lface = eOrg->Lface;; 461 ; 462 return eNew;; 463}; 464 ; 46",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:1760,Security,authoriz,authorization,1760,"ftware is furnished to do so, subject to the following conditions:; 11 *; 12 * The above copyright notice including the dates of first publication and; 13 * either this permission notice or a reference to; 14 * http://oss.sgi.com/projects/FreeB/; 15 * shall be included in all copies or substantial portions of the Software.; 16 *; 17 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS; 18 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,; 19 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL; 20 * SILICON GRAPHICS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,; 21 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF; 22 * OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE; 23 * SOFTWARE.; 24 *; 25 * Except as contained in this notice, the name of Silicon Graphics, Inc.; 26 * shall not be used in advertising or otherwise to promote the sale, use or; 27 * other dealings in this Software without prior written authorization from; 28 * Silicon Graphics, Inc.; 29 */; 30/*; 31** Author: Eric Veach, July 1994.; 32**; 33*/; 34 ; 35#include ""gluos.h""; 36#include <stddef.h>; 37#include <assert.h>; 38#include ""mesh.h""; 39#include ""memalloc.h""; 40 ; 41#ifndef TRUE; 42#define TRUE 1; 43#endif; 44#ifndef FALSE; 45#define FALSE 0; 46#endif; 47 ; 48static GLUvertex *allocVertex(); 49{; 50 return (GLUvertex *)memAlloc( sizeof( GLUvertex ));; 51}; 52 ; 53static GLUface *allocFace(); 54{; 55 return (GLUface *)memAlloc( sizeof( GLUface ));; 56}; 57 ; 58/************************ Utility Routines ************************/; 59 ; 60/* MakeEdge creates a new pair of half-edges which form their own loop.; 61 * No vertex or face structures are allocated, but these must be assigned; 62 * before the current edge operation is completed.; 63 */; 64static GLUhalfEdge *MakeEdge( GLUhalfEdge *eNext ); 65{; 66 GLUhalfEdge *e;; 67 GLUhalfEdge *eSym;; 68 GLUhalfEdge *ePrev;; 69 ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:1933,Testability,assert,assert,1933,"://oss.sgi.com/projects/FreeB/; 15 * shall be included in all copies or substantial portions of the Software.; 16 *; 17 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS; 18 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,; 19 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL; 20 * SILICON GRAPHICS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,; 21 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF; 22 * OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE; 23 * SOFTWARE.; 24 *; 25 * Except as contained in this notice, the name of Silicon Graphics, Inc.; 26 * shall not be used in advertising or otherwise to promote the sale, use or; 27 * other dealings in this Software without prior written authorization from; 28 * Silicon Graphics, Inc.; 29 */; 30/*; 31** Author: Eric Veach, July 1994.; 32**; 33*/; 34 ; 35#include ""gluos.h""; 36#include <stddef.h>; 37#include <assert.h>; 38#include ""mesh.h""; 39#include ""memalloc.h""; 40 ; 41#ifndef TRUE; 42#define TRUE 1; 43#endif; 44#ifndef FALSE; 45#define FALSE 0; 46#endif; 47 ; 48static GLUvertex *allocVertex(); 49{; 50 return (GLUvertex *)memAlloc( sizeof( GLUvertex ));; 51}; 52 ; 53static GLUface *allocFace(); 54{; 55 return (GLUface *)memAlloc( sizeof( GLUface ));; 56}; 57 ; 58/************************ Utility Routines ************************/; 59 ; 60/* MakeEdge creates a new pair of half-edges which form their own loop.; 61 * No vertex or face structures are allocated, but these must be assigned; 62 * before the current edge operation is completed.; 63 */; 64static GLUhalfEdge *MakeEdge( GLUhalfEdge *eNext ); 65{; 66 GLUhalfEdge *e;; 67 GLUhalfEdge *eSym;; 68 GLUhalfEdge *ePrev;; 69 EdgePair *pair = (EdgePair *)memAlloc( sizeof( EdgePair ));; 70 if (pair == NULL) return NULL;; 71 ; 72 e = &pair->e;; 73 eSym = &pair->eSym;; 74 ; 75 /* Make sure eNext points to the first edge of the edge pair */; ",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:4792,Testability,assert,assert,4792,"alfEdge *b ); 113{; 114 GLUhalfEdge *aOnext = a->Onext;; 115 GLUhalfEdge *bOnext = b->Onext;; 116 ; 117 aOnext->Sym->Lnext = b;; 118 bOnext->Sym->Lnext = a;; 119 a->Onext = bOnext;; 120 b->Onext = aOnext;; 121}; 122 ; 123/* MakeVertex( newVertex, eOrig, vNext ) attaches a new vertex and makes it the; 124 * origin of all edges in the vertex loop to which eOrig belongs. ""vNext"" gives; 125 * a place to insert the new vertex in the global vertex list. We insert; 126 * the new vertex *before* vNext so that algorithms which walk the vertex; 127 * list will not see the newly created vertices.; 128 */; 129static void MakeVertex( GLUvertex *newVertex,; 130 GLUhalfEdge *eOrig, GLUvertex *vNext ); 131{; 132 GLUhalfEdge *e;; 133 GLUvertex *vPrev;; 134 GLUvertex *vNew = newVertex;; 135 ; 136 assert(vNew != NULL);; 137 ; 138 /* insert in circular doubly-linked list before vNext */; 139 vPrev = vNext->prev;; 140 vNew->prev = vPrev;; 141 vPrev->next = vNew;; 142 vNew->next = vNext;; 143 vNext->prev = vNew;; 144 ; 145 vNew->anEdge = eOrig;; 146 vNew->data = NULL;; 147 /* leave coords, s, t undefined */; 148 ; 149 /* fix other edges on this vertex loop */; 150 e = eOrig;; 151 do {; 152 e->Org = vNew;; 153 e = e->Onext;; 154 } while( e != eOrig );; 155}; 156 ; 157/* MakeFace( newFace, eOrig, fNext ) attaches a new face and makes it the left; 158 * face of all edges in the face loop to which eOrig belongs. ""fNext"" gives; 159 * a place to insert the new face in the global face list. We insert; 160 * the new face *before* fNext so that algorithms which walk the face; 161 * list will not see the newly created faces.; 162 */; 163static void MakeFace( GLUface *newFace, GLUhalfEdge *eOrig, GLUface *fNext ); 164{; 165 GLUhalfEdge *e;; 166 GLUface *fPrev;; 167 GLUface *fNew = newFace;; 168 ; 169 assert(fNew != NULL);; 170 ; 171 /* insert in circular doubly-linked list before fNext */; 172 fPrev = fNext->prev;; 173 fNew->prev = fPrev;; 174 fPrev->next = fNew;; 175 fNew->next = fNext;; 176 fNext-",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:5801,Testability,assert,assert,5801,";; 140 vNew->prev = vPrev;; 141 vPrev->next = vNew;; 142 vNew->next = vNext;; 143 vNext->prev = vNew;; 144 ; 145 vNew->anEdge = eOrig;; 146 vNew->data = NULL;; 147 /* leave coords, s, t undefined */; 148 ; 149 /* fix other edges on this vertex loop */; 150 e = eOrig;; 151 do {; 152 e->Org = vNew;; 153 e = e->Onext;; 154 } while( e != eOrig );; 155}; 156 ; 157/* MakeFace( newFace, eOrig, fNext ) attaches a new face and makes it the left; 158 * face of all edges in the face loop to which eOrig belongs. ""fNext"" gives; 159 * a place to insert the new face in the global face list. We insert; 160 * the new face *before* fNext so that algorithms which walk the face; 161 * list will not see the newly created faces.; 162 */; 163static void MakeFace( GLUface *newFace, GLUhalfEdge *eOrig, GLUface *fNext ); 164{; 165 GLUhalfEdge *e;; 166 GLUface *fPrev;; 167 GLUface *fNew = newFace;; 168 ; 169 assert(fNew != NULL);; 170 ; 171 /* insert in circular doubly-linked list before fNext */; 172 fPrev = fNext->prev;; 173 fNew->prev = fPrev;; 174 fPrev->next = fNew;; 175 fNew->next = fNext;; 176 fNext->prev = fNew;; 177 ; 178 fNew->anEdge = eOrig;; 179 fNew->data = NULL;; 180 fNew->trail = NULL;; 181 fNew->marked = FALSE;; 182 ; 183 /* The new face is marked ""inside"" if the old one was. This is a; 184 * convenience for the common case where a face has been split in two.; 185 */; 186 fNew->inside = fNext->inside;; 187 ; 188 /* fix other edges on this face loop */; 189 e = eOrig;; 190 do {; 191 e->Lface = fNew;; 192 e = e->Lnext;; 193 } while( e != eOrig );; 194}; 195 ; 196/* KillEdge( eDel ) destroys an edge (the half-edges eDel and eDel->Sym),; 197 * and removes from the global edge list.; 198 */; 199static void KillEdge( GLUhalfEdge *eDel ); 200{; 201 GLUhalfEdge *ePrev, *eNext;; 202 ; 203 /* Half-edges are allocated in pairs, see EdgePair above */; 204 if( eDel->Sym < eDel ) { eDel = eDel->Sym; }; 205 ; 206 /* delete from circular doubly-linked list */; 207 eNext = eDel->next;; 208 ePre",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:22061,Testability,assert,assert,22061,"Edge *e1 = &mesh1->eHead;; 655 GLUface *f2 = &mesh2->fHead;; 656 GLUvertex *v2 = &mesh2->vHead;; 657 GLUhalfEdge *e2 = &mesh2->eHead;; 658 ; 659 /* Add the faces, vertices, and edges of mesh2 to those of mesh1 */; 660 if( f2->next != f2 ) {; 661 f1->prev->next = f2->next;; 662 f2->next->prev = f1->prev;; 663 f2->prev->next = f1;; 664 f1->prev = f2->prev;; 665 }; 666 ; 667 if( v2->next != v2 ) {; 668 v1->prev->next = v2->next;; 669 v2->next->prev = v1->prev;; 670 v2->prev->next = v1;; 671 v1->prev = v2->prev;; 672 }; 673 ; 674 if( e2->next != e2 ) {; 675 e1->Sym->next->Sym->next = e2->next;; 676 e2->next->Sym->next = e1->Sym->next;; 677 e2->Sym->next->Sym->next = e1;; 678 e1->Sym->next = e2->Sym->next;; 679 }; 680 ; 681 memFree( mesh2 );; 682 return mesh1;; 683}; 684 ; 685 ; 686#ifdef DELETE_BY_ZAPPING; 687 ; 688/* __gl_meshDeleteMesh( mesh ) will free all storage for any valid mesh.; 689 */; 690void __gl_meshDeleteMesh( GLUmesh *mesh ); 691{; 692 GLUface *fHead = &mesh->fHead;; 693 ; 694 while( fHead->next != fHead ) {; 695 __gl_meshZapFace( fHead->next );; 696 }; 697 assert( mesh->vHead.next == &mesh->vHead );; 698 ; 699 memFree( mesh );; 700}; 701 ; 702#else; 703 ; 704/* __gl_meshDeleteMesh( mesh ) will free all storage for any valid mesh.; 705 */; 706void __gl_meshDeleteMesh( GLUmesh *mesh ); 707{; 708 GLUface *f, *fNext;; 709 GLUvertex *v, *vNext;; 710 GLUhalfEdge *e, *eNext;; 711 ; 712 for( f = mesh->fHead.next; f != &mesh->fHead; f = fNext ) {; 713 fNext = f->next;; 714 memFree( f );; 715 }; 716 ; 717 for( v = mesh->vHead.next; v != &mesh->vHead; v = vNext ) {; 718 vNext = v->next;; 719 memFree( v );; 720 }; 721 ; 722 for( e = mesh->eHead.next; e != &mesh->eHead; e = eNext ) {; 723 /* One call frees both e and e->Sym (see EdgePair above) */; 724 eNext = e->next;; 725 memFree( e );; 726 }; 727 ; 728 memFree( mesh );; 729}; 730 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_m",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23305,Testability,assert,assert,23305,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23368,Testability,assert,assert,23368,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23396,Testability,assert,assert,23396,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23429,Testability,assert,assert,23429,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23471,Testability,assert,assert,23471,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23513,Testability,assert,assert,23513,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23601,Testability,assert,assert,23601,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23767,Testability,assert,assert,23767,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23830,Testability,assert,assert,23830,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23858,Testability,assert,assert,23858,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23891,Testability,assert,assert,23891,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23933,Testability,assert,assert,23933,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:23975,Testability,assert,assert,23975,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24061,Testability,assert,assert,24061,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24227,Testability,assert,assert,24227,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24270,Testability,assert,assert,24270,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24298,Testability,assert,assert,24298,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24331,Testability,assert,assert,24331,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24362,Testability,assert,assert,24362,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24393,Testability,assert,assert,24393,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24435,Testability,assert,assert,24435,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/mesh_8c_source.html:24484,Testability,assert,assert,24484,"30 ; 731#endif; 732 ; 733#ifndef NDEBUG; 734 ; 735/* __gl_meshCheckMesh( mesh ) checks a mesh for self-consistency.; 736 */; 737void __gl_meshCheckMesh( GLUmesh *mesh ); 738{; 739 GLUface *fHead = &mesh->fHead;; 740 GLUvertex *vHead = &mesh->vHead;; 741 GLUhalfEdge *eHead = &mesh->eHead;; 742 GLUface *f, *fPrev;; 743 GLUvertex *v, *vPrev;; 744 GLUhalfEdge *e, *ePrev;; 745 ; 746 fPrev = fHead;; 747 for( fPrev = fHead ; (f = fPrev->next) != fHead; fPrev = f) {; 748 assert( f->prev == fPrev );; 749 e = f->anEdge;; 750 do {; 751 assert( e->Sym != e );; 752 assert( e->Sym->Sym == e );; 753 assert( e->Lnext->Onext->Sym == e );; 754 assert( e->Onext->Sym->Lnext == e );; 755 assert( e->Lface == f );; 756 e = e->Lnext;; 757 } while( e != f->anEdge );; 758 }; 759 assert( f->prev == fPrev && f->anEdge == NULL && f->data == NULL );; 760 ; 761 vPrev = vHead;; 762 for( vPrev = vHead ; (v = vPrev->next) != vHead; vPrev = v) {; 763 assert( v->prev == vPrev );; 764 e = v->anEdge;; 765 do {; 766 assert( e->Sym != e );; 767 assert( e->Sym->Sym == e );; 768 assert( e->Lnext->Onext->Sym == e );; 769 assert( e->Onext->Sym->Lnext == e );; 770 assert( e->Org == v );; 771 e = e->Onext;; 772 } while( e != v->anEdge );; 773 }; 774 assert( v->prev == vPrev && v->anEdge == NULL && v->data == NULL );; 775 ; 776 ePrev = eHead;; 777 for( ePrev = eHead ; (e = ePrev->next) != eHead; ePrev = e) {; 778 assert( e->Sym->next == ePrev->Sym );; 779 assert( e->Sym != e );; 780 assert( e->Sym->Sym == e );; 781 assert( e->Org != NULL );; 782 assert( e->Dst != NULL );; 783 assert( e->Lnext->Onext->Sym == e );; 784 assert( e->Onext->Sym->Lnext == e );; 785 }; 786 assert( e->Sym->next == ePrev->Sym; 787 && e->Sym == &mesh->eHeadSym; 788 && e->Sym->Sym == e; 789 && e->Org == NULL && e->Dst == NULL; 790 && e->Lface == NULL && e->Rface == NULL );; 791}; 792 ; 793#endif; b#define b(i)Definition RSha256.hxx:100; f#define f(i)Definition RSha256.hxx:104; a#define a(i)Definition RSha256.hxx:99; e#define e(i)Definition R",MatchSource.WIKI,doc/master/mesh_8c_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mesh_8c_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:2565,Availability,error,errors,2565,"* (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodBase; 34#define ROOT_TMVA_MethodBase; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodBase //; 39// //; 40// Virtual base class for all TMVA method //; 41// //; 42//////////////////////////////////////////////////////////////////////////; 43 ; 44#include <iosfwd>; 45#include <vector>; 46#include <map>; 47#include ""assert.h""; 48 ; 49#include ""TString.h""; 50 ; 51#include ""TMVA/IMethod.h""; 52#include ""TMVA/Configurable.h""; 53#include ""TMVA/Types.h""; 54#include ""TMVA/DataSet.h""; 55#include ""TMVA/Event.h""; 56#include ""TMVA/TransformationHandler.h""; 57#include <TMVA/Results.h>; 58#include ""TMVA/TrainingHistory.h""; 59 ; 60#include <TFile.h>; 61 ; 62class TGraph;; 63class TTree;; 64class TDirectory;; 65class TSpline;; 66class TH1F;; 67class TH1D;; 68class TMultiGraph;; 69 ; 70/*! \class TMVA::IPythonInteractive; 71\ingroup TMVA; 72 ; 73This class is needed by JsMVA, and it's a helper class for tracking errors during; 74the training in Jupyter notebook. It’s only initialized in Jupyter notebook context.; 75In initialization we specify some title, and a TGraph will be created for every title.; 76We can add new data points easily to all TGraphs. These graphs are added to a; 77TMultiGraph, and during an interactive training we get this TMultiGraph object; 78and plot it with JsROOT.; 79*/; 80 ; 81namespace TMVA {; 82 ; 83 class Ranking;; 84 class PDF;; 85 class TSpline1;; 86 class MethodCuts;; 87 class MethodBoost;; 88 class DataSetInfo;; 89 namespace Experimental {; 90 class Classification;; 91 }; 92 class TrainingHistory;; 93 ; 94 class IPythonInteractive {; 95 public:; 96 IPythonInteractive();; 97 ~IPythonInteractive();; 98 void Init(std::vector<TString>& graphTitles);; 99 void ClearGraphs();; 100 void AddPoint(Double_t x, Double_t y1, Double_t y2);; 101 void AddPoint(std::vecto",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:6929,Availability,error,error,6929,"t=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:6952,Availability,error,error,6952,"erforms multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(const TMVA::Event* cons",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:7313,Availability,error,errors,7313,"e );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(const TMVA::Event* const ev){; 215 fTmpEvent = ev;; 216 const std::vector<Float_t>* ptr = &GetRegressionValues();; 217 fTmpEvent = nullptr;; 218 return (*ptr);; 219 }; 220 ; 221 virtual const std::vector<Float_t>& GetRegressionValues() {; 222 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 223 return (*ptr);; 224 }; 225 ; 226 // multiclass classification response; 227 virt",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:20546,Availability,error,errors,20546,"AnalysisType type ) { fAnalysisType = type; }; 437 Types::EAnalysisType GetAnalysisType() const { return fAnalysisType; }; 438 Bool_t DoRegression() const { return fAnalysisType == Types::kRegression; }; 439 Bool_t DoMulticlass() const { return fAnalysisType == Types::kMulticlass; }; 440 ; 441 // setter method for suppressing writing to XML and writing of standalone classes; 442 void DisableWriting(Bool_t setter){ fModelPersistence = setter?kFALSE:kTRUE; }//DEPRECATED; 443 ; 444 protected:; 445 mutable const Event *fTmpEvent; //! temporary event when testing on a different DataSet than the own one; 446 DataSet *fTmpData = nullptr; //! temporary dataset used when evaluating on a different data (used by MethodCategory::GetMvaValues); 447 // helper variables for JsMVA; 448 IPythonInteractive *fInteractive = nullptr;; 449 bool fExitFromTraining = false;; 450 UInt_t fIPyMaxIter = 0, fIPyCurrentIter = 0;; 451 ; 452 public:; 453 ; 454 // initializing IPythonInteractive class (for JsMVA only); 455 inline void InitIPythonInteractive(){; 456 if (fInteractive) delete fInteractive;; 457 fInteractive = new IPythonInteractive();; 458 }; 459 ; 460 // get training errors (for JsMVA only); 461 inline TMultiGraph* GetInteractiveTrainingError(){return fInteractive->Get();}; 462 ; 463 // stop's the training process (for JsMVA only); 464 inline void ExitFromTraining(){; 465 fExitFromTraining = true;; 466 }; 467 ; 468 // check's if the training ended (for JsMVA only); 469 inline bool TrainingEnded(){; 470 if (fExitFromTraining && fInteractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:37242,Availability,error,errors,37242,"DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::EventDefinition Event.h:51; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::IPythonInteractiveThis class is needed by JsMVA, and it's a helper class for tracking errors during the training in Jup...Definition MethodBase.h:94; TMVA::IPythonInteractive::Initvoid Init(std::vector< TString > &graphTitles)This function gets some title and it creates a TGraph for every title.Definition MethodBase.cxx:169; TMVA::IPythonInteractive::IPythonInteractiveIPythonInteractive()standard constructorDefinition MethodBase.cxx:146; TMVA::IPythonInteractive::fMultiGraphTMultiGraph * fMultiGraphDefinition MethodBase.h:105; TMVA::IPythonInteractive::fGraphsstd::vector< TGraph * > fGraphsDefinition MethodBase.h:106; TMVA::IPythonInteractive::fIndexInt_t fIndexDefinition MethodBase.h:108; TMVA::IPythonInteractive::~IPythonInteractive~IPythonInteractive()standard destructorDefinition MethodBase.cxx:154; TMVA::IPythonInteractive::GetTMultiGraph * Get()Definition MethodBase.h:102; TMVA::IPythonInteractive::ClearGraphsvoid ClearGraphs()This function sets the point number to 0 for all graphs.Definition MethodBase.cxx:193; TMVA::IPythonInterac",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:61595,Availability,avail,available,61595,"::GetTransformationHandlerTransformationHandler & GetTransformationHandler(Bool_t takeReroutedIfAvailable=true)Definition MethodBase.h:394; TMVA::MethodBase::IsSilentFileBool_t IsSilentFile() constDefinition MethodBase.h:379; TMVA::MethodBase::GetMethodTypeTypes::EMVA GetMethodType() constDefinition MethodBase.h:333; TMVA::MethodBase::AddTargetsXMLTovoid AddTargetsXMLTo(void *parent) constwrite target info to XMLDefinition MethodBase.cxx:1821; TMVA::MethodBase::fResultsResults * fResultsDefinition MethodBase.h:730; TMVA::MethodBase::fXminDouble_t fXminminimum (signal and background)Definition MethodBase.h:665; TMVA::MethodBase::fDefaultPDFPDF * fDefaultPDFdefault PDF definitionsDefinition MethodBase.h:644; TMVA::MethodBase::ReadTargetsFromXMLvoid ReadTargetsFromXML(void *tarnode)read target info from XMLDefinition MethodBase.cxx:1959; TMVA::MethodBase::SetFilevoid SetFile(TFile *file)Definition MethodBase.h:375; TMVA::MethodBase::ProcessBaseOptionsvoid ProcessBaseOptions()the option string is decoded, for available options see ""DeclareOptions""Definition MethodBase.cxx:540; TMVA::MethodBase::Resetvirtual void Reset()Definition MethodBase.h:193; TMVA::MethodBase::fROOTTrainingVersionUInt_t fROOTTrainingVersionDefinition MethodBase.h:619; TMVA::MethodBase::ResetThisBasevoid ResetThisBase(); TMVA::MethodBase::ReadStateFromXMLvoid ReadStateFromXML(void *parent)Definition MethodBase.cxx:1480; TMVA::MethodBase::GetSignalReferenceCutOrientationDouble_t GetSignalReferenceCutOrientation() constDefinition MethodBase.h:361; TMVA::MethodBase::SetSignalReferenceCutvoid SetSignalReferenceCut(Double_t cut)Definition MethodBase.h:364; TMVA::MethodBase::fInputVarsstd::vector< TString > * fInputVarsDefinition MethodBase.h:588; TMVA::MethodBase::NoErrorCalcvoid NoErrorCalc(Double_t *const err, Double_t *const errUpper)Definition MethodBase.cxx:837; TMVA::MethodBase::fSignalReferenceCutDouble_t fSignalReferenceCutthe data set information (sometimes needed)Definition MethodBase.h:609; TM",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:445,Deployability,integrat,integrated,445,". ROOT: tmva/tmva/inc/TMVA/MethodBase.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodBase.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodBase *; 8 * *; 9 * *; 10 * Description: *; 11 * Virtual base class for all MVA method *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <Joerg.Stelzer@cern.ch> - CERN, Switzerland *; 17 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 18 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005-2011: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * U. of Bonn, Germany *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodBase; 34#define ROOT_TMVA_MethodBase; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodBase //; 39// //; 40// Virtual base class for all TMVA method //; 41// //; 42//////////////////////////////////////////////////////////////////////////; 43 ; 44#include <iosfwd>; 45#include <vector>; 46#include <map>; 47#include ""assert.h""; 48 ; 49#include """,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:4614,Deployability,configurat,configuration,4614,"ultiGraph;; 106 std::vector<TGraph*> fGraphs;; 107 Int_t fNumGraphs;; 108 Int_t fIndex;; 109 };; 110 ; 111 class MethodBase : virtual public IMethod, public Configurable {; 112 ; 113 friend class CrossValidation;; 114 friend class Factory;; 115 friend class RootFinder;; 116 friend class MethodBoost;; 117 friend class MethodCrossValidation;; 118 friend class Experimental::Classification;; 119 ; 120 public:; 121 ; 122 enum EWeightFileType { kROOT=0, kTEXT };; 123 ; 124 // default constructor; 125 MethodBase( const TString& jobName,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // stor",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:18195,Deployability,update,update,18195,,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:63792,Deployability,configurat,configuration,63792,"89; TMVA::MethodBase::EWeightFileTypeEWeightFileTypeDefinition MethodBase.h:122; TMVA::MethodBase::kTEXT@ kTEXTDefinition MethodBase.h:122; TMVA::MethodBase::kROOT@ kROOTDefinition MethodBase.h:122; TMVA::MethodBase::fMeanBDouble_t fMeanBmean (background)Definition MethodBase.h:662; TMVA::MethodBase::GetTransformationHandlerconst TransformationHandler & GetTransformationHandler(Bool_t takeReroutedIfAvailable=true) constDefinition MethodBase.h:398; TMVA::MethodBase::fNsmoothMVAPdfInt_t fNsmoothMVAPdfDefinition MethodBase.h:727; TMVA::MethodBase::SetSignalReferenceCutOrientationvoid SetSignalReferenceCutOrientation(Double_t cutOrientation)Definition MethodBase.h:365; TMVA::MethodBase::fBaseDirTDirectory * fBaseDirDefinition MethodBase.h:625; TMVA::MethodBase::fIgnoreNegWeightsInTrainingBool_t fIgnoreNegWeightsInTrainingIf true, events with negative weights are not used in training.Definition MethodBase.h:682; TMVA::MethodBase::WriteStateToFilevoid WriteStateToFile() constwrite options and weights to file note that each one text file for the main configuration information...Definition MethodBase.cxx:1404; TMVA::MethodBase::AddClassesXMLTovoid AddClassesXMLTo(void *parent) constwrite class info to XMLDefinition MethodBase.cxx:1801; TMVA::MethodBase::GetInputLabelconst TString & GetInputLabel(Int_t i) constDefinition MethodBase.h:350; TMVA::MethodBase::GetCutOrientationECutOrientation GetCutOrientation() constDefinition MethodBase.h:552; TMVA::MethodBase::fRankingRanking * fRankingDefinition MethodBase.h:587; TMVA::MethodBase::fTrainHistoryTrainingHistory fTrainHistoryDefinition MethodBase.h:425; TMVA::MethodBase::SetMethodBaseDirvoid SetMethodBaseDir(TDirectory *methodDir)Definition MethodBase.h:374; TMVA::MethodBase::AddClassifierOutputvirtual void AddClassifierOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:869; TMVA::MethodBase::fMVAPdfBPDF * fMVAPdfBbackground MVA PDFDefinition MethodBase.h:646; TMV",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:11598,Energy Efficiency,monitor,monitoring,11598,"268 private:; 269 friend class MethodCategory;; 270 friend class MethodCompositeBase;; 271 void WriteStateToXML ( void* parent ) const;; 272 void ReadStateFromXML ( void* parent );; 273 void WriteStateToStream ( std::ostream& tf ) const; // needed for MakeClass; 274 void WriteVarsToStream ( std::ostream& tf, const TString& prefix = """" ) const; // needed for MakeClass; 275 ; 276 ; 277 public: // these two need to be public, they are used to read in-memory weight-files; 278 void ReadStateFromStream ( std::istream& tf ); // backward compatibility; 279 void ReadStateFromStream ( TFile& rf ); // backward compatibility; 280 void ReadStateFromXMLString( const char* xmlstr ); // for reading from memory; 281 ; 282 private:; 283 // the variable information; 284 void AddVarsXMLTo ( void* parent ) const;; 285 void AddSpectatorsXMLTo ( void* parent ) const;; 286 void AddTargetsXMLTo ( void* parent ) const;; 287 void AddClassesXMLTo ( void* parent ) const;; 288 void ReadVariablesFromXML ( void* varnode );; 289 void ReadSpectatorsFromXML( void* specnode);; 290 void ReadTargetsFromXML ( void* tarnode );; 291 void ReadClassesFromXML ( void* clsnode );; 292 void ReadVarsFromStream ( std::istream& istr ); // backward compatibility; 293 ; 294 public:; 295 // ---------------------------------------------------------------------------; 296 ; 297 // write evaluation histograms into target file; 298 virtual void WriteEvaluationHistosToFile(Types::ETreeType treetype);; 299 ; 300 // write classifier-specific monitoring information to target file; 301 virtual void WriteMonitoringHistosToFile() const;; 302 ; 303 // ---------- public evaluation methods --------------------------------------; 304 ; 305 // individual initialization for testing of each method; 306 // overload this one for individual initialisation of the testing,; 307 // it is then called automatically within the global ""TestInit""; 308 ; 309 // variables (and private member functions) for the Evaluation:; 310 // get the efficiency.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:54629,Energy Efficiency,monitor,monitoring,54629,"Name(TString)set the weight file name (depreciated)Definition MethodBase.cxx:2068; TMVA::MethodBase::DataInfoDataSetInfo & DataInfo() constDefinition MethodBase.h:410; TMVA::MethodBase::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) constcreate reader class for method (classification only at present)Definition MethodBase.cxx:3003; TMVA::MethodBase::GetWeightFileNameTString GetWeightFileName() constretrieve weight file nameDefinition MethodBase.cxx:2076; TMVA::MethodBase::SetTestTimevoid SetTestTime(Double_t testTime)Definition MethodBase.h:165; TMVA::MethodBase::fMethodTypeTypes::EMVA fMethodTypeDefinition MethodBase.h:616; TMVA::MethodBase::TestClassificationvirtual void TestClassification()initializationDefinition MethodBase.cxx:1127; TMVA::MethodBase::AddOutputvoid AddOutput(Types::ETreeType type, Types::EAnalysisType analysisType)Definition MethodBase.cxx:1315; TMVA::MethodBase::WriteMonitoringHistosToFilevirtual void WriteMonitoringHistosToFile() constwrite special monitoring histograms to file dummy implementation here --------------—Definition MethodBase.cxx:2133; TMVA::MethodBase::GetNVariablesUInt_t GetNVariables() constDefinition MethodBase.h:345; TMVA::MethodBase::fAnalysisTypeTypes::EAnalysisType fAnalysisTypeDefinition MethodBase.h:595; TMVA::MethodBase::AddRegressionOutputvirtual void AddRegressionOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:744; TMVA::MethodBase::InitBasevoid InitBase()default initialization called by all constructorsDefinition MethodBase.cxx:441; TMVA::MethodBase::fEventCollectionsstd::vector< const std::vector< TMVA::Event * > * > fEventCollectionsDefinition MethodBase.h:708; TMVA::MethodBase::TrainingEndedbool TrainingEnded()Definition MethodBase.h:469; TMVA::MethodBase::GetRegressionDeviationvirtual void GetRegressionDeviation(UInt_t tgtNum, Types::ETreeType type, Double_t &stddev, Double_t &stddev90Percent) constDefinition MethodBase",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:445,Integrability,integrat,integrated,445,". ROOT: tmva/tmva/inc/TMVA/MethodBase.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodBase.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodBase *; 8 * *; 9 * *; 10 * Description: *; 11 * Virtual base class for all MVA method *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <Joerg.Stelzer@cern.ch> - CERN, Switzerland *; 17 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 18 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005-2011: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * U. of Bonn, Germany *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodBase; 34#define ROOT_TMVA_MethodBase; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodBase //; 39// //; 40// Virtual base class for all TMVA method //; 41// //; 42//////////////////////////////////////////////////////////////////////////; 43 ; 44#include <iosfwd>; 45#include <vector>; 46#include <map>; 47#include ""assert.h""; 48 ; 49#include """,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:9510,Integrability,message,message,9510,,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:30298,Integrability,depend,depends,30298,"ean (background); 663 Double_t fRmsS; ///< RMS (signal); 664 Double_t fRmsB; ///< RMS (background); 665 Double_t fXmin; ///< minimum (signal and background); 666 Double_t fXmax; ///< maximum (signal and background); 667 ; 668 // variable preprocessing; 669 TString fVarTransformString; ///< labels variable transform method; 670 ; 671 TransformationHandler* fTransformationPointer; ///< pointer to the rest of transformations; 672 TransformationHandler fTransformation; ///< the list of transformations; 673 ; 674 ; 675 // help and verbosity; 676 Bool_t fVerbose; ///< verbose flag; 677 TString fVerbosityLevelString; ///< verbosity level (user input string); 678 EMsgType fVerbosityLevel; ///< verbosity level; 679 Bool_t fHelp; ///< help flag; 680 Bool_t fHasMVAPdfs; ///< MVA Pdfs are created for this classifier; 681 ; 682 Bool_t fIgnoreNegWeightsInTraining; ///< If true, events with negative weights are not used in training; 683 ; 684 protected:; 685 ; 686 Bool_t IgnoreEventsWithNegWeightsInTraining() const { return fIgnoreNegWeightsInTraining; }; 687 ; 688 // for signal/background; 689 UInt_t fSignalClass; // index of the Signal-class; 690 UInt_t fBackgroundClass; // index of the Background-class; 691 ; 692 private:; 693 ; 694 // timing variables; 695 Double_t fTrainTime; // for timing measurements; 696 Double_t fTestTime; // for timing measurements; 697 ; 698 // orientation of cut: depends on signal and background mean values; 699 ECutOrientation fCutOrientation; // +1 if Sig>Bkg, -1 otherwise; 700 ; 701 // for root finder; 702 TSpline1* fSplRefS; // helper splines for RootFinder (signal); 703 TSpline1* fSplRefB; // helper splines for RootFinder (background); 704 ; 705 TSpline1* fSplTrainRefS; // helper splines for RootFinder (signal); 706 TSpline1* fSplTrainRefB; // helper splines for RootFinder (background); 707 ; 708 mutable std::vector<const std::vector<TMVA::Event*>*> fEventCollections; // if the method needs the complete event-collection, the transformed event coll.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:67673,Integrability,wrap,wrapper,67673,"ulticlassOutputvirtual void AddMulticlassOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:794; TMVA::MethodBase::IsConstructedFromWeightFileBool_t IsConstructedFromWeightFile() constDefinition MethodBase.h:540; TMVA::MethodBase::CheckSetupvirtual void CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodBase.cxx:433; TMVA::MethodBoostClass for boosting a TMVA method.Definition MethodBoost.h:58; TMVA::MethodCategoryClass for categorizing the phase space.Definition MethodCategory.h:58; TMVA::MethodCompositeBaseVirtual base class for combining several TMVA method.Definition MethodCompositeBase.h:50; TMVA::MethodCrossValidationDefinition MethodCrossValidation.h:38; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::RootFinderRoot finding using Brents algorithm (translated from CERNLIB function RZERO)Definition RootFinder.h:48; TMVA::TSpline1Linear interpolation of TGraph.Definition TSpline1.h:43; TMVA::TrainingHistoryTracking data from training.Definition TrainingHistory.h:31; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::Transformconst Event * Transform(const Event *) constthe transformationDefinition TransformationHandler.cxx:152; TMVA::TransformationHandler::GetRMSDouble_t GetRMS(Int_t ivar, Int_t cls=-1) constDefinition TransformationHandler.cxx:955; TMVA::TransformationHandler::GetMeanDouble_t GetMean(Int_t ivar, Int_t cls=-1) constDefinition TransformationHan",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:4614,Modifiability,config,configuration,4614,"ultiGraph;; 106 std::vector<TGraph*> fGraphs;; 107 Int_t fNumGraphs;; 108 Int_t fIndex;; 109 };; 110 ; 111 class MethodBase : virtual public IMethod, public Configurable {; 112 ; 113 friend class CrossValidation;; 114 friend class Factory;; 115 friend class RootFinder;; 116 friend class MethodBoost;; 117 friend class MethodCrossValidation;; 118 friend class Experimental::Classification;; 119 ; 120 public:; 121 ; 122 enum EWeightFileType { kROOT=0, kTEXT };; 123 ; 124 // default constructor; 125 MethodBase( const TString& jobName,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // stor",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:4917,Modifiability,variab,variable,4917,"; 117 friend class MethodCrossValidation;; 118 friend class Experimental::Classification;; 119 ; 120 public:; 121 ; 122 enum EWeightFileType { kROOT=0, kTEXT };; 123 ; 124 // default constructor; 125 MethodBase( const TString& jobName,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:10826,Modifiability,variab,variable,10826,"oid* parent ) const = 0;; 264 virtual void ReadWeightsFromXML ( void* wghtnode ) = 0;; 265 virtual void ReadWeightsFromStream( std::istream& ) = 0; // backward compatibility; 266 virtual void ReadWeightsFromStream( TFile& ) {} // backward compatibility; 267 ; 268 private:; 269 friend class MethodCategory;; 270 friend class MethodCompositeBase;; 271 void WriteStateToXML ( void* parent ) const;; 272 void ReadStateFromXML ( void* parent );; 273 void WriteStateToStream ( std::ostream& tf ) const; // needed for MakeClass; 274 void WriteVarsToStream ( std::ostream& tf, const TString& prefix = """" ) const; // needed for MakeClass; 275 ; 276 ; 277 public: // these two need to be public, they are used to read in-memory weight-files; 278 void ReadStateFromStream ( std::istream& tf ); // backward compatibility; 279 void ReadStateFromStream ( TFile& rf ); // backward compatibility; 280 void ReadStateFromXMLString( const char* xmlstr ); // for reading from memory; 281 ; 282 private:; 283 // the variable information; 284 void AddVarsXMLTo ( void* parent ) const;; 285 void AddSpectatorsXMLTo ( void* parent ) const;; 286 void AddTargetsXMLTo ( void* parent ) const;; 287 void AddClassesXMLTo ( void* parent ) const;; 288 void ReadVariablesFromXML ( void* varnode );; 289 void ReadSpectatorsFromXML( void* specnode);; 290 void ReadTargetsFromXML ( void* tarnode );; 291 void ReadClassesFromXML ( void* clsnode );; 292 void ReadVarsFromStream ( std::istream& istr ); // backward compatibility; 293 ; 294 public:; 295 // ---------------------------------------------------------------------------; 296 ; 297 // write evaluation histograms into target file; 298 virtual void WriteEvaluationHistosToFile(Types::ETreeType treetype);; 299 ; 300 // write classifier-specific monitoring information to target file; 301 virtual void WriteMonitoringHistosToFile() const;; 302 ; 303 // ---------- public evaluation methods --------------------------------------; 304 ; 305 // individual initialization for testin",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:12003,Modifiability,variab,variables,12003,"268 private:; 269 friend class MethodCategory;; 270 friend class MethodCompositeBase;; 271 void WriteStateToXML ( void* parent ) const;; 272 void ReadStateFromXML ( void* parent );; 273 void WriteStateToStream ( std::ostream& tf ) const; // needed for MakeClass; 274 void WriteVarsToStream ( std::ostream& tf, const TString& prefix = """" ) const; // needed for MakeClass; 275 ; 276 ; 277 public: // these two need to be public, they are used to read in-memory weight-files; 278 void ReadStateFromStream ( std::istream& tf ); // backward compatibility; 279 void ReadStateFromStream ( TFile& rf ); // backward compatibility; 280 void ReadStateFromXMLString( const char* xmlstr ); // for reading from memory; 281 ; 282 private:; 283 // the variable information; 284 void AddVarsXMLTo ( void* parent ) const;; 285 void AddSpectatorsXMLTo ( void* parent ) const;; 286 void AddTargetsXMLTo ( void* parent ) const;; 287 void AddClassesXMLTo ( void* parent ) const;; 288 void ReadVariablesFromXML ( void* varnode );; 289 void ReadSpectatorsFromXML( void* specnode);; 290 void ReadTargetsFromXML ( void* tarnode );; 291 void ReadClassesFromXML ( void* clsnode );; 292 void ReadVarsFromStream ( std::istream& istr ); // backward compatibility; 293 ; 294 public:; 295 // ---------------------------------------------------------------------------; 296 ; 297 // write evaluation histograms into target file; 298 virtual void WriteEvaluationHistosToFile(Types::ETreeType treetype);; 299 ; 300 // write classifier-specific monitoring information to target file; 301 virtual void WriteMonitoringHistosToFile() const;; 302 ; 303 // ---------- public evaluation methods --------------------------------------; 304 ; 305 // individual initialization for testing of each method; 306 // overload this one for individual initialisation of the testing,; 307 // it is then called automatically within the global ""TestInit""; 308 ; 309 // variables (and private member functions) for the Evaluation:; 310 // get the efficiency.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:14247,Modifiability,variab,variable,14247,"; 326 virtual void GetRegressionDeviation(UInt_t tgtNum, Types::ETreeType type, Double_t& stddev,Double_t& stddev90Percent ) const;; 327 // ---------- public accessors -----------------------------------------------; 328 ; 329 // classifier naming (a lot of names ... aren't they ;-); 330 const TString& GetJobName () const { return fJobName; }; 331 const TString& GetMethodName () const { return fMethodName; }; 332 TString GetMethodTypeName() const { return Types::Instance().GetMethodName(fMethodType); }; 333 Types::EMVA GetMethodType () const { return fMethodType; }; 334 const char* GetName () const { return fMethodName.Data(); }; 335 const TString& GetTestvarName () const { return fTestvar; }; 336 const TString GetProbaName () const { return fTestvar + ""_Proba""; }; 337 TString GetWeightFileName() const;; 338 ; 339 // build classifier name in Test tree; 340 // MVA prefix (e.g., ""TMVA_""); 341 void SetTestvarName ( const TString & v="""" ) { fTestvar = (v=="""") ? (""MVA_"" + GetMethodName()) : v; }; 342 ; 343 // number of input variable used by classifier; 344 UInt_t GetNvar() const { return DataInfo().GetNVariables(); }; 345 UInt_t GetNVariables() const { return DataInfo().GetNVariables(); }; 346 UInt_t GetNTargets() const { return DataInfo().GetNTargets(); };; 347 ; 348 // internal names and expressions of input variables; 349 const TString& GetInputVar ( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetInternalName(); }; 350 const TString& GetInputLabel( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetLabel(); }; 351 const char * GetInputTitle( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetTitle(); }; 352 ; 353 // normalisation and limit accessors; 354 Double_t GetMean( Int_t ivar ) const { return GetTransformationHandler().GetMean(ivar); }; 355 Double_t GetRMS ( Int_t ivar ) const { return GetTransformationHandler().GetRMS(ivar); }; 356 Double_t GetXmin( Int_t ivar ) const { return GetTransformationHandler().GetMin(ivar); }; 357 Double_t G",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:14539,Modifiability,variab,variables,14539," fJobName; }; 331 const TString& GetMethodName () const { return fMethodName; }; 332 TString GetMethodTypeName() const { return Types::Instance().GetMethodName(fMethodType); }; 333 Types::EMVA GetMethodType () const { return fMethodType; }; 334 const char* GetName () const { return fMethodName.Data(); }; 335 const TString& GetTestvarName () const { return fTestvar; }; 336 const TString GetProbaName () const { return fTestvar + ""_Proba""; }; 337 TString GetWeightFileName() const;; 338 ; 339 // build classifier name in Test tree; 340 // MVA prefix (e.g., ""TMVA_""); 341 void SetTestvarName ( const TString & v="""" ) { fTestvar = (v=="""") ? (""MVA_"" + GetMethodName()) : v; }; 342 ; 343 // number of input variable used by classifier; 344 UInt_t GetNvar() const { return DataInfo().GetNVariables(); }; 345 UInt_t GetNVariables() const { return DataInfo().GetNVariables(); }; 346 UInt_t GetNTargets() const { return DataInfo().GetNTargets(); };; 347 ; 348 // internal names and expressions of input variables; 349 const TString& GetInputVar ( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetInternalName(); }; 350 const TString& GetInputLabel( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetLabel(); }; 351 const char * GetInputTitle( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetTitle(); }; 352 ; 353 // normalisation and limit accessors; 354 Double_t GetMean( Int_t ivar ) const { return GetTransformationHandler().GetMean(ivar); }; 355 Double_t GetRMS ( Int_t ivar ) const { return GetTransformationHandler().GetRMS(ivar); }; 356 Double_t GetXmin( Int_t ivar ) const { return GetTransformationHandler().GetMin(ivar); }; 357 Double_t GetXmax( Int_t ivar ) const { return GetTransformationHandler().GetMax(ivar); }; 358 ; 359 // sets the minimum requirement on the MVA output to declare an event signal-like; 360 Double_t GetSignalReferenceCut() const { return fSignalReferenceCut; }; 361 Double_t GetSignalReferenceCutOrientation() const { return fSignalReferenceCutO",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:20135,Modifiability,variab,variables,20135,"_S(x) dx } = Int_[xC,+oo] { PDF_B(x) dx }; 431 virtual Bool_t IsSignalLike();; 432 virtual Bool_t IsSignalLike(Double_t mvaVal);; 433 ; 434 ; 435 Bool_t HasMVAPdfs() const { return fHasMVAPdfs; }; 436 virtual void SetAnalysisType( Types::EAnalysisType type ) { fAnalysisType = type; }; 437 Types::EAnalysisType GetAnalysisType() const { return fAnalysisType; }; 438 Bool_t DoRegression() const { return fAnalysisType == Types::kRegression; }; 439 Bool_t DoMulticlass() const { return fAnalysisType == Types::kMulticlass; }; 440 ; 441 // setter method for suppressing writing to XML and writing of standalone classes; 442 void DisableWriting(Bool_t setter){ fModelPersistence = setter?kFALSE:kTRUE; }//DEPRECATED; 443 ; 444 protected:; 445 mutable const Event *fTmpEvent; //! temporary event when testing on a different DataSet than the own one; 446 DataSet *fTmpData = nullptr; //! temporary dataset used when evaluating on a different data (used by MethodCategory::GetMvaValues); 447 // helper variables for JsMVA; 448 IPythonInteractive *fInteractive = nullptr;; 449 bool fExitFromTraining = false;; 450 UInt_t fIPyMaxIter = 0, fIPyCurrentIter = 0;; 451 ; 452 public:; 453 ; 454 // initializing IPythonInteractive class (for JsMVA only); 455 inline void InitIPythonInteractive(){; 456 if (fInteractive) delete fInteractive;; 457 fInteractive = new IPythonInteractive();; 458 }; 459 ; 460 // get training errors (for JsMVA only); 461 inline TMultiGraph* GetInteractiveTrainingError(){return fInteractive->Get();}; 462 ; 463 // stop's the training process (for JsMVA only); 464 inline void ExitFromTraining(){; 465 fExitFromTraining = true;; 466 }; 467 ; 468 // check's if the training ended (for JsMVA only); 469 inline bool TrainingEnded(){; 470 if (fExitFromTraining && fInteractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurre",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:21460,Modifiability,config,config,21460,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:21467,Modifiability,variab,variable,21467,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:21660,Modifiability,variab,variables,21660,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:21835,Modifiability,variab,variables,21835,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:22193,Modifiability,variab,variables,22193,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:25412,Modifiability,variab,variables,25412,"Rarity); 562 void CreateMVAPdfs();; 563 ; 564 // for root finder; 565 //virtual method to find ROOT; 566 virtual Double_t GetValueForRoot ( Double_t ); // implementation; 567 ; 568 // used for file parsing; 569 Bool_t GetLine( std::istream& fin, char * buf );; 570 ; 571 // fill test tree with classification or regression results; 572 virtual void AddClassifierOutput ( Types::ETreeType type );; 573 virtual void AddClassifierOutputProb( Types::ETreeType type );; 574 virtual void AddRegressionOutput ( Types::ETreeType type );; 575 virtual void AddMulticlassOutput ( Types::ETreeType type );; 576 ; 577 private:; 578 ; 579 void AddInfoItem( void* gi, const TString& name,; 580 const TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientati",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:25512,Modifiability,variab,variable,25512,"; 566 virtual Double_t GetValueForRoot ( Double_t ); // implementation; 567 ; 568 // used for file parsing; 569 Bool_t GetLine( std::istream& fin, char * buf );; 570 ; 571 // fill test tree with classification or regression results; 572 virtual void AddClassifierOutput ( Types::ETreeType type );; 573 virtual void AddClassifierOutputProb( Types::ETreeType type );; 574 virtual void AddRegressionOutput ( Types::ETreeType type );; 575 virtual void AddMulticlassOutput ( Types::ETreeType type );; 576 ; 577 private:; 578 ; 579 void AddInfoItem( void* gi, const TString& name,; 580 const TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientation; // minimum requirement on the MVA output to declare an event signal-like; 611 Types::ESBType fVa",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:26065,Modifiability,variab,variables,26065,"const TString& name,; 580 const TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientation; // minimum requirement on the MVA output to declare an event signal-like; 611 Types::ESBType fVariableTransformType; // this is the event type (sig or bgd) assumed for variable transform; 612 ; 613 // naming and versioning; 614 TString fJobName; // name of job -> user defined, appears in weight files; 615 TString fMethodName; // name of the method (set in derived class); 616 Types::EMVA fMethodType; // type of method (set in derived class); 617 TString fTestvar; // variable used in evaluation, etc (mostly the MVA); 618 UInt_t fTMVATrainingVersion; // TMVA version used for training; 619 UInt_t fROOTTrainingVersion; // ROOT version used for tr",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:26589,Modifiability,variab,variable,26589,"istograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientation; // minimum requirement on the MVA output to declare an event signal-like; 611 Types::ESBType fVariableTransformType; // this is the event type (sig or bgd) assumed for variable transform; 612 ; 613 // naming and versioning; 614 TString fJobName; // name of job -> user defined, appears in weight files; 615 TString fMethodName; // name of the method (set in derived class); 616 Types::EMVA fMethodType; // type of method (set in derived class); 617 TString fTestvar; // variable used in evaluation, etc (mostly the MVA); 618 UInt_t fTMVATrainingVersion; // TMVA version used for training; 619 UInt_t fROOTTrainingVersion; // ROOT version used for training; 620 Bool_t fConstructedFromWeightFile; // is it obtained from weight file?; 621 ; 622 // Directory structure: dataloader/fMethodBaseDir/fBaseDir; 623 // where the first directory name is defined by the method type; 624 // and the second is user supplied (the title given in Factory::BookMethod()); 625 TDirectory* fBaseDir; // base directory for the instance, needed to know where to jump back from localDir; 626 mutable TDirectory* fMethodBaseDir; // base directory for the method; 627 //this will be the next way t",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:26891,Modifiability,variab,variable,26891,"loat_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientation; // minimum requirement on the MVA output to declare an event signal-like; 611 Types::ESBType fVariableTransformType; // this is the event type (sig or bgd) assumed for variable transform; 612 ; 613 // naming and versioning; 614 TString fJobName; // name of job -> user defined, appears in weight files; 615 TString fMethodName; // name of the method (set in derived class); 616 Types::EMVA fMethodType; // type of method (set in derived class); 617 TString fTestvar; // variable used in evaluation, etc (mostly the MVA); 618 UInt_t fTMVATrainingVersion; // TMVA version used for training; 619 UInt_t fROOTTrainingVersion; // ROOT version used for training; 620 Bool_t fConstructedFromWeightFile; // is it obtained from weight file?; 621 ; 622 // Directory structure: dataloader/fMethodBaseDir/fBaseDir; 623 // where the first directory name is defined by the method type; 624 // and the second is user supplied (the title given in Factory::BookMethod()); 625 TDirectory* fBaseDir; // base directory for the instance, needed to know where to jump back from localDir; 626 mutable TDirectory* fMethodBaseDir; // base directory for the method; 627 //this will be the next way to save results; 628 TFile *fFile;; 629 ; 630 //SilentFile; 631 Bool_t fSilentFile;; 632 //Model Persistence; 633 Bool_t fModelPersistence;; 634 ; 635 TString fParentDir; ///< method parent name, like booster name; 636 ; 637 TString fFileDir; ///< unix sub-directory for weight files (default: DataLoade",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:29127,Modifiability,variab,variable,29127,,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:29196,Modifiability,variab,variable,29196,,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:30149,Modifiability,variab,variables,30149,"ean (background); 663 Double_t fRmsS; ///< RMS (signal); 664 Double_t fRmsB; ///< RMS (background); 665 Double_t fXmin; ///< minimum (signal and background); 666 Double_t fXmax; ///< maximum (signal and background); 667 ; 668 // variable preprocessing; 669 TString fVarTransformString; ///< labels variable transform method; 670 ; 671 TransformationHandler* fTransformationPointer; ///< pointer to the rest of transformations; 672 TransformationHandler fTransformation; ///< the list of transformations; 673 ; 674 ; 675 // help and verbosity; 676 Bool_t fVerbose; ///< verbose flag; 677 TString fVerbosityLevelString; ///< verbosity level (user input string); 678 EMsgType fVerbosityLevel; ///< verbosity level; 679 Bool_t fHelp; ///< help flag; 680 Bool_t fHasMVAPdfs; ///< MVA Pdfs are created for this classifier; 681 ; 682 Bool_t fIgnoreNegWeightsInTraining; ///< If true, events with negative weights are not used in training; 683 ; 684 protected:; 685 ; 686 Bool_t IgnoreEventsWithNegWeightsInTraining() const { return fIgnoreNegWeightsInTraining; }; 687 ; 688 // for signal/background; 689 UInt_t fSignalClass; // index of the Signal-class; 690 UInt_t fBackgroundClass; // index of the Background-class; 691 ; 692 private:; 693 ; 694 // timing variables; 695 Double_t fTrainTime; // for timing measurements; 696 Double_t fTestTime; // for timing measurements; 697 ; 698 // orientation of cut: depends on signal and background mean values; 699 ECutOrientation fCutOrientation; // +1 if Sig>Bkg, -1 otherwise; 700 ; 701 // for root finder; 702 TSpline1* fSplRefS; // helper splines for RootFinder (signal); 703 TSpline1* fSplRefB; // helper splines for RootFinder (background); 704 ; 705 TSpline1* fSplTrainRefS; // helper splines for RootFinder (signal); 706 TSpline1* fSplTrainRefB; // helper splines for RootFinder (background); 707 ; 708 mutable std::vector<const std::vector<TMVA::Event*>*> fEventCollections; // if the method needs the complete event-collection, the transformed event coll.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:31424,Modifiability,variab,variables,31424,"r (background); 707 ; 708 mutable std::vector<const std::vector<TMVA::Event*>*> fEventCollections; // if the method needs the complete event-collection, the transformed event coll. ist stored here.; 709 ; 710 public:; 711 Bool_t fSetupCompleted; // is method setup; 712 ; 713 private:; 714 ; 715 // This is a workaround for OSx where static thread_local data members are; 716 // not supported. The C++ solution would indeed be the following:; 717// static MethodBase*& GetThisBaseThreadLocal() {TTHREAD_TLS(MethodBase*) fgThisBase(nullptr); return fgThisBase; };; 718 ; 719 // ===== depreciated options, kept for backward compatibility =====; 720 private:; 721 ; 722 Bool_t fNormalise; // normalise input variables; 723 Bool_t fUseDecorr; // synonymous for decorrelation; 724 TString fVariableTransformTypeString; // labels variable transform type; 725 Bool_t fTxtWeightsOnly; // if TRUE, write weights only to text files; 726 Int_t fNbinsMVAPdf; // number of bins used in histogram that creates PDF; 727 Int_t fNsmoothMVAPdf; // number of times a histogram is smoothed before creating the PDF; 728 ; 729 protected:; 730 Results *fResults;; 731 ClassDef(MethodBase,0); // Virtual base class for all TMVA method; 732 ; 733 };; 734} // namespace TMVA; 735 ; 736 ; 737 ; 738 ; 739 ; 740 ; 741 ; 742// ========== INLINE FUNCTIONS =========================================================; 743 ; 744 ; 745//_______________________________________________________________________; 746inline const TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:31543,Modifiability,variab,variable,31543,"r (background); 707 ; 708 mutable std::vector<const std::vector<TMVA::Event*>*> fEventCollections; // if the method needs the complete event-collection, the transformed event coll. ist stored here.; 709 ; 710 public:; 711 Bool_t fSetupCompleted; // is method setup; 712 ; 713 private:; 714 ; 715 // This is a workaround for OSx where static thread_local data members are; 716 // not supported. The C++ solution would indeed be the following:; 717// static MethodBase*& GetThisBaseThreadLocal() {TTHREAD_TLS(MethodBase*) fgThisBase(nullptr); return fgThisBase; };; 718 ; 719 // ===== depreciated options, kept for backward compatibility =====; 720 private:; 721 ; 722 Bool_t fNormalise; // normalise input variables; 723 Bool_t fUseDecorr; // synonymous for decorrelation; 724 TString fVariableTransformTypeString; // labels variable transform type; 725 Bool_t fTxtWeightsOnly; // if TRUE, write weights only to text files; 726 Int_t fNbinsMVAPdf; // number of bins used in histogram that creates PDF; 727 Int_t fNsmoothMVAPdf; // number of times a histogram is smoothed before creating the PDF; 728 ; 729 protected:; 730 Results *fResults;; 731 ClassDef(MethodBase,0); // Virtual base class for all TMVA method; 732 ; 733 };; 734} // namespace TMVA; 735 ; 736 ; 737 ; 738 ; 739 ; 740 ; 741 ; 742// ========== INLINE FUNCTIONS =========================================================; 743 ; 744 ; 745//_______________________________________________________________________; 746inline const TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:41529,Modifiability,variab,variables,41529," TMVA::MethodBase::fFileDirTString fFileDirunix sub-directory for weight files (default: DataLoader's Name + ""weights"")Definition MethodBase.h:637; TMVA::MethodBase::fTmpDataDataSet * fTmpDatatemporary event when testing on a different DataSet than the own oneDefinition MethodBase.h:446; TMVA::MethodBase::HasTrainingTreeBool_t HasTrainingTree() constDefinition MethodBase.h:513; TMVA::MethodBase::DeclareOptionsvirtual void DeclareOptions()=0; TMVA::MethodBase::SetSilentFilevoid SetSilentFile(Bool_t status)Definition MethodBase.h:378; TMVA::MethodBase::ReadClassesFromXMLvoid ReadClassesFromXML(void *clsnode)read number of classes from XMLDefinition MethodBase.cxx:1917; TMVA::MethodBase::GetInteractiveTrainingErrorTMultiGraph * GetInteractiveTrainingError()Definition MethodBase.h:461; TMVA::MethodBase::SetWeightFileDirvoid SetWeightFileDir(TString fileDir)set directory of weight fileDefinition MethodBase.cxx:2059; TMVA::MethodBase::WriteStateToXMLvoid WriteStateToXML(void *parent) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1331; TMVA::MethodBase::fSilentFileBool_t fSilentFileDefinition MethodBase.h:631; TMVA::MethodBase::DeclareBaseOptionsvoid DeclareBaseOptions()define the options (their key words) that can be set in the option string here the options valid for ...Definition MethodBase.cxx:509; TMVA::MethodBase::GetMaxIterUInt_t GetMaxIter()Definition MethodBase.h:478; TMVA::MethodBase::fRmsBDouble_t fRmsBRMS (background)Definition MethodBase.h:664; TMVA::MethodBase::GetXminDouble_t GetXmin(Int_t ivar) constDefinition MethodBase.h:356; TMVA::MethodBase::VerboseBool_t Verbose() constDefinition MethodBase.h:503; TMVA::MethodBase::TestRegressionvirtual void TestRegression(Double_t &bias, Double_t &biasT, Double_t &dev, Double_t &devT, Double_t &rms, Double_t &rmsT, Double_t &mInf, Double_t &mInfT, Double_t &corr, Types::ETreeType type)calculate <sum-of-deviation-squared> of regression output ver",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:48691,Modifiability,variab,variableDefinition,48691,"> GetMulticlassEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2703; TMVA::MethodBase::GetNTargetsUInt_t GetNTargets() constDefinition MethodBase.h:346; TMVA::MethodBase::fVerbosityLevelEMsgType fVerbosityLevelverbosity levelDefinition MethodBase.h:678; TMVA::MethodBase::GetProbaNameconst TString GetProbaName() constDefinition MethodBase.h:336; TMVA::MethodBase::fIPyMaxIterUInt_t fIPyMaxIterDefinition MethodBase.h:450; TMVA::MethodBase::fTransformationTransformationHandler fTransformationthe list of transformationsDefinition MethodBase.h:672; TMVA::MethodBase::ReadWeightsFromStreamvirtual void ReadWeightsFromStream(std::istream &)=0; TMVA::MethodBase::AddInfoItemvoid AddInfoItem(void *gi, const TString &name, const TString &value) constxml writingDefinition MethodBase.cxx:1306; TMVA::MethodBase::fMethodBaseDirTDirectory * fMethodBaseDirDefinition MethodBase.h:626; TMVA::MethodBase::AddClassifierOutputProbvirtual void AddClassifierOutputProb(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:951; TMVA::MethodBase::fSplSPDF * fSplSPDFs of MVA distribution (signal)Definition MethodBase.h:650; TMVA::MethodBase::MakeClassSpecificvirtual void MakeClassSpecific(std::ostream &, const TString &="""") constDefinition MethodBase.h:520; TMVA::MethodBase::fSignalReferenceCutOrientationDouble_t fSignalReferenceCutOrientationDefinition MethodBase.h:610; TMVA::MethodBase::GetEfficiencyvirtual Double_t GetEfficiency(const TString &, Types::ETreeType, Double_t &err)fill background efficiency (resp.Definition MethodBase.cxx:2302; TMVA::MethodBase::fNbinsMVAoutputInt_t fNbinsMVAoutputDefinition MethodBase.h:592; TMVA::MethodBase::GetTrainingTMVAVersionStringTString GetTrainingTMVAVersionString() constcalculates the TMVA version string from the training version code on the flyDefinition MethodBase.cxx:3369; TMVA::MethodBase::fSpleffBvsSTSpline * fSpleffBvsSsplines for signal eff. versus background",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:50631,Modifiability,variab,variable,50631,"se::fSpleffBvsSTSpline * fSpleffBvsSsplines for signal eff. versus background eff.Definition MethodBase.h:652; TMVA::MethodBase::fVariableTransformTypeStringTString fVariableTransformTypeStringDefinition MethodBase.h:724; TMVA::MethodBase::fSplTrainRefBTSpline1 * fSplTrainRefBDefinition MethodBase.h:706; TMVA::MethodBase::GetWeightFileDirconst TString & GetWeightFileDir() constDefinition MethodBase.h:492; TMVA::MethodBase::SetAnalysisTypevirtual void SetAnalysisType(Types::EAnalysisType type)Definition MethodBase.h:436; TMVA::MethodBase::GetCurrentIterUInt_t GetCurrentIter()Definition MethodBase.h:481; TMVA::MethodBase::fTestvarTString fTestvarDefinition MethodBase.h:617; TMVA::MethodBase::GetMethodNameconst TString & GetMethodName() constDefinition MethodBase.h:331; TMVA::MethodBase::TxtWeightsOnlyBool_t TxtWeightsOnly() constDefinition MethodBase.h:534; TMVA::MethodBase::Statisticsvoid Statistics(Types::ETreeType treeType, const TString &theVarName, Double_t &, Double_t &, Double_t &, Double_t &, Double_t &, Double_t &)calculates rms,mean, xmin, xmax of the event variable this can be either done for the variables as th...Definition MethodBase.cxx:2942; TMVA::MethodBase::ReadWeightsFromStreamvirtual void ReadWeightsFromStream(TFile &)Definition MethodBase.h:266; TMVA::MethodBase::ExitFromTrainingvoid ExitFromTraining()Definition MethodBase.h:464; TMVA::MethodBase::GetNEventsUInt_t GetNEvents() constDefinition MethodBase.h:416; TMVA::MethodBase::DoRegressionBool_t DoRegression() constDefinition MethodBase.h:438; TMVA::MethodBase::fRegressionReturnValstd::vector< Float_t > * fRegressionReturnValDefinition MethodBase.h:597; TMVA::MethodBase::fMulticlassReturnValstd::vector< Float_t > * fMulticlassReturnValDefinition MethodBase.h:598; TMVA::MethodBase::fRmsSDouble_t fRmsSRMS (signal)Definition MethodBase.h:663; TMVA::MethodBase::GetLineBool_t GetLine(std::istream &fin, char *buf)reads one line from the input stream checks for certain keywords and interprets the line if",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:50672,Modifiability,variab,variables,50672,"se::fSpleffBvsSTSpline * fSpleffBvsSsplines for signal eff. versus background eff.Definition MethodBase.h:652; TMVA::MethodBase::fVariableTransformTypeStringTString fVariableTransformTypeStringDefinition MethodBase.h:724; TMVA::MethodBase::fSplTrainRefBTSpline1 * fSplTrainRefBDefinition MethodBase.h:706; TMVA::MethodBase::GetWeightFileDirconst TString & GetWeightFileDir() constDefinition MethodBase.h:492; TMVA::MethodBase::SetAnalysisTypevirtual void SetAnalysisType(Types::EAnalysisType type)Definition MethodBase.h:436; TMVA::MethodBase::GetCurrentIterUInt_t GetCurrentIter()Definition MethodBase.h:481; TMVA::MethodBase::fTestvarTString fTestvarDefinition MethodBase.h:617; TMVA::MethodBase::GetMethodNameconst TString & GetMethodName() constDefinition MethodBase.h:331; TMVA::MethodBase::TxtWeightsOnlyBool_t TxtWeightsOnly() constDefinition MethodBase.h:534; TMVA::MethodBase::Statisticsvoid Statistics(Types::ETreeType treeType, const TString &theVarName, Double_t &, Double_t &, Double_t &, Double_t &, Double_t &, Double_t &)calculates rms,mean, xmin, xmax of the event variable this can be either done for the variables as th...Definition MethodBase.cxx:2942; TMVA::MethodBase::ReadWeightsFromStreamvirtual void ReadWeightsFromStream(TFile &)Definition MethodBase.h:266; TMVA::MethodBase::ExitFromTrainingvoid ExitFromTraining()Definition MethodBase.h:464; TMVA::MethodBase::GetNEventsUInt_t GetNEvents() constDefinition MethodBase.h:416; TMVA::MethodBase::DoRegressionBool_t DoRegression() constDefinition MethodBase.h:438; TMVA::MethodBase::fRegressionReturnValstd::vector< Float_t > * fRegressionReturnValDefinition MethodBase.h:597; TMVA::MethodBase::fMulticlassReturnValstd::vector< Float_t > * fMulticlassReturnValDefinition MethodBase.h:598; TMVA::MethodBase::fRmsSDouble_t fRmsSRMS (signal)Definition MethodBase.h:663; TMVA::MethodBase::GetLineBool_t GetLine(std::istream &fin, char *buf)reads one line from the input stream checks for certain keywords and interprets the line if",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:51750,Modifiability,variab,variable,51750,"Base::ReadWeightsFromStreamvirtual void ReadWeightsFromStream(TFile &)Definition MethodBase.h:266; TMVA::MethodBase::ExitFromTrainingvoid ExitFromTraining()Definition MethodBase.h:464; TMVA::MethodBase::GetNEventsUInt_t GetNEvents() constDefinition MethodBase.h:416; TMVA::MethodBase::DoRegressionBool_t DoRegression() constDefinition MethodBase.h:438; TMVA::MethodBase::fRegressionReturnValstd::vector< Float_t > * fRegressionReturnValDefinition MethodBase.h:597; TMVA::MethodBase::fMulticlassReturnValstd::vector< Float_t > * fMulticlassReturnValDefinition MethodBase.h:598; TMVA::MethodBase::fRmsSDouble_t fRmsSRMS (signal)Definition MethodBase.h:663; TMVA::MethodBase::GetLineBool_t GetLine(std::istream &fin, char *buf)reads one line from the input stream checks for certain keywords and interprets the line if keywords ...Definition MethodBase.cxx:2142; TMVA::MethodBase::GetEventconst Event * GetEvent() constDefinition MethodBase.h:751; TMVA::MethodBase::fVarTransformStringTString fVarTransformStringlabels variable transform methodDefinition MethodBase.h:669; TMVA::MethodBase::ProcessSetupvoid ProcessSetup()process all options the ""CheckForUnusedOptions"" is done in an independent call, since it may be overr...Definition MethodBase.cxx:423; TMVA::MethodBase::ProcessOptionsvirtual void ProcessOptions()=0; TMVA::MethodBase::GetMvaValuesvirtual std::vector< Double_t > GetMvaValues(Long64_t firstEvt=0, Long64_t lastEvt=-1, Bool_t logProgress=false)get all the MVA values for the events of the current Data typeDefinition MethodBase.cxx:898; TMVA::MethodBase::IsSignalLikevirtual Bool_t IsSignalLike()uses a pre-set cut on the MVA output (SetSignalReferenceCut and SetSignalReferenceCutOrientation) for...Definition MethodBase.cxx:855; TMVA::MethodBase::RerouteTransformationHandlervoid RerouteTransformationHandler(TransformationHandler *fTargetTransformation)Definition MethodBase.h:403; TMVA::MethodBase::~MethodBasevirtual ~MethodBase()destructorDefinition MethodBase.cxx:364; TMVA::Me",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:55059,Modifiability,variab,variableDefinition,55059,"WeightFileNameTString GetWeightFileName() constretrieve weight file nameDefinition MethodBase.cxx:2076; TMVA::MethodBase::SetTestTimevoid SetTestTime(Double_t testTime)Definition MethodBase.h:165; TMVA::MethodBase::fMethodTypeTypes::EMVA fMethodTypeDefinition MethodBase.h:616; TMVA::MethodBase::TestClassificationvirtual void TestClassification()initializationDefinition MethodBase.cxx:1127; TMVA::MethodBase::AddOutputvoid AddOutput(Types::ETreeType type, Types::EAnalysisType analysisType)Definition MethodBase.cxx:1315; TMVA::MethodBase::WriteMonitoringHistosToFilevirtual void WriteMonitoringHistosToFile() constwrite special monitoring histograms to file dummy implementation here --------------—Definition MethodBase.cxx:2133; TMVA::MethodBase::GetNVariablesUInt_t GetNVariables() constDefinition MethodBase.h:345; TMVA::MethodBase::fAnalysisTypeTypes::EAnalysisType fAnalysisTypeDefinition MethodBase.h:595; TMVA::MethodBase::AddRegressionOutputvirtual void AddRegressionOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:744; TMVA::MethodBase::InitBasevoid InitBase()default initialization called by all constructorsDefinition MethodBase.cxx:441; TMVA::MethodBase::fEventCollectionsstd::vector< const std::vector< TMVA::Event * > * > fEventCollectionsDefinition MethodBase.h:708; TMVA::MethodBase::TrainingEndedbool TrainingEnded()Definition MethodBase.h:469; TMVA::MethodBase::GetRegressionDeviationvirtual void GetRegressionDeviation(UInt_t tgtNum, Types::ETreeType type, Double_t &stddev, Double_t &stddev90Percent) constDefinition MethodBase.cxx:724; TMVA::MethodBase::InitIPythonInteractivevoid InitIPythonInteractive()Definition MethodBase.h:455; TMVA::MethodBase::ReadStateFromXMLStringvoid ReadStateFromXMLString(const char *xmlstr)for reading from memoryDefinition MethodBase.cxx:1469; TMVA::MethodBase::fSetupCompletedBool_t fSetupCompletedDefinition MethodBase.h:711; TMVA::MethodBase::CreateMVAPdfsvoid CreateMVAPd",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:56032,Modifiability,variab,variables,56032,"reeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:744; TMVA::MethodBase::InitBasevoid InitBase()default initialization called by all constructorsDefinition MethodBase.cxx:441; TMVA::MethodBase::fEventCollectionsstd::vector< const std::vector< TMVA::Event * > * > fEventCollectionsDefinition MethodBase.h:708; TMVA::MethodBase::TrainingEndedbool TrainingEnded()Definition MethodBase.h:469; TMVA::MethodBase::GetRegressionDeviationvirtual void GetRegressionDeviation(UInt_t tgtNum, Types::ETreeType type, Double_t &stddev, Double_t &stddev90Percent) constDefinition MethodBase.cxx:724; TMVA::MethodBase::InitIPythonInteractivevoid InitIPythonInteractive()Definition MethodBase.h:455; TMVA::MethodBase::ReadStateFromXMLStringvoid ReadStateFromXMLString(const char *xmlstr)for reading from memoryDefinition MethodBase.cxx:1469; TMVA::MethodBase::fSetupCompletedBool_t fSetupCompletedDefinition MethodBase.h:711; TMVA::MethodBase::CreateMVAPdfsvoid CreateMVAPdfs()Create PDFs of the MVA output variables.Definition MethodBase.cxx:2185; TMVA::MethodBase::GetTrainingROOTVersionStringTString GetTrainingROOTVersionString() constcalculates the ROOT version string from the training version code on the flyDefinition MethodBase.cxx:3381; TMVA::MethodBase::GetValueForRootvirtual Double_t GetValueForRoot(Double_t)returns efficiency as function of cutDefinition MethodBase.cxx:3320; TMVA::MethodBase::GetTrainingROOTVersionCodeUInt_t GetTrainingROOTVersionCode() constDefinition MethodBase.h:390; TMVA::MethodBase::ReadStateFromFilevoid ReadStateFromFile()Function to write options and weights to file.Definition MethodBase.cxx:1426; TMVA::MethodBase::WriteVarsToStreamvoid WriteVarsToStream(std::ostream &tf, const TString &prefix="""") constwrite the list of variables (name, min, max) for a given data transformation method to the streamDefinition MethodBase.cxx:1710; TMVA::MethodBase::ReadVarsFromStreamvoid ReadVarsFromStream(std::istream &istr)Read the va",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:56792,Modifiability,variab,variables,56792,"tateFromXMLString(const char *xmlstr)for reading from memoryDefinition MethodBase.cxx:1469; TMVA::MethodBase::fSetupCompletedBool_t fSetupCompletedDefinition MethodBase.h:711; TMVA::MethodBase::CreateMVAPdfsvoid CreateMVAPdfs()Create PDFs of the MVA output variables.Definition MethodBase.cxx:2185; TMVA::MethodBase::GetTrainingROOTVersionStringTString GetTrainingROOTVersionString() constcalculates the ROOT version string from the training version code on the flyDefinition MethodBase.cxx:3381; TMVA::MethodBase::GetValueForRootvirtual Double_t GetValueForRoot(Double_t)returns efficiency as function of cutDefinition MethodBase.cxx:3320; TMVA::MethodBase::GetTrainingROOTVersionCodeUInt_t GetTrainingROOTVersionCode() constDefinition MethodBase.h:390; TMVA::MethodBase::ReadStateFromFilevoid ReadStateFromFile()Function to write options and weights to file.Definition MethodBase.cxx:1426; TMVA::MethodBase::WriteVarsToStreamvoid WriteVarsToStream(std::ostream &tf, const TString &prefix="""") constwrite the list of variables (name, min, max) for a given data transformation method to the streamDefinition MethodBase.cxx:1710; TMVA::MethodBase::ReadVarsFromStreamvoid ReadVarsFromStream(std::istream &istr)Read the variables (name, min, max) for a given data transformation method from the stream.Definition MethodBase.cxx:1725; TMVA::MethodBase::AddWeightsXMLTovirtual void AddWeightsXMLTo(void *parent) const =0; TMVA::MethodBase::Initvirtual void Init()=0; TMVA::MethodBase::ReadSpectatorsFromXMLvoid ReadSpectatorsFromXML(void *specnode)read spectator info from XMLDefinition MethodBase.cxx:1877; TMVA::MethodBase::GetTrainingHistoryvirtual const std::vector< Float_t > & GetTrainingHistory(const char *)Definition MethodBase.h:233; TMVA::MethodBase::fTmpEventconst Event * fTmpEventDefinition MethodBase.h:445; TMVA::MethodBase::fExitFromTrainingbool fExitFromTrainingDefinition MethodBase.h:449; TMVA::MethodBase::SetNormalisedvoid SetNormalised(Bool_t norm)Definition MethodBase.h:497; TMVA::",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:56991,Modifiability,variab,variables,56991,"oid CreateMVAPdfs()Create PDFs of the MVA output variables.Definition MethodBase.cxx:2185; TMVA::MethodBase::GetTrainingROOTVersionStringTString GetTrainingROOTVersionString() constcalculates the ROOT version string from the training version code on the flyDefinition MethodBase.cxx:3381; TMVA::MethodBase::GetValueForRootvirtual Double_t GetValueForRoot(Double_t)returns efficiency as function of cutDefinition MethodBase.cxx:3320; TMVA::MethodBase::GetTrainingROOTVersionCodeUInt_t GetTrainingROOTVersionCode() constDefinition MethodBase.h:390; TMVA::MethodBase::ReadStateFromFilevoid ReadStateFromFile()Function to write options and weights to file.Definition MethodBase.cxx:1426; TMVA::MethodBase::WriteVarsToStreamvoid WriteVarsToStream(std::ostream &tf, const TString &prefix="""") constwrite the list of variables (name, min, max) for a given data transformation method to the streamDefinition MethodBase.cxx:1710; TMVA::MethodBase::ReadVarsFromStreamvoid ReadVarsFromStream(std::istream &istr)Read the variables (name, min, max) for a given data transformation method from the stream.Definition MethodBase.cxx:1725; TMVA::MethodBase::AddWeightsXMLTovirtual void AddWeightsXMLTo(void *parent) const =0; TMVA::MethodBase::Initvirtual void Init()=0; TMVA::MethodBase::ReadSpectatorsFromXMLvoid ReadSpectatorsFromXML(void *specnode)read spectator info from XMLDefinition MethodBase.cxx:1877; TMVA::MethodBase::GetTrainingHistoryvirtual const std::vector< Float_t > & GetTrainingHistory(const char *)Definition MethodBase.h:233; TMVA::MethodBase::fTmpEventconst Event * fTmpEventDefinition MethodBase.h:445; TMVA::MethodBase::fExitFromTrainingbool fExitFromTrainingDefinition MethodBase.h:449; TMVA::MethodBase::SetNormalisedvoid SetNormalised(Bool_t norm)Definition MethodBase.h:497; TMVA::MethodBase::SetTestvarNamevoid SetTestvarName(const TString &v="""")Definition MethodBase.h:341; TMVA::MethodBase::ReadVariablesFromXMLvoid ReadVariablesFromXML(void *varnode)read variable info from XMLDefiniti",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:57953,Modifiability,variab,variable,57953,"romStreamvoid ReadVarsFromStream(std::istream &istr)Read the variables (name, min, max) for a given data transformation method from the stream.Definition MethodBase.cxx:1725; TMVA::MethodBase::AddWeightsXMLTovirtual void AddWeightsXMLTo(void *parent) const =0; TMVA::MethodBase::Initvirtual void Init()=0; TMVA::MethodBase::ReadSpectatorsFromXMLvoid ReadSpectatorsFromXML(void *specnode)read spectator info from XMLDefinition MethodBase.cxx:1877; TMVA::MethodBase::GetTrainingHistoryvirtual const std::vector< Float_t > & GetTrainingHistory(const char *)Definition MethodBase.h:233; TMVA::MethodBase::fTmpEventconst Event * fTmpEventDefinition MethodBase.h:445; TMVA::MethodBase::fExitFromTrainingbool fExitFromTrainingDefinition MethodBase.h:449; TMVA::MethodBase::SetNormalisedvoid SetNormalised(Bool_t norm)Definition MethodBase.h:497; TMVA::MethodBase::SetTestvarNamevoid SetTestvarName(const TString &v="""")Definition MethodBase.h:341; TMVA::MethodBase::ReadVariablesFromXMLvoid ReadVariablesFromXML(void *varnode)read variable info from XMLDefinition MethodBase.cxx:1837; TMVA::MethodBase::fConstructedFromWeightFileBool_t fConstructedFromWeightFileDefinition MethodBase.h:620; TMVA::MethodBase::fHelpBool_t fHelphelp flagDefinition MethodBase.h:679; TMVA::MethodBase::GetNvarUInt_t GetNvar() constDefinition MethodBase.h:344; TMVA::MethodBase::OptimizeTuningParametersvirtual std::map< TString, Double_t > OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")call the Optimizer with the set of parameters and ranges that are meant to be tuned.Definition MethodBase.cxx:623; TMVA::MethodBase::SetTrainTimevoid SetTrainTime(Double_t trainTime)Definition MethodBase.h:161; TMVA::MethodBase::GetXmaxDouble_t GetXmax(Int_t ivar) constDefinition MethodBase.h:357; TMVA::MethodBase::GetMulticlassTrainingEfficiencyvirtual std::vector< Float_t > GetMulticlassTrainingEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2715; TMVA::MethodBase::f",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:59273,Modifiability,variab,variables,59273,"hodBase::GetNvarUInt_t GetNvar() constDefinition MethodBase.h:344; TMVA::MethodBase::OptimizeTuningParametersvirtual std::map< TString, Double_t > OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")call the Optimizer with the set of parameters and ranges that are meant to be tuned.Definition MethodBase.cxx:623; TMVA::MethodBase::SetTrainTimevoid SetTrainTime(Double_t trainTime)Definition MethodBase.h:161; TMVA::MethodBase::GetXmaxDouble_t GetXmax(Int_t ivar) constDefinition MethodBase.h:357; TMVA::MethodBase::GetMulticlassTrainingEfficiencyvirtual std::vector< Float_t > GetMulticlassTrainingEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2715; TMVA::MethodBase::fDataSetInfoDataSetInfo & fDataSetInfoDefinition MethodBase.h:607; TMVA::MethodBase::fHasMVAPdfsBool_t fHasMVAPdfsMVA Pdfs are created for this classifier.Definition MethodBase.h:680; TMVA::MethodBase::WriteStateToStreamvoid WriteStateToStream(std::ostream &tf) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1267; TMVA::MethodBase::GetRarityvirtual Double_t GetRarity(Double_t mvaVal, Types::ESBType reftype=Types::kBackground) constcompute rarity:Definition MethodBase.cxx:2285; TMVA::MethodBase::fUseDecorrBool_t fUseDecorrDefinition MethodBase.h:723; TMVA::MethodBase::SetTuneParametersvirtual void SetTuneParameters(std::map< TString, Double_t > tuneParameters)set the tuning parameters according to the argument This is just a dummy .Definition MethodBase.cxx:644; TMVA::MethodBase::GetTrainTimeDouble_t GetTrainTime() constDefinition MethodBase.h:162; TMVA::MethodBase::SetBaseDirvoid SetBaseDir(TDirectory *methodDir)Definition MethodBase.h:373; TMVA::MethodBase::ReadStateFromStreamvoid ReadStateFromStream(std::istream &tf)read the header from the weight files of the different MVA methodsDefinition MethodBase.cxx:1590; TMVA::MethodBase::AddVarsXMLTovoid AddVarsXMLTo(void *parent)",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:60207,Modifiability,variab,variable,60207,"&tf) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1267; TMVA::MethodBase::GetRarityvirtual Double_t GetRarity(Double_t mvaVal, Types::ESBType reftype=Types::kBackground) constcompute rarity:Definition MethodBase.cxx:2285; TMVA::MethodBase::fUseDecorrBool_t fUseDecorrDefinition MethodBase.h:723; TMVA::MethodBase::SetTuneParametersvirtual void SetTuneParameters(std::map< TString, Double_t > tuneParameters)set the tuning parameters according to the argument This is just a dummy .Definition MethodBase.cxx:644; TMVA::MethodBase::GetTrainTimeDouble_t GetTrainTime() constDefinition MethodBase.h:162; TMVA::MethodBase::SetBaseDirvoid SetBaseDir(TDirectory *methodDir)Definition MethodBase.h:373; TMVA::MethodBase::ReadStateFromStreamvoid ReadStateFromStream(std::istream &tf)read the header from the weight files of the different MVA methodsDefinition MethodBase.cxx:1590; TMVA::MethodBase::AddVarsXMLTovoid AddVarsXMLTo(void *parent) constwrite variable info to XMLDefinition MethodBase.cxx:1762; TMVA::MethodBase::HelpBool_t Help() constDefinition MethodBase.h:504; TMVA::MethodBase::fSplRefBTSpline1 * fSplRefBDefinition MethodBase.h:703; TMVA::MethodBase::fJobNameTString fJobNameDefinition MethodBase.h:614; TMVA::MethodBase::fIPyCurrentIterUInt_t fIPyCurrentIterDefinition MethodBase.h:450; TMVA::MethodBase::GetTransformationHandlerTransformationHandler & GetTransformationHandler(Bool_t takeReroutedIfAvailable=true)Definition MethodBase.h:394; TMVA::MethodBase::IsSilentFileBool_t IsSilentFile() constDefinition MethodBase.h:379; TMVA::MethodBase::GetMethodTypeTypes::EMVA GetMethodType() constDefinition MethodBase.h:333; TMVA::MethodBase::AddTargetsXMLTovoid AddTargetsXMLTo(void *parent) constwrite target info to XMLDefinition MethodBase.cxx:1821; TMVA::MethodBase::fResultsResults * fResultsDefinition MethodBase.h:730; TMVA::MethodBase::fXminDouble_t fXminminimum (signal and background)Definition MethodBase.h:",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:63792,Modifiability,config,configuration,63792,"89; TMVA::MethodBase::EWeightFileTypeEWeightFileTypeDefinition MethodBase.h:122; TMVA::MethodBase::kTEXT@ kTEXTDefinition MethodBase.h:122; TMVA::MethodBase::kROOT@ kROOTDefinition MethodBase.h:122; TMVA::MethodBase::fMeanBDouble_t fMeanBmean (background)Definition MethodBase.h:662; TMVA::MethodBase::GetTransformationHandlerconst TransformationHandler & GetTransformationHandler(Bool_t takeReroutedIfAvailable=true) constDefinition MethodBase.h:398; TMVA::MethodBase::fNsmoothMVAPdfInt_t fNsmoothMVAPdfDefinition MethodBase.h:727; TMVA::MethodBase::SetSignalReferenceCutOrientationvoid SetSignalReferenceCutOrientation(Double_t cutOrientation)Definition MethodBase.h:365; TMVA::MethodBase::fBaseDirTDirectory * fBaseDirDefinition MethodBase.h:625; TMVA::MethodBase::fIgnoreNegWeightsInTrainingBool_t fIgnoreNegWeightsInTrainingIf true, events with negative weights are not used in training.Definition MethodBase.h:682; TMVA::MethodBase::WriteStateToFilevoid WriteStateToFile() constwrite options and weights to file note that each one text file for the main configuration information...Definition MethodBase.cxx:1404; TMVA::MethodBase::AddClassesXMLTovoid AddClassesXMLTo(void *parent) constwrite class info to XMLDefinition MethodBase.cxx:1801; TMVA::MethodBase::GetInputLabelconst TString & GetInputLabel(Int_t i) constDefinition MethodBase.h:350; TMVA::MethodBase::GetCutOrientationECutOrientation GetCutOrientation() constDefinition MethodBase.h:552; TMVA::MethodBase::fRankingRanking * fRankingDefinition MethodBase.h:587; TMVA::MethodBase::fTrainHistoryTrainingHistory fTrainHistoryDefinition MethodBase.h:425; TMVA::MethodBase::SetMethodBaseDirvoid SetMethodBaseDir(TDirectory *methodDir)Definition MethodBase.h:374; TMVA::MethodBase::AddClassifierOutputvirtual void AddClassifierOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:869; TMVA::MethodBase::fMVAPdfBPDF * fMVAPdfBbackground MVA PDFDefinition MethodBase.h:646; TMV",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:64603,Modifiability,variab,variableDefinition,64603,"egWeightsInTrainingIf true, events with negative weights are not used in training.Definition MethodBase.h:682; TMVA::MethodBase::WriteStateToFilevoid WriteStateToFile() constwrite options and weights to file note that each one text file for the main configuration information...Definition MethodBase.cxx:1404; TMVA::MethodBase::AddClassesXMLTovoid AddClassesXMLTo(void *parent) constwrite class info to XMLDefinition MethodBase.cxx:1801; TMVA::MethodBase::GetInputLabelconst TString & GetInputLabel(Int_t i) constDefinition MethodBase.h:350; TMVA::MethodBase::GetCutOrientationECutOrientation GetCutOrientation() constDefinition MethodBase.h:552; TMVA::MethodBase::fRankingRanking * fRankingDefinition MethodBase.h:587; TMVA::MethodBase::fTrainHistoryTrainingHistory fTrainHistoryDefinition MethodBase.h:425; TMVA::MethodBase::SetMethodBaseDirvoid SetMethodBaseDir(TDirectory *methodDir)Definition MethodBase.h:374; TMVA::MethodBase::AddClassifierOutputvirtual void AddClassifierOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:869; TMVA::MethodBase::fMVAPdfBPDF * fMVAPdfBbackground MVA PDFDefinition MethodBase.h:646; TMVA::MethodBase::DataDataSet * Data() constDefinition MethodBase.h:409; TMVA::MethodBase::AddSpectatorsXMLTovoid AddSpectatorsXMLTo(void *parent) constwrite spectator info to XMLDefinition MethodBase.cxx:1778; TMVA::MethodBase::SetModelPersistencevoid SetModelPersistence(Bool_t status)Definition MethodBase.h:382; TMVA::MethodBase::fWeightFileTString fWeightFileweight file nameDefinition MethodBase.h:638; TMVA::MethodBase::IsNormalisedBool_t IsNormalised() constDefinition MethodBase.h:496; TMVA::MethodBase::GetInternalVarNameconst TString & GetInternalVarName(Int_t ivar) constDefinition MethodBase.h:510; TMVA::MethodBase::GetTrainingEventconst Event * GetTrainingEvent(Long64_t ievt) constDefinition MethodBase.h:771; TMVA::MethodBase::GetSignalReferenceCutDouble_t GetSignalReferenceCut() constDefinition",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:66818,Modifiability,variab,variableDefinition,66818,"l Double_t GetROCIntegral(TH1D *histS, TH1D *histB) constcalculate the area (integral) under the ROC curve as a overall quality measure of the classificationDefinition MethodBase.cxx:2822; TMVA::MethodBase::fCutOrientationECutOrientation fCutOrientationDefinition MethodBase.h:699; TMVA::MethodBase::CreateRankingvirtual const Ranking * CreateRanking()=0; TMVA::MethodBase::GetRMSDouble_t GetRMS(Int_t ivar) constDefinition MethodBase.h:355; TMVA::MethodBase::fInteractiveIPythonInteractive * fInteractivetemporary dataset used when evaluating on a different data (used by MethodCategory::GetMvaValues)Definition MethodBase.h:448; TMVA::MethodBase::GetInputTitleconst char * GetInputTitle(Int_t i) constDefinition MethodBase.h:351; TMVA::MethodBase::GetOriginalVarNameconst TString & GetOriginalVarName(Int_t ivar) constDefinition MethodBase.h:511; TMVA::MethodBase::fNbinsHInt_t fNbinsHDefinition MethodBase.h:593; TMVA::MethodBase::AddMulticlassOutputvirtual void AddMulticlassOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:794; TMVA::MethodBase::IsConstructedFromWeightFileBool_t IsConstructedFromWeightFile() constDefinition MethodBase.h:540; TMVA::MethodBase::CheckSetupvirtual void CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodBase.cxx:433; TMVA::MethodBoostClass for boosting a TMVA method.Definition MethodBoost.h:58; TMVA::MethodCategoryClass for categorizing the phase space.Definition MethodCategory.h:58; TMVA::MethodCompositeBaseVirtual base class for combining several TMVA method.Definition MethodCompositeBase.h:50; TMVA::MethodCrossValidationDefinition MethodCrossValidation.h:38; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:67782,Modifiability,variab,variables,67782,"method's discriminating variableDefinition MethodBase.cxx:794; TMVA::MethodBase::IsConstructedFromWeightFileBool_t IsConstructedFromWeightFile() constDefinition MethodBase.h:540; TMVA::MethodBase::CheckSetupvirtual void CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodBase.cxx:433; TMVA::MethodBoostClass for boosting a TMVA method.Definition MethodBoost.h:58; TMVA::MethodCategoryClass for categorizing the phase space.Definition MethodCategory.h:58; TMVA::MethodCompositeBaseVirtual base class for combining several TMVA method.Definition MethodCompositeBase.h:50; TMVA::MethodCrossValidationDefinition MethodCrossValidation.h:38; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::RootFinderRoot finding using Brents algorithm (translated from CERNLIB function RZERO)Definition RootFinder.h:48; TMVA::TSpline1Linear interpolation of TGraph.Definition TSpline1.h:43; TMVA::TrainingHistoryTracking data from training.Definition TrainingHistory.h:31; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::Transformconst Event * Transform(const Event *) constthe transformationDefinition TransformationHandler.cxx:152; TMVA::TransformationHandler::GetRMSDouble_t GetRMS(Int_t ivar, Int_t cls=-1) constDefinition TransformationHandler.cxx:955; TMVA::TransformationHandler::GetMeanDouble_t GetMean(Int_t ivar, Int_t cls=-1) constDefinition TransformationHandler.cxx:936; TMVA::TransformationHandler::GetMinDouble_t GetMin(Int_t ivar, Int_t cls=-1) constDef",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:70598,Modifiability,variab,variable,70598,":TransformationHandler::GetMaxDouble_t GetMax(Int_t ivar, Int_t cls=-1) constDefinition TransformationHandler.cxx:993; TMVA::Types::GetMethodNameTString GetMethodName(Types::EMVA method) constDefinition Types.cxx:136; TMVA::Types::Instancestatic Types & Instance()The single instance of ""Types"" if existing already, or create it (Singleton)Definition Types.cxx:70; TMVA::Types::ESBTypeESBTypeDefinition Types.h:134; TMVA::Types::kBackground@ kBackgroundDefinition Types.h:136; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMVA::VariableInfo::GetLabelconst TString & GetLabel() constDefinition VariableInfo.h:59; TMVA::VariableInfo::GetExpressionconst TString & GetExpression() constDefinition VariableInfo.h:57; TMVA::VariableInfo::GetInternalNameconst TString & GetInternalName() constDefinition VariableInfo.h:58; TMatrixT< Double_t >; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TNamed::GetTitleconst char * GetTitle() const overrideReturns title of object.Definition TNamed.h:48; TSplineBase class for spline implementation containing the Draw/Paint methods.Definition TSpline.h:31; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; double; int; unsigned int; xDouble_t x[n]Definition legend1.C:17; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; v@ vDefinition rootcling_impl.cxx:3699; Types.h. tmvatmvaincTMVAMethodBase.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5021,Performance,perform,performs,5021,"lic:; 121 ; 122 enum EWeightFileType { kROOT=0, kTEXT };; 123 ; 124 // default constructor; 125 MethodBase( const TString& jobName,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass(",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5150,Performance,optimiz,optimize,5150,"e,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& de",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5362,Performance,tune,tuneParameters,5362," 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5799,Performance,perform,performs,5799,"testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation stil",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5954,Performance,perform,performs,5954,"eType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error c",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:6043,Performance,perform,performs,6043,"ng; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = n",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:35904,Performance,perform,perform,35904,"nst char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; y1Option_t Option_t TPoint TPoint const char y1Definition TGWin32VirtualXProxy.cxx:70; namechar name[80]Definition TGX11.cxx:110; TString.h; TrainingHistory.h; TransformationHandler.h; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1D1-D histogram with a double per channel (see TH1 documentation)Definition TH1.h:670; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TH1TH1 is the base class of all histogram classes in ROOT.Definition TH1.h:59; ClassificationClass to perform two class classification.; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::EventDefinition E",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:36017,Performance,perform,perform,36017,"Long_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; y1Option_t Option_t TPoint TPoint const char y1Definition TGWin32VirtualXProxy.cxx:70; namechar name[80]Definition TGX11.cxx:110; TString.h; TrainingHistory.h; TransformationHandler.h; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1D1-D histogram with a double per channel (see TH1 documentation)Definition TH1.h:670; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TH1TH1 is the base class of all histogram classes in ROOT.Definition TH1.h:59; ClassificationClass to perform two class classification.; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::EventDefinition Event.h:51; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::FactoryThis is the main MVA steering cla",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:58500,Performance,tune,tuned,58500,"A::MethodBase::GetTrainingHistoryvirtual const std::vector< Float_t > & GetTrainingHistory(const char *)Definition MethodBase.h:233; TMVA::MethodBase::fTmpEventconst Event * fTmpEventDefinition MethodBase.h:445; TMVA::MethodBase::fExitFromTrainingbool fExitFromTrainingDefinition MethodBase.h:449; TMVA::MethodBase::SetNormalisedvoid SetNormalised(Bool_t norm)Definition MethodBase.h:497; TMVA::MethodBase::SetTestvarNamevoid SetTestvarName(const TString &v="""")Definition MethodBase.h:341; TMVA::MethodBase::ReadVariablesFromXMLvoid ReadVariablesFromXML(void *varnode)read variable info from XMLDefinition MethodBase.cxx:1837; TMVA::MethodBase::fConstructedFromWeightFileBool_t fConstructedFromWeightFileDefinition MethodBase.h:620; TMVA::MethodBase::fHelpBool_t fHelphelp flagDefinition MethodBase.h:679; TMVA::MethodBase::GetNvarUInt_t GetNvar() constDefinition MethodBase.h:344; TMVA::MethodBase::OptimizeTuningParametersvirtual std::map< TString, Double_t > OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")call the Optimizer with the set of parameters and ranges that are meant to be tuned.Definition MethodBase.cxx:623; TMVA::MethodBase::SetTrainTimevoid SetTrainTime(Double_t trainTime)Definition MethodBase.h:161; TMVA::MethodBase::GetXmaxDouble_t GetXmax(Int_t ivar) constDefinition MethodBase.h:357; TMVA::MethodBase::GetMulticlassTrainingEfficiencyvirtual std::vector< Float_t > GetMulticlassTrainingEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2715; TMVA::MethodBase::fDataSetInfoDataSetInfo & fDataSetInfoDefinition MethodBase.h:607; TMVA::MethodBase::fHasMVAPdfsBool_t fHasMVAPdfsMVA Pdfs are created for this classifier.Definition MethodBase.h:680; TMVA::MethodBase::WriteStateToStreamvoid WriteStateToStream(std::ostream &tf) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1267; TMVA::MethodBase::GetRarityvirtual Double_t GetRarity(Double_t ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:59654,Performance,tune,tuneParameters,59654,"se::GetXmaxDouble_t GetXmax(Int_t ivar) constDefinition MethodBase.h:357; TMVA::MethodBase::GetMulticlassTrainingEfficiencyvirtual std::vector< Float_t > GetMulticlassTrainingEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2715; TMVA::MethodBase::fDataSetInfoDataSetInfo & fDataSetInfoDefinition MethodBase.h:607; TMVA::MethodBase::fHasMVAPdfsBool_t fHasMVAPdfsMVA Pdfs are created for this classifier.Definition MethodBase.h:680; TMVA::MethodBase::WriteStateToStreamvoid WriteStateToStream(std::ostream &tf) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1267; TMVA::MethodBase::GetRarityvirtual Double_t GetRarity(Double_t mvaVal, Types::ESBType reftype=Types::kBackground) constcompute rarity:Definition MethodBase.cxx:2285; TMVA::MethodBase::fUseDecorrBool_t fUseDecorrDefinition MethodBase.h:723; TMVA::MethodBase::SetTuneParametersvirtual void SetTuneParameters(std::map< TString, Double_t > tuneParameters)set the tuning parameters according to the argument This is just a dummy .Definition MethodBase.cxx:644; TMVA::MethodBase::GetTrainTimeDouble_t GetTrainTime() constDefinition MethodBase.h:162; TMVA::MethodBase::SetBaseDirvoid SetBaseDir(TDirectory *methodDir)Definition MethodBase.h:373; TMVA::MethodBase::ReadStateFromStreamvoid ReadStateFromStream(std::istream &tf)read the header from the weight files of the different MVA methodsDefinition MethodBase.cxx:1590; TMVA::MethodBase::AddVarsXMLTovoid AddVarsXMLTo(void *parent) constwrite variable info to XMLDefinition MethodBase.cxx:1762; TMVA::MethodBase::HelpBool_t Help() constDefinition MethodBase.h:504; TMVA::MethodBase::fSplRefBTSpline1 * fSplRefBDefinition MethodBase.h:703; TMVA::MethodBase::fJobNameTString fJobNameDefinition MethodBase.h:614; TMVA::MethodBase::fIPyCurrentIterUInt_t fIPyCurrentIterDefinition MethodBase.h:450; TMVA::MethodBase::GetTransformationHandlerTransformationHandler & GetTransformationHandle",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:13369,Security,access,accessors,13369,"/ overload this one for individual initialisation of the testing,; 307 // it is then called automatically within the global ""TestInit""; 308 ; 309 // variables (and private member functions) for the Evaluation:; 310 // get the efficiency. It fills a histogram for efficiency/vs/bkg; 311 // and returns the one value fo the efficiency demanded for; 312 // in the TString argument. (Watch the string format); 313 virtual Double_t GetEfficiency( const TString&, Types::ETreeType, Double_t& err );; 314 virtual Double_t GetTrainingEfficiency(const TString& );; 315 virtual std::vector<Float_t> GetMulticlassEfficiency( std::vector<std::vector<Float_t> >& purity );; 316 virtual std::vector<Float_t> GetMulticlassTrainingEfficiency(std::vector<std::vector<Float_t> >& purity );; 317 virtual TMatrixD GetMulticlassConfusionMatrix(Double_t effB, Types::ETreeType type);; 318 virtual Double_t GetSignificance() const;; 319 virtual Double_t GetROCIntegral(TH1D *histS, TH1D *histB) const;; 320 virtual Double_t GetROCIntegral(PDF *pdfS=nullptr, PDF *pdfB=nullptr) const;; 321 virtual Double_t GetMaximumSignificance( Double_t SignalEvents, Double_t BackgroundEvents,; 322 Double_t& optimal_significance_value ) const;; 323 virtual Double_t GetSeparation( TH1*, TH1* ) const;; 324 virtual Double_t GetSeparation( PDF* pdfS = nullptr, PDF* pdfB = nullptr ) const;; 325 ; 326 virtual void GetRegressionDeviation(UInt_t tgtNum, Types::ETreeType type, Double_t& stddev,Double_t& stddev90Percent ) const;; 327 // ---------- public accessors -----------------------------------------------; 328 ; 329 // classifier naming (a lot of names ... aren't they ;-); 330 const TString& GetJobName () const { return fJobName; }; 331 const TString& GetMethodName () const { return fMethodName; }; 332 TString GetMethodTypeName() const { return Types::Instance().GetMethodName(fMethodType); }; 333 Types::EMVA GetMethodType () const { return fMethodType; }; 334 const char* GetName () const { return fMethodName.Data(); }; 335 c",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:14903,Security,access,accessors,14903,"st TString GetProbaName () const { return fTestvar + ""_Proba""; }; 337 TString GetWeightFileName() const;; 338 ; 339 // build classifier name in Test tree; 340 // MVA prefix (e.g., ""TMVA_""); 341 void SetTestvarName ( const TString & v="""" ) { fTestvar = (v=="""") ? (""MVA_"" + GetMethodName()) : v; }; 342 ; 343 // number of input variable used by classifier; 344 UInt_t GetNvar() const { return DataInfo().GetNVariables(); }; 345 UInt_t GetNVariables() const { return DataInfo().GetNVariables(); }; 346 UInt_t GetNTargets() const { return DataInfo().GetNTargets(); };; 347 ; 348 // internal names and expressions of input variables; 349 const TString& GetInputVar ( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetInternalName(); }; 350 const TString& GetInputLabel( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetLabel(); }; 351 const char * GetInputTitle( Int_t i ) const { return DataInfo().GetVariableInfo(i).GetTitle(); }; 352 ; 353 // normalisation and limit accessors; 354 Double_t GetMean( Int_t ivar ) const { return GetTransformationHandler().GetMean(ivar); }; 355 Double_t GetRMS ( Int_t ivar ) const { return GetTransformationHandler().GetRMS(ivar); }; 356 Double_t GetXmin( Int_t ivar ) const { return GetTransformationHandler().GetMin(ivar); }; 357 Double_t GetXmax( Int_t ivar ) const { return GetTransformationHandler().GetMax(ivar); }; 358 ; 359 // sets the minimum requirement on the MVA output to declare an event signal-like; 360 Double_t GetSignalReferenceCut() const { return fSignalReferenceCut; }; 361 Double_t GetSignalReferenceCutOrientation() const { return fSignalReferenceCutOrientation; }; 362 ; 363 // sets the minimum requirement on the MVA output to declare an event signal-like; 364 void SetSignalReferenceCut( Double_t cut ) { fSignalReferenceCut = cut; }; 365 void SetSignalReferenceCutOrientation( Double_t cutOrientation ) { fSignalReferenceCutOrientation = cutOrientation; }; 366 ; 367 // pointers to ROOT directories; 368 TDirectory* BaseDir()",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:17791,Security,access,accessors,17791,"() const {return fSilentFile;}; 380 ; 381 //Model Persistence; 382 void SetModelPersistence(Bool_t status){fModelPersistence=status;}//added support to create/remove dir here if exits or not; 383 Bool_t IsModelPersistence() const {return fModelPersistence;}; 384 ; 385 // the TMVA version can be obtained and checked using; 386 // if (GetTrainingTMVAVersionCode()>TMVA_VERSION(3,7,2)) {...}; 387 // or; 388 // if (GetTrainingROOTVersionCode()>ROOT_VERSION(5,15,5)) {...}; 389 UInt_t GetTrainingTMVAVersionCode() const { return fTMVATrainingVersion; }; 390 UInt_t GetTrainingROOTVersionCode() const { return fROOTTrainingVersion; }; 391 TString GetTrainingTMVAVersionString() const;; 392 TString GetTrainingROOTVersionString() const;; 393 ; 394 TransformationHandler& GetTransformationHandler(Bool_t takeReroutedIfAvailable=true); 395 {; 396 if(fTransformationPointer && takeReroutedIfAvailable) return *fTransformationPointer; else return fTransformation;; 397 }; 398 const TransformationHandler& GetTransformationHandler(Bool_t takeReroutedIfAvailable=true) const; 399 {; 400 if(fTransformationPointer && takeReroutedIfAvailable) return *fTransformationPointer; else return fTransformation;; 401 }; 402 ; 403 void RerouteTransformationHandler (TransformationHandler* fTargetTransformation) { fTransformationPointer=fTargetTransformation; }; 404 ; 405 // ---------- event accessors ------------------------------------------------; 406 ; 407 // returns reference to data set; 408 // NOTE: this DataSet is the ""original"" dataset, i.e. the one seen by ALL Classifiers WITHOUT transformation; 409 DataSet* Data() const { return (fTmpData) ? fTmpData : DataInfo().GetDataSet(); }; 410 DataSetInfo& DataInfo() const { return fDataSetInfo; }; 411 ; 412 ; 413 // event reference and update; 414 // NOTE: these Event accessors make sure that you get the events transformed according to the; 415 // particular classifiers transformation chosen; 416 UInt_t GetNEvents () const { return Data()->GetNEvents(); };",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:18228,Security,access,accessors,18228,,MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:21267,Security,access,accessors,21267,"AnalysisType type ) { fAnalysisType = type; }; 437 Types::EAnalysisType GetAnalysisType() const { return fAnalysisType; }; 438 Bool_t DoRegression() const { return fAnalysisType == Types::kRegression; }; 439 Bool_t DoMulticlass() const { return fAnalysisType == Types::kMulticlass; }; 440 ; 441 // setter method for suppressing writing to XML and writing of standalone classes; 442 void DisableWriting(Bool_t setter){ fModelPersistence = setter?kFALSE:kTRUE; }//DEPRECATED; 443 ; 444 protected:; 445 mutable const Event *fTmpEvent; //! temporary event when testing on a different DataSet than the own one; 446 DataSet *fTmpData = nullptr; //! temporary dataset used when evaluating on a different data (used by MethodCategory::GetMvaValues); 447 // helper variables for JsMVA; 448 IPythonInteractive *fInteractive = nullptr;; 449 bool fExitFromTraining = false;; 450 UInt_t fIPyMaxIter = 0, fIPyCurrentIter = 0;; 451 ; 452 public:; 453 ; 454 // initializing IPythonInteractive class (for JsMVA only); 455 inline void InitIPythonInteractive(){; 456 if (fInteractive) delete fInteractive;; 457 fInteractive = new IPythonInteractive();; 458 }; 459 ; 460 // get training errors (for JsMVA only); 461 inline TMultiGraph* GetInteractiveTrainingError(){return fInteractive->Get();}; 462 ; 463 // stop's the training process (for JsMVA only); 464 inline void ExitFromTraining(){; 465 fExitFromTraining = true;; 466 }; 467 ; 468 // check's if the training ended (for JsMVA only); 469 inline bool TrainingEnded(){; 470 if (fExitFromTraining && fInteractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:22124,Security,access,accessors,22124,"ractive){; 471 delete fInteractive;; 472 fInteractive = nullptr;; 473 }; 474 return fExitFromTraining;; 475 }; 476 ; 477 // get fIPyMaxIter; 478 inline UInt_t GetMaxIter(){ return fIPyMaxIter; }; 479 ; 480 // get fIPyCurrentIter; 481 inline UInt_t GetCurrentIter(){ return fIPyCurrentIter; }; 482 ; 483 protected:; 484 ; 485 // ---------- protected accessors -------------------------------------------; 486 ; 487 //TDirectory* LocalTDir() const { return Data().LocalRootDir(); }; 488 ; 489 // weight file name and directory (given by global config variable); 490 void SetWeightFileName( TString );; 491 ; 492 const TString& GetWeightFileDir() const { return fFileDir; }; 493 void SetWeightFileDir( TString fileDir );; 494 ; 495 // are input variables normalised ?; 496 Bool_t IsNormalised() const { return fNormalise; }; 497 void SetNormalised( Bool_t norm ) { fNormalise = norm; }; 498 ; 499 // set number of input variables (only used by MethodCuts, could perhaps be removed); 500 // void SetNvar( Int_t n ) { fNvar = n; }; 501 ; 502 // verbose and help flags; 503 Bool_t Verbose() const { return fVerbose; }; 504 Bool_t Help () const { return fHelp; }; 505 ; 506 // ---------- protected event and tree accessors -----------------------------; 507 ; 508 // names of input variables (if the original names are expressions, they are; 509 // transformed into regexps); 510 const TString& GetInternalVarName( Int_t ivar ) const { return (*fInputVars)[ivar]; }; 511 const TString& GetOriginalVarName( Int_t ivar ) const { return DataInfo().GetVariableInfo(ivar).GetExpression(); }; 512 ; 513 Bool_t HasTrainingTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxili",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:23519,Security,access,access,23519,"ningTree() const { return Data()->GetNTrainingEvents() != 0; }; 514 ; 515 // ---------- protected auxiliary methods ------------------------------------; 516 ; 517 protected:; 518 ; 519 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 520 virtual void MakeClassSpecific( std::ostream&, const TString& = """" ) const {}; 521 ; 522 // header and auxiliary classes; 523 virtual void MakeClassSpecificHeader( std::ostream&, const TString& = """" ) const {}; 524 ; 525 // static pointer to this object - required for ROOT finder (to be solved differently)(solved by Omar); 526 //static MethodBase* GetThisBase();; 527 ; 528 // some basic statistical analysis; 529 void Statistics( Types::ETreeType treeType, const TString& theVarName,; 530 Double_t&, Double_t&, Double_t&,; 531 Double_t&, Double_t&, Double_t& );; 532 ; 533 // if TRUE, write weights only to text files; 534 Bool_t TxtWeightsOnly() const { return kTRUE; }; 535 ; 536 protected:; 537 ; 538 // access to event information that needs method-specific information; 539 ; 540 Bool_t IsConstructedFromWeightFile() const { return fConstructedFromWeightFile; }; 541 ; 542 private:; 543 ; 544 // ---------- private definitions --------------------------------------------; 545 // Initialisation; 546 void InitBase();; 547 void DeclareBaseOptions();; 548 void ProcessBaseOptions();; 549 ; 550 // used in efficiency computation; 551 enum ECutOrientation { kNegative = -1, kPositive = +1 };; 552 ECutOrientation GetCutOrientation() const { return fCutOrientation; }; 553 ; 554 // ---------- private accessors ---------------------------------------------; 555 ; 556 // reset required for RootFinder; 557 void ResetThisBase();; 558 ; 559 // ---------- private auxiliary methods --------------------------------------; 560 ; 561 // PDFs for classifier response (required to compute signal probability and Rarity); 562 void CreateMVAPdfs();; 563 ; 564 // for root finder; 565 //virtual method to find ROOT; 566 v",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:24113,Security,access,accessors,24113,"olved by Omar); 526 //static MethodBase* GetThisBase();; 527 ; 528 // some basic statistical analysis; 529 void Statistics( Types::ETreeType treeType, const TString& theVarName,; 530 Double_t&, Double_t&, Double_t&,; 531 Double_t&, Double_t&, Double_t& );; 532 ; 533 // if TRUE, write weights only to text files; 534 Bool_t TxtWeightsOnly() const { return kTRUE; }; 535 ; 536 protected:; 537 ; 538 // access to event information that needs method-specific information; 539 ; 540 Bool_t IsConstructedFromWeightFile() const { return fConstructedFromWeightFile; }; 541 ; 542 private:; 543 ; 544 // ---------- private definitions --------------------------------------------; 545 // Initialisation; 546 void InitBase();; 547 void DeclareBaseOptions();; 548 void ProcessBaseOptions();; 549 ; 550 // used in efficiency computation; 551 enum ECutOrientation { kNegative = -1, kPositive = +1 };; 552 ECutOrientation GetCutOrientation() const { return fCutOrientation; }; 553 ; 554 // ---------- private accessors ---------------------------------------------; 555 ; 556 // reset required for RootFinder; 557 void ResetThisBase();; 558 ; 559 // ---------- private auxiliary methods --------------------------------------; 560 ; 561 // PDFs for classifier response (required to compute signal probability and Rarity); 562 void CreateMVAPdfs();; 563 ; 564 // for root finder; 565 //virtual method to find ROOT; 566 virtual Double_t GetValueForRoot ( Double_t ); // implementation; 567 ; 568 // used for file parsing; 569 Bool_t GetLine( std::istream& fin, char * buf );; 570 ; 571 // fill test tree with classification or regression results; 572 virtual void AddClassifierOutput ( Types::ETreeType type );; 573 virtual void AddClassifierOutputProb( Types::ETreeType type );; 574 virtual void AddRegressionOutput ( Types::ETreeType type );; 575 virtual void AddMulticlassOutput ( Types::ETreeType type );; 576 ; 577 private:; 578 ; 579 void AddInfoItem( void* gi, const TString& name,; 580 const TString& value) ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:25258,Security,access,accessors,25258,"- private auxiliary methods --------------------------------------; 560 ; 561 // PDFs for classifier response (required to compute signal probability and Rarity); 562 void CreateMVAPdfs();; 563 ; 564 // for root finder; 565 //virtual method to find ROOT; 566 virtual Double_t GetValueForRoot ( Double_t ); // implementation; 567 ; 568 // used for file parsing; 569 Bool_t GetLine( std::istream& fin, char * buf );; 570 ; 571 // fill test tree with classification or regression results; 572 virtual void AddClassifierOutput ( Types::ETreeType type );; 573 virtual void AddClassifierOutputProb( Types::ETreeType type );; 574 virtual void AddRegressionOutput ( Types::ETreeType type );; 575 virtual void AddMulticlassOutput ( Types::ETreeType type );; 576 ; 577 private:; 578 ; 579 void AddInfoItem( void* gi, const TString& name,; 580 const TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 6",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:26098,Security,access,access,26098," TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType fAnalysisType; // method-mode : true --> regression, false --> classification; 596 ; 597 std::vector<Float_t>* fRegressionReturnVal; // holds the return-values for the regression; 598 std::vector<Float_t>* fMulticlassReturnVal; // holds the return-values for the multiclass classification; 599 ; 600 private:; 601 ; 602 // MethodCuts redefines some of the evaluation variables and histograms -> must access private members; 603 friend class MethodCuts;; 604 ; 605 ; 606 // data sets; 607 DataSetInfo& fDataSetInfo; //! the data set information (sometimes needed); 608 ; 609 Double_t fSignalReferenceCut; // minimum requirement on the MVA output to declare an event signal-like; 610 Double_t fSignalReferenceCutOrientation; // minimum requirement on the MVA output to declare an event signal-like; 611 Types::ESBType fVariableTransformType; // this is the event type (sig or bgd) assumed for variable transform; 612 ; 613 // naming and versioning; 614 TString fJobName; // name of job -> user defined, appears in weight files; 615 TString fMethodName; // name of the method (set in derived class); 616 Types::EMVA fMethodType; // type of method (set in derived class); 617 TString fTestvar; // variable used in evaluation, etc (mostly the MVA); 618 UInt_t fTMVATrainingVersion; // TMVA version used for training; 619 UInt_t fROOTTrainingVersion; // ROOT version used for training; 620 Bool_t fConstructedF",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:36031,Security,validat,validation,36031,"Long_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; y1Option_t Option_t TPoint TPoint const char y1Definition TGWin32VirtualXProxy.cxx:70; namechar name[80]Definition TGX11.cxx:110; TString.h; TrainingHistory.h; TransformationHandler.h; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1D1-D histogram with a double per channel (see TH1 documentation)Definition TH1.h:670; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TH1TH1 is the base class of all histogram classes in ROOT.Definition TH1.h:59; ClassificationClass to perform two class classification.; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::EventDefinition Event.h:51; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::FactoryThis is the main MVA steering cla",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:1973,Testability,assert,assert,1973," 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <Joerg.Stelzer@cern.ch> - CERN, Switzerland *; 17 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 18 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005-2011: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * U. of Bonn, Germany *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodBase; 34#define ROOT_TMVA_MethodBase; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodBase //; 39// //; 40// Virtual base class for all TMVA method //; 41// //; 42//////////////////////////////////////////////////////////////////////////; 43 ; 44#include <iosfwd>; 45#include <vector>; 46#include <map>; 47#include ""assert.h""; 48 ; 49#include ""TString.h""; 50 ; 51#include ""TMVA/IMethod.h""; 52#include ""TMVA/Configurable.h""; 53#include ""TMVA/Types.h""; 54#include ""TMVA/DataSet.h""; 55#include ""TMVA/Event.h""; 56#include ""TMVA/TransformationHandler.h""; 57#include <TMVA/Results.h>; 58#include ""TMVA/TrainingHistory.h""; 59 ; 60#include <TFile.h>; 61 ; 62class TGraph;; 63class TTree;; 64class TDirectory;; 65class TSpline;; 66class TH1F;; 67class TH1D;; 68class TMultiGraph;; 69 ; 70/*! \class TMVA::IPythonInteractive; 71\ingroup TMVA; 72 ; 73This class is needed by JsMVA, and it's a helper class for tracking errors during; 74the training in Jupyter notebook. It’s only ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:4803,Testability,test,testing,4803,"end class CrossValidation;; 114 friend class Factory;; 115 friend class RootFinder;; 116 friend class MethodBoost;; 117 friend class MethodCrossValidation;; 118 friend class Experimental::Classification;; 119 ; 120 public:; 121 ; 122 enum EWeightFileType { kROOT=0, kTEXT };; 123 ; 124 // default constructor; 125 MethodBase( const TString& jobName,; 126 Types::EMVA methodType,; 127 const TString& methodTitle,; 128 DataSetInfo& dsi,; 129 const TString& theOption = """" );; 130 ; 131 // constructor used for Testing + Application of the MVA, only (no training),; 132 // using given weight file; 133 MethodBase( Types::EMVA methodType,; 134 DataSetInfo& dsi,; 135 const TString& weightFile );; 136 ; 137 // default destructor; 138 virtual ~MethodBase();; 139 ; 140 // declaration, processing and checking of configuration options; 141 void SetupMethod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5650,Testability,test,testing,5650,"hod();; 142 void ProcessSetup();; 143 virtual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset()",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5691,Testability,test,testTime,5691,"tual void CheckSetup(); // may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a d",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5716,Testability,test,testTime,5716,"/ may be overwritten by derived classes; 144 ; 145 // ---------- main training and testing methods ------------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5819,Testability,test,testing,5819,"--------------------------; 146 ; 147 // prepare tree branch with the method's discriminating variable; 148 void AddOutput( Types::ETreeType type, Types::EAnalysisType analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 vir",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:5985,Testability,test,testing,5985,"e analysisType );; 149 ; 150 // performs classifier training; 151 // calls methods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:6063,Testability,test,testing,6063,"hods Train() implemented by derived classes; 152 void TrainMethod();; 153 ; 154 // optimize tuning parameters; 155 virtual std::map<TString,Double_t> OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 156 virtual void SetTuneParameters(std::map<TString,Double_t> tuneParameters);; 157 ; 158 virtual void Train() = 0;; 159 ; 160 // store and retrieve time used for training; 161 void SetTrainTime( Double_t trainTime ) { fTrainTime = trainTime; }; 162 Double_t GetTrainTime() const { return fTrainTime; }; 163 ; 164 // store and retrieve time used for testing; 165 void SetTestTime ( Double_t testTime ) { fTestTime = testTime; }; 166 Double_t GetTestTime () const { return fTestTime; }; 167 ; 168 // performs classifier testing; 169 virtual void TestClassification();; 170 virtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"");; 171 ; 172 // performs multiclass classifier testing; 173 virtual void TestMulticlass();; 174 ; 175 // performs regression testing; 176 virtual void TestRegression( Double_t& bias, Double_t& biasT,; 177 Double_t& dev, Double_t& devT,; 178 Double_t& rms, Double_t& rmsT,; 179 Double_t& mInf, Double_t& mInfT, // mutual information; 180 Double_t& corr,; 181 Types::ETreeType type );; 182 ; 183 // options treatment; 184 virtual void Init() = 0;; 185 virtual void DeclareOptions() = 0;; 186 virtual void ProcessOptions() = 0;; 187 virtual void DeclareCompatibilityOptions(); // declaration of past options; 188 ; 189 // reset the Method --> As if it was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* e",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:7581,Testability,log,logProgress,7581,"was not yet trained, just instantiated; 190 // virtual void Reset() = 0;; 191 //for the moment, I provide a dummy (that would not work) default, just to make; 192 // compilation/running w/o parameter optimisation still possible; 193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(const TMVA::Event* const ev){; 215 fTmpEvent = ev;; 216 const std::vector<Float_t>* ptr = &GetRegressionValues();; 217 fTmpEvent = nullptr;; 218 return (*ptr);; 219 }; 220 ; 221 virtual const std::vector<Float_t>& GetRegressionValues() {; 222 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 223 return (*ptr);; 224 }; 225 ; 226 // multiclass classification response; 227 virtual const std::vector<Float_t>& GetMulticlassValues() {; 228 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 229 return (*ptr);; 230 }; 231 ; 232 // Training history; 233 virtual const std::vector<Float_t>& GetTrainingHistory(const char* /*name*/ ) {; 234 std:",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:7810,Testability,log,logProgress,7810,"193 virtual void Reset(){return;}; 194 ; 195 // classifier response:; 196 // some methods may return a per-event error estimate; 197 // error calculation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(const TMVA::Event* const ev){; 215 fTmpEvent = ev;; 216 const std::vector<Float_t>* ptr = &GetRegressionValues();; 217 fTmpEvent = nullptr;; 218 return (*ptr);; 219 }; 220 ; 221 virtual const std::vector<Float_t>& GetRegressionValues() {; 222 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 223 return (*ptr);; 224 }; 225 ; 226 // multiclass classification response; 227 virtual const std::vector<Float_t>& GetMulticlassValues() {; 228 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 229 return (*ptr);; 230 }; 231 ; 232 // Training history; 233 virtual const std::vector<Float_t>& GetTrainingHistory(const char* /*name*/ ) {; 234 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 235 return (*ptr);; 236 }; 237 ; 238 // probability of classifier response (mvaval) to be signal (requires ""CreateMvaPdf"" option set); 239 virtual Double_t GetProba( const Eve",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:11825,Testability,test,testing,11825,"268 private:; 269 friend class MethodCategory;; 270 friend class MethodCompositeBase;; 271 void WriteStateToXML ( void* parent ) const;; 272 void ReadStateFromXML ( void* parent );; 273 void WriteStateToStream ( std::ostream& tf ) const; // needed for MakeClass; 274 void WriteVarsToStream ( std::ostream& tf, const TString& prefix = """" ) const; // needed for MakeClass; 275 ; 276 ; 277 public: // these two need to be public, they are used to read in-memory weight-files; 278 void ReadStateFromStream ( std::istream& tf ); // backward compatibility; 279 void ReadStateFromStream ( TFile& rf ); // backward compatibility; 280 void ReadStateFromXMLString( const char* xmlstr ); // for reading from memory; 281 ; 282 private:; 283 // the variable information; 284 void AddVarsXMLTo ( void* parent ) const;; 285 void AddSpectatorsXMLTo ( void* parent ) const;; 286 void AddTargetsXMLTo ( void* parent ) const;; 287 void AddClassesXMLTo ( void* parent ) const;; 288 void ReadVariablesFromXML ( void* varnode );; 289 void ReadSpectatorsFromXML( void* specnode);; 290 void ReadTargetsFromXML ( void* tarnode );; 291 void ReadClassesFromXML ( void* clsnode );; 292 void ReadVarsFromStream ( std::istream& istr ); // backward compatibility; 293 ; 294 public:; 295 // ---------------------------------------------------------------------------; 296 ; 297 // write evaluation histograms into target file; 298 virtual void WriteEvaluationHistosToFile(Types::ETreeType treetype);; 299 ; 300 // write classifier-specific monitoring information to target file; 301 virtual void WriteMonitoringHistosToFile() const;; 302 ; 303 // ---------- public evaluation methods --------------------------------------; 304 ; 305 // individual initialization for testing of each method; 306 // overload this one for individual initialisation of the testing,; 307 // it is then called automatically within the global ""TestInit""; 308 ; 309 // variables (and private member functions) for the Evaluation:; 310 // get the efficiency.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:11911,Testability,test,testing,11911,"268 private:; 269 friend class MethodCategory;; 270 friend class MethodCompositeBase;; 271 void WriteStateToXML ( void* parent ) const;; 272 void ReadStateFromXML ( void* parent );; 273 void WriteStateToStream ( std::ostream& tf ) const; // needed for MakeClass; 274 void WriteVarsToStream ( std::ostream& tf, const TString& prefix = """" ) const; // needed for MakeClass; 275 ; 276 ; 277 public: // these two need to be public, they are used to read in-memory weight-files; 278 void ReadStateFromStream ( std::istream& tf ); // backward compatibility; 279 void ReadStateFromStream ( TFile& rf ); // backward compatibility; 280 void ReadStateFromXMLString( const char* xmlstr ); // for reading from memory; 281 ; 282 private:; 283 // the variable information; 284 void AddVarsXMLTo ( void* parent ) const;; 285 void AddSpectatorsXMLTo ( void* parent ) const;; 286 void AddTargetsXMLTo ( void* parent ) const;; 287 void AddClassesXMLTo ( void* parent ) const;; 288 void ReadVariablesFromXML ( void* varnode );; 289 void ReadSpectatorsFromXML( void* specnode);; 290 void ReadTargetsFromXML ( void* tarnode );; 291 void ReadClassesFromXML ( void* clsnode );; 292 void ReadVarsFromStream ( std::istream& istr ); // backward compatibility; 293 ; 294 public:; 295 // ---------------------------------------------------------------------------; 296 ; 297 // write evaluation histograms into target file; 298 virtual void WriteEvaluationHistosToFile(Types::ETreeType treetype);; 299 ; 300 // write classifier-specific monitoring information to target file; 301 virtual void WriteMonitoringHistosToFile() const;; 302 ; 303 // ---------- public evaluation methods --------------------------------------; 304 ; 305 // individual initialization for testing of each method; 306 // overload this one for individual initialisation of the testing,; 307 // it is then called automatically within the global ""TestInit""; 308 ; 309 // variables (and private member functions) for the Evaluation:; 310 // get the efficiency.",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:19936,Testability,test,testing,19936,"------------------------------; 427 ; 428 // this method is used to decide whether an event is signal- or background-like; 429 // the reference cut ""xC"" is taken to be where; 430 // Int_[-oo,xC] { PDF_S(x) dx } = Int_[xC,+oo] { PDF_B(x) dx }; 431 virtual Bool_t IsSignalLike();; 432 virtual Bool_t IsSignalLike(Double_t mvaVal);; 433 ; 434 ; 435 Bool_t HasMVAPdfs() const { return fHasMVAPdfs; }; 436 virtual void SetAnalysisType( Types::EAnalysisType type ) { fAnalysisType = type; }; 437 Types::EAnalysisType GetAnalysisType() const { return fAnalysisType; }; 438 Bool_t DoRegression() const { return fAnalysisType == Types::kRegression; }; 439 Bool_t DoMulticlass() const { return fAnalysisType == Types::kMulticlass; }; 440 ; 441 // setter method for suppressing writing to XML and writing of standalone classes; 442 void DisableWriting(Bool_t setter){ fModelPersistence = setter?kFALSE:kTRUE; }//DEPRECATED; 443 ; 444 protected:; 445 mutable const Event *fTmpEvent; //! temporary event when testing on a different DataSet than the own one; 446 DataSet *fTmpData = nullptr; //! temporary dataset used when evaluating on a different data (used by MethodCategory::GetMvaValues); 447 // helper variables for JsMVA; 448 IPythonInteractive *fInteractive = nullptr;; 449 bool fExitFromTraining = false;; 450 UInt_t fIPyMaxIter = 0, fIPyCurrentIter = 0;; 451 ; 452 public:; 453 ; 454 // initializing IPythonInteractive class (for JsMVA only); 455 inline void InitIPythonInteractive(){; 456 if (fInteractive) delete fInteractive;; 457 fInteractive = new IPythonInteractive();; 458 }; 459 ; 460 // get training errors (for JsMVA only); 461 inline TMultiGraph* GetInteractiveTrainingError(){return fInteractive->Get();}; 462 ; 463 // stop's the training process (for JsMVA only); 464 inline void ExitFromTraining(){; 465 fExitFromTraining = true;; 466 }; 467 ; 468 // check's if the training ended (for JsMVA only); 469 inline bool TrainingEnded(){; 470 if (fExitFromTraining && fInteractive){; 471 delete ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:24696,Testability,test,test,24696,"; 543 ; 544 // ---------- private definitions --------------------------------------------; 545 // Initialisation; 546 void InitBase();; 547 void DeclareBaseOptions();; 548 void ProcessBaseOptions();; 549 ; 550 // used in efficiency computation; 551 enum ECutOrientation { kNegative = -1, kPositive = +1 };; 552 ECutOrientation GetCutOrientation() const { return fCutOrientation; }; 553 ; 554 // ---------- private accessors ---------------------------------------------; 555 ; 556 // reset required for RootFinder; 557 void ResetThisBase();; 558 ; 559 // ---------- private auxiliary methods --------------------------------------; 560 ; 561 // PDFs for classifier response (required to compute signal probability and Rarity); 562 void CreateMVAPdfs();; 563 ; 564 // for root finder; 565 //virtual method to find ROOT; 566 virtual Double_t GetValueForRoot ( Double_t ); // implementation; 567 ; 568 // used for file parsing; 569 Bool_t GetLine( std::istream& fin, char * buf );; 570 ; 571 // fill test tree with classification or regression results; 572 virtual void AddClassifierOutput ( Types::ETreeType type );; 573 virtual void AddClassifierOutputProb( Types::ETreeType type );; 574 virtual void AddRegressionOutput ( Types::ETreeType type );; 575 virtual void AddMulticlassOutput ( Types::ETreeType type );; 576 ; 577 private:; 578 ; 579 void AddInfoItem( void* gi, const TString& name,; 580 const TString& value) const;; 581 ; 582 // ========== class members ==================================================; 583 ; 584 protected:; 585 ; 586 // direct accessors; 587 Ranking* fRanking; // pointer to ranking object (created by derived classifiers); 588 std::vector<TString>* fInputVars; // vector of input variables used in MVA; 589 ; 590 // histogram binning; 591 Int_t fNbins; // number of bins in input variable histograms; 592 Int_t fNbinsMVAoutput; // number of bins in MVA output histograms; 593 Int_t fNbinsH; // number of bins in evaluation histograms; 594 ; 595 Types::EAnalysisType f",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:32687,Testability,assert,assert,32687,"NbinsMVAPdf; // number of bins used in histogram that creates PDF; 727 Int_t fNsmoothMVAPdf; // number of times a histogram is smoothed before creating the PDF; 728 ; 729 protected:; 730 Results *fResults;; 731 ClassDef(MethodBase,0); // Virtual base class for all TMVA method; 732 ; 733 };; 734} // namespace TMVA; 735 ; 736 ; 737 ; 738 ; 739 ; 740 ; 741 ; 742// ========== INLINE FUNCTIONS =========================================================; 743 ; 744 ; 745//_______________________________________________________________________; 746inline const TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762 return GetTransformationHandler().Transform(Data()->GetEvent(ievt));; 763}; 764 ; 765inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt, Types::ETreeType type ) const; 766{; 767 assert(fTmpEvent==nullptr);; 768 return GetTransformationHandler().Transform(Data()->GetEvent(ievt, type));; 769}; 770 ; 771inline const TMVA::Event* TMVA::MethodBase::GetTrainingEvent( Long64_t ievt ) const; 772{; 773 assert(fTmpEvent==nullptr);; 774 return GetEvent(ievt, Types::kTraining);; 775}; 776 ; 777inline const TMVA::Event* TMVA::MethodBase::GetTestingEvent( Long64_t ievt ) const; 778{; 779 assert(fTmpEvent==nullptr);; 780 return GetEvent(ievt, Types::kTesting);; 781}; 782 ; 783#endif; Configurable.h; DataSet.h; Event.h; IMethod.h; Results.h; Char_tchar Char_tDefinition RtypesCore.h:37; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDe",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:32915,Testability,assert,assert,32915,"sDef(MethodBase,0); // Virtual base class for all TMVA method; 732 ; 733 };; 734} // namespace TMVA; 735 ; 736 ; 737 ; 738 ; 739 ; 740 ; 741 ; 742// ========== INLINE FUNCTIONS =========================================================; 743 ; 744 ; 745//_______________________________________________________________________; 746inline const TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762 return GetTransformationHandler().Transform(Data()->GetEvent(ievt));; 763}; 764 ; 765inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt, Types::ETreeType type ) const; 766{; 767 assert(fTmpEvent==nullptr);; 768 return GetTransformationHandler().Transform(Data()->GetEvent(ievt, type));; 769}; 770 ; 771inline const TMVA::Event* TMVA::MethodBase::GetTrainingEvent( Long64_t ievt ) const; 772{; 773 assert(fTmpEvent==nullptr);; 774 return GetEvent(ievt, Types::kTraining);; 775}; 776 ; 777inline const TMVA::Event* TMVA::MethodBase::GetTestingEvent( Long64_t ievt ) const; 778{; 779 assert(fTmpEvent==nullptr);; 780 return GetEvent(ievt, Types::kTesting);; 781}; 782 ; 783#endif; Configurable.h; DataSet.h; Event.h; IMethod.h; Results.h; Char_tchar Char_tDefinition RtypesCore.h:37; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDefinition RtypesCore.h:69; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassDef#define ClassDef(name, id)Definition Rtypes.h:342; TFile.h; dataOption_t Option_t TPoint TPoint const char GetTextMagnitude G",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:33134,Testability,assert,assert,33134,"onst TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762 return GetTransformationHandler().Transform(Data()->GetEvent(ievt));; 763}; 764 ; 765inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt, Types::ETreeType type ) const; 766{; 767 assert(fTmpEvent==nullptr);; 768 return GetTransformationHandler().Transform(Data()->GetEvent(ievt, type));; 769}; 770 ; 771inline const TMVA::Event* TMVA::MethodBase::GetTrainingEvent( Long64_t ievt ) const; 772{; 773 assert(fTmpEvent==nullptr);; 774 return GetEvent(ievt, Types::kTraining);; 775}; 776 ; 777inline const TMVA::Event* TMVA::MethodBase::GetTestingEvent( Long64_t ievt ) const; 778{; 779 assert(fTmpEvent==nullptr);; 780 return GetEvent(ievt, Types::kTesting);; 781}; 782 ; 783#endif; Configurable.h; DataSet.h; Event.h; IMethod.h; Results.h; Char_tchar Char_tDefinition RtypesCore.h:37; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDefinition RtypesCore.h:69; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassDef#define ClassDef(name, id)Definition Rtypes.h:342; TFile.h; dataOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void dataDefinition TGWin32VirtualXProxy.cxx:104; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXPro",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:33318,Testability,assert,assert,33318,"onst TMVA::Event* TMVA::MethodBase::GetEvent( const TMVA::Event* ev ) const; 747{; 748 return GetTransformationHandler().Transform(ev);; 749}; 750 ; 751inline const TMVA::Event* TMVA::MethodBase::GetEvent() const; 752{; 753 if(fTmpEvent); 754 return GetTransformationHandler().Transform(fTmpEvent);; 755 else; 756 return GetTransformationHandler().Transform(Data()->GetEvent());; 757}; 758 ; 759inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt ) const; 760{; 761 assert(fTmpEvent==nullptr);; 762 return GetTransformationHandler().Transform(Data()->GetEvent(ievt));; 763}; 764 ; 765inline const TMVA::Event* TMVA::MethodBase::GetEvent( Long64_t ievt, Types::ETreeType type ) const; 766{; 767 assert(fTmpEvent==nullptr);; 768 return GetTransformationHandler().Transform(Data()->GetEvent(ievt, type));; 769}; 770 ; 771inline const TMVA::Event* TMVA::MethodBase::GetTrainingEvent( Long64_t ievt ) const; 772{; 773 assert(fTmpEvent==nullptr);; 774 return GetEvent(ievt, Types::kTraining);; 775}; 776 ; 777inline const TMVA::Event* TMVA::MethodBase::GetTestingEvent( Long64_t ievt ) const; 778{; 779 assert(fTmpEvent==nullptr);; 780 return GetEvent(ievt, Types::kTesting);; 781}; 782 ; 783#endif; Configurable.h; DataSet.h; Event.h; IMethod.h; Results.h; Char_tchar Char_tDefinition RtypesCore.h:37; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDefinition RtypesCore.h:69; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassDef#define ClassDef(name, id)Definition Rtypes.h:342; TFile.h; dataOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void dataDefinition TGWin32VirtualXProxy.cxx:104; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXPro",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:40667,Testability,test,testing,40667," MethodBase.h:651; TMVA::MethodBase::fVerboseBool_t fVerboseverbose flagDefinition MethodBase.h:676; TMVA::MethodBase::fNormaliseBool_t fNormaliseDefinition MethodBase.h:722; TMVA::MethodBase::GetKSTrainingVsTestvirtual Double_t GetKSTrainingVsTest(Char_t SorB, TString opt=""X"")Definition MethodBase.cxx:3392; TMVA::MethodBase::fNbinsInt_t fNbinsDefinition MethodBase.h:591; TMVA::MethodBase::fMethodNameTString fMethodNameDefinition MethodBase.h:615; TMVA::MethodBase::GetMvaValuevirtual Double_t GetMvaValue(Double_t *errLower=nullptr, Double_t *errUpper=nullptr)=0; TMVA::MethodBase::GetFileTFile * GetFile() constDefinition MethodBase.h:370; TMVA::MethodBase::GetSeparationvirtual Double_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodBase.cxx:2789; TMVA::MethodBase::fFileDirTString fFileDirunix sub-directory for weight files (default: DataLoader's Name + ""weights"")Definition MethodBase.h:637; TMVA::MethodBase::fTmpDataDataSet * fTmpDatatemporary event when testing on a different DataSet than the own oneDefinition MethodBase.h:446; TMVA::MethodBase::HasTrainingTreeBool_t HasTrainingTree() constDefinition MethodBase.h:513; TMVA::MethodBase::DeclareOptionsvirtual void DeclareOptions()=0; TMVA::MethodBase::SetSilentFilevoid SetSilentFile(Bool_t status)Definition MethodBase.h:378; TMVA::MethodBase::ReadClassesFromXMLvoid ReadClassesFromXML(void *clsnode)read number of classes from XMLDefinition MethodBase.cxx:1917; TMVA::MethodBase::GetInteractiveTrainingErrorTMultiGraph * GetInteractiveTrainingError()Definition MethodBase.h:461; TMVA::MethodBase::SetWeightFileDirvoid SetWeightFileDir(TString fileDir)set directory of weight fileDefinition MethodBase.cxx:2059; TMVA::MethodBase::WriteStateToXMLvoid WriteStateToXML(void *parent) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1331; TMVA::MethodBase::fSilentFileBool_t fSilentFileDefinition MethodBase.h:631; TMVA::MethodBase",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:42477,Testability,test,test,42477," weight fileDefinition MethodBase.cxx:2059; TMVA::MethodBase::WriteStateToXMLvoid WriteStateToXML(void *parent) constgeneral method used in writing the header of the weight files where the used variables,...Definition MethodBase.cxx:1331; TMVA::MethodBase::fSilentFileBool_t fSilentFileDefinition MethodBase.h:631; TMVA::MethodBase::DeclareBaseOptionsvoid DeclareBaseOptions()define the options (their key words) that can be set in the option string here the options valid for ...Definition MethodBase.cxx:509; TMVA::MethodBase::GetMaxIterUInt_t GetMaxIter()Definition MethodBase.h:478; TMVA::MethodBase::fRmsBDouble_t fRmsBRMS (background)Definition MethodBase.h:664; TMVA::MethodBase::GetXminDouble_t GetXmin(Int_t ivar) constDefinition MethodBase.h:356; TMVA::MethodBase::VerboseBool_t Verbose() constDefinition MethodBase.h:503; TMVA::MethodBase::TestRegressionvirtual void TestRegression(Double_t &bias, Double_t &biasT, Double_t &dev, Double_t &devT, Double_t &rms, Double_t &rmsT, Double_t &mInf, Double_t &mInfT, Double_t &corr, Types::ETreeType type)calculate <sum-of-deviation-squared> of regression output versus ""true"" value from test sampleDefinition MethodBase.cxx:992; TMVA::MethodBase::GetMeanDouble_t GetMean(Int_t ivar) constDefinition MethodBase.h:354; TMVA::MethodBase::DeclareCompatibilityOptionsvirtual void DeclareCompatibilityOptions()options that are used ONLY for the READER to ensure backward compatibility they are hence without any...Definition MethodBase.cxx:596; TMVA::MethodBase::fSplTrainBPDF * fSplTrainBPDFs of training MVA distribution (background)Definition MethodBase.h:655; TMVA::MethodBase::GetMethodTypeNameTString GetMethodTypeName() constDefinition MethodBase.h:332; TMVA::MethodBase::DoMulticlassBool_t DoMulticlass() constDefinition MethodBase.h:439; TMVA::MethodBase::GetSignificancevirtual Double_t GetSignificance() constcompute significance of mean differenceDefinition MethodBase.cxx:2776; TMVA::MethodBase::fFileTFile * fFileDefinition MethodBase.h:6",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:46468,Testability,test,test,46468,"stprints out method-specific help methodDefinition MethodBase.cxx:3264; TMVA::MethodBase::SetMethodDirvoid SetMethodDir(TDirectory *methodDir)Definition MethodBase.h:372; TMVA::MethodBase::GetJobNameconst TString & GetJobName() constDefinition MethodBase.h:330; TMVA::MethodBase::IgnoreEventsWithNegWeightsInTrainingBool_t IgnoreEventsWithNegWeightsInTraining() constDefinition MethodBase.h:686; TMVA::MethodBase::ECutOrientationECutOrientationDefinition MethodBase.h:551; TMVA::MethodBase::kPositive@ kPositiveDefinition MethodBase.h:551; TMVA::MethodBase::kNegative@ kNegativeDefinition MethodBase.h:551; TMVA::MethodBase::TrainMethodvoid TrainMethod()Definition MethodBase.cxx:650; TMVA::MethodBase::fTrainTimeDouble_t fTrainTimeDefinition MethodBase.h:695; TMVA::MethodBase::WriteEvaluationHistosToFilevirtual void WriteEvaluationHistosToFile(Types::ETreeType treetype)writes all MVA evaluation histograms to fileDefinition MethodBase.cxx:2094; TMVA::MethodBase::TestMulticlassvirtual void TestMulticlass()test multiclass classificationDefinition MethodBase.cxx:1100; TMVA::MethodBase::fModelPersistenceBool_t fModelPersistenceDefinition MethodBase.h:633; TMVA::MethodBase::GetTestvarNameconst TString & GetTestvarName() constDefinition MethodBase.h:335; TMVA::MethodBase::GetEventCollectionconst std::vector< TMVA::Event * > & GetEventCollection(Types::ETreeType type)returns the event collection (i.e.Definition MethodBase.cxx:3347; TMVA::MethodBase::GetDataMvaValuesvirtual std::vector< Double_t > GetDataMvaValues(DataSet *data=nullptr, Long64_t firstEvt=0, Long64_t lastEvt=-1, Bool_t logProgress=false)get all the MVA values for the events of the given Data typeDefinition MethodBase.cxx:940; TMVA::MethodBase::SetupMethodvoid SetupMethod()setup of methodsDefinition MethodBase.cxx:406; TMVA::MethodBase::BaseDirTDirectory * BaseDir() constreturns the ROOT directory where info/histograms etc of the corresponding MVA method instance are sto...Definition MethodBase.cxx:1980; TMVA::MethodBas",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:47052,Testability,log,logProgress,47052,"gative@ kNegativeDefinition MethodBase.h:551; TMVA::MethodBase::TrainMethodvoid TrainMethod()Definition MethodBase.cxx:650; TMVA::MethodBase::fTrainTimeDouble_t fTrainTimeDefinition MethodBase.h:695; TMVA::MethodBase::WriteEvaluationHistosToFilevirtual void WriteEvaluationHistosToFile(Types::ETreeType treetype)writes all MVA evaluation histograms to fileDefinition MethodBase.cxx:2094; TMVA::MethodBase::TestMulticlassvirtual void TestMulticlass()test multiclass classificationDefinition MethodBase.cxx:1100; TMVA::MethodBase::fModelPersistenceBool_t fModelPersistenceDefinition MethodBase.h:633; TMVA::MethodBase::GetTestvarNameconst TString & GetTestvarName() constDefinition MethodBase.h:335; TMVA::MethodBase::GetEventCollectionconst std::vector< TMVA::Event * > & GetEventCollection(Types::ETreeType type)returns the event collection (i.e.Definition MethodBase.cxx:3347; TMVA::MethodBase::GetDataMvaValuesvirtual std::vector< Double_t > GetDataMvaValues(DataSet *data=nullptr, Long64_t firstEvt=0, Long64_t lastEvt=-1, Bool_t logProgress=false)get all the MVA values for the events of the given Data typeDefinition MethodBase.cxx:940; TMVA::MethodBase::SetupMethodvoid SetupMethod()setup of methodsDefinition MethodBase.cxx:406; TMVA::MethodBase::BaseDirTDirectory * BaseDir() constreturns the ROOT directory where info/histograms etc of the corresponding MVA method instance are sto...Definition MethodBase.cxx:1980; TMVA::MethodBase::GetTestingEventconst Event * GetTestingEvent(Long64_t ievt) constDefinition MethodBase.h:777; TMVA::MethodBase::GetMulticlassEfficiencyvirtual std::vector< Float_t > GetMulticlassEfficiency(std::vector< std::vector< Float_t > > &purity)Definition MethodBase.cxx:2703; TMVA::MethodBase::GetNTargetsUInt_t GetNTargets() constDefinition MethodBase.h:346; TMVA::MethodBase::fVerbosityLevelEMsgType fVerbosityLevelverbosity levelDefinition MethodBase.h:678; TMVA::MethodBase::GetProbaNameconst TString GetProbaName() constDefinition MethodBase.h:336; TMVA::Method",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:52177,Testability,log,logProgress,52177,":vector< Float_t > * fRegressionReturnValDefinition MethodBase.h:597; TMVA::MethodBase::fMulticlassReturnValstd::vector< Float_t > * fMulticlassReturnValDefinition MethodBase.h:598; TMVA::MethodBase::fRmsSDouble_t fRmsSRMS (signal)Definition MethodBase.h:663; TMVA::MethodBase::GetLineBool_t GetLine(std::istream &fin, char *buf)reads one line from the input stream checks for certain keywords and interprets the line if keywords ...Definition MethodBase.cxx:2142; TMVA::MethodBase::GetEventconst Event * GetEvent() constDefinition MethodBase.h:751; TMVA::MethodBase::fVarTransformStringTString fVarTransformStringlabels variable transform methodDefinition MethodBase.h:669; TMVA::MethodBase::ProcessSetupvoid ProcessSetup()process all options the ""CheckForUnusedOptions"" is done in an independent call, since it may be overr...Definition MethodBase.cxx:423; TMVA::MethodBase::ProcessOptionsvirtual void ProcessOptions()=0; TMVA::MethodBase::GetMvaValuesvirtual std::vector< Double_t > GetMvaValues(Long64_t firstEvt=0, Long64_t lastEvt=-1, Bool_t logProgress=false)get all the MVA values for the events of the current Data typeDefinition MethodBase.cxx:898; TMVA::MethodBase::IsSignalLikevirtual Bool_t IsSignalLike()uses a pre-set cut on the MVA output (SetSignalReferenceCut and SetSignalReferenceCutOrientation) for...Definition MethodBase.cxx:855; TMVA::MethodBase::RerouteTransformationHandlervoid RerouteTransformationHandler(TransformationHandler *fTargetTransformation)Definition MethodBase.h:403; TMVA::MethodBase::~MethodBasevirtual ~MethodBase()destructorDefinition MethodBase.cxx:364; TMVA::MethodBase::HasMVAPdfsBool_t HasMVAPdfs() constDefinition MethodBase.h:435; TMVA::MethodBase::fMVAPdfSPDF * fMVAPdfSsignal MVA PDFDefinition MethodBase.h:645; TMVA::MethodBase::fBackgroundClassUInt_t fBackgroundClassDefinition MethodBase.h:690; TMVA::MethodBase::fNbinsMVAPdfInt_t fNbinsMVAPdfDefinition MethodBase.h:726; TMVA::MethodBase::fTestTimeDouble_t fTestTimeDefinition MethodBase.h:696; ",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:54157,Testability,test,testTime,54157,"dBase::GetMaximumSignificancevirtual Double_t GetMaximumSignificance(Double_t SignalEvents, Double_t BackgroundEvents, Double_t &optimal_significance_value) constplot significance, , curve for given number of signal and background events; returns cut for maximum ...Definition MethodBase.cxx:2886; TMVA::MethodBase::GetTrainingEfficiencyvirtual Double_t GetTrainingEfficiency(const TString &)Definition MethodBase.cxx:2528; TMVA::MethodBase::SetWeightFileNamevoid SetWeightFileName(TString)set the weight file name (depreciated)Definition MethodBase.cxx:2068; TMVA::MethodBase::DataInfoDataSetInfo & DataInfo() constDefinition MethodBase.h:410; TMVA::MethodBase::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) constcreate reader class for method (classification only at present)Definition MethodBase.cxx:3003; TMVA::MethodBase::GetWeightFileNameTString GetWeightFileName() constretrieve weight file nameDefinition MethodBase.cxx:2076; TMVA::MethodBase::SetTestTimevoid SetTestTime(Double_t testTime)Definition MethodBase.h:165; TMVA::MethodBase::fMethodTypeTypes::EMVA fMethodTypeDefinition MethodBase.h:616; TMVA::MethodBase::TestClassificationvirtual void TestClassification()initializationDefinition MethodBase.cxx:1127; TMVA::MethodBase::AddOutputvoid AddOutput(Types::ETreeType type, Types::EAnalysisType analysisType)Definition MethodBase.cxx:1315; TMVA::MethodBase::WriteMonitoringHistosToFilevirtual void WriteMonitoringHistosToFile() constwrite special monitoring histograms to file dummy implementation here --------------—Definition MethodBase.cxx:2133; TMVA::MethodBase::GetNVariablesUInt_t GetNVariables() constDefinition MethodBase.h:345; TMVA::MethodBase::fAnalysisTypeTypes::EAnalysisType fAnalysisTypeDefinition MethodBase.h:595; TMVA::MethodBase::AddRegressionOutputvirtual void AddRegressionOutput(Types::ETreeType type)prepare tree branch with the method's discriminating variableDefinition MethodBase.cxx:744; TMVA::MethodBase::InitBasevoid InitBase()d",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodBase_8h_source.html:8832,Usability,simpl,simple,8832,"culation is skipped if err==0; 198 virtual Double_t GetMvaValue( Double_t* errLower = nullptr, Double_t* errUpper = nullptr) = 0;; 199 ; 200 // signal/background classification response; 201 Double_t GetMvaValue( const TMVA::Event* const ev, Double_t* err = nullptr, Double_t* errUpper = nullptr );; 202 ; 203 protected:; 204 // helper function to set errors to -1; 205 void NoErrorCalc(Double_t* const err, Double_t* const errUpper);; 206 ; 207 // signal/background classification response for all current set of data; 208 virtual std::vector<Double_t> GetMvaValues(Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 209 // same as above but using a provided data set (used by MethodCategory); 210 virtual std::vector<Double_t> GetDataMvaValues(DataSet *data = nullptr, Long64_t firstEvt = 0, Long64_t lastEvt = -1, Bool_t logProgress = false);; 211 ; 212 public:; 213 // regression response; 214 const std::vector<Float_t>& GetRegressionValues(const TMVA::Event* const ev){; 215 fTmpEvent = ev;; 216 const std::vector<Float_t>* ptr = &GetRegressionValues();; 217 fTmpEvent = nullptr;; 218 return (*ptr);; 219 }; 220 ; 221 virtual const std::vector<Float_t>& GetRegressionValues() {; 222 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 223 return (*ptr);; 224 }; 225 ; 226 // multiclass classification response; 227 virtual const std::vector<Float_t>& GetMulticlassValues() {; 228 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 229 return (*ptr);; 230 }; 231 ; 232 // Training history; 233 virtual const std::vector<Float_t>& GetTrainingHistory(const char* /*name*/ ) {; 234 std::vector<Float_t>* ptr = new std::vector<Float_t>(0);; 235 return (*ptr);; 236 }; 237 ; 238 // probability of classifier response (mvaval) to be signal (requires ""CreateMvaPdf"" option set); 239 virtual Double_t GetProba( const Event *ev); // the simple one, automatically calculates the mvaVal and uses the SAME sig/bkg ratio as given in the training sample (typically 50/50 .",MatchSource.WIKI,doc/master/MethodBase_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodBase_8h_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:10847,Availability,avail,available,10847,"];; 288 }; 289 ; 290 if (NULL != fCutMin) delete [] fCutMin;; 291 if (NULL != fCutMax) delete [] fCutMax;; 292 ; 293 if (NULL != fTmpCutMin) delete [] fTmpCutMin;; 294 if (NULL != fTmpCutMax) delete [] fTmpCutMax;; 295 ; 296 if (NULL != fBinaryTreeS) delete fBinaryTreeS;; 297 if (NULL != fBinaryTreeB) delete fBinaryTreeB;; 298}; 299 ; 300////////////////////////////////////////////////////////////////////////////////; 301/// define the options (their key words) that can be set in the option string.; 302///; 303/// know options:; 304/// - Method `<string>` Minimisation method. Available values are:; 305/// - MC Monte Carlo `<default>`; 306/// - GA Genetic Algorithm; 307/// - SA Simulated annealing; 308///; 309/// - EffMethod `<string>` Efficiency selection method. Available values are:; 310/// - EffSel `<default>`; 311/// - EffPDF; 312///; 313/// - VarProp `<string>` Property of variable 1 for the MC method (taking precedence over the; 314/// globale setting. The same values as for the global option are available. Variables 1..10 can be; 315/// set this way; 316///; 317/// - CutRangeMin/Max `<float>` user-defined ranges in which cuts are varied; 318 ; 319void TMVA::MethodCuts::DeclareOptions(); 320{; 321 DeclareOptionRef(fFitMethodS = ""GA"", ""FitMethod"", ""Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated)"");; 322 AddPreDefVal(TString(""GA""));; 323 AddPreDefVal(TString(""SA""));; 324 AddPreDefVal(TString(""MC""));; 325 AddPreDefVal(TString(""MCEvents""));; 326 AddPreDefVal(TString(""MINUIT""));; 327 AddPreDefVal(TString(""EventScan""));; 328 ; 329 // selection type; 330 DeclareOptionRef(fEffMethodS = ""EffSel"", ""EffMethod"", ""Selection Method"");; 331 AddPreDefVal(TString(""EffSel""));; 332 AddPreDefVal(TString(""EffPDF""));; 333 ; 334 // cut ranges; 335 fCutRange.resize(GetNvar());; 336 fCutRangeMin = new Double_t[GetNvar()];; 337 fCutRangeMax = new Double_t[GetNvar()];; 338 for (UInt_t ivar=",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:13566,Availability,avail,available,13566,"eDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm."" << Endl;; 369 Log() << kWARNING << ""Please remove \""Normalise\"" option from booking option string"" << Endl;; 370 Log() << kWARNING << ""==> Will reset normalisation flag to \""False\"""" << Endl;; 371 SetNormalised( kFALSE );; 372 }; 373 ; 374 if (IgnoreEventsWithNegWeightsInTraining()) {; 375 Log() << kFATAL << ""Mechanism to ignore events with negative weights in training not yet available for method: ""; 376 << GetMethodTypeName(); 377 << "" --> Please remove \""IgnoreNegWeightsInTraining\"" option from booking string.""; 378 << Endl;; 379 }; 380 ; 381 if (fFitMethodS == ""MC"" ) fFitMethod = kUseMonteCarlo;; 382 else if (fFitMethodS == ""MCEvents"") fFitMethod = kUseMonteCarloEvents;; 383 else if (fFitMethodS == ""GA"" ) fFitMethod = kUseGeneticAlgorithm;; 384 else if (fFitMethodS == ""SA"" ) fFitMethod = kUseSimulatedAnnealing;; 385 else if (fFitMethodS == ""MINUIT"" ) {; 386 fFitMethod = kUseMinuit;; 387 Log() << kWARNING << ""poor performance of MINUIT in MethodCuts; preferred fit method: GA"" << Endl;; 388 }; 389 else if (fFitMethodS == ""EventScan"" ) fFitMethod = kUseEventScan;; 390 else Log() << kFATAL << ""unknown minimisation method: "" << fFitMethodS << Endl;; 391 ; 392 if (fEffMethodS == ""EFFSEL"" ) fEffMethod = kUseEventSelection; // highly recommen",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:16287,Availability,error,error,16287,"meters theFitP = kNotEnforced;; 413 if (fAllVarsI[ivar] == """" || fAllVarsI[ivar] == ""NotEnforced"") theFitP = kNotEnforced;; 414 else if (fAllVarsI[ivar] == ""FMax"" ) theFitP = kForceMax;; 415 else if (fAllVarsI[ivar] == ""FMin"" ) theFitP = kForceMin;; 416 else if (fAllVarsI[ivar] == ""FSmart"" ) theFitP = kForceSmart;; 417 else {; 418 Log() << kFATAL << ""unknown value \'"" << fAllVarsI[ivar]; 419 << ""\' for fit parameter option "" << Form(""VarProp[%i]"",ivar) << Endl;; 420 }; 421 (*fFitParams)[ivar] = theFitP;; 422 ; 423 if (theFitP != kNotEnforced); 424 Log() << kINFO << ""Use \"""" << fAllVarsI[ivar]; 425 << ""\"" cuts for variable: "" << ""'"" << (*fInputVars)[ivar] << ""'"" << Endl;; 426 }; 427}; 428 ; 429////////////////////////////////////////////////////////////////////////////////; 430/// cut evaluation: returns 1.0 if event passed, 0.0 otherwise; 431 ; 432Double_t TMVA::MethodCuts::GetMvaValue( Double_t* err, Double_t* errUpper ); 433{; 434 // cannot determine error; 435 NoErrorCalc(err, errUpper);; 436 ; 437 // sanity check; 438 if (fCutMin == NULL || fCutMax == NULL || fNbins == 0) {; 439 Log() << kFATAL << ""<Eval_Cuts> fCutMin/Max have zero pointer. ""; 440 << ""Did you book Cuts ?"" << Endl;; 441 }; 442 ; 443 const Event* ev = GetEvent();; 444 ; 445 // sanity check; 446 if (fTestSignalEff > 0) {; 447 // get efficiency bin; 448 Int_t ibin = fEffBvsSLocal->FindBin( fTestSignalEff );; 449 if (ibin < 0 ) ibin = 0;; 450 else if (ibin >= fNbins) ibin = fNbins - 1;; 451 ; 452 Bool_t passed = kTRUE;; 453 for (UInt_t ivar=0; ivar<GetNvar(); ivar++); 454 passed &= ( (ev->GetValue(ivar) > fCutMin[ivar][ibin]) &&; 455 (ev->GetValue(ivar) <= fCutMax[ivar][ibin]) );; 456 ; 457 return passed ? 1. : 0. ;; 458 }; 459 else return 0;; 460}; 461 ; 462////////////////////////////////////////////////////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 4",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:37049,Availability,error,error,37049,,MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:40127,Availability,error,error,40127," ) Log() << kWARNING << ""Negative background efficiency found and set to 0. This is probably due to many events with negative weights in a certain cut-region."" << Endl;; 1042 fNegEffWarning = kTRUE;; 1043 }; 1044}; 1045 ; 1046////////////////////////////////////////////////////////////////////////////////; 1047/// compute signal and background efficiencies from event counting; 1048/// for given cut sample; 1049 ; 1050void TMVA::MethodCuts::GetEffsfromSelection( Double_t* cutMin, Double_t* cutMax,; 1051 Double_t& effS, Double_t& effB); 1052{; 1053 Float_t nTotS = 0, nTotB = 0;; 1054 Float_t nSelS = 0, nSelB = 0;; 1055 ; 1056 Volume* volume = new Volume( cutMin, cutMax, GetNvar() );; 1057 ; 1058 // search for all events lying in the volume, and add up their weights; 1059 nSelS = fBinaryTreeS->SearchVolume( volume );; 1060 nSelB = fBinaryTreeB->SearchVolume( volume );; 1061 ; 1062 delete volume;; 1063 ; 1064 // total number of ""events"" (sum of weights) as reference to compute efficiency; 1065 nTotS = fBinaryTreeS->GetSumOfWeights();; 1066 nTotB = fBinaryTreeB->GetSumOfWeights();; 1067 ; 1068 // sanity check; 1069 if (nTotS == 0 && nTotB == 0) {; 1070 Log() << kFATAL << ""<GetEffsfromSelection> fatal error in zero total number of events:""; 1071 << "" nTotS, nTotB: "" << nTotS << "" "" << nTotB << "" ***"" << Endl;; 1072 }; 1073 ; 1074 // efficiencies; 1075 if (nTotS == 0 ) {; 1076 effS = 0;; 1077 effB = nSelB/nTotB;; 1078 Log() << kWARNING << ""<ComputeEstimator> zero number of signal events"" << Endl;; 1079 }; 1080 else if (nTotB == 0) {; 1081 effB = 0;; 1082 effS = nSelS/nTotS;; 1083 Log() << kWARNING << ""<ComputeEstimator> zero number of background events"" << Endl;; 1084 }; 1085 else {; 1086 effS = nSelS/nTotS;; 1087 effB = nSelB/nTotB;; 1088 }; 1089 ; 1090 // quick fix to prevent from efficiencies < 0; 1091 if( effS < 0.0 ) {; 1092 effS = 0.0;; 1093 if( !fNegEffWarning ) Log() << kWARNING << ""Negative signal efficiency found and set to 0. This is probably due to many events w",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:46343,Availability,error,error,46343,"ar] = new PDF( TString(GetName()) + "" PDF Var Sig "" + GetInputVar( ivar ), (*fVarHistS_smooth)[ivar], PDF::kSpline2 );; 1210 (*fVarPdfB)[ivar] = new PDF( TString(GetName()) + "" PDF Var Bkg "" + GetInputVar( ivar ), (*fVarHistB_smooth)[ivar], PDF::kSpline2 );; 1211 }; 1212}; 1213 ; 1214////////////////////////////////////////////////////////////////////////////////; 1215/// read the cuts from stream; 1216 ; 1217void TMVA::MethodCuts::ReadWeightsFromStream( std::istream& istr ); 1218{; 1219 TString dummy;; 1220 UInt_t dummyInt;; 1221 ; 1222 // first the dimensions; 1223 istr >> dummy >> dummy;; 1224 // coverity[tainted_data_argument]; 1225 istr >> dummy >> fNbins;; 1226 ; 1227 // get rid of one read-in here because we read in once all ready to check for decorrelation; 1228 istr >> dummy >> dummy >> dummy >> dummy >> dummy >> dummy >> dummyInt >> dummy ;; 1229 ; 1230 // sanity check; 1231 if (dummyInt != Data()->GetNVariables()) {; 1232 Log() << kFATAL << ""<ReadWeightsFromStream> fatal error: mismatch ""; 1233 << ""in number of variables: "" << dummyInt << "" != "" << Data()->GetNVariables() << Endl;; 1234 }; 1235 //SetNvar(dummyInt);; 1236 ; 1237 // print some information; 1238 if (fFitMethod == kUseMonteCarlo) {; 1239 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1240 }; 1241 else if (fFitMethod == kUseMonteCarloEvents) {; 1242 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1243 }; 1244 else if (fFitMethod == kUseGeneticAlgorithm) {; 1245 Log() << kINFO << ""Read cuts optimised using Genetic Algorithm"" << Endl;; 1246 }; 1247 else if (fFitMethod == kUseSimulatedAnnealing) {; 1248 Log() << kINFO << ""Read cuts optimised using Simulated Annealing algorithm"" << Endl;; 1249 }; 1250 else if (fFitMethod == kUseEventScan) {; 1251 Log() << kINFO << ""Read cuts optimised using Full Event Scan"" << Endl;; 1252 }; 1253 else {; 1254 Log() << kWARNING << ""unknown method: "" << fFitMethod << Endl;; 1255 }; 1256 Log() << kINFO <<",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:70840,Availability,failure,failures,70840," scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * number of variables)"" << Endl;; 1771 Log() << "" -> increase \""nsteps\"""" << Endl;; 1772 Log() << """" << Endl;; 1773 Log() << bold << ""Simulated Annealing (SA) algorithm:"" << resbold << Endl;; 1774 Log() << """" << Endl;; 1775 Log() << ""\""Increasing Adaptive\"" approach:"" << Endl;; 1776 Log() << """" << Endl;; 1777 Log() << ""The algorithm seeks local minima and explores their neighborhoods, while"" << Endl;; 1778 Log() << ""changing the ambient temperature depending on the number of failures"" << Endl;; 1779 Log() << ""in the previous steps. The performance can be improved by increasing"" << Endl;; 1780 Log() << ""the number of iteration steps (\""MaxCalls\""), or by adjusting the"" << Endl;; 1781 Log() << ""minimal temperature (\""MinTemperature\""). Manual adjustments of the"" << Endl;; 1782 Log() << ""speed of the temperature increase (\""TemperatureScale\"" and \""AdaptiveSpeed\"")"" << Endl;; 1783 Log() << ""to individual data sets should also help. Summary:"" << brk << Endl;; 1784 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1785 Log() << "" -> adjust \""MinTemperature\"""" << brk << Endl;; 178",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:431,Deployability,integrat,integrated,431,". ROOT: tmva/tmva/src/MethodCuts.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodCuts.cxx. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Matt Jachowski, Peter Speckmayer, Eckhard von Toerne, Helge Voss, Kai Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate Data analysis *; 6 * Package: TMVA *; 7 * Class : TMVA::MethodCuts *; 8 * *; 9 * *; 10 * Description: *; 11 * Implementation (see header for description) *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 16 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 17 * Eckhard von Toerne <evt@physik.uni-bonn.de> - U. of Bonn, Germany *; 18 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 19 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 20 * *; 21 * Copyright (c) 2005: *; 22 * CERN, Switzerland *; 23 * U. of Victoria, Canada *; 24 * MPI-K Heidelberg, Germany *; 25 * *; 26 * Redistribution and use in source and binary forms, with or without *; 27 * modification, are permitted according to the terms listed in LICENSE *; 28 * (see tmva/doc/LICENSE) *; 29 **********************************************************************************/; 30 ; 31/*! \class TMVA::MethodCuts; 32\ingroup TMVA; 33 ; 34 Multivariate optimisation of signal efficiency for given background; 35 efficiency, applying rectangular minimum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the si",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:22837,Deployability,update,update,22837,"ar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree();; 591 fBinaryTreeS->Fill( GetEventCollection(Types::kTraining), fSignalClass );; 592 fBinaryTreeB = new BinarySearchTree();; 593 fBinaryTreeB->Fill( GetEventCollection(Types::kTraining), fBackgroundClass );; 594 ; 595 for (UInt_t ivar =0; ivar < Data()->GetNVariables(); ivar++) {; 596 (*fMeanS)[ivar] = fBinaryTreeS->Mean(Types::kSignal, ivar);; 597 (*fRmsS)[ivar] = fBinaryTreeS->RMS (Types::kSignal, ivar);; 598 (*fMeanB)[ivar] = fBinaryTreeB->Mean(Types::kBackground, ivar);; 599 (*fRmsB)[ivar] = fBinaryTreeB->RMS (Types::kBackground, ivar);; 600 ; 601 // update interval ?; 602 Double_t xmin = TMath::Min(fBinaryTreeS->Min(Types::kSignal, ivar),; 603 fBinaryTreeB->Min(Types::kBackground, ivar));; 604 Double_t xmax = TMath::Max(fBinaryTreeS->Max(Types::kSignal, ivar),; 605 fBinaryTreeB->Max(Types::kBackground, ivar));; 606 ; 607 // redefine ranges to be slightly smaller and larger than xmin and xmax, respectively; 608 Double_t eps = 0.01*(xmax - xmin);; 609 xmin -= eps;; 610 xmax += eps;; 611 ; 612 if (TMath::Abs(fCutRange[",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68976,Deployability,configurat,configuration,68976,"og() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true m",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:69372,Deployability,configurat,configuration,69372,"iables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * numbe",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:54027,Energy Efficiency,monitor,monitoring,54027," {; 1395 Log() << kFATAL << ""Mismatch in bins: "" << tmpbin-1 << "" >= "" << fNbins << Endl;; 1396 }; 1397 ; 1398 fEffBvsSLocal->SetBinContent( tmpbin, tmpeffB );; 1399 void* ct = gTools().GetChild(ch);; 1400 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1401 gTools().ReadAttr( ct, TString::Format( ""cutMin_%i"", ivar ), fCutMin[ivar][tmpbin-1] );; 1402 gTools().ReadAttr( ct, TString::Format( ""cutMax_%i"", ivar ), fCutMax[ivar][tmpbin-1] );; 1403 }; 1404 ch = gTools().GetNextChild(ch, ""Bin"");; 1405 }; 1406}; 1407 ; 1408////////////////////////////////////////////////////////////////////////////////; 1409/// write histograms and PDFs to file for monitoring purposes; 1410 ; 1411void TMVA::MethodCuts::WriteMonitoringHistosToFile( void ) const; 1412{; 1413 Log() << kINFO << ""Write monitoring histograms to file: "" << BaseDir()->GetPath() << Endl;; 1414 ; 1415 fEffBvsSLocal->Write();; 1416 ; 1417 // save reference histograms to file; 1418 if (fEffMethod == kUsePDFs) {; 1419 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1420 (*fVarHistS)[ivar]->Write();; 1421 (*fVarHistB)[ivar]->Write();; 1422 (*fVarHistS_smooth)[ivar]->Write();; 1423 (*fVarHistB_smooth)[ivar]->Write();; 1424 (*fVarPdfS)[ivar]->GetPDFHist()->Write();; 1425 (*fVarPdfB)[ivar]->GetPDFHist()->Write();; 1426 }; 1427 }; 1428}; 1429 ; 1430////////////////////////////////////////////////////////////////////////////////; 1431/// Overloaded function to create background efficiency (rejection) versus; 1432/// signal efficiency plot (first call of this function).; 1433///; 1434/// The function returns the signal efficiency at background efficiency; 1435/// indicated in theString; 1436///; 1437/// ""theString"" must have two entries:; 1438/// - `[0]`: ""Efficiency""; 1439/// - `[1]`: the value of background efficiency at which the signal efficiency; 1440/// is to be returned; 1441 ; 1442Double_t TMVA::MethodCuts::GetTrainingEfficiency(const TString& theString); 1443{; 1444 // parse input string for required background efficien",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:54162,Energy Efficiency,monitor,monitoring,54162," {; 1395 Log() << kFATAL << ""Mismatch in bins: "" << tmpbin-1 << "" >= "" << fNbins << Endl;; 1396 }; 1397 ; 1398 fEffBvsSLocal->SetBinContent( tmpbin, tmpeffB );; 1399 void* ct = gTools().GetChild(ch);; 1400 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1401 gTools().ReadAttr( ct, TString::Format( ""cutMin_%i"", ivar ), fCutMin[ivar][tmpbin-1] );; 1402 gTools().ReadAttr( ct, TString::Format( ""cutMax_%i"", ivar ), fCutMax[ivar][tmpbin-1] );; 1403 }; 1404 ch = gTools().GetNextChild(ch, ""Bin"");; 1405 }; 1406}; 1407 ; 1408////////////////////////////////////////////////////////////////////////////////; 1409/// write histograms and PDFs to file for monitoring purposes; 1410 ; 1411void TMVA::MethodCuts::WriteMonitoringHistosToFile( void ) const; 1412{; 1413 Log() << kINFO << ""Write monitoring histograms to file: "" << BaseDir()->GetPath() << Endl;; 1414 ; 1415 fEffBvsSLocal->Write();; 1416 ; 1417 // save reference histograms to file; 1418 if (fEffMethod == kUsePDFs) {; 1419 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1420 (*fVarHistS)[ivar]->Write();; 1421 (*fVarHistB)[ivar]->Write();; 1422 (*fVarHistS_smooth)[ivar]->Write();; 1423 (*fVarHistB_smooth)[ivar]->Write();; 1424 (*fVarPdfS)[ivar]->GetPDFHist()->Write();; 1425 (*fVarPdfB)[ivar]->GetPDFHist()->Write();; 1426 }; 1427 }; 1428}; 1429 ; 1430////////////////////////////////////////////////////////////////////////////////; 1431/// Overloaded function to create background efficiency (rejection) versus; 1432/// signal efficiency plot (first call of this function).; 1433///; 1434/// The function returns the signal efficiency at background efficiency; 1435/// indicated in theString; 1436///; 1437/// ""theString"" must have two entries:; 1438/// - `[0]`: ""Efficiency""; 1439/// - `[1]`: the value of background efficiency at which the signal efficiency; 1440/// is to be returned; 1441 ; 1442Double_t TMVA::MethodCuts::GetTrainingEfficiency(const TString& theString); 1443{; 1444 // parse input string for required background efficien",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68258,Energy Efficiency,power,power,68258,"sation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68675,Energy Efficiency,reduce,reduces,68675,"trongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within ",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:81572,Energy Efficiency,monitor,monitoring,81572,"for given cut sampleDefinition MethodCuts.cxx:1050; TMVA::MethodCuts::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constcreate XML description for LD classification and regression (for arbitrary number of output classes/t...Definition MethodCuts.cxx:1287; TMVA::MethodCuts::Initvoid Init(void)default initialisation called by all constructorsDefinition MethodCuts.cxx:220; TMVA::MethodCuts::GetTrainingEfficiencyDouble_t GetTrainingEfficiency(const TString &)Overloaded function to create background efficiency (rejection) versus signal efficiency plot (first ...Definition MethodCuts.cxx:1442; TMVA::MethodCuts::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)Cuts can only handle classification with 2 classes.Definition MethodCuts.cxx:211; TMVA::MethodCuts::ProcessOptionsvoid ProcessOptions()process user options.Definition MethodCuts.cxx:363; TMVA::MethodCuts::WriteMonitoringHistosToFilevoid WriteMonitoringHistosToFile(void) constwrite histograms and PDFs to file for monitoring purposesDefinition MethodCuts.cxx:1411; TMVA::MethodCuts::EEffMethodEEffMethodDefinition MethodCuts.h:157; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(const std::vector< Double_t > &, Double_t *, Double_t *)translates parameters into cutsDefinition MethodCuts.cxx:974; TMVA::MethodCuts::~MethodCutsvirtual ~MethodCuts(void)destructorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCut",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:431,Integrability,integrat,integrated,431,". ROOT: tmva/tmva/src/MethodCuts.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodCuts.cxx. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Matt Jachowski, Peter Speckmayer, Eckhard von Toerne, Helge Voss, Kai Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate Data analysis *; 6 * Package: TMVA *; 7 * Class : TMVA::MethodCuts *; 8 * *; 9 * *; 10 * Description: *; 11 * Implementation (see header for description) *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 16 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 17 * Eckhard von Toerne <evt@physik.uni-bonn.de> - U. of Bonn, Germany *; 18 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 19 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 20 * *; 21 * Copyright (c) 2005: *; 22 * CERN, Switzerland *; 23 * U. of Victoria, Canada *; 24 * MPI-K Heidelberg, Germany *; 25 * *; 26 * Redistribution and use in source and binary forms, with or without *; 27 * modification, are permitted according to the terms listed in LICENSE *; 28 * (see tmva/doc/LICENSE) *; 29 **********************************************************************************/; 30 ; 31/*! \class TMVA::MethodCuts; 32\ingroup TMVA; 33 ; 34 Multivariate optimisation of signal efficiency for given background; 35 efficiency, applying rectangular minimum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the si",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:2676,Integrability,depend,depending,2676,"imum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the signal significance-; 43 squared, \f$ \frac{S^2}{(S+B)} \f$, with S and B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:66615,Integrability,message,message,66615," bins until the background eff. matches the requirement; 1681 for (Int_t bini=1; bini<=nbins_; bini++) {; 1682 // get corresponding signal and background efficiencies; 1683 effS = (bini - 0.5)/Float_t(nbins_);; 1684 effB = fSpleffBvsS->Eval( effS );; 1685 ; 1686 // find signal efficiency that corresponds to required background efficiency; 1687 if ((effB - effBref)*(effB_ - effBref) < 0) break;; 1688 effS_ = effS;; 1689 effB_ = effB;; 1690 }; 1691 ; 1692 effS = 0.5*(effS + effS_);; 1693 effSerr = 0;; 1694 if (Data()->GetNEvtSigTest() > 0); 1695 effSerr = TMath::Sqrt( effS*(1.0 - effS)/Double_t(Data()->GetNEvtSigTest()) );; 1696 ; 1697 return effS;; 1698 ; 1699 }; 1700 ; 1701 return -1;; 1702}; 1703 ; 1704////////////////////////////////////////////////////////////////////////////////; 1705/// write specific classifier response; 1706 ; 1707void TMVA::MethodCuts::MakeClassSpecific( std::ostream& fout, const TString& className ) const; 1708{; 1709 fout << "" // not implemented for class: \"""" << className << ""\"""" << std::endl;; 1710 fout << ""};"" << std::endl;; 1711}; 1712 ; 1713////////////////////////////////////////////////////////////////////////////////; 1714/// get help message text; 1715///; 1716/// typical length of text line:; 1717/// ""|--------------------------------------------------------------|""; 1718 ; 1719void TMVA::MethodCuts::GetHelpMessage() const; 1720{; 1721 TString bold = gConfig().WriteOptionsReference() ? ""<b>"" : """";; 1722 TString resbold = gConfig().WriteOptionsReference() ? ""</b>"" : """";; 1723 TString brk = gConfig().WriteOptionsReference() ? ""<br>"" : """";; 1724 ; 1725 Log() << Endl;; 1726 Log() << gTools().Color(""bold"") << ""--- Short description:"" << gTools().Color(""reset"") << Endl;; 1727 Log() << Endl;; 1728 Log() << ""The optimisation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three op",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:70813,Integrability,depend,depending,70813," scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * number of variables)"" << Endl;; 1771 Log() << "" -> increase \""nsteps\"""" << Endl;; 1772 Log() << """" << Endl;; 1773 Log() << bold << ""Simulated Annealing (SA) algorithm:"" << resbold << Endl;; 1774 Log() << """" << Endl;; 1775 Log() << ""\""Increasing Adaptive\"" approach:"" << Endl;; 1776 Log() << """" << Endl;; 1777 Log() << ""The algorithm seeks local minima and explores their neighborhoods, while"" << Endl;; 1778 Log() << ""changing the ambient temperature depending on the number of failures"" << Endl;; 1779 Log() << ""in the previous steps. The performance can be improved by increasing"" << Endl;; 1780 Log() << ""the number of iteration steps (\""MaxCalls\""), or by adjusting the"" << Endl;; 1781 Log() << ""minimal temperature (\""MinTemperature\""). Manual adjustments of the"" << Endl;; 1782 Log() << ""speed of the temperature increase (\""TemperatureScale\"" and \""AdaptiveSpeed\"")"" << Endl;; 1783 Log() << ""to individual data sets should also help. Summary:"" << brk << Endl;; 1784 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1785 Log() << "" -> adjust \""MinTemperature\"""" << brk << Endl;; 178",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:77770,Integrability,interface,interface,77770,"Int_t idx) const overrideReturns the object at position idx. Returns 0 if idx is out of range.Definition TList.cxx:355; TMVA::BinarySearchTreeA simple Binary search tree including a volume search method.Definition BinarySearchTree.h:65; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::FitterBaseBase class for TMVA fitters.Definition FitterBase.h:51; TMVA::FitterBase::SetIPythonInteractivevoid SetIPythonInteractive(bool *ExitFromTraining, UInt_t *fIPyMaxIter_, UInt_t *fIPyCurrentIter_)Definition FitterBase.h:73; TMVA::FitterBase::RunDouble_t Run()estimator function interface for fittingDefinition FitterBase.cxx:74; TMVA::GeneticFitterFitter using a Genetic Algorithm.Definition GeneticFitter.h:44; TMVA::IntervalThe TMVA::Interval Class.Definition Interval.h:61; TMVA::MCFitterFitter using Monte Carlo sampling of parameters.Definition MCFitter.h:44; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::MethodCutsfriend class MethodCutsDefinition MethodBase.h:603; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::MethodCuts::ComputeEstimatorDouble_t ComputeEstimator(std::vector< Double_t > &)returns estimator for ""cut fitness"" used by GA.Definition MethodCuts.cxx:893; TMVA::MethodCuts::MakeClassSpecificvoid MakeClassSpecific(std::ostream &, const TString &) constwrite specific classifier responseDefinition MethodCuts.cxx:1707; TMVA::MethodCuts::ReadWeightsFromStreamvoid ReadWeightsFromStream(st",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:79627,Integrability,message,message,79627," const TString &) constwrite specific classifier responseDefinition MethodCuts.cxx:1707; TMVA::MethodCuts::ReadWeightsFromStreamvoid ReadWeightsFromStream(std::istream &i)read the cuts from streamDefinition MethodCuts.cxx:1217; TMVA::MethodCuts::GetEfficiencyDouble_t GetEfficiency(const TString &, Types::ETreeType, Double_t &)Overloaded function to create background efficiency (rejection) versus signal efficiency plot (first ...Definition MethodCuts.cxx:1555; TMVA::MethodCuts::EstimatorFunctionDouble_t EstimatorFunction(std::vector< Double_t > &)returns estimator for ""cut fitness"" used by GADefinition MethodCuts.cxx:878; TMVA::MethodCuts::DeclareOptionsvoid DeclareOptions()define the options (their key words) that can be set in the option string.Definition MethodCuts.cxx:319; TMVA::MethodCuts::MatchCutsToParsvoid MatchCutsToPars(std::vector< Double_t > &, Double_t *, Double_t *)translates cuts into parametersDefinition MethodCuts.cxx:1009; TMVA::MethodCuts::GetHelpMessagevoid GetHelpMessage() constget help message textDefinition MethodCuts.cxx:1719; TMVA::MethodCuts::Trainvoid Train(void)training method: here the cuts are optimised for the training sampleDefinition MethodCuts.cxx:578; TMVA::MethodCuts::fgMaxAbsCutValstatic const Double_t fgMaxAbsCutValDefinition MethodCuts.h:130; TMVA::MethodCuts::CreateVariablePDFsvoid CreateVariablePDFs(void)for PDF method: create efficiency reference histograms and PDFsDefinition MethodCuts.cxx:1106; TMVA::MethodCuts::GetMvaValueDouble_t GetMvaValue(Double_t *err=nullptr, Double_t *errUpper=nullptr)cut evaluation: returns 1.0 if event passed, 0.0 otherwiseDefinition MethodCuts.cxx:432; TMVA::MethodCuts::EFitParametersEFitParametersDefinition MethodCuts.h:161; TMVA::MethodCuts::GetEffsfromSelectionvoid GetEffsfromSelection(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from event counting for given cut sampleDefinition MethodCuts.cxx:1050; TMVA::MethodCuts::AddWeightsX",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:82900,Integrability,wrap,wrapper,82900,"ctorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCuts::GetCutsDouble_t GetCuts(Double_t effS, std::vector< Double_t > &cutMin, std::vector< Double_t > &cutMax) constretrieve cut values for given signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::MinuitFitter/Fitter using MINUITDefinition MinuitFitter.h:48; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::PDF::kSpline2@ kSpline2Definition PDF.h:70; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Results::Storevoid Store(TObject *obj, const char *alias=nullptr)Definition Results.cxx:86; TMVA::Results::GetHistTH1 * GetHist(const TString &alias) constDefinition Results.cxx:136; TMVA::SimulatedAnnealingFitterFitter using a Simulated Annealing Algorithm.Definition SimulatedAnnealingFitter.h:49; TMVA::TSpline1Linear interpolation of TGraph.Definition TSpline1.h:43; TMVA::TimerTiming information for training and evaluation of MVA methods.Definition Timer.h:58; TMVA::Timer::DrawProgressBarvoid DrawProgressBar(Int_t, const TString &comment="""")draws progress bar in color or B&W caution:Definition Timer.cxx:202; TMVA::Tools::ParseFormatLineTList * ParseFormatLine(TString theString, const char *sep="":"")Parse the string and cut into labels separated by "":"".Definition Tools.cxx:401; TMV",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:1853,Modifiability,variab,variables,1853,"* Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 17 * Eckhard von Toerne <evt@physik.uni-bonn.de> - U. of Bonn, Germany *; 18 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 19 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 20 * *; 21 * Copyright (c) 2005: *; 22 * CERN, Switzerland *; 23 * U. of Victoria, Canada *; 24 * MPI-K Heidelberg, Germany *; 25 * *; 26 * Redistribution and use in source and binary forms, with or without *; 27 * modification, are permitted according to the terms listed in LICENSE *; 28 * (see tmva/doc/LICENSE) *; 29 **********************************************************************************/; 30 ; 31/*! \class TMVA::MethodCuts; 32\ingroup TMVA; 33 ; 34 Multivariate optimisation of signal efficiency for given background; 35 efficiency, applying rectangular minimum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the signal significance-; 43 squared, \f$ \frac{S^2}{(S+B)} \f$, with S and B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 5",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:2498,Modifiability,variab,variable,2498,"hodCuts; 32\ingroup TMVA; 33 ; 34 Multivariate optimisation of signal efficiency for given background; 35 efficiency, applying rectangular minimum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the signal significance-; 43 squared, \f$ \frac{S^2}{(S+B)} \f$, with S and B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is t",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:2926,Modifiability,variab,variable,2926," space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the signal significance-; 43 squared, \f$ \frac{S^2}{(S+B)} \f$, with S and B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description for Method Likelihood for a detailed explanation.; 75*/; 76 ; 77#include ""TMVA/MethodCuts.h""; 78 ; 79#include ""TMVA/BinarySearchTree.h""; 80#include ""TMVA/ClassifierFactory.h""; 81#include ""TMVA/Config.h""; 82#include ""TMVA/Co",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:3043,Modifiability,config,configurable,3043," B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description for Method Likelihood for a detailed explanation.; 75*/; 76 ; 77#include ""TMVA/MethodCuts.h""; 78 ; 79#include ""TMVA/BinarySearchTree.h""; 80#include ""TMVA/ClassifierFactory.h""; 81#include ""TMVA/Config.h""; 82#include ""TMVA/Configurable.h""; 83#include ""TMVA/DataSet.h""; 84#include ""TMVA/DataSetInfo.h""; 85#include ""TMVA/Event.h""; 86#include ""TMVA/IFitterTarget.h""; 87#include ""TMVA/IMethod.h""; 88#include ""TMVA/Geneti",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:3147,Modifiability,variab,variables,3147,"equires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description for Method Likelihood for a detailed explanation.; 75*/; 76 ; 77#include ""TMVA/MethodCuts.h""; 78 ; 79#include ""TMVA/BinarySearchTree.h""; 80#include ""TMVA/ClassifierFactory.h""; 81#include ""TMVA/Config.h""; 82#include ""TMVA/Configurable.h""; 83#include ""TMVA/DataSet.h""; 84#include ""TMVA/DataSetInfo.h""; 85#include ""TMVA/Event.h""; 86#include ""TMVA/IFitterTarget.h""; 87#include ""TMVA/IMethod.h""; 88#include ""TMVA/GeneticFitter.h""; 89#include ""TMVA/Interval.h""; 90#include ""TMVA/FitterBase.h""; 91#include ""TMVA/MCFitter.h""; 92#include ""TMVA/MethodBase.h""; 93#include ""TMVA/MethodFDA.h""; 94#incl",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:8347,Modifiability,variab,variable,8347,"AnalysisType type, UInt_t numberClasses,; 212 UInt_t /*numberTargets*/ ); 213{; 214 return (type == Types::kClassification && numberClasses == 2);; 215}; 216 ; 217////////////////////////////////////////////////////////////////////////////////; 218/// default initialisation called by all constructors; 219 ; 220void TMVA::MethodCuts::Init( void ); 221{; 222 fVarHistS = fVarHistB = 0;; 223 fVarHistS_smooth = fVarHistB_smooth = 0;; 224 fVarPdfS = fVarPdfB = 0;; 225 fFitParams = 0;; 226 fBinaryTreeS = fBinaryTreeB = 0;; 227 fEffSMin = 0;; 228 fEffSMax = 0;; 229 ; 230 // vector with fit results; 231 fNpar = 2*GetNvar();; 232 fRangeSign = new std::vector<Int_t> ( GetNvar() );; 233 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) (*fRangeSign)[ivar] = +1;; 234 ; 235 fMeanS = new std::vector<Double_t>( GetNvar() );; 236 fMeanB = new std::vector<Double_t>( GetNvar() );; 237 fRmsS = new std::vector<Double_t>( GetNvar() );; 238 fRmsB = new std::vector<Double_t>( GetNvar() );; 239 ; 240 // get the variable specific options, first initialize default; 241 fFitParams = new std::vector<EFitParameters>( GetNvar() );; 242 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) (*fFitParams)[ivar] = kNotEnforced;; 243 ; 244 fFitMethod = kUseMonteCarlo;; 245 fTestSignalEff = -1;; 246 ; 247 // create LUT for cuts; 248 fCutMin = new Double_t*[GetNvar()];; 249 fCutMax = new Double_t*[GetNvar()];; 250 for (UInt_t i=0; i<GetNvar(); i++) {; 251 fCutMin[i] = new Double_t[fNbins];; 252 fCutMax[i] = new Double_t[fNbins];; 253 }; 254 ; 255 // init; 256 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 257 for (Int_t ibin=0; ibin<fNbins; ibin++) {; 258 fCutMin[ivar][ibin] = 0;; 259 fCutMax[ivar][ibin] = 0;; 260 }; 261 }; 262 ; 263 fTmpCutMin = new Double_t[GetNvar()];; 264 fTmpCutMax = new Double_t[GetNvar()];; 265}; 266 ; 267////////////////////////////////////////////////////////////////////////////////; 268/// destructor; 269 ; 270TMVA::MethodCuts::~MethodCuts( void ); 271{; 272 delete fRangeSign;; 273 delete fMea",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:10720,Modifiability,variab,variable,10720,"i] ) delete [] fCutMin[i];; 286 if (NULL != fCutMax[i] ) delete [] fCutMax[i];; 287 if (NULL != fCutRange[i]) delete fCutRange[i];; 288 }; 289 ; 290 if (NULL != fCutMin) delete [] fCutMin;; 291 if (NULL != fCutMax) delete [] fCutMax;; 292 ; 293 if (NULL != fTmpCutMin) delete [] fTmpCutMin;; 294 if (NULL != fTmpCutMax) delete [] fTmpCutMax;; 295 ; 296 if (NULL != fBinaryTreeS) delete fBinaryTreeS;; 297 if (NULL != fBinaryTreeB) delete fBinaryTreeB;; 298}; 299 ; 300////////////////////////////////////////////////////////////////////////////////; 301/// define the options (their key words) that can be set in the option string.; 302///; 303/// know options:; 304/// - Method `<string>` Minimisation method. Available values are:; 305/// - MC Monte Carlo `<default>`; 306/// - GA Genetic Algorithm; 307/// - SA Simulated annealing; 308///; 309/// - EffMethod `<string>` Efficiency selection method. Available values are:; 310/// - EffSel `<default>`; 311/// - EffPDF; 312///; 313/// - VarProp `<string>` Property of variable 1 for the MC method (taking precedence over the; 314/// globale setting. The same values as for the global option are available. Variables 1..10 can be; 315/// set this way; 316///; 317/// - CutRangeMin/Max `<float>` user-defined ranges in which cuts are varied; 318 ; 319void TMVA::MethodCuts::DeclareOptions(); 320{; 321 DeclareOptionRef(fFitMethodS = ""GA"", ""FitMethod"", ""Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated)"");; 322 AddPreDefVal(TString(""GA""));; 323 AddPreDefVal(TString(""SA""));; 324 AddPreDefVal(TString(""MC""));; 325 AddPreDefVal(TString(""MCEvents""));; 326 AddPreDefVal(TString(""MINUIT""));; 327 AddPreDefVal(TString(""EventScan""));; 328 ; 329 // selection type; 330 DeclareOptionRef(fEffMethodS = ""EffSel"", ""EffMethod"", ""Selection Method"");; 331 AddPreDefVal(TString(""EffSel""));; 332 AddPreDefVal(TString(""EffPDF""));; 333 ; 334 // cut ranges; 335 fCutRange.resi",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:12050,Modifiability,variab,variable,12050,"s have been introduced for testing purposes and are depreciated)"");; 322 AddPreDefVal(TString(""GA""));; 323 AddPreDefVal(TString(""SA""));; 324 AddPreDefVal(TString(""MC""));; 325 AddPreDefVal(TString(""MCEvents""));; 326 AddPreDefVal(TString(""MINUIT""));; 327 AddPreDefVal(TString(""EventScan""));; 328 ; 329 // selection type; 330 DeclareOptionRef(fEffMethodS = ""EffSel"", ""EffMethod"", ""Selection Method"");; 331 AddPreDefVal(TString(""EffSel""));; 332 AddPreDefVal(TString(""EffPDF""));; 333 ; 334 // cut ranges; 335 fCutRange.resize(GetNvar());; 336 fCutRangeMin = new Double_t[GetNvar()];; 337 fCutRangeMax = new Double_t[GetNvar()];; 338 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 339 fCutRange[ivar] = 0;; 340 fCutRangeMin[ivar] = fCutRangeMax[ivar] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm.",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:12166,Modifiability,variab,variable,12166,"s have been introduced for testing purposes and are depreciated)"");; 322 AddPreDefVal(TString(""GA""));; 323 AddPreDefVal(TString(""SA""));; 324 AddPreDefVal(TString(""MC""));; 325 AddPreDefVal(TString(""MCEvents""));; 326 AddPreDefVal(TString(""MINUIT""));; 327 AddPreDefVal(TString(""EventScan""));; 328 ; 329 // selection type; 330 DeclareOptionRef(fEffMethodS = ""EffSel"", ""EffMethod"", ""Selection Method"");; 331 AddPreDefVal(TString(""EffSel""));; 332 AddPreDefVal(TString(""EffPDF""));; 333 ; 334 // cut ranges; 335 fCutRange.resize(GetNvar());; 336 fCutRangeMin = new Double_t[GetNvar()];; 337 fCutRangeMax = new Double_t[GetNvar()];; 338 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 339 fCutRange[ivar] = 0;; 340 fCutRangeMin[ivar] = fCutRangeMax[ivar] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm.",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:12724,Modifiability,variab,variables,12724,"] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm."" << Endl;; 369 Log() << kWARNING << ""Please remove \""Normalise\"" option from booking option string"" << Endl;; 370 Log() << kWARNING << ""==> Will reset normalisation flag to \""False\"""" << Endl;; 371 SetNormalised( kFALSE );; 372 }; 373 ; 374 if (IgnoreEventsWithNegWeightsInTraining()) {; 375 Log() << kFATAL << ""Mechanism to ignore events with negative weights in training not yet available for method: ""; 376 << GetMethodTypeName(); 377 << "" --> Please remove \""IgnoreNegWeightsInTraining\"" option from booking string.""; 378 << Endl;; 379 }; 380 ; 381 if (fFitMethodS == ""MC"" ) fFitMethod = kUseMonteCarlo;; 382 else if (fFitMethodS == ""MCEvents"") fFitMethod = kUseMonteCarloEvents;; 383 else if (fFitMethodS == ""GA"" ) fFitMethod = kUseGenet",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:12959,Modifiability,variab,variables,12959,"] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm."" << Endl;; 369 Log() << kWARNING << ""Please remove \""Normalise\"" option from booking option string"" << Endl;; 370 Log() << kWARNING << ""==> Will reset normalisation flag to \""False\"""" << Endl;; 371 SetNormalised( kFALSE );; 372 }; 373 ; 374 if (IgnoreEventsWithNegWeightsInTraining()) {; 375 Log() << kFATAL << ""Mechanism to ignore events with negative weights in training not yet available for method: ""; 376 << GetMethodTypeName(); 377 << "" --> Please remove \""IgnoreNegWeightsInTraining\"" option from booking string.""; 378 << Endl;; 379 }; 380 ; 381 if (fFitMethodS == ""MC"" ) fFitMethod = kUseMonteCarlo;; 382 else if (fFitMethodS == ""MCEvents"") fFitMethod = kUseMonteCarloEvents;; 383 else if (fFitMethodS == ""GA"" ) fFitMethod = kUseGenet",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:15941,Modifiability,variab,variable,15941,"IT in MethodCuts; preferred fit method: GA"" << Endl;; 388 }; 389 else if (fFitMethodS == ""EventScan"" ) fFitMethod = kUseEventScan;; 390 else Log() << kFATAL << ""unknown minimisation method: "" << fFitMethodS << Endl;; 391 ; 392 if (fEffMethodS == ""EFFSEL"" ) fEffMethod = kUseEventSelection; // highly recommended; 393 else if (fEffMethodS == ""EFFPDF"" ) fEffMethod = kUsePDFs;; 394 else fEffMethod = kUseEventSelection;; 395 ; 396 // options output; 397 Log() << kINFO << Form(""Use optimization method: \""%s\"""",; 398 (fFitMethod == kUseMonteCarlo) ? ""Monte Carlo"" :; 399 (fFitMethod == kUseMonteCarlo) ? ""Monte-Carlo-Event sampling"" :; 400 (fFitMethod == kUseEventScan) ? ""Full Event Scan (slow)"" :; 401 (fFitMethod == kUseMinuit) ? ""MINUIT"" : ""Genetic Algorithm"" ) << Endl;; 402 Log() << kINFO << Form(""Use efficiency computation method: \""%s\"""",; 403 (fEffMethod == kUseEventSelection) ? ""Event Selection"" : ""PDF"" ) << Endl;; 404 ; 405 // cut ranges; 406 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 407 fCutRange[ivar] = new Interval( fCutRangeMin[ivar], fCutRangeMax[ivar] );; 408 }; 409 ; 410 // individual options; 411 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 412 EFitParameters theFitP = kNotEnforced;; 413 if (fAllVarsI[ivar] == """" || fAllVarsI[ivar] == ""NotEnforced"") theFitP = kNotEnforced;; 414 else if (fAllVarsI[ivar] == ""FMax"" ) theFitP = kForceMax;; 415 else if (fAllVarsI[ivar] == ""FMin"" ) theFitP = kForceMin;; 416 else if (fAllVarsI[ivar] == ""FSmart"" ) theFitP = kForceSmart;; 417 else {; 418 Log() << kFATAL << ""unknown value \'"" << fAllVarsI[ivar]; 419 << ""\' for fit parameter option "" << Form(""VarProp[%i]"",ivar) << Endl;; 420 }; 421 (*fFitParams)[ivar] = theFitP;; 422 ; 423 if (theFitP != kNotEnforced); 424 Log() << kINFO << ""Use \"""" << fAllVarsI[ivar]; 425 << ""\"" cuts for variable: "" << ""'"" << (*fInputVars)[ivar] << ""'"" << Endl;; 426 }; 427}; 428 ; 429////////////////////////////////////////////////////////////////////////////////; 430/// cut evaluation: returns 1.",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:17457,Modifiability,variab,variable,17457,"rCalc(err, errUpper);; 436 ; 437 // sanity check; 438 if (fCutMin == NULL || fCutMax == NULL || fNbins == 0) {; 439 Log() << kFATAL << ""<Eval_Cuts> fCutMin/Max have zero pointer. ""; 440 << ""Did you book Cuts ?"" << Endl;; 441 }; 442 ; 443 const Event* ev = GetEvent();; 444 ; 445 // sanity check; 446 if (fTestSignalEff > 0) {; 447 // get efficiency bin; 448 Int_t ibin = fEffBvsSLocal->FindBin( fTestSignalEff );; 449 if (ibin < 0 ) ibin = 0;; 450 else if (ibin >= fNbins) ibin = fNbins - 1;; 451 ; 452 Bool_t passed = kTRUE;; 453 for (UInt_t ivar=0; ivar<GetNvar(); ivar++); 454 passed &= ( (ev->GetValue(ivar) > fCutMin[ivar][ibin]) &&; 455 (ev->GetValue(ivar) <= fCutMax[ivar][ibin]) );; 456 ; 457 return passed ? 1. : 0. ;; 458 }; 459 else return 0;; 460}; 461 ; 462////////////////////////////////////////////////////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 469 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 470 ; 471 Double_t trueEffS = GetCuts( effS, cutsMin, cutsMax );; 472 ; 473 // retrieve variable expressions (could be transformations); 474 std::vector<TString>* varVec = 0;; 475 if (GetTransformationHandler().GetNumOfTransformations() == 0) {; 476 // no transformation applied, replace by current variables; 477 varVec = new std::vector<TString>;; 478 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 479 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() );; 480 }; 481 }; 482 else if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 483 // get transformation string; 484 varVec = GetTransformationHandler().GetTransformationStringsOfLastTransform();; 485 }; 486 else {; 487 // replace transformation print by current variables and indicated incompleteness; 488 varVec = new std::vector<TString>;; 489 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 490 varVec->push_back( DataInfo().GetVariableInfo",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:17668,Modifiability,variab,variables,17668,"ibin = fEffBvsSLocal->FindBin( fTestSignalEff );; 449 if (ibin < 0 ) ibin = 0;; 450 else if (ibin >= fNbins) ibin = fNbins - 1;; 451 ; 452 Bool_t passed = kTRUE;; 453 for (UInt_t ivar=0; ivar<GetNvar(); ivar++); 454 passed &= ( (ev->GetValue(ivar) > fCutMin[ivar][ibin]) &&; 455 (ev->GetValue(ivar) <= fCutMax[ivar][ibin]) );; 456 ; 457 return passed ? 1. : 0. ;; 458 }; 459 else return 0;; 460}; 461 ; 462////////////////////////////////////////////////////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 469 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 470 ; 471 Double_t trueEffS = GetCuts( effS, cutsMin, cutsMax );; 472 ; 473 // retrieve variable expressions (could be transformations); 474 std::vector<TString>* varVec = 0;; 475 if (GetTransformationHandler().GetNumOfTransformations() == 0) {; 476 // no transformation applied, replace by current variables; 477 varVec = new std::vector<TString>;; 478 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 479 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() );; 480 }; 481 }; 482 else if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 483 // get transformation string; 484 varVec = GetTransformationHandler().GetTransformationStringsOfLastTransform();; 485 }; 486 else {; 487 // replace transformation print by current variables and indicated incompleteness; 488 varVec = new std::vector<TString>;; 489 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 490 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() + "" [transformed]"" );; 491 }; 492 }; 493 ; 494 UInt_t maxL = 0;; 495 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 496 if ((UInt_t)(*varVec)[ivar].Length() > maxL) maxL = (*varVec)[ivar].Length();; 497 }; 498 UInt_t maxLine = 20+maxL+16;; 499 ; 500 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 501 Log() << Endl;; 502 Log() << kHEADER <<",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:18119,Modifiability,variab,variables,18119,"////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 469 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 470 ; 471 Double_t trueEffS = GetCuts( effS, cutsMin, cutsMax );; 472 ; 473 // retrieve variable expressions (could be transformations); 474 std::vector<TString>* varVec = 0;; 475 if (GetTransformationHandler().GetNumOfTransformations() == 0) {; 476 // no transformation applied, replace by current variables; 477 varVec = new std::vector<TString>;; 478 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 479 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() );; 480 }; 481 }; 482 else if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 483 // get transformation string; 484 varVec = GetTransformationHandler().GetTransformationStringsOfLastTransform();; 485 }; 486 else {; 487 // replace transformation print by current variables and indicated incompleteness; 488 varVec = new std::vector<TString>;; 489 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 490 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() + "" [transformed]"" );; 491 }; 492 }; 493 ; 494 UInt_t maxL = 0;; 495 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 496 if ((UInt_t)(*varVec)[ivar].Length() > maxL) maxL = (*varVec)[ivar].Length();; 497 }; 498 UInt_t maxLine = 20+maxL+16;; 499 ; 500 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 501 Log() << Endl;; 502 Log() << kHEADER << ""Cut values for requested signal efficiency: "" << trueEffS << Endl;; 503 Log() << kINFO << ""Corresponding background efficiency : "" << fEffBvsSLocal->GetBinContent( ibin ) << Endl;; 504 if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 505 Log() << kINFO << ""Transformation applied to input variables : \""""; 506 << GetTransformationHandler().GetNameOfLastTransform() << ""\"""" << Endl;; 507 }; 508 else if (GetTransformationHandler().GetNum",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:18976,Modifiability,variab,variables,18976,"tion string; 484 varVec = GetTransformationHandler().GetTransformationStringsOfLastTransform();; 485 }; 486 else {; 487 // replace transformation print by current variables and indicated incompleteness; 488 varVec = new std::vector<TString>;; 489 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 490 varVec->push_back( DataInfo().GetVariableInfo(ivar).GetLabel() + "" [transformed]"" );; 491 }; 492 }; 493 ; 494 UInt_t maxL = 0;; 495 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 496 if ((UInt_t)(*varVec)[ivar].Length() > maxL) maxL = (*varVec)[ivar].Length();; 497 }; 498 UInt_t maxLine = 20+maxL+16;; 499 ; 500 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 501 Log() << Endl;; 502 Log() << kHEADER << ""Cut values for requested signal efficiency: "" << trueEffS << Endl;; 503 Log() << kINFO << ""Corresponding background efficiency : "" << fEffBvsSLocal->GetBinContent( ibin ) << Endl;; 504 if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 505 Log() << kINFO << ""Transformation applied to input variables : \""""; 506 << GetTransformationHandler().GetNameOfLastTransform() << ""\"""" << Endl;; 507 }; 508 else if (GetTransformationHandler().GetNumOfTransformations() > 1) {; 509 Log() << kINFO << ""[ More than one (="" << GetTransformationHandler().GetNumOfTransformations() << "") ""; 510 << "" transformations applied in transformation chain; cuts applied on transformed quantities ] "" << Endl;; 511 }; 512 else {; 513 Log() << kINFO << ""Transformation applied to input variables : None"" << Endl;; 514 }; 515 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 516 Log() << Endl;; 517 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 518 Log() << kINFO; 519 << ""Cut["" << std::setw(2) << ivar << ""]: ""; 520 << std::setw(10) << cutsMin[ivar]; 521 << "" < ""; 522 << std::setw(maxL) << (*varVec)[ivar]; 523 << "" <= ""; 524 << std::setw(10) << cutsMax[ivar] << Endl;; 525 }; 526 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 527 Log() << Endl;; 528 ; 529 delete varVec; // yes, ownership h",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:19444,Modifiability,variab,variables,19444,"r=0; ivar<cutsMin.size(); ivar++) {; 496 if ((UInt_t)(*varVec)[ivar].Length() > maxL) maxL = (*varVec)[ivar].Length();; 497 }; 498 UInt_t maxLine = 20+maxL+16;; 499 ; 500 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 501 Log() << Endl;; 502 Log() << kHEADER << ""Cut values for requested signal efficiency: "" << trueEffS << Endl;; 503 Log() << kINFO << ""Corresponding background efficiency : "" << fEffBvsSLocal->GetBinContent( ibin ) << Endl;; 504 if (GetTransformationHandler().GetNumOfTransformations() == 1) {; 505 Log() << kINFO << ""Transformation applied to input variables : \""""; 506 << GetTransformationHandler().GetNameOfLastTransform() << ""\"""" << Endl;; 507 }; 508 else if (GetTransformationHandler().GetNumOfTransformations() > 1) {; 509 Log() << kINFO << ""[ More than one (="" << GetTransformationHandler().GetNumOfTransformations() << "") ""; 510 << "" transformations applied in transformation chain; cuts applied on transformed quantities ] "" << Endl;; 511 }; 512 else {; 513 Log() << kINFO << ""Transformation applied to input variables : None"" << Endl;; 514 }; 515 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 516 Log() << Endl;; 517 for (UInt_t ivar=0; ivar<cutsMin.size(); ivar++) {; 518 Log() << kINFO; 519 << ""Cut["" << std::setw(2) << ivar << ""]: ""; 520 << std::setw(10) << cutsMin[ivar]; 521 << "" < ""; 522 << std::setw(maxL) << (*varVec)[ivar]; 523 << "" <= ""; 524 << std::setw(10) << cutsMax[ivar] << Endl;; 525 }; 526 for (UInt_t i=0; i<maxLine; i++) Log() << ""-"";; 527 Log() << Endl;; 528 ; 529 delete varVec; // yes, ownership has been given to us; 530}; 531 ; 532////////////////////////////////////////////////////////////////////////////////; 533/// retrieve cut values for given signal efficiency; 534/// assume vector of correct size !!; 535 ; 536Double_t TMVA::MethodCuts::GetCuts( Double_t effS, Double_t* cutMin, Double_t* cutMax ) const; 537{; 538 std::vector<Double_t> cMin( GetNvar() );; 539 std::vector<Double_t> cMax( GetNvar() );; 540 Double_t trueEffS = GetCuts(",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:21772,Modifiability,variab,variables,21772,"ar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree();; 591 fBinaryTreeS->Fill( GetEventCollection(Types::kTraining), fSignalClass );; 592 fBinaryTreeB = new BinarySearchTree();; 593 fBinaryTreeB->Fill( GetEventCollection(Types::kTraining), fBackgroundClass );; 594 ; 595 for (UInt_t ivar =0; ivar < Data()->GetNVariables(); ivar++) {; 596 (*fMeanS)[ivar] = fBinaryTreeS->Mean(Types::kSignal, ivar);; 597 (*fRmsS)[ivar] = fBinaryTreeS->RMS (Types::kSignal, ivar);; 598 (*fMeanB)[ivar] = fBinaryTreeB->Mean(Types::kBackground, ivar);; 599 (*fRmsB)[ivar] = fBinaryTreeB->RMS (Types::kBackground, ivar);; 600 ; 601 // update interval ?; 602 Double_t xmin = TMath::Min(fBinaryTreeS->Min(Types::kSignal, ivar),; 603 fBinaryTreeB->Min(Types::kBackground, ivar));; 604 Double_t xmax = TMath::Max(fBinaryTreeS->Max(Types::kSignal, ivar),; 605 fBinaryTreeB->Max(Types::kBackground, ivar));; 606 ; 607 // redefine ranges to be slightly smaller and larger than xmin and xmax, respectively; 608 Double_t eps = 0.01*(xmax - xmin);; 609 xmin -= eps;; 610 xmax += eps;; 611 ; 612 if (TMath::Abs(fCutRange[",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:21831,Modifiability,variab,variables,21831,"ar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree();; 591 fBinaryTreeS->Fill( GetEventCollection(Types::kTraining), fSignalClass );; 592 fBinaryTreeB = new BinarySearchTree();; 593 fBinaryTreeB->Fill( GetEventCollection(Types::kTraining), fBackgroundClass );; 594 ; 595 for (UInt_t ivar =0; ivar < Data()->GetNVariables(); ivar++) {; 596 (*fMeanS)[ivar] = fBinaryTreeS->Mean(Types::kSignal, ivar);; 597 (*fRmsS)[ivar] = fBinaryTreeS->RMS (Types::kSignal, ivar);; 598 (*fMeanB)[ivar] = fBinaryTreeB->Mean(Types::kBackground, ivar);; 599 (*fRmsB)[ivar] = fBinaryTreeB->RMS (Types::kBackground, ivar);; 600 ; 601 // update interval ?; 602 Double_t xmin = TMath::Min(fBinaryTreeS->Min(Types::kSignal, ivar),; 603 fBinaryTreeB->Min(Types::kBackground, ivar));; 604 Double_t xmax = TMath::Max(fBinaryTreeS->Max(Types::kSignal, ivar),; 605 fBinaryTreeB->Max(Types::kBackground, ivar));; 606 ; 607 // redefine ranges to be slightly smaller and larger than xmin and xmax, respectively; 608 Double_t eps = 0.01*(xmax - xmin);; 609 xmin -= eps;; 610 xmax += eps;; 611 ; 612 if (TMath::Abs(fCutRange[",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:22028,Modifiability,variab,variables,22028,"ar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree();; 591 fBinaryTreeS->Fill( GetEventCollection(Types::kTraining), fSignalClass );; 592 fBinaryTreeB = new BinarySearchTree();; 593 fBinaryTreeB->Fill( GetEventCollection(Types::kTraining), fBackgroundClass );; 594 ; 595 for (UInt_t ivar =0; ivar < Data()->GetNVariables(); ivar++) {; 596 (*fMeanS)[ivar] = fBinaryTreeS->Mean(Types::kSignal, ivar);; 597 (*fRmsS)[ivar] = fBinaryTreeS->RMS (Types::kSignal, ivar);; 598 (*fMeanB)[ivar] = fBinaryTreeB->Mean(Types::kBackground, ivar);; 599 (*fRmsB)[ivar] = fBinaryTreeB->RMS (Types::kBackground, ivar);; 600 ; 601 // update interval ?; 602 Double_t xmin = TMath::Min(fBinaryTreeS->Min(Types::kSignal, ivar),; 603 fBinaryTreeB->Min(Types::kBackground, ivar));; 604 Double_t xmax = TMath::Max(fBinaryTreeS->Max(Types::kSignal, ivar),; 605 fBinaryTreeB->Max(Types::kBackground, ivar));; 606 ; 607 // redefine ranges to be slightly smaller and larger than xmin and xmax, respectively; 608 Double_t eps = 0.01*(xmax - xmin);; 609 xmin -= eps;; 610 xmax += eps;; 611 ; 612 if (TMath::Abs(fCutRange[",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:28571,Modifiability,variab,variable,28571," fIPyMaxIter = nsamples;; 709 ; 710 Log() << kINFO << ""Running full event scan: "" << Endl;; 711 for (Int_t ievt1=0; ievt1<nevents; ievt1++) {; 712 for (Int_t ievt2=ievt1+1; ievt2<nevents; ievt2++) {; 713 ; 714 fIPyCurrentIter = ic;; 715 if (fExitFromTraining) break;; 716 EstimatorFunction( ievt1, ievt2 );; 717 ; 718 // what's the time please?; 719 ic++;; 720 if ((nsamples<10000) || ic%10000 == 0) timer.DrawProgressBar( ic );; 721 }; 722 }; 723 }; 724 // --------------------------------------------------------------------------; 725 else if (fFitMethod == kUseMonteCarloEvents) {; 726 ; 727 Int_t nsamples = 200000;; 728 UInt_t seed = 100;; 729 DeclareOptionRef( nsamples, ""SampleSize"", ""Number of Monte-Carlo-Event samples"" );; 730 DeclareOptionRef( seed, ""Seed"", ""Seed for the random generator (0 takes random seeds)"" );; 731 ParseOptions();; 732 ; 733 Int_t nevents = Data()->GetNEvents();; 734 Int_t ic = 0;; 735 ; 736 // timing of MC; 737 Timer timer( nsamples, GetName() );; 738 fIPyMaxIter = nsamples;; 739 ; 740 // random generator; 741 TRandom3*rnd = new TRandom3( seed );; 742 ; 743 Log() << kINFO << ""Running Monte-Carlo-Event sampling over "" << nsamples << "" events"" << Endl;; 744 std::vector<Double_t> pars( 2*GetNvar() );; 745 ; 746 for (Int_t itoy=0; itoy<nsamples; itoy++) {; 747 fIPyCurrentIter = ic;; 748 if (fExitFromTraining) break;; 749 ; 750 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 751 ; 752 // generate minimum and delta cuts for this variable; 753 ; 754 // retrieve signal events; 755 Bool_t isSignal = kFALSE;; 756 Int_t ievt1, ievt2;; 757 Double_t evt1 = 0., evt2 = 0.;; 758 Int_t nbreak = 0;; 759 while (!isSignal) {; 760 ievt1 = Int_t(rnd->Uniform(0.,1.)*nevents);; 761 ievt2 = Int_t(rnd->Uniform(0.,1.)*nevents);; 762 ; 763 const Event *ev1 = GetEvent(ievt1);; 764 isSignal = DataInfo().IsSignal(ev1);; 765 evt1 = ev1->GetValue( ivar );; 766 ; 767 const Event *ev2 = GetEvent(ievt2);; 768 isSignal &= DataInfo().IsSignal(ev2);; 769 evt2 = ev2->GetValue( ivar )",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:35386,Modifiability,variab,variables,35386,"0 ; 931 // if the average of the bin right and left is larger than this one, add the difference to; 932 // the current value of the estimator (because you can do at least so much better); 933 eta = ( -TMath::Abs(effBH-average) + (1.0 - (effBH - effB))) / (1.0 + effS);; 934 // alternative idea; 935 //if (effBH<0) eta = (1.e-6+effB)/(1.0 + effS);; 936 //else eta = (effB - effBH) * (1.0 + 10.* effS);; 937 ; 938 // if a point is found which is better than an existing one, ... replace it.; 939 // preliminary best event -> backup; 940 if (effBH < 0 || effBH > effB) {; 941 fEffBvsSLocal->SetBinContent( ibinS, effB );; 942 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 943 fCutMin[ivar][ibinS-1] = fTmpCutMin[ivar]; // bin 1 stored in index 0; 944 fCutMax[ivar][ibinS-1] = fTmpCutMax[ivar];; 945 }; 946 }; 947 ; 948 // caution (!) this value is not good for a decision for MC, .. it is designed for GA; 949 // but .. it doesn't matter, as MC samplings are independent from the former ones; 950 // and the replacement of the best variables by better ones is done about 10 lines above.; 951 // ( if (effBH < 0 || effBH > effB) { .... ); 952 ; 953 if (ibinS<=1) {; 954 // add penalty for effS=0 bin; 955 // to avoid that the minimizer gets stuck in the zero-bin; 956 // force it towards higher efficiency; 957 Double_t penalty=0.,diff=0.;; 958 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 959 diff=(fCutRange[ivar]->GetMax()-fTmpCutMax[ivar])/(fCutRange[ivar]->GetMax()-fCutRange[ivar]->GetMin());; 960 penalty+=diff*diff;; 961 diff=(fCutRange[ivar]->GetMin()-fTmpCutMin[ivar])/(fCutRange[ivar]->GetMax()-fCutRange[ivar]->GetMin());; 962 penalty+=4.*diff*diff;; 963 }; 964 ; 965 if (effS<1.e-4) return 10.0+penalty;; 966 else return 10.*(1.-10.*effS);; 967 }; 968 return eta;; 969}; 970 ; 971////////////////////////////////////////////////////////////////////////////////; 972/// translates parameters into cuts; 973 ; 974void TMVA::MethodCuts::MatchParsToCuts( const std::vector<Double_t> & pars,; 9",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:46384,Modifiability,variab,variables,46384,"ar Sig "" + GetInputVar( ivar ), (*fVarHistS_smooth)[ivar], PDF::kSpline2 );; 1210 (*fVarPdfB)[ivar] = new PDF( TString(GetName()) + "" PDF Var Bkg "" + GetInputVar( ivar ), (*fVarHistB_smooth)[ivar], PDF::kSpline2 );; 1211 }; 1212}; 1213 ; 1214////////////////////////////////////////////////////////////////////////////////; 1215/// read the cuts from stream; 1216 ; 1217void TMVA::MethodCuts::ReadWeightsFromStream( std::istream& istr ); 1218{; 1219 TString dummy;; 1220 UInt_t dummyInt;; 1221 ; 1222 // first the dimensions; 1223 istr >> dummy >> dummy;; 1224 // coverity[tainted_data_argument]; 1225 istr >> dummy >> fNbins;; 1226 ; 1227 // get rid of one read-in here because we read in once all ready to check for decorrelation; 1228 istr >> dummy >> dummy >> dummy >> dummy >> dummy >> dummy >> dummyInt >> dummy ;; 1229 ; 1230 // sanity check; 1231 if (dummyInt != Data()->GetNVariables()) {; 1232 Log() << kFATAL << ""<ReadWeightsFromStream> fatal error: mismatch ""; 1233 << ""in number of variables: "" << dummyInt << "" != "" << Data()->GetNVariables() << Endl;; 1234 }; 1235 //SetNvar(dummyInt);; 1236 ; 1237 // print some information; 1238 if (fFitMethod == kUseMonteCarlo) {; 1239 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1240 }; 1241 else if (fFitMethod == kUseMonteCarloEvents) {; 1242 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1243 }; 1244 else if (fFitMethod == kUseGeneticAlgorithm) {; 1245 Log() << kINFO << ""Read cuts optimised using Genetic Algorithm"" << Endl;; 1246 }; 1247 else if (fFitMethod == kUseSimulatedAnnealing) {; 1248 Log() << kINFO << ""Read cuts optimised using Simulated Annealing algorithm"" << Endl;; 1249 }; 1250 else if (fFitMethod == kUseEventScan) {; 1251 Log() << kINFO << ""Read cuts optimised using Full Event Scan"" << Endl;; 1252 }; 1253 else {; 1254 Log() << kWARNING << ""unknown method: "" << fFitMethod << Endl;; 1255 }; 1256 Log() << kINFO << ""in "" << fNbins << "" signal efficiency bin",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:47419,Modifiability,variab,variables,47419,"(GetName()) + "" PDF Var Bkg "" + GetInputVar( ivar ), (*fVarHistB_smooth)[ivar], PDF::kSpline2 );; 1211 }; 1212}; 1213 ; 1214////////////////////////////////////////////////////////////////////////////////; 1215/// read the cuts from stream; 1216 ; 1217void TMVA::MethodCuts::ReadWeightsFromStream( std::istream& istr ); 1218{; 1219 TString dummy;; 1220 UInt_t dummyInt;; 1221 ; 1222 // first the dimensions; 1223 istr >> dummy >> dummy;; 1224 // coverity[tainted_data_argument]; 1225 istr >> dummy >> fNbins;; 1226 ; 1227 // get rid of one read-in here because we read in once all ready to check for decorrelation; 1228 istr >> dummy >> dummy >> dummy >> dummy >> dummy >> dummy >> dummyInt >> dummy ;; 1229 ; 1230 // sanity check; 1231 if (dummyInt != Data()->GetNVariables()) {; 1232 Log() << kFATAL << ""<ReadWeightsFromStream> fatal error: mismatch ""; 1233 << ""in number of variables: "" << dummyInt << "" != "" << Data()->GetNVariables() << Endl;; 1234 }; 1235 //SetNvar(dummyInt);; 1236 ; 1237 // print some information; 1238 if (fFitMethod == kUseMonteCarlo) {; 1239 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1240 }; 1241 else if (fFitMethod == kUseMonteCarloEvents) {; 1242 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1243 }; 1244 else if (fFitMethod == kUseGeneticAlgorithm) {; 1245 Log() << kINFO << ""Read cuts optimised using Genetic Algorithm"" << Endl;; 1246 }; 1247 else if (fFitMethod == kUseSimulatedAnnealing) {; 1248 Log() << kINFO << ""Read cuts optimised using Simulated Annealing algorithm"" << Endl;; 1249 }; 1250 else if (fFitMethod == kUseEventScan) {; 1251 Log() << kINFO << ""Read cuts optimised using Full Event Scan"" << Endl;; 1252 }; 1253 else {; 1254 Log() << kWARNING << ""unknown method: "" << fFitMethod << Endl;; 1255 }; 1256 Log() << kINFO << ""in "" << fNbins << "" signal efficiency bins and for "" << GetNvar() << "" variables"" << Endl;; 1257 ; 1258 // now read the cuts; 1259 char buffer[200];; 1260 istr.",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:49039,Modifiability,variab,variables,49039,"or (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1275 istr >> fCutMin[ivar][ibin] >> fCutMax[ivar][ibin];; 1276 }; 1277 }; 1278 ; 1279 fEffSMin = fEffBvsSLocal->GetBinCenter(1);; 1280 fEffSMax = fEffBvsSLocal->GetBinCenter(fNbins);; 1281}; 1282 ; 1283////////////////////////////////////////////////////////////////////////////////; 1284/// create XML description for LD classification and regression; 1285/// (for arbitrary number of output classes/targets); 1286 ; 1287void TMVA::MethodCuts::AddWeightsXMLTo( void* parent ) const; 1288{; 1289 // write all necessary information to the stream; 1290 std::vector<Double_t> cutsMin;; 1291 std::vector<Double_t> cutsMax;; 1292 ; 1293 void* wght = gTools().AddChild(parent, ""Weights"");; 1294 gTools().AddAttr( wght, ""OptimisationMethod"", (Int_t)fEffMethod);; 1295 gTools().AddAttr( wght, ""FitMethod"", (Int_t)fFitMethod );; 1296 gTools().AddAttr( wght, ""nbins"", fNbins );; 1297 gTools().AddComment( wght, TString::Format( ""Below are the optimised cuts for %i variables: Format: ibin(hist) effS effB cutMin[ivar=0] cutMax[ivar=0] ... cutMin[ivar=n-1] cutMax[ivar=n-1]"", GetNvar() ) );; 1298 ; 1299 // NOTE: The signal efficiency written out into; 1300 // the weight file does not correspond to the center of the bin within which the; 1301 // background rejection is maximised (as before) but to the lower left edge of it.; 1302 // This is because the cut optimisation algorithm determines the best background; 1303 // rejection for all signal efficiencies belonging into a bin. Since the best background; 1304 // rejection is in general obtained for the lowest possible signal efficiency, the; 1305 // reference signal efficiency is the lowest value in the bin.; 1306 ; 1307 for (Int_t ibin=0; ibin<fNbins; ibin++) {; 1308 Double_t effS = fEffBvsSLocal->GetBinCenter ( ibin + 1 );; 1309 Double_t trueEffS = GetCuts( effS, cutsMin, cutsMax );; 1310 if (TMath::Abs(trueEffS) < 1e-10) trueEffS = 0;; 1311 ; 1312 void* binxml = gTools().AddChild( wght, ""Bin"" );; 1",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:52289,Modifiability,variab,variables,52289,"i] != 0) delete [] fCutMin[i];; 1332 if (fCutMax[i] != 0) delete [] fCutMax[i];; 1333 }; 1334 if (fCutMin != 0) delete [] fCutMin;; 1335 if (fCutMax != 0) delete [] fCutMax;; 1336 ; 1337 Int_t tmpEffMethod, tmpFitMethod;; 1338 gTools().ReadAttr( wghtnode, ""OptimisationMethod"", tmpEffMethod );; 1339 gTools().ReadAttr( wghtnode, ""FitMethod"", tmpFitMethod );; 1340 gTools().ReadAttr( wghtnode, ""nbins"", fNbins );; 1341 ; 1342 fEffMethod = (EEffMethod)tmpEffMethod;; 1343 fFitMethod = (EFitMethodType)tmpFitMethod;; 1344 ; 1345 // print some information; 1346 if (fFitMethod == kUseMonteCarlo) {; 1347 Log() << kINFO << ""Read cuts optimised using sample of MC events"" << Endl;; 1348 }; 1349 else if (fFitMethod == kUseMonteCarloEvents) {; 1350 Log() << kINFO << ""Read cuts optimised using sample of MC-Event events"" << Endl;; 1351 }; 1352 else if (fFitMethod == kUseGeneticAlgorithm) {; 1353 Log() << kINFO << ""Read cuts optimised using Genetic Algorithm"" << Endl;; 1354 }; 1355 else if (fFitMethod == kUseSimulatedAnnealing) {; 1356 Log() << kINFO << ""Read cuts optimised using Simulated Annealing algorithm"" << Endl;; 1357 }; 1358 else if (fFitMethod == kUseEventScan) {; 1359 Log() << kINFO << ""Read cuts optimised using Full Event Scan"" << Endl;; 1360 }; 1361 else {; 1362 Log() << kWARNING << ""unknown method: "" << fFitMethod << Endl;; 1363 }; 1364 Log() << kINFO << ""Reading "" << fNbins << "" signal efficiency bins for "" << GetNvar() << "" variables"" << Endl;; 1365 ; 1366 delete fEffBvsSLocal;; 1367 fEffBvsSLocal = new TH1F( GetTestvarName() + ""_effBvsSLocal"",; 1368 TString(GetName()) + "" efficiency of B vs S"", fNbins, 0.0, 1.0 );; 1369 fEffBvsSLocal->SetDirectory(nullptr); // it's local; 1370 for (Int_t ibin=1; ibin<=fNbins; ibin++) fEffBvsSLocal->SetBinContent( ibin, -0.1 ); // Init; 1371 ; 1372 fCutMin = new Double_t*[GetNvar()];; 1373 fCutMax = new Double_t*[GetNvar()];; 1374 for (UInt_t i=0;i<GetNvar();i++) {; 1375 fCutMin[i] = new Double_t[fNbins];; 1376 fCutMax[i] = new Double_t[f",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:61495,Modifiability,variab,variables,61495,") > 2) {; 1565 delete list;; 1566 Log() << kFATAL << ""<GetEfficiency> wrong number of arguments""; 1567 << "" in string: "" << theString; 1568 << "" | required format, e.g., Efficiency:0.05, or empty string"" << Endl;; 1569 return -1;; 1570 }; 1571 ; 1572 // sanity check; 1573 Bool_t computeArea = (list->GetSize() < 2); // the area is computed; 1574 ; 1575 // that will be the value of the efficiency retured (does not affect; 1576 // the efficiency-vs-bkg plot which is done anyway.; 1577 Float_t effBref = (computeArea?1.:atof( ((TObjString*)list->At(1))->GetString() ));; 1578 ; 1579 delete list;; 1580 ; 1581 ; 1582 // first round ? --> create histograms; 1583 if (results->GetHist(""MVA_EFF_BvsS"")==0) {; 1584 ; 1585 if (fBinaryTreeS!=0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 1586 if (fBinaryTreeB!=0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 1587 ; 1588 // the variables may be transformed by a transformation method: to coherently; 1589 // treat signal and background one must decide which transformation type shall; 1590 // be used: our default is signal-type; 1591 fBinaryTreeS = new BinarySearchTree();; 1592 fBinaryTreeS->Fill( GetEventCollection(Types::kTesting), fSignalClass );; 1593 fBinaryTreeB = new BinarySearchTree();; 1594 fBinaryTreeB->Fill( GetEventCollection(Types::kTesting), fBackgroundClass );; 1595 ; 1596 // there is no really good equivalent to the fEffS; fEffB (efficiency vs cutvalue); 1597 // for the ""Cuts"" method (unless we had only one cut). Maybe later I might add here; 1598 // histograms for each of the cuts...but this would require also a change in the; 1599 // base class, and it is not really necessary, as we get exactly THIS info from the; 1600 // ""evaluateAllVariables"" anyway.; 1601 ; 1602 // now create efficiency curve: background versus signal; 1603 TH1* eff_BvsS = new TH1F( GetTestvarName() + ""_effBvsS"", GetTestvarName() + """", fNbins, 0, 1 );; 1604 for (Int_t ibin=1; ibin<=fNbins; ibin++) eff_BvsS->SetBinContent( ibin, -0.1 ); // Init; 1605 T",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:67836,Modifiability,variab,variables,67836,"9void TMVA::MethodCuts::GetHelpMessage() const; 1720{; 1721 TString bold = gConfig().WriteOptionsReference() ? ""<b>"" : """";; 1722 TString resbold = gConfig().WriteOptionsReference() ? ""</b>"" : """";; 1723 TString brk = gConfig().WriteOptionsReference() ? ""<br>"" : """";; 1724 ; 1725 Log() << Endl;; 1726 Log() << gTools().Color(""bold"") << ""--- Short description:"" << gTools().Color(""reset"") << Endl;; 1727 Log() << Endl;; 1728 Log() << ""The optimisation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Lo",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68175,Modifiability,variab,variables,68175,"sation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68271,Modifiability,variab,variables,68271,"sation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68364,Modifiability,variab,variables,68364,"ll range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration op",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68718,Modifiability,variab,variable,68718,"trongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within ",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:68976,Modifiability,config,configuration,68976,"og() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true m",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:69214,Modifiability,variab,variables,69214," input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" <<",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:69372,Modifiability,config,configuration,69372,"iables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). Choosing this option"" << Endl;; 1747 Log() << ""reduces the number of requirements for the variable from 2 (min/max)"" << Endl;; 1748 Log() << ""to a single one (TMVA finds out whether it is to be interpreted as"" << Endl;; 1749 Log() << ""min or max)."" << Endl;; 1750 Log() << Endl;; 1751 Log() << gTools().Color(""bold"") << ""--- Performance tuning via configuration options:"" << gTools().Color(""reset"") << Endl;; 1752 Log() << """" << Endl;; 1753 Log() << bold << ""Monte Carlo sampling:"" << resbold << Endl;; 1754 Log() << """" << Endl;; 1755 Log() << ""Apart form the \""Fsmart\"" option for the variables, the only way"" << Endl;; 1756 Log() << ""to improve the MC sampling is to increase the sampling rate. This"" << Endl;; 1757 Log() << ""is done via the configuration option \""MC_NRandCuts\"". The execution"" << Endl;; 1758 Log() << ""time scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * numbe",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:70372,Modifiability,variab,variables,70372," scales linearly with the sampling rate."" << Endl;; 1759 Log() << """" << Endl;; 1760 Log() << bold << ""Genetic Algorithm:"" << resbold << Endl;; 1761 Log() << """" << Endl;; 1762 Log() << ""The algorithm terminates if no significant fitness increase has"" << Endl;; 1763 Log() << ""been achieved within the last \""nsteps\"" steps of the calculation."" << Endl;; 1764 Log() << ""Wiggles in the ROC curve or constant background rejection of 1"" << Endl;; 1765 Log() << ""indicate that the GA failed to always converge at the true maximum"" << Endl;; 1766 Log() << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * number of variables)"" << Endl;; 1771 Log() << "" -> increase \""nsteps\"""" << Endl;; 1772 Log() << """" << Endl;; 1773 Log() << bold << ""Simulated Annealing (SA) algorithm:"" << resbold << Endl;; 1774 Log() << """" << Endl;; 1775 Log() << ""\""Increasing Adaptive\"" approach:"" << Endl;; 1776 Log() << """" << Endl;; 1777 Log() << ""The algorithm seeks local minima and explores their neighborhoods, while"" << Endl;; 1778 Log() << ""changing the ambient temperature depending on the number of failures"" << Endl;; 1779 Log() << ""in the previous steps. The performance can be improved by increasing"" << Endl;; 1780 Log() << ""the number of iteration steps (\""MaxCalls\""), or by adjusting the"" << Endl;; 1781 Log() << ""minimal temperature (\""MinTemperature\""). Manual adjustments of the"" << Endl;; 1782 Log() << ""speed of the temperature increase (\""TemperatureScale\"" and \""AdaptiveSpeed\"")"" << Endl;; 1783 Log() << ""to individual data sets should also help. Summary:"" << brk << Endl;; 1784 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1785 Log() << "" -> adjust \""MinTemperature\"""" << brk << Endl;; 178",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:77445,Modifiability,variab,variableDefinition,77445,"finition TH1.h:59; TH1::GetXaxisTAxis * GetXaxis()Definition TH1.h:324; TH1::SetBinContentvirtual void SetBinContent(Int_t bin, Double_t content)Set bin content see convention for numbering bins in TH1::GetBin In case the bin number is greater th...Definition TH1.cxx:9222; TListA doubly linked list.Definition TList.h:38; TList::AtTObject * At(Int_t idx) const overrideReturns the object at position idx. Returns 0 if idx is out of range.Definition TList.cxx:355; TMVA::BinarySearchTreeA simple Binary search tree including a volume search method.Definition BinarySearchTree.h:65; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::FitterBaseBase class for TMVA fitters.Definition FitterBase.h:51; TMVA::FitterBase::SetIPythonInteractivevoid SetIPythonInteractive(bool *ExitFromTraining, UInt_t *fIPyMaxIter_, UInt_t *fIPyCurrentIter_)Definition FitterBase.h:73; TMVA::FitterBase::RunDouble_t Run()estimator function interface for fittingDefinition FitterBase.cxx:74; TMVA::GeneticFitterFitter using a Genetic Algorithm.Definition GeneticFitter.h:44; TMVA::IntervalThe TMVA::Interval Class.Definition Interval.h:61; TMVA::MCFitterFitter using Monte Carlo sampling of parameters.Definition MCFitter.h:44; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::MethodCutsfriend class MethodCutsDefinition MethodBase.h:603; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::MethodCuts::ComputeEstimatorDouble_t Compu",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:2516,Performance,perform,performed,2516,"hodCuts; 32\ingroup TMVA; 33 ; 34 Multivariate optimisation of signal efficiency for given background; 35 efficiency, applying rectangular minimum and maximum requirements.; 36 ; 37 Also implemented is a ""decorrelate/diagonalized cuts approach"",; 38 which improves over the uncorrelated cuts approach by; 39 transforming linearly the input variables into a diagonal space,; 40 using the square-root of the covariance matrix.; 41 ; 42 Other optimisation criteria, such as maximising the signal significance-; 43 squared, \f$ \frac{S^2}{(S+B)} \f$, with S and B being the signal and background yields,; 44 correspond to a particular point in the optimised background rejection; 45 versus signal efficiency curve. This working point requires the knowledge; 46 of the expected yields, which is not the case in general. Note also that; 47 for rare signals, Poissonian statistics should be used, which modifies; 48 the significance criterion.; 49 ; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is t",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:3404,Performance,perform,performed,3404,"; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description for Method Likelihood for a detailed explanation.; 75*/; 76 ; 77#include ""TMVA/MethodCuts.h""; 78 ; 79#include ""TMVA/BinarySearchTree.h""; 80#include ""TMVA/ClassifierFactory.h""; 81#include ""TMVA/Config.h""; 82#include ""TMVA/Configurable.h""; 83#include ""TMVA/DataSet.h""; 84#include ""TMVA/DataSetInfo.h""; 85#include ""TMVA/Event.h""; 86#include ""TMVA/IFitterTarget.h""; 87#include ""TMVA/IMethod.h""; 88#include ""TMVA/GeneticFitter.h""; 89#include ""TMVA/Interval.h""; 90#include ""TMVA/FitterBase.h""; 91#include ""TMVA/MCFitter.h""; 92#include ""TMVA/MethodBase.h""; 93#include ""TMVA/MethodFDA.h""; 94#include ""TMVA/MinuitFitter.h""; 95#include ""TMVA/MsgLogger.h""; 96#include ""TMVA/PDF.h""; 97#include ""TMVA/Results.h""; 98#include ""TMVA/SimulatedAnnealingFitter.h""; 99#include ""TMVA/Timer.h""; 100#include ""TMVA/Tools.h",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:13155,Performance,perform,performance,13155,"] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm."" << Endl;; 369 Log() << kWARNING << ""Please remove \""Normalise\"" option from booking option string"" << Endl;; 370 Log() << kWARNING << ""==> Will reset normalisation flag to \""False\"""" << Endl;; 371 SetNormalised( kFALSE );; 372 }; 373 ; 374 if (IgnoreEventsWithNegWeightsInTraining()) {; 375 Log() << kFATAL << ""Mechanism to ignore events with negative weights in training not yet available for method: ""; 376 << GetMethodTypeName(); 377 << "" --> Please remove \""IgnoreNegWeightsInTraining\"" option from booking string.""; 378 << Endl;; 379 }; 380 ; 381 if (fFitMethodS == ""MC"" ) fFitMethod = kUseMonteCarlo;; 382 else if (fFitMethodS == ""MCEvents"") fFitMethod = kUseMonteCarloEvents;; 383 else if (fFitMethodS == ""GA"" ) fFitMethod = kUseGenet",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:14117,Performance,perform,performance,14117,,MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:14616,Performance,optimiz,optimization,14616,,MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:26611,Performance,perform,perform,26611,"ivar] == kForceMax) {; 657 ranges.push_back( new Interval( fCutRange[ivar]->GetMin(), fCutRange[ivar]->GetMax(), nbins ) );; 658 ranges.push_back( new Interval( fCutRange[ivar]->GetMax() - fCutRange[ivar]->GetMin(),; 659 fCutRange[ivar]->GetMax() - fCutRange[ivar]->GetMin(), nbins ) );; 660 }; 661 else {; 662 ranges.push_back( new Interval( fCutRange[ivar]->GetMin(), fCutRange[ivar]->GetMax(), nbins ) );; 663 ranges.push_back( new Interval( 0, fCutRange[ivar]->GetMax() - fCutRange[ivar]->GetMin(), nbins ) );; 664 }; 665 }; 666 ; 667 // create the fitter; 668 FitterBase* fitter = NULL;; 669 ; 670 switch (fFitMethod) {; 671 case kUseGeneticAlgorithm:; 672 fitter = new GeneticFitter( *this, TString::Format(""%sFitter_GA"", GetName()), ranges, GetOptions() );; 673 break;; 674 case kUseMonteCarlo:; 675 fitter = new MCFitter ( *this, TString::Format(""%sFitter_MC"", GetName()), ranges, GetOptions() );; 676 break;; 677 case kUseMinuit:; 678 fitter = new MinuitFitter ( *this, TString::Format(""%sFitter_MINUIT"", GetName()), ranges, GetOptions() );; 679 break;; 680 case kUseSimulatedAnnealing:; 681 fitter = new SimulatedAnnealingFitter( *this, TString::Format(""%sFitter_SA"", GetName()), ranges, GetOptions() );; 682 break;; 683 default:; 684 Log() << kFATAL << ""Wrong fit method: "" << fFitMethod << Endl;; 685 }; 686 ; 687 if (fInteractive) fitter->SetIPythonInteractive(&fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 688 ; 689 fitter->CheckForUnusedOptions();; 690 ; 691 // perform the fit; 692 fitter->Run();; 693 ; 694 // clean up; 695 for (UInt_t ivar=0; ivar<ranges.size(); ivar++) delete ranges[ivar];; 696 delete fitter;; 697 ; 698 }; 699 // --------------------------------------------------------------------------; 700 else if (fFitMethod == kUseEventScan) {; 701 ; 702 Int_t nevents = Data()->GetNEvents();; 703 Int_t ic = 0;; 704 ; 705 // timing of MC; 706 Int_t nsamples = Int_t(0.5*nevents*(nevents - 1));; 707 Timer timer( nsamples, GetName() );; 708 fIPyMaxIter = nsamples;; ",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:33698,Performance,optimiz,optimizes,33698,"e_t TMVA::MethodCuts::EstimatorFunction( std::vector<Double_t>& pars ); 879{; 880 return ComputeEstimator( pars );; 881}; 882 ; 883////////////////////////////////////////////////////////////////////////////////; 884/// returns estimator for ""cut fitness"" used by GA.; 885///; 886/// there are two requirements:; 887/// 1. the signal efficiency must be equal to the required one in the; 888/// efficiency scan; 889/// 2. the background efficiency must be as small as possible; 890///; 891/// the requirement 1. has priority over 2.; 892 ; 893Double_t TMVA::MethodCuts::ComputeEstimator( std::vector<Double_t>& pars ); 894{; 895 // caution: the npar gives the _free_ parameters; 896 // however: the ""pars"" array contains all parameters; 897 ; 898 // determine cuts; 899 Double_t effS = 0, effB = 0;; 900 this->MatchParsToCuts( pars, &fTmpCutMin[0], &fTmpCutMax[0] );; 901 ; 902 // retrieve signal and background efficiencies for given cut; 903 switch (fEffMethod) {; 904 case kUsePDFs:; 905 this->GetEffsfromPDFs (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 906 break;; 907 case kUseEventSelection:; 908 this->GetEffsfromSelection (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 909 break;; 910 default:; 911 this->GetEffsfromSelection (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 912 }; 913 ; 914 Double_t eta = 0;; 915 ; 916 // test for a estimator function which optimizes on the whole background-rejection signal-efficiency plot; 917 ; 918 // get the backg-reject. and sig-eff for the parameters given to this function; 919 // effS, effB; 920 ; 921 // get best background rejection for given signal efficiency; 922 Int_t ibinS = fEffBvsSLocal->FindBin( effS );; 923 ; 924 Double_t effBH = fEffBvsSLocal->GetBinContent( ibinS );; 925 Double_t effBH_left = (ibinS > 1 ) ? fEffBvsSLocal->GetBinContent( ibinS-1 ) : effBH;; 926 Double_t effBH_right = (ibinS < fNbins) ? fEffBvsSLocal->GetBinContent( ibinS+1 ) : effBH;; 927 ; 928 Double_t average = 0.5*(effBH_left + effBH_right);; 929 if (effBH < e",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:67231,Performance,perform,performed,67231,"7void TMVA::MethodCuts::MakeClassSpecific( std::ostream& fout, const TString& className ) const; 1708{; 1709 fout << "" // not implemented for class: \"""" << className << ""\"""" << std::endl;; 1710 fout << ""};"" << std::endl;; 1711}; 1712 ; 1713////////////////////////////////////////////////////////////////////////////////; 1714/// get help message text; 1715///; 1716/// typical length of text line:; 1717/// ""|--------------------------------------------------------------|""; 1718 ; 1719void TMVA::MethodCuts::GetHelpMessage() const; 1720{; 1721 TString bold = gConfig().WriteOptionsReference() ? ""<b>"" : """";; 1722 TString resbold = gConfig().WriteOptionsReference() ? ""</b>"" : """";; 1723 TString brk = gConfig().WriteOptionsReference() ? ""<br>"" : """";; 1724 ; 1725 Log() << Endl;; 1726 Log() << gTools().Color(""bold"") << ""--- Short description:"" << gTools().Color(""reset"") << Endl;; 1727 Log() << Endl;; 1728 Log() << ""The optimisation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the varia",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:67649,Performance,perform,perform,67649,"1715///; 1716/// typical length of text line:; 1717/// ""|--------------------------------------------------------------|""; 1718 ; 1719void TMVA::MethodCuts::GetHelpMessage() const; 1720{; 1721 TString bold = gConfig().WriteOptionsReference() ? ""<b>"" : """";; 1722 TString resbold = gConfig().WriteOptionsReference() ? ""</b>"" : """";; 1723 TString brk = gConfig().WriteOptionsReference() ? ""<br>"" : """";; 1724 ; 1725 Log() << Endl;; 1726 Log() << gTools().Color(""bold"") << ""--- Short description:"" << gTools().Color(""reset"") << Endl;; 1727 Log() << Endl;; 1728 Log() << ""The optimisation of rectangular cuts performed by TMVA maximises "" << Endl;; 1729 Log() << ""the background rejection at given signal efficiency, and scans "" << Endl;; 1730 Log() << ""over the full range of the latter quantity. Three optimisation"" << Endl;; 1731 Log() << ""methods are optional: Monte Carlo sampling (MC), a Genetics"" << Endl;; 1732 Log() << ""Algorithm (GA), and Simulated Annealing (SA). GA and SA are"" << Endl;; 1733 Log() << ""expected to perform best."" << Endl;; 1734 Log() << Endl;; 1735 Log() << ""The difficulty to find the optimal cuts strongly increases with"" << Endl;; 1736 Log() << ""the dimensionality (number of input variables) of the problem."" << Endl;; 1737 Log() << ""This behavior is due to the non-uniqueness of the solution space.""<< Endl;; 1738 Log() << Endl;; 1739 Log() << gTools().Color(""bold"") << ""--- Performance optimisation:"" << gTools().Color(""reset"") << Endl;; 1740 Log() << Endl;; 1741 Log() << ""If the dimensionality exceeds, say, 4 input variables, it is "" << Endl;; 1742 Log() << ""advisable to scrutinize the separation power of the variables,"" << Endl;; 1743 Log() << ""and to remove the weakest ones. If some among the input variables"" << Endl;; 1744 Log() << ""can be described by a single cut (e.g., because signal tends to be"" << Endl;; 1745 Log() << ""larger than background), this can be indicated to MethodCuts via"" << Endl;; 1746 Log() << ""the \""Fsmart\"" options (see option string). C",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:70902,Performance,perform,performance,70902," << ""fitness. In such a case, it is recommended to broaden the search "" << Endl;; 1767 Log() << ""by increasing the population size (\""popSize\"") and to give the GA "" << Endl;; 1768 Log() << ""more time to find improvements by increasing the number of steps"" << Endl;; 1769 Log() << ""(\""nsteps\"")"" << Endl;; 1770 Log() << "" -> increase \""popSize\"" (at least >10 * number of variables)"" << Endl;; 1771 Log() << "" -> increase \""nsteps\"""" << Endl;; 1772 Log() << """" << Endl;; 1773 Log() << bold << ""Simulated Annealing (SA) algorithm:"" << resbold << Endl;; 1774 Log() << """" << Endl;; 1775 Log() << ""\""Increasing Adaptive\"" approach:"" << Endl;; 1776 Log() << """" << Endl;; 1777 Log() << ""The algorithm seeks local minima and explores their neighborhoods, while"" << Endl;; 1778 Log() << ""changing the ambient temperature depending on the number of failures"" << Endl;; 1779 Log() << ""in the previous steps. The performance can be improved by increasing"" << Endl;; 1780 Log() << ""the number of iteration steps (\""MaxCalls\""), or by adjusting the"" << Endl;; 1781 Log() << ""minimal temperature (\""MinTemperature\""). Manual adjustments of the"" << Endl;; 1782 Log() << ""speed of the temperature increase (\""TemperatureScale\"" and \""AdaptiveSpeed\"")"" << Endl;; 1783 Log() << ""to individual data sets should also help. Summary:"" << brk << Endl;; 1784 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1785 Log() << "" -> adjust \""MinTemperature\"""" << brk << Endl;; 1786 Log() << "" -> adjust \""TemperatureScale\"""" << brk << Endl;; 1787 Log() << "" -> adjust \""AdaptiveSpeed\"""" << Endl;; 1788 Log() << """" << Endl;; 1789 Log() << ""\""Decreasing Adaptive\"" approach:"" << Endl;; 1790 Log() << """" << Endl;; 1791 Log() << ""The algorithm calculates the initial temperature (based on the effect-"" << Endl;; 1792 Log() << ""iveness of large steps) and the multiplier that ensures to reach the"" << Endl;; 1793 Log() << ""minimal temperature with the requested number of iteration steps."" << Endl;; 1794 Log() << ""The performance",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:71989,Performance,perform,performance,71989," and \""AdaptiveSpeed\"")"" << Endl;; 1783 Log() << ""to individual data sets should also help. Summary:"" << brk << Endl;; 1784 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1785 Log() << "" -> adjust \""MinTemperature\"""" << brk << Endl;; 1786 Log() << "" -> adjust \""TemperatureScale\"""" << brk << Endl;; 1787 Log() << "" -> adjust \""AdaptiveSpeed\"""" << Endl;; 1788 Log() << """" << Endl;; 1789 Log() << ""\""Decreasing Adaptive\"" approach:"" << Endl;; 1790 Log() << """" << Endl;; 1791 Log() << ""The algorithm calculates the initial temperature (based on the effect-"" << Endl;; 1792 Log() << ""iveness of large steps) and the multiplier that ensures to reach the"" << Endl;; 1793 Log() << ""minimal temperature with the requested number of iteration steps."" << Endl;; 1794 Log() << ""The performance can be improved by adjusting the minimal temperature"" << Endl;; 1795 Log() << "" (\""MinTemperature\"") and by increasing number of steps (\""MaxCalls\""):"" << brk << Endl;; 1796 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1797 Log() << "" -> adjust \""MinTemperature\"""" << Endl;; 1798 Log() << "" "" << Endl;; 1799 Log() << ""Other kernels:"" << Endl;; 1800 Log() << """" << Endl;; 1801 Log() << ""Alternative ways of counting the temperature change are implemented. "" << Endl;; 1802 Log() << ""Each of them starts with the maximum temperature (\""MaxTemperature\"")"" << Endl;; 1803 Log() << ""and decreases while changing the temperature according to a given"" << Endl;; 1804 Log() << ""prescription:"" << brk << Endl;; 1805 Log() << ""CurrentTemperature ="" << brk << Endl;; 1806 Log() << "" - Sqrt: InitialTemperature / Sqrt(StepNumber+2) * TemperatureScale"" << brk << Endl;; 1807 Log() << "" - Log: InitialTemperature / Log(StepNumber+2) * TemperatureScale"" << brk << Endl;; 1808 Log() << "" - Homo: InitialTemperature / (StepNumber+2) * TemperatureScale"" << brk << Endl;; 1809 Log() << "" - Sin: (Sin(StepNumber / TemperatureScale) + 1) / (StepNumber + 1)*InitialTemperature + Eps"" << brk << Endl;; 1810 Log() << "" - Geo:",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:73309,Performance,perform,performance,73309,"g() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1797 Log() << "" -> adjust \""MinTemperature\"""" << Endl;; 1798 Log() << "" "" << Endl;; 1799 Log() << ""Other kernels:"" << Endl;; 1800 Log() << """" << Endl;; 1801 Log() << ""Alternative ways of counting the temperature change are implemented. "" << Endl;; 1802 Log() << ""Each of them starts with the maximum temperature (\""MaxTemperature\"")"" << Endl;; 1803 Log() << ""and decreases while changing the temperature according to a given"" << Endl;; 1804 Log() << ""prescription:"" << brk << Endl;; 1805 Log() << ""CurrentTemperature ="" << brk << Endl;; 1806 Log() << "" - Sqrt: InitialTemperature / Sqrt(StepNumber+2) * TemperatureScale"" << brk << Endl;; 1807 Log() << "" - Log: InitialTemperature / Log(StepNumber+2) * TemperatureScale"" << brk << Endl;; 1808 Log() << "" - Homo: InitialTemperature / (StepNumber+2) * TemperatureScale"" << brk << Endl;; 1809 Log() << "" - Sin: (Sin(StepNumber / TemperatureScale) + 1) / (StepNumber + 1)*InitialTemperature + Eps"" << brk << Endl;; 1810 Log() << "" - Geo: CurrentTemperature * TemperatureScale"" << Endl;; 1811 Log() << """" << Endl;; 1812 Log() << ""Their performance can be improved by adjusting initial temperature"" << Endl;; 1813 Log() << ""(\""InitialTemperature\""), the number of iteration steps (\""MaxCalls\""),"" << Endl;; 1814 Log() << ""and the multiplier that scales the temperature decrease"" << Endl;; 1815 Log() << ""(\""TemperatureScale\"")"" << brk << Endl;; 1816 Log() << "" -> increase \""MaxCalls\"""" << brk << Endl;; 1817 Log() << "" -> adjust \""InitialTemperature\"""" << brk << Endl;; 1818 Log() << "" -> adjust \""TemperatureScale\"""" << brk << Endl;; 1819 Log() << "" -> adjust \""KernelTemperature\"""" << Endl;; 1820}; BinarySearchTree.h; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; DataSetInfo.h; DataSet.h; Event.h; FitterBase.h; GeneticFitter.h; IFitterTarget.h; IMethod.h; Interval.h; MCFitter.h; MethodBase.h; MethodCuts.h; MethodFDA",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:12687,Safety,sanity check,sanity check,12687,"] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i] = ""NotEnforced"";; 349 ; 350 DeclareOptionRef(fAllVarsI, GetNvar(), ""VarProp"", ""Categorisation of cuts"");; 351 AddPreDefVal(TString(""NotEnforced""));; 352 AddPreDefVal(TString(""FMax""));; 353 AddPreDefVal(TString(""FMin""));; 354 AddPreDefVal(TString(""FSmart""));; 355}; 356 ; 357////////////////////////////////////////////////////////////////////////////////; 358/// process user options.; 359///; 360/// sanity check, do not allow the input variables to be normalised, because this; 361/// only creates problems when interpreting the cuts; 362 ; 363void TMVA::MethodCuts::ProcessOptions(); 364{; 365 if (IsNormalised()) {; 366 Log() << kWARNING << ""Normalisation of the input variables for cut optimisation is not"" << Endl;; 367 Log() << kWARNING << ""supported because this provides intransparent cut values, and no"" << Endl;; 368 Log() << kWARNING << ""improvement in the performance of the algorithm."" << Endl;; 369 Log() << kWARNING << ""Please remove \""Normalise\"" option from booking option string"" << Endl;; 370 Log() << kWARNING << ""==> Will reset normalisation flag to \""False\"""" << Endl;; 371 SetNormalised( kFALSE );; 372 }; 373 ; 374 if (IgnoreEventsWithNegWeightsInTraining()) {; 375 Log() << kFATAL << ""Mechanism to ignore events with negative weights in training not yet available for method: ""; 376 << GetMethodTypeName(); 377 << "" --> Please remove \""IgnoreNegWeightsInTraining\"" option from booking string.""; 378 << Endl;; 379 }; 380 ; 381 if (fFitMethodS == ""MC"" ) fFitMethod = kUseMonteCarlo;; 382 else if (fFitMethodS == ""MCEvents"") fFitMethod = kUseMonteCarloEvents;; 383 else if (fFitMethodS == ""GA"" ) fFitMethod = kUseGenet",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:16340,Safety,sanity check,sanity check,16340,"meters theFitP = kNotEnforced;; 413 if (fAllVarsI[ivar] == """" || fAllVarsI[ivar] == ""NotEnforced"") theFitP = kNotEnforced;; 414 else if (fAllVarsI[ivar] == ""FMax"" ) theFitP = kForceMax;; 415 else if (fAllVarsI[ivar] == ""FMin"" ) theFitP = kForceMin;; 416 else if (fAllVarsI[ivar] == ""FSmart"" ) theFitP = kForceSmart;; 417 else {; 418 Log() << kFATAL << ""unknown value \'"" << fAllVarsI[ivar]; 419 << ""\' for fit parameter option "" << Form(""VarProp[%i]"",ivar) << Endl;; 420 }; 421 (*fFitParams)[ivar] = theFitP;; 422 ; 423 if (theFitP != kNotEnforced); 424 Log() << kINFO << ""Use \"""" << fAllVarsI[ivar]; 425 << ""\"" cuts for variable: "" << ""'"" << (*fInputVars)[ivar] << ""'"" << Endl;; 426 }; 427}; 428 ; 429////////////////////////////////////////////////////////////////////////////////; 430/// cut evaluation: returns 1.0 if event passed, 0.0 otherwise; 431 ; 432Double_t TMVA::MethodCuts::GetMvaValue( Double_t* err, Double_t* errUpper ); 433{; 434 // cannot determine error; 435 NoErrorCalc(err, errUpper);; 436 ; 437 // sanity check; 438 if (fCutMin == NULL || fCutMax == NULL || fNbins == 0) {; 439 Log() << kFATAL << ""<Eval_Cuts> fCutMin/Max have zero pointer. ""; 440 << ""Did you book Cuts ?"" << Endl;; 441 }; 442 ; 443 const Event* ev = GetEvent();; 444 ; 445 // sanity check; 446 if (fTestSignalEff > 0) {; 447 // get efficiency bin; 448 Int_t ibin = fEffBvsSLocal->FindBin( fTestSignalEff );; 449 if (ibin < 0 ) ibin = 0;; 450 else if (ibin >= fNbins) ibin = fNbins - 1;; 451 ; 452 Bool_t passed = kTRUE;; 453 for (UInt_t ivar=0; ivar<GetNvar(); ivar++); 454 passed &= ( (ev->GetValue(ivar) > fCutMin[ivar][ibin]) &&; 455 (ev->GetValue(ivar) <= fCutMax[ivar][ibin]) );; 456 ; 457 return passed ? 1. : 0. ;; 458 }; 459 else return 0;; 460}; 461 ; 462////////////////////////////////////////////////////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 4",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:16586,Safety,sanity check,sanity check,16586,"Form(""VarProp[%i]"",ivar) << Endl;; 420 }; 421 (*fFitParams)[ivar] = theFitP;; 422 ; 423 if (theFitP != kNotEnforced); 424 Log() << kINFO << ""Use \"""" << fAllVarsI[ivar]; 425 << ""\"" cuts for variable: "" << ""'"" << (*fInputVars)[ivar] << ""'"" << Endl;; 426 }; 427}; 428 ; 429////////////////////////////////////////////////////////////////////////////////; 430/// cut evaluation: returns 1.0 if event passed, 0.0 otherwise; 431 ; 432Double_t TMVA::MethodCuts::GetMvaValue( Double_t* err, Double_t* errUpper ); 433{; 434 // cannot determine error; 435 NoErrorCalc(err, errUpper);; 436 ; 437 // sanity check; 438 if (fCutMin == NULL || fCutMax == NULL || fNbins == 0) {; 439 Log() << kFATAL << ""<Eval_Cuts> fCutMin/Max have zero pointer. ""; 440 << ""Did you book Cuts ?"" << Endl;; 441 }; 442 ; 443 const Event* ev = GetEvent();; 444 ; 445 // sanity check; 446 if (fTestSignalEff > 0) {; 447 // get efficiency bin; 448 Int_t ibin = fEffBvsSLocal->FindBin( fTestSignalEff );; 449 if (ibin < 0 ) ibin = 0;; 450 else if (ibin >= fNbins) ibin = fNbins - 1;; 451 ; 452 Bool_t passed = kTRUE;; 453 for (UInt_t ivar=0; ivar<GetNvar(); ivar++); 454 passed &= ( (ev->GetValue(ivar) > fCutMin[ivar][ibin]) &&; 455 (ev->GetValue(ivar) <= fCutMax[ivar][ibin]) );; 456 ; 457 return passed ? 1. : 0. ;; 458 }; 459 else return 0;; 460}; 461 ; 462////////////////////////////////////////////////////////////////////////////////; 463/// print cuts; 464 ; 465void TMVA::MethodCuts::PrintCuts( Double_t effS ) const; 466{; 467 std::vector<Double_t> cutsMin;; 468 std::vector<Double_t> cutsMax;; 469 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 470 ; 471 Double_t trueEffS = GetCuts( effS, cutsMin, cutsMax );; 472 ; 473 // retrieve variable expressions (could be transformations); 474 std::vector<TString>* varVec = 0;; 475 if (GetTransformationHandler().GetNumOfTransformations() == 0) {; 476 // no transformation applied, replace by current variables; 477 varVec = new std::vector<TString>;; 478 for (UInt_t ivar=0; ivar<cutsM",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:35564,Safety,avoid,avoid,35564,"(1.0 - (effBH - effB))) / (1.0 + effS);; 934 // alternative idea; 935 //if (effBH<0) eta = (1.e-6+effB)/(1.0 + effS);; 936 //else eta = (effB - effBH) * (1.0 + 10.* effS);; 937 ; 938 // if a point is found which is better than an existing one, ... replace it.; 939 // preliminary best event -> backup; 940 if (effBH < 0 || effBH > effB) {; 941 fEffBvsSLocal->SetBinContent( ibinS, effB );; 942 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 943 fCutMin[ivar][ibinS-1] = fTmpCutMin[ivar]; // bin 1 stored in index 0; 944 fCutMax[ivar][ibinS-1] = fTmpCutMax[ivar];; 945 }; 946 }; 947 ; 948 // caution (!) this value is not good for a decision for MC, .. it is designed for GA; 949 // but .. it doesn't matter, as MC samplings are independent from the former ones; 950 // and the replacement of the best variables by better ones is done about 10 lines above.; 951 // ( if (effBH < 0 || effBH > effB) { .... ); 952 ; 953 if (ibinS<=1) {; 954 // add penalty for effS=0 bin; 955 // to avoid that the minimizer gets stuck in the zero-bin; 956 // force it towards higher efficiency; 957 Double_t penalty=0.,diff=0.;; 958 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 959 diff=(fCutRange[ivar]->GetMax()-fTmpCutMax[ivar])/(fCutRange[ivar]->GetMax()-fCutRange[ivar]->GetMin());; 960 penalty+=diff*diff;; 961 diff=(fCutRange[ivar]->GetMin()-fTmpCutMin[ivar])/(fCutRange[ivar]->GetMax()-fCutRange[ivar]->GetMin());; 962 penalty+=4.*diff*diff;; 963 }; 964 ; 965 if (effS<1.e-4) return 10.0+penalty;; 966 else return 10.*(1.-10.*effS);; 967 }; 968 return eta;; 969}; 970 ; 971////////////////////////////////////////////////////////////////////////////////; 972/// translates parameters into cuts; 973 ; 974void TMVA::MethodCuts::MatchParsToCuts( const std::vector<Double_t> & pars,; 975 Double_t* cutMin, Double_t* cutMax ); 976{; 977 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 978 Int_t ipar = 2*ivar;; 979 cutMin[ivar] = ((*fRangeSign)[ivar] > 0) ? pars[ipar] : pars[ipar] - pars[ipar+1];; 980 cutMax[ivar] ",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:40021,Safety,sanity check,sanity check,40021," ) Log() << kWARNING << ""Negative background efficiency found and set to 0. This is probably due to many events with negative weights in a certain cut-region."" << Endl;; 1042 fNegEffWarning = kTRUE;; 1043 }; 1044}; 1045 ; 1046////////////////////////////////////////////////////////////////////////////////; 1047/// compute signal and background efficiencies from event counting; 1048/// for given cut sample; 1049 ; 1050void TMVA::MethodCuts::GetEffsfromSelection( Double_t* cutMin, Double_t* cutMax,; 1051 Double_t& effS, Double_t& effB); 1052{; 1053 Float_t nTotS = 0, nTotB = 0;; 1054 Float_t nSelS = 0, nSelB = 0;; 1055 ; 1056 Volume* volume = new Volume( cutMin, cutMax, GetNvar() );; 1057 ; 1058 // search for all events lying in the volume, and add up their weights; 1059 nSelS = fBinaryTreeS->SearchVolume( volume );; 1060 nSelB = fBinaryTreeB->SearchVolume( volume );; 1061 ; 1062 delete volume;; 1063 ; 1064 // total number of ""events"" (sum of weights) as reference to compute efficiency; 1065 nTotS = fBinaryTreeS->GetSumOfWeights();; 1066 nTotB = fBinaryTreeB->GetSumOfWeights();; 1067 ; 1068 // sanity check; 1069 if (nTotS == 0 && nTotB == 0) {; 1070 Log() << kFATAL << ""<GetEffsfromSelection> fatal error in zero total number of events:""; 1071 << "" nTotS, nTotB: "" << nTotS << "" "" << nTotB << "" ***"" << Endl;; 1072 }; 1073 ; 1074 // efficiencies; 1075 if (nTotS == 0 ) {; 1076 effS = 0;; 1077 effB = nSelB/nTotB;; 1078 Log() << kWARNING << ""<ComputeEstimator> zero number of signal events"" << Endl;; 1079 }; 1080 else if (nTotB == 0) {; 1081 effB = 0;; 1082 effS = nSelS/nTotS;; 1083 Log() << kWARNING << ""<ComputeEstimator> zero number of background events"" << Endl;; 1084 }; 1085 else {; 1086 effS = nSelS/nTotS;; 1087 effB = nSelB/nTotB;; 1088 }; 1089 ; 1090 // quick fix to prevent from efficiencies < 0; 1091 if( effS < 0.0 ) {; 1092 effS = 0.0;; 1093 if( !fNegEffWarning ) Log() << kWARNING << ""Negative signal efficiency found and set to 0. This is probably due to many events w",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:46225,Safety,sanity check,sanity check,46225,"; 1205 // smooth; 1206 (*fVarHistB_smooth)[ivar]->Smooth(nsmooth);; 1207 ; 1208 // create PDFs; 1209 (*fVarPdfS)[ivar] = new PDF( TString(GetName()) + "" PDF Var Sig "" + GetInputVar( ivar ), (*fVarHistS_smooth)[ivar], PDF::kSpline2 );; 1210 (*fVarPdfB)[ivar] = new PDF( TString(GetName()) + "" PDF Var Bkg "" + GetInputVar( ivar ), (*fVarHistB_smooth)[ivar], PDF::kSpline2 );; 1211 }; 1212}; 1213 ; 1214////////////////////////////////////////////////////////////////////////////////; 1215/// read the cuts from stream; 1216 ; 1217void TMVA::MethodCuts::ReadWeightsFromStream( std::istream& istr ); 1218{; 1219 TString dummy;; 1220 UInt_t dummyInt;; 1221 ; 1222 // first the dimensions; 1223 istr >> dummy >> dummy;; 1224 // coverity[tainted_data_argument]; 1225 istr >> dummy >> fNbins;; 1226 ; 1227 // get rid of one read-in here because we read in once all ready to check for decorrelation; 1228 istr >> dummy >> dummy >> dummy >> dummy >> dummy >> dummy >> dummyInt >> dummy ;; 1229 ; 1230 // sanity check; 1231 if (dummyInt != Data()->GetNVariables()) {; 1232 Log() << kFATAL << ""<ReadWeightsFromStream> fatal error: mismatch ""; 1233 << ""in number of variables: "" << dummyInt << "" != "" << Data()->GetNVariables() << Endl;; 1234 }; 1235 //SetNvar(dummyInt);; 1236 ; 1237 // print some information; 1238 if (fFitMethod == kUseMonteCarlo) {; 1239 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1240 }; 1241 else if (fFitMethod == kUseMonteCarloEvents) {; 1242 Log() << kWARNING << ""Read cuts optimised using sample of MC events"" << Endl;; 1243 }; 1244 else if (fFitMethod == kUseGeneticAlgorithm) {; 1245 Log() << kINFO << ""Read cuts optimised using Genetic Algorithm"" << Endl;; 1246 }; 1247 else if (fFitMethod == kUseSimulatedAnnealing) {; 1248 Log() << kINFO << ""Read cuts optimised using Simulated Annealing algorithm"" << Endl;; 1249 }; 1250 else if (fFitMethod == kUseEventScan) {; 1251 Log() << kINFO << ""Read cuts optimised using Full Event Scan"" << Endl;; 1252 }",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:53321,Safety,sanity check,sanity check,53321,"TString(GetName()) + "" efficiency of B vs S"", fNbins, 0.0, 1.0 );; 1369 fEffBvsSLocal->SetDirectory(nullptr); // it's local; 1370 for (Int_t ibin=1; ibin<=fNbins; ibin++) fEffBvsSLocal->SetBinContent( ibin, -0.1 ); // Init; 1371 ; 1372 fCutMin = new Double_t*[GetNvar()];; 1373 fCutMax = new Double_t*[GetNvar()];; 1374 for (UInt_t i=0;i<GetNvar();i++) {; 1375 fCutMin[i] = new Double_t[fNbins];; 1376 fCutMax[i] = new Double_t[fNbins];; 1377 }; 1378 ; 1379 // read efficiencies and cuts; 1380 Int_t tmpbin;; 1381 Float_t tmpeffS, tmpeffB;; 1382 void* ch = gTools().GetChild(wghtnode,""Bin"");; 1383 while (ch) {; 1384 // if (strcmp(gTools().GetName(ch),""Bin"") !=0) {; 1385 // ch = gTools().GetNextChild(ch);; 1386 // continue;; 1387 // }; 1388 ; 1389 gTools().ReadAttr( ch, ""ibin"", tmpbin );; 1390 gTools().ReadAttr( ch, ""effS"", tmpeffS );; 1391 gTools().ReadAttr( ch, ""effB"", tmpeffB );; 1392 ; 1393 // sanity check; 1394 if (tmpbin-1 >= fNbins || tmpbin-1 < 0) {; 1395 Log() << kFATAL << ""Mismatch in bins: "" << tmpbin-1 << "" >= "" << fNbins << Endl;; 1396 }; 1397 ; 1398 fEffBvsSLocal->SetBinContent( tmpbin, tmpeffB );; 1399 void* ct = gTools().GetChild(ch);; 1400 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1401 gTools().ReadAttr( ct, TString::Format( ""cutMin_%i"", ivar ), fCutMin[ivar][tmpbin-1] );; 1402 gTools().ReadAttr( ct, TString::Format( ""cutMax_%i"", ivar ), fCutMax[ivar][tmpbin-1] );; 1403 }; 1404 ch = gTools().GetNextChild(ch, ""Bin"");; 1405 }; 1406}; 1407 ; 1408////////////////////////////////////////////////////////////////////////////////; 1409/// write histograms and PDFs to file for monitoring purposes; 1410 ; 1411void TMVA::MethodCuts::WriteMonitoringHistosToFile( void ) const; 1412{; 1413 Log() << kINFO << ""Write monitoring histograms to file: "" << BaseDir()->GetPath() << Endl;; 1414 ; 1415 fEffBvsSLocal->Write();; 1416 ; 1417 // save reference histograms to file; 1418 if (fEffMethod == kUsePDFs) {; 1419 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1420 (*fVarHist",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:55450,Safety,sanity check,sanity check,55450,"HistB_smooth)[ivar]->Write();; 1424 (*fVarPdfS)[ivar]->GetPDFHist()->Write();; 1425 (*fVarPdfB)[ivar]->GetPDFHist()->Write();; 1426 }; 1427 }; 1428}; 1429 ; 1430////////////////////////////////////////////////////////////////////////////////; 1431/// Overloaded function to create background efficiency (rejection) versus; 1432/// signal efficiency plot (first call of this function).; 1433///; 1434/// The function returns the signal efficiency at background efficiency; 1435/// indicated in theString; 1436///; 1437/// ""theString"" must have two entries:; 1438/// - `[0]`: ""Efficiency""; 1439/// - `[1]`: the value of background efficiency at which the signal efficiency; 1440/// is to be returned; 1441 ; 1442Double_t TMVA::MethodCuts::GetTrainingEfficiency(const TString& theString); 1443{; 1444 // parse input string for required background efficiency; 1445 TList* list = gTools().ParseFormatLine( theString );; 1446 // sanity check; 1447 if (list->GetSize() != 2) {; 1448 Log() << kFATAL << ""<GetTrainingEfficiency> wrong number of arguments""; 1449 << "" in string: "" << theString; 1450 << "" | required format, e.g., Efficiency:0.05"" << Endl;; 1451 return -1;; 1452 }; 1453 ; 1454 Results* results = Data()->GetResults(GetMethodName(), Types::kTesting, GetAnalysisType());; 1455 ; 1456 // that will be the value of the efficiency retured (does not affect; 1457 // the efficiency-vs-bkg plot which is done anyway.; 1458 Float_t effBref = atof( ((TObjString*)list->At(1))->GetString() );; 1459 ; 1460 delete list;; 1461 ; 1462 // first round ? --> create histograms; 1463 if (results->GetHist(""EFF_BVSS_TR"")==0) {; 1464 ; 1465 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 1466 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 1467 ; 1468 fBinaryTreeS = new BinarySearchTree();; 1469 fBinaryTreeS->Fill( GetEventCollection(Types::kTraining), fSignalClass );; 1470 fBinaryTreeB = new BinarySearchTree();; 1471 fBinaryTreeB->Fill( GetEventCollection(Types::kTrai",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:60877,Safety,sanity check,sanity check,60877,"ncy; 1548/// indicated in theString; 1549///; 1550/// ""theString"" must have two entries:; 1551/// - `[0]`: ""Efficiency""; 1552/// - `[1]`: the value of background efficiency at which the signal efficiency; 1553/// is to be returned; 1554 ; 1555Double_t TMVA::MethodCuts::GetEfficiency( const TString& theString, Types::ETreeType type, Double_t& effSerr ); 1556{; 1557 Data()->SetCurrentType(type);; 1558 ; 1559 Results* results = Data()->GetResults( GetMethodName(), Types::kTesting, GetAnalysisType() );; 1560 ; 1561 // parse input string for required background efficiency; 1562 TList* list = gTools().ParseFormatLine( theString, "":"" );; 1563 ; 1564 if (list->GetSize() > 2) {; 1565 delete list;; 1566 Log() << kFATAL << ""<GetEfficiency> wrong number of arguments""; 1567 << "" in string: "" << theString; 1568 << "" | required format, e.g., Efficiency:0.05, or empty string"" << Endl;; 1569 return -1;; 1570 }; 1571 ; 1572 // sanity check; 1573 Bool_t computeArea = (list->GetSize() < 2); // the area is computed; 1574 ; 1575 // that will be the value of the efficiency retured (does not affect; 1576 // the efficiency-vs-bkg plot which is done anyway.; 1577 Float_t effBref = (computeArea?1.:atof( ((TObjString*)list->At(1))->GetString() ));; 1578 ; 1579 delete list;; 1580 ; 1581 ; 1582 // first round ? --> create histograms; 1583 if (results->GetHist(""MVA_EFF_BvsS"")==0) {; 1584 ; 1585 if (fBinaryTreeS!=0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 1586 if (fBinaryTreeB!=0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 1587 ; 1588 // the variables may be transformed by a transformation method: to coherently; 1589 // treat signal and background one must decide which transformation type shall; 1590 // be used: our default is signal-type; 1591 fBinaryTreeS = new BinarySearchTree();; 1592 fBinaryTreeS->Fill( GetEventCollection(Types::kTesting), fSignalClass );; 1593 fBinaryTreeB = new BinarySearchTree();; 1594 fBinaryTreeB->Fill( GetEventCollection(Types::kTesting), fBackgroundClass );; 1595 ;",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:3390,Testability,test,tests,3390,"; 50 The rectangular cut of a volume in the variable space is performed using; 51 a binary tree to sort the training events. This provides a significant; 52 reduction in computing time (up to several orders of magnitudes, depending; 53 on the complexity of the problem at hand).; 54 ; 55 Technically, optimisation is achieved in TMVA by two methods:; 56 ; 57 1. Monte Carlo generation using uniform priors for the lower cut value,; 58 and the cut width, thrown within the variable ranges.; 59 ; 60 2. A Genetic Algorithm (GA) searches for the optimal (""fittest"") cut sample.; 61 The GA is configurable by many external settings through the option; 62 string. For difficult cases (such as many variables), some tuning; 63 may be necessary to achieve satisfying results; 64 ; 65 Attempts to use Minuit fits (Simplex ot Migrad) instead have not shown; 66 superior results, and often failed due to convergence at local minima.; 67 ; 68 The tests we have performed so far showed that in generic applications,; 69 the GA is superior to MC sampling, and hence GA is the default method.; 70 It is worthwhile trying both anyway.; 71 ; 72 **Decorrelated (or ""diagonalized"") Cuts**; 73 ; 74 See class description for Method Likelihood for a detailed explanation.; 75*/; 76 ; 77#include ""TMVA/MethodCuts.h""; 78 ; 79#include ""TMVA/BinarySearchTree.h""; 80#include ""TMVA/ClassifierFactory.h""; 81#include ""TMVA/Config.h""; 82#include ""TMVA/Configurable.h""; 83#include ""TMVA/DataSet.h""; 84#include ""TMVA/DataSetInfo.h""; 85#include ""TMVA/Event.h""; 86#include ""TMVA/IFitterTarget.h""; 87#include ""TMVA/IMethod.h""; 88#include ""TMVA/GeneticFitter.h""; 89#include ""TMVA/Interval.h""; 90#include ""TMVA/FitterBase.h""; 91#include ""TMVA/MCFitter.h""; 92#include ""TMVA/MethodBase.h""; 93#include ""TMVA/MethodFDA.h""; 94#include ""TMVA/MinuitFitter.h""; 95#include ""TMVA/MsgLogger.h""; 96#include ""TMVA/PDF.h""; 97#include ""TMVA/Results.h""; 98#include ""TMVA/SimulatedAnnealingFitter.h""; 99#include ""TMVA/Timer.h""; 100#include ""TMVA/Tools.h",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:11211,Testability,test,testing,11211,"key words) that can be set in the option string.; 302///; 303/// know options:; 304/// - Method `<string>` Minimisation method. Available values are:; 305/// - MC Monte Carlo `<default>`; 306/// - GA Genetic Algorithm; 307/// - SA Simulated annealing; 308///; 309/// - EffMethod `<string>` Efficiency selection method. Available values are:; 310/// - EffSel `<default>`; 311/// - EffPDF; 312///; 313/// - VarProp `<string>` Property of variable 1 for the MC method (taking precedence over the; 314/// globale setting. The same values as for the global option are available. Variables 1..10 can be; 315/// set this way; 316///; 317/// - CutRangeMin/Max `<float>` user-defined ranges in which cuts are varied; 318 ; 319void TMVA::MethodCuts::DeclareOptions(); 320{; 321 DeclareOptionRef(fFitMethodS = ""GA"", ""FitMethod"", ""Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated)"");; 322 AddPreDefVal(TString(""GA""));; 323 AddPreDefVal(TString(""SA""));; 324 AddPreDefVal(TString(""MC""));; 325 AddPreDefVal(TString(""MCEvents""));; 326 AddPreDefVal(TString(""MINUIT""));; 327 AddPreDefVal(TString(""EventScan""));; 328 ; 329 // selection type; 330 DeclareOptionRef(fEffMethodS = ""EffSel"", ""EffMethod"", ""Selection Method"");; 331 AddPreDefVal(TString(""EffSel""));; 332 AddPreDefVal(TString(""EffPDF""));; 333 ; 334 // cut ranges; 335 fCutRange.resize(GetNvar());; 336 fCutRangeMin = new Double_t[GetNvar()];; 337 fCutRangeMax = new Double_t[GetNvar()];; 338 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 339 fCutRange[ivar] = 0;; 340 fCutRangeMin[ivar] = fCutRangeMax[ivar] = -1;; 341 }; 342 ; 343 DeclareOptionRef( fCutRangeMin, GetNvar(), ""CutRangeMin"", ""Minimum of allowed cut range (set per variable)"" );; 344 DeclareOptionRef( fCutRangeMax, GetNvar(), ""CutRangeMax"", ""Maximum of allowed cut range (set per variable)"" );; 345 ; 346 fAllVarsI = new TString[GetNvar()];; 347 ; 348 for (UInt_t i=0; i<GetNvar(); i++) fAllVarsI[i]",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:30944,Testability,test,test,30944,"B; fBinaryTreeB = 0; }; 800 ; 801 // force cut ranges within limits; 802 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 803 for (Int_t ibin=0; ibin<fNbins; ibin++) {; 804 ; 805 if ((*fFitParams)[ivar] == kForceMin && fCutMin[ivar][ibin] > -fgMaxAbsCutVal) {; 806 fCutMin[ivar][ibin] = -fgMaxAbsCutVal;; 807 }; 808 if ((*fFitParams)[ivar] == kForceMax && fCutMax[ivar][ibin] < fgMaxAbsCutVal) {; 809 fCutMax[ivar][ibin] = fgMaxAbsCutVal;; 810 }; 811 }; 812 }; 813 ; 814 // some output; 815 // the efficiency which is asked for has to be slightly higher than the bin-borders.; 816 // if not, then the wrong bin is taken in some cases.; 817 Double_t epsilon = 0.0001;; 818 for (Double_t eff=0.1; eff<0.95; eff += 0.1) PrintCuts( eff+epsilon );; 819 ; 820 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 821 ExitFromTraining();; 822}; 823 ; 824////////////////////////////////////////////////////////////////////////////////; 825/// nothing to test; 826 ; 827void TMVA::MethodCuts::TestClassification(); 828{; 829}; 830 ; 831////////////////////////////////////////////////////////////////////////////////; 832/// for full event scan; 833 ; 834Double_t TMVA::MethodCuts::EstimatorFunction( Int_t ievt1, Int_t ievt2 ); 835{; 836 const Event *ev1 = GetEvent(ievt1);; 837 if (!DataInfo().IsSignal(ev1)) return -1;; 838 ; 839 const Event *ev2 = GetEvent(ievt2);; 840 if (!DataInfo().IsSignal(ev2)) return -1;; 841 ; 842 const Int_t nvar = GetNvar();; 843 Double_t* evt1 = new Double_t[nvar];; 844 Double_t* evt2 = new Double_t[nvar];; 845 ; 846 for (Int_t ivar=0; ivar<nvar; ivar++) {; 847 evt1[ivar] = ev1->GetValue( ivar );; 848 evt2[ivar] = ev2->GetValue( ivar );; 849 }; 850 ; 851 // determine cuts; 852 std::vector<Double_t> pars;; 853 for (Int_t ivar=0; ivar<nvar; ivar++) {; 854 Double_t cutMin;; 855 Double_t cutMax;; 856 if (evt1[ivar] < evt2[ivar]) {; 857 cutMin = evt1[ivar];; 858 cutMax = evt2[ivar];; 859 }; 860 else {; 861 cutMin = evt2[ivar];; 862 cutMax = evt1[ivar];; 863 }; 864 ; 865",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:33662,Testability,test,test,33662,"e_t TMVA::MethodCuts::EstimatorFunction( std::vector<Double_t>& pars ); 879{; 880 return ComputeEstimator( pars );; 881}; 882 ; 883////////////////////////////////////////////////////////////////////////////////; 884/// returns estimator for ""cut fitness"" used by GA.; 885///; 886/// there are two requirements:; 887/// 1. the signal efficiency must be equal to the required one in the; 888/// efficiency scan; 889/// 2. the background efficiency must be as small as possible; 890///; 891/// the requirement 1. has priority over 2.; 892 ; 893Double_t TMVA::MethodCuts::ComputeEstimator( std::vector<Double_t>& pars ); 894{; 895 // caution: the npar gives the _free_ parameters; 896 // however: the ""pars"" array contains all parameters; 897 ; 898 // determine cuts; 899 Double_t effS = 0, effB = 0;; 900 this->MatchParsToCuts( pars, &fTmpCutMin[0], &fTmpCutMax[0] );; 901 ; 902 // retrieve signal and background efficiencies for given cut; 903 switch (fEffMethod) {; 904 case kUsePDFs:; 905 this->GetEffsfromPDFs (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 906 break;; 907 case kUseEventSelection:; 908 this->GetEffsfromSelection (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 909 break;; 910 default:; 911 this->GetEffsfromSelection (&fTmpCutMin[0], &fTmpCutMax[0], effS, effB);; 912 }; 913 ; 914 Double_t eta = 0;; 915 ; 916 // test for a estimator function which optimizes on the whole background-rejection signal-efficiency plot; 917 ; 918 // get the backg-reject. and sig-eff for the parameters given to this function; 919 // effS, effB; 920 ; 921 // get best background rejection for given signal efficiency; 922 Int_t ibinS = fEffBvsSLocal->FindBin( effS );; 923 ; 924 Double_t effBH = fEffBvsSLocal->GetBinContent( ibinS );; 925 Double_t effBH_left = (ibinS > 1 ) ? fEffBvsSLocal->GetBinContent( ibinS-1 ) : effBH;; 926 Double_t effBH_right = (ibinS < fNbins) ? fEffBvsSLocal->GetBinContent( ibinS+1 ) : effBH;; 927 ; 928 Double_t average = 0.5*(effBH_left + effBH_right);; 929 if (effBH < e",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:82029,Testability,test,testDefinition,82029,"nction to create background efficiency (rejection) versus signal efficiency plot (first ...Definition MethodCuts.cxx:1442; TMVA::MethodCuts::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)Cuts can only handle classification with 2 classes.Definition MethodCuts.cxx:211; TMVA::MethodCuts::ProcessOptionsvoid ProcessOptions()process user options.Definition MethodCuts.cxx:363; TMVA::MethodCuts::WriteMonitoringHistosToFilevoid WriteMonitoringHistosToFile(void) constwrite histograms and PDFs to file for monitoring purposesDefinition MethodCuts.cxx:1411; TMVA::MethodCuts::EEffMethodEEffMethodDefinition MethodCuts.h:157; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(const std::vector< Double_t > &, Double_t *, Double_t *)translates parameters into cutsDefinition MethodCuts.cxx:974; TMVA::MethodCuts::~MethodCutsvirtual ~MethodCuts(void)destructorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCuts::GetCutsDouble_t GetCuts(Double_t effS, std::vector< Double_t > &cutMin, std::vector< Double_t > &cutMax) constretrieve cut values for given signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::MinuitFitter/Fitter using MINUITDefinition MinuitFitter.h:48; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::PDF::kSpline",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:21263,Usability,clear,clear,21263,"t; 537{; 538 std::vector<Double_t> cMin( GetNvar() );; 539 std::vector<Double_t> cMax( GetNvar() );; 540 Double_t trueEffS = GetCuts( effS, cMin, cMax );; 541 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 542 cutMin[ivar] = cMin[ivar];; 543 cutMax[ivar] = cMax[ivar];; 544 }; 545 return trueEffS;; 546}; 547 ; 548////////////////////////////////////////////////////////////////////////////////; 549/// retrieve cut values for given signal efficiency; 550 ; 551Double_t TMVA::MethodCuts::GetCuts( Double_t effS,; 552 std::vector<Double_t>& cutMin,; 553 std::vector<Double_t>& cutMax ) const; 554{; 555 // find corresponding bin; 556 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 557 ; 558 // get the true efficiency which is the one on the ""left hand"" side of the bin; 559 Double_t trueEffS = fEffBvsSLocal->GetBinLowEdge( ibin );; 560 ; 561 ibin--; // the 'cut' vector has 0...fNbins indices; 562 if (ibin < 0 ) ibin = 0;; 563 else if (ibin >= fNbins) ibin = fNbins - 1;; 564 ; 565 cutMin.clear();; 566 cutMax.clear();; 567 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree(",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:21284,Usability,clear,clear,21284,"r() );; 539 std::vector<Double_t> cMax( GetNvar() );; 540 Double_t trueEffS = GetCuts( effS, cMin, cMax );; 541 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 542 cutMin[ivar] = cMin[ivar];; 543 cutMax[ivar] = cMax[ivar];; 544 }; 545 return trueEffS;; 546}; 547 ; 548////////////////////////////////////////////////////////////////////////////////; 549/// retrieve cut values for given signal efficiency; 550 ; 551Double_t TMVA::MethodCuts::GetCuts( Double_t effS,; 552 std::vector<Double_t>& cutMin,; 553 std::vector<Double_t>& cutMax ) const; 554{; 555 // find corresponding bin; 556 Int_t ibin = fEffBvsSLocal->FindBin( effS );; 557 ; 558 // get the true efficiency which is the one on the ""left hand"" side of the bin; 559 Double_t trueEffS = fEffBvsSLocal->GetBinLowEdge( ibin );; 560 ; 561 ibin--; // the 'cut' vector has 0...fNbins indices; 562 if (ibin < 0 ) ibin = 0;; 563 else if (ibin >= fNbins) ibin = fNbins - 1;; 564 ; 565 cutMin.clear();; 566 cutMax.clear();; 567 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 568 cutMin.push_back( fCutMin[ivar][ibin] );; 569 cutMax.push_back( fCutMax[ivar][ibin] );; 570 }; 571 ; 572 return trueEffS;; 573}; 574 ; 575////////////////////////////////////////////////////////////////////////////////; 576/// training method: here the cuts are optimised for the training sample; 577 ; 578void TMVA::MethodCuts::Train( void ); 579{; 580 if (fEffMethod == kUsePDFs) CreateVariablePDFs(); // create PDFs for variables; 581 ; 582 // create binary trees (global member variables) for signal and background; 583 if (fBinaryTreeS != 0) { delete fBinaryTreeS; fBinaryTreeS = 0; }; 584 if (fBinaryTreeB != 0) { delete fBinaryTreeB; fBinaryTreeB = 0; }; 585 ; 586 // the variables may be transformed by a transformation method: to coherently; 587 // treat signal and background one must decide which transformation type shall; 588 // be used: our default is signal-type; 589 ; 590 fBinaryTreeS = new BinarySearchTree();; 591 fBinaryTreeS->Fill( GetEventCollection",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:76905,Usability,simpl,simple,76905,"ection::GetSizevirtual Int_t GetSize() constReturn the capacity of the collection, i.e.Definition TCollection.h:184; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TGraph::SetPointvirtual void SetPoint(Int_t i, Double_t x, Double_t y)Set x and y values for point number i.Definition TGraph.cxx:2342; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TH1TH1 is the base class of all histogram classes in ROOT.Definition TH1.h:59; TH1::GetXaxisTAxis * GetXaxis()Definition TH1.h:324; TH1::SetBinContentvirtual void SetBinContent(Int_t bin, Double_t content)Set bin content see convention for numbering bins in TH1::GetBin In case the bin number is greater th...Definition TH1.cxx:9222; TListA doubly linked list.Definition TList.h:38; TList::AtTObject * At(Int_t idx) const overrideReturns the object at position idx. Returns 0 if idx is out of range.Definition TList.cxx:355; TMVA::BinarySearchTreeA simple Binary search tree including a volume search method.Definition BinarySearchTree.h:65; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::FitterBaseBase class for TMVA fitters.Definition FitterBase.h:51; TMVA::FitterBase::SetIPythonInteractivevoid SetIPythonInteractive(bool *ExitFromTraining, UInt_t *fIPyMaxIter_, UInt_t *fIPyCurrentIter_)Definition FitterBase.h:73; TMVA::FitterBase::RunDouble_t Run()estimator function interface for fittingDefinition FitterBase.cxx:74; TMVA::GeneticFitterFitter using a Genetic Algorithm.Definition GeneticFitter.h:44; TMVA::Interval",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8cxx_source.html:83685,Usability,progress bar,progress bar,83685,"en signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::MinuitFitter/Fitter using MINUITDefinition MinuitFitter.h:48; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::PDF::kSpline2@ kSpline2Definition PDF.h:70; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Results::Storevoid Store(TObject *obj, const char *alias=nullptr)Definition Results.cxx:86; TMVA::Results::GetHistTH1 * GetHist(const TString &alias) constDefinition Results.cxx:136; TMVA::SimulatedAnnealingFitterFitter using a Simulated Annealing Algorithm.Definition SimulatedAnnealingFitter.h:49; TMVA::TSpline1Linear interpolation of TGraph.Definition TSpline1.h:43; TMVA::TimerTiming information for training and evaluation of MVA methods.Definition Timer.h:58; TMVA::Timer::DrawProgressBarvoid DrawProgressBar(Int_t, const TString &comment="""")draws progress bar in color or B&W caution:Definition Timer.cxx:202; TMVA::Tools::ParseFormatLineTList * ParseFormatLine(TString theString, const char *sep="":"")Parse the string and cut into labels separated by "":"".Definition Tools.cxx:401; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::ReadAttrvoid ReadAttr(void *node, const char *, T &value)read attribute from xmlDefinition Tools.h:329; TMVA::Tools::AddCommentBool_t AddComment(void *node, const char *comment)Definition Tools.cxx:1132; TMVA::Tools::GetChildvoid * GetChild(void *parent, const char *childname=nullptr)get child nodeDefinition Tools.cxx:1150; TMVA::Tools::AddAttrvoid AddAttr(void *node, const char *, const T &value, Int_t precision=16)add attribute to xmlDefinition Tools.h:347; TMVA::Tools::AddChildvoid * AddChild(void *parent, const char *childname, const char *content=nullptr, bool isRootNode=false)add child nodeDefinition Tools.cxx",MatchSource.WIKI,doc/master/MethodCuts_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8cxx_source.html
https://root.cern/doc/master/MethodCuts_8h.html:360,Integrability,depend,dependency,360,". ROOT: tmva/tmva/inc/TMVA/MethodCuts.h File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Namespaces ; MethodCuts.h File Reference. #include <vector>; #include ""TMVA/MethodBase.h""; #include ""TMVA/BinarySearchTree.h""; #include ""TMVA/PDF.h""; #include ""TMatrixDfwd.h""; #include ""IFitterTarget.h"". Include dependency graph for MethodCuts.h:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; class  TMVA::MethodCuts;  Multivariate optimisation of signal efficiency for given background efficiency, applying rectangular minimum and maximum requirements. More...;  . Namespaces; namespace  TMVA;  create variable transformations ;  . tmvatmvaincTMVAMethodCuts.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:25 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodCuts_8h.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h.html
https://root.cern/doc/master/MethodCuts_8h.html:857,Modifiability,variab,variable,857,". ROOT: tmva/tmva/inc/TMVA/MethodCuts.h File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Namespaces ; MethodCuts.h File Reference. #include <vector>; #include ""TMVA/MethodBase.h""; #include ""TMVA/BinarySearchTree.h""; #include ""TMVA/PDF.h""; #include ""TMatrixDfwd.h""; #include ""IFitterTarget.h"". Include dependency graph for MethodCuts.h:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; class  TMVA::MethodCuts;  Multivariate optimisation of signal efficiency for given background efficiency, applying rectangular minimum and maximum requirements. More...;  . Namespaces; namespace  TMVA;  create variable transformations ;  . tmvatmvaincTMVAMethodCuts.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:25 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodCuts_8h.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h.html
https://root.cern/doc/master/MethodCuts_8h_source.html:4760,Availability,avail,available,4760," 103 Double_t GetmuTransform ( TTree *) { return -1; }; 104 Double_t GetEfficiency ( const TString&, Types::ETreeType, Double_t& );; 105 Double_t GetTrainingEfficiency(const TString& );; 106 ; 107 // rarity distributions (signal or background (default) is uniform in [0,1]); 108 Double_t GetRarity( Double_t, Types::ESBType ) const { return 0; }; 109 ; 110 // accessors for Minuit; 111 Double_t ComputeEstimator( std::vector<Double_t> & );; 112 ; 113 Double_t EstimatorFunction( std::vector<Double_t> & );; 114 Double_t EstimatorFunction( Int_t ievt1, Int_t ievt2 );; 115 ; 116 void SetTestSignalEfficiency( Double_t effS ) { fTestSignalEff = effS; }; 117 ; 118 // retrieve cut values for given signal efficiency; 119 void PrintCuts( Double_t effS ) const;; 120 Double_t GetCuts ( Double_t effS, std::vector<Double_t>& cutMin, std::vector<Double_t>& cutMax ) const;; 121 Double_t GetCuts ( Double_t effS, Double_t* cutMin, Double_t* cutMax ) const;; 122 ; 123 // ranking of input variables (not available for cuts); 124 const Ranking* CreateRanking() { return nullptr; }; 125 ; 126 void DeclareOptions();; 127 void ProcessOptions();; 128 ; 129 // maximum |cut| value; 130 static const Double_t fgMaxAbsCutVal;; 131 ; 132 // no check of options at this place; 133 void CheckSetup() {}; 134 ; 135 protected:; 136 ; 137 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 138 void MakeClassSpecific( std::ostream&, const TString& ) const;; 139 ; 140 // get help message text; 141 void GetHelpMessage() const;; 142 ; 143 private:; 144 ; 145 // optimisation method; 146 enum EFitMethodType { kUseMonteCarlo = 0,; 147 kUseGeneticAlgorithm,; 148 kUseSimulatedAnnealing,; 149 kUseMinuit,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:412,Deployability,integrat,integrated,412,". ROOT: tmva/tmva/inc/TMVA/MethodCuts.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodCuts.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Matt Jachowski, Peter Speckmayer, Helge Voss, Kai Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodCuts *; 8 * *; 9 * *; 10 * Description: *; 11 * Multivariate optimisation of signal efficiency for given background *; 12 * efficiency, using rectangular minimum and maximum requirements on *; 13 * input variables *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 18 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * LAPP, Annecy, France *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodCuts; 34#define ROOT_TMVA_MethodCuts; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodCuts //; 39// //; 40// Multivariate optimisation of signal efficiency for given background //; 41// efficiency, using rectangular minimum and maximum requirements on //; 42// input variables //; 43// //; 44//////////////////////////////////////////////////////////////////////////; 45 ; 46#i",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:18416,Energy Efficiency,monitor,monitoring,18416,"for given cut sampleDefinition MethodCuts.cxx:1050; TMVA::MethodCuts::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constcreate XML description for LD classification and regression (for arbitrary number of output classes/t...Definition MethodCuts.cxx:1287; TMVA::MethodCuts::Initvoid Init(void)default initialisation called by all constructorsDefinition MethodCuts.cxx:220; TMVA::MethodCuts::GetTrainingEfficiencyDouble_t GetTrainingEfficiency(const TString &)Overloaded function to create background efficiency (rejection) versus signal efficiency plot (first ...Definition MethodCuts.cxx:1442; TMVA::MethodCuts::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)Cuts can only handle classification with 2 classes.Definition MethodCuts.cxx:211; TMVA::MethodCuts::ProcessOptionsvoid ProcessOptions()process user options.Definition MethodCuts.cxx:363; TMVA::MethodCuts::WriteMonitoringHistosToFilevoid WriteMonitoringHistosToFile(void) constwrite histograms and PDFs to file for monitoring purposesDefinition MethodCuts.cxx:1411; TMVA::MethodCuts::EEffMethodEEffMethodDefinition MethodCuts.h:157; TMVA::MethodCuts::kUsePDFs@ kUsePDFsDefinition MethodCuts.h:158; TMVA::MethodCuts::kUseEventSelection@ kUseEventSelectionDefinition MethodCuts.h:157; TMVA::MethodCuts::CheckSetupvoid CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodCuts.h:133; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(const std::vector< Double_t > &, Double_t *, Double_t *)translates parameters into cutsDefinition MethodCuts.cxx:974; TMVA::MethodCuts::~MethodCutsvirtual ~MethodCuts(void)destructorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMinuit@ kUseMinuitDefinition MethodC",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:412,Integrability,integrat,integrated,412,". ROOT: tmva/tmva/inc/TMVA/MethodCuts.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodCuts.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Matt Jachowski, Peter Speckmayer, Helge Voss, Kai Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodCuts *; 8 * *; 9 * *; 10 * Description: *; 11 * Multivariate optimisation of signal efficiency for given background *; 12 * efficiency, using rectangular minimum and maximum requirements on *; 13 * input variables *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 18 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * LAPP, Annecy, France *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodCuts; 34#define ROOT_TMVA_MethodCuts; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodCuts //; 39// //; 40// Multivariate optimisation of signal efficiency for given background //; 41// efficiency, using rectangular minimum and maximum requirements on //; 42// input variables //; 43// //; 44//////////////////////////////////////////////////////////////////////////; 45 ; 46#i",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:5269,Integrability,message,message,5269,"14 Double_t EstimatorFunction( Int_t ievt1, Int_t ievt2 );; 115 ; 116 void SetTestSignalEfficiency( Double_t effS ) { fTestSignalEff = effS; }; 117 ; 118 // retrieve cut values for given signal efficiency; 119 void PrintCuts( Double_t effS ) const;; 120 Double_t GetCuts ( Double_t effS, std::vector<Double_t>& cutMin, std::vector<Double_t>& cutMax ) const;; 121 Double_t GetCuts ( Double_t effS, Double_t* cutMin, Double_t* cutMax ) const;; 122 ; 123 // ranking of input variables (not available for cuts); 124 const Ranking* CreateRanking() { return nullptr; }; 125 ; 126 void DeclareOptions();; 127 void ProcessOptions();; 128 ; 129 // maximum |cut| value; 130 static const Double_t fgMaxAbsCutVal;; 131 ; 132 // no check of options at this place; 133 void CheckSetup() {}; 134 ; 135 protected:; 136 ; 137 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 138 void MakeClassSpecific( std::ostream&, const TString& ) const;; 139 ; 140 // get help message text; 141 void GetHelpMessage() const;; 142 ; 143 private:; 144 ; 145 // optimisation method; 146 enum EFitMethodType { kUseMonteCarlo = 0,; 147 kUseGeneticAlgorithm,; 148 kUseSimulatedAnnealing,; 149 kUseMinuit,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calc",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:14688,Integrability,message,message,14688,"TMVA::MethodCuts::fFitMethodEFitMethodType fFitMethodchosen fit methodDefinition MethodCuts.h:168; TMVA::MethodCuts::CreateRankingconst Ranking * CreateRanking()Definition MethodCuts.h:124; TMVA::MethodCuts::fNegEffWarningBool_t fNegEffWarningflag risen in case of negative efficiency warningDefinition MethodCuts.h:213; TMVA::MethodCuts::fEffSMinDouble_t fEffSMinused to test optimized signal efficiencyDefinition MethodCuts.h:173; TMVA::MethodCuts::fCutRangeMaxDouble_t * fCutRangeMaxmaximum of allowed cut rangeDefinition MethodCuts.h:176; TMVA::MethodCuts::GetSignificanceDouble_t GetSignificance(void) constcompute significance of mean differenceDefinition MethodCuts.h:102; TMVA::MethodCuts::MatchCutsToParsvoid MatchCutsToPars(std::vector< Double_t > &, Double_t *, Double_t *)translates cuts into parametersDefinition MethodCuts.cxx:1009; TMVA::MethodCuts::DynamicCaststatic MethodCuts * DynamicCast(IMethod *method)Definition MethodCuts.h:74; TMVA::MethodCuts::GetHelpMessagevoid GetHelpMessage() constget help message textDefinition MethodCuts.cxx:1719; TMVA::MethodCuts::Trainvoid Train(void)training method: here the cuts are optimised for the training sampleDefinition MethodCuts.cxx:578; TMVA::MethodCuts::GetRarityDouble_t GetRarity(Double_t, Types::ESBType) constcompute rarity:Definition MethodCuts.h:108; TMVA::MethodCuts::fgMaxAbsCutValstatic const Double_t fgMaxAbsCutValDefinition MethodCuts.h:130; TMVA::MethodCuts::fVarHistBstd::vector< TH1 * > * fVarHistBreference histograms (background)Definition MethodCuts.h:206; TMVA::MethodCuts::CreateVariablePDFsvoid CreateVariablePDFs(void)for PDF method: create efficiency reference histograms and PDFsDefinition MethodCuts.cxx:1106; TMVA::MethodCuts::fVarPdfBstd::vector< PDF * > * fVarPdfBreference PDFs (background)Definition MethodCuts.h:210; TMVA::MethodCuts::fEffMethodEEffMethod fEffMethodchosen efficiency calculation methodDefinition MethodCuts.h:170; TMVA::MethodCuts::fTmpCutMinDouble_t * fTmpCutMintemporary minimum requi",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:22208,Integrability,wrap,wrapper,22208,"MaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::fEffBvsSLocalTH1 * fEffBvsSLocalintermediate eff. background versus eff signal histoDefinition MethodCuts.h:202; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCuts::GetCutsDouble_t GetCuts(Double_t effS, std::vector< Double_t > &cutMin, std::vector< Double_t > &cutMax) constretrieve cut values for given signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Types::ESBTypeESBTypeDefinition Types.h:134; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TRandomThis is the base class for the ROOT Random number generators.Definition TRandom.h:27; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; double; int; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22. tmvatmvaincTMVAMethodCuts.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:708,Modifiability,variab,variables,708,". ROOT: tmva/tmva/inc/TMVA/MethodCuts.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodCuts.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Matt Jachowski, Peter Speckmayer, Helge Voss, Kai Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodCuts *; 8 * *; 9 * *; 10 * Description: *; 11 * Multivariate optimisation of signal efficiency for given background *; 12 * efficiency, using rectangular minimum and maximum requirements on *; 13 * input variables *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 18 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * LAPP, Annecy, France *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodCuts; 34#define ROOT_TMVA_MethodCuts; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodCuts //; 39// //; 40// Multivariate optimisation of signal efficiency for given background //; 41// efficiency, using rectangular minimum and maximum requirements on //; 42// input variables //; 43// //; 44//////////////////////////////////////////////////////////////////////////; 45 ; 46#i",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:1891,Modifiability,variab,variables,1891,"round *; 12 * efficiency, using rectangular minimum and maximum requirements on *; 13 * input variables *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Matt Jachowski <jachowski@stanford.edu> - Stanford University, USA *; 18 * Peter Speckmayer <speckmay@mail.cern.ch> - CERN, Switzerland *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * *; 22 * Copyright (c) 2005: *; 23 * CERN, Switzerland *; 24 * U. of Victoria, Canada *; 25 * MPI-K Heidelberg, Germany *; 26 * LAPP, Annecy, France *; 27 * *; 28 * Redistribution and use in source and binary forms, with or without *; 29 * modification, are permitted according to the terms listed in LICENSE *; 30 * (see tmva/doc/LICENSE) *; 31 **********************************************************************************/; 32 ; 33#ifndef ROOT_TMVA_MethodCuts; 34#define ROOT_TMVA_MethodCuts; 35 ; 36//////////////////////////////////////////////////////////////////////////; 37// //; 38// MethodCuts //; 39// //; 40// Multivariate optimisation of signal efficiency for given background //; 41// efficiency, using rectangular minimum and maximum requirements on //; 42// input variables //; 43// //; 44//////////////////////////////////////////////////////////////////////////; 45 ; 46#include <vector>; 47 ; 48 ; 49#include ""TMVA/MethodBase.h""; 50#include ""TMVA/BinarySearchTree.h""; 51#include ""TMVA/PDF.h""; 52#include ""TMatrixDfwd.h""; 53#include ""IFitterTarget.h""; 54 ; 55class TRandom;; 56 ; 57namespace TMVA {; 58 ; 59 class Interval;; 60 ; 61 class MethodCuts : public MethodBase, public IFitterTarget {; 62 ; 63 public:; 64 ; 65 MethodCuts( const TString& jobName,; 66 const TString& methodTitle,; 67 DataSetInfo& theData,; 68 const TString& theOption = ""MC:150:10000:"");; 69 ; 70 MethodCuts( DataSetInfo& theData,; 71 const TString& theWeightFile);; 72 ; 73 // this is a workaround which is ne",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:4745,Modifiability,variab,variables,4745,"{ return -1; }; 103 Double_t GetmuTransform ( TTree *) { return -1; }; 104 Double_t GetEfficiency ( const TString&, Types::ETreeType, Double_t& );; 105 Double_t GetTrainingEfficiency(const TString& );; 106 ; 107 // rarity distributions (signal or background (default) is uniform in [0,1]); 108 Double_t GetRarity( Double_t, Types::ESBType ) const { return 0; }; 109 ; 110 // accessors for Minuit; 111 Double_t ComputeEstimator( std::vector<Double_t> & );; 112 ; 113 Double_t EstimatorFunction( std::vector<Double_t> & );; 114 Double_t EstimatorFunction( Int_t ievt1, Int_t ievt2 );; 115 ; 116 void SetTestSignalEfficiency( Double_t effS ) { fTestSignalEff = effS; }; 117 ; 118 // retrieve cut values for given signal efficiency; 119 void PrintCuts( Double_t effS ) const;; 120 Double_t GetCuts ( Double_t effS, std::vector<Double_t>& cutMin, std::vector<Double_t>& cutMax ) const;; 121 Double_t GetCuts ( Double_t effS, Double_t* cutMin, Double_t* cutMax ) const;; 122 ; 123 // ranking of input variables (not available for cuts); 124 const Ranking* CreateRanking() { return nullptr; }; 125 ; 126 void DeclareOptions();; 127 void ProcessOptions();; 128 ; 129 // maximum |cut| value; 130 static const Double_t fgMaxAbsCutVal;; 131 ; 132 // no check of options at this place; 133 void CheckSetup() {}; 134 ; 135 protected:; 136 ; 137 // make ROOT-independent C++ class for classifier response (classifier-specific implementation); 138 void MakeClassSpecific( std::ostream&, const TString& ) const;; 139 ; 140 // get help message text; 141 void GetHelpMessage() const;; 142 ; 143 private:; 144 ; 145 // optimisation method; 146 enum EFitMethodType { kUseMonteCarlo = 0,; 147 kUseGeneticAlgorithm,; 148 kUseSimulatedAnnealing,; 149 kUseMinuit,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compu",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:7214,Modifiability,variab,variables,7214,"1 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // basic statistics; 197 std::vector<Double_t>* fMeanS; ///< means of variables (signal); 198 std::vector<Double_t>* fMeanB; ///< means of variables (background); 199 std::vector<Double_t>* fRmsS; ///< RMSs of variables (signal); 200 std::vector<Double_t>* fRmsB; ///< RMSs of variables (background); 201 ; 202 TH1* fEffBvsSLocal; ///< intermediate eff.",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:7630,Modifiability,variab,variables,7630,"1 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // basic statistics; 197 std::vector<Double_t>* fMeanS; ///< means of variables (signal); 198 std::vector<Double_t>* fMeanB; ///< means of variables (background); 199 std::vector<Double_t>* fRmsS; ///< RMSs of variables (signal); 200 std::vector<Double_t>* fRmsB; ///< RMSs of variables (background); 201 ; 202 TH1* fEffBvsSLocal; ///< intermediate eff.",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:7699,Modifiability,variab,variables,7699,"1 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // basic statistics; 197 std::vector<Double_t>* fMeanS; ///< means of variables (signal); 198 std::vector<Double_t>* fMeanB; ///< means of variables (background); 199 std::vector<Double_t>* fRmsS; ///< RMSs of variables (signal); 200 std::vector<Double_t>* fRmsB; ///< RMSs of variables (background); 201 ; 202 TH1* fEffBvsSLocal; ///< intermediate eff.",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:7770,Modifiability,variab,variables,7770,"1 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // basic statistics; 197 std::vector<Double_t>* fMeanS; ///< means of variables (signal); 198 std::vector<Double_t>* fMeanB; ///< means of variables (background); 199 std::vector<Double_t>* fRmsS; ///< RMSs of variables (signal); 200 std::vector<Double_t>* fRmsB; ///< RMSs of variables (background); 201 ; 202 TH1* fEffBvsSLocal; ///< intermediate eff.",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:7837,Modifiability,variab,variables,7837,"1 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // basic statistics; 197 std::vector<Double_t>* fMeanS; ///< means of variables (signal); 198 std::vector<Double_t>* fMeanB; ///< means of variables (background); 199 std::vector<Double_t>* fRmsS; ///< RMSs of variables (signal); 200 std::vector<Double_t>* fRmsB; ///< RMSs of variables (background); 201 ; 202 TH1* fEffBvsSLocal; ///< intermediate eff.",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:16312,Modifiability,variab,variables,16312,"ficiency reference histograms and PDFsDefinition MethodCuts.cxx:1106; TMVA::MethodCuts::fVarPdfBstd::vector< PDF * > * fVarPdfBreference PDFs (background)Definition MethodCuts.h:210; TMVA::MethodCuts::fEffMethodEEffMethod fEffMethodchosen efficiency calculation methodDefinition MethodCuts.h:170; TMVA::MethodCuts::fTmpCutMinDouble_t * fTmpCutMintemporary minimum requirementDefinition MethodCuts.h:186; TMVA::MethodCuts::GetMvaValueDouble_t GetMvaValue(Double_t *err=nullptr, Double_t *errUpper=nullptr)cut evaluation: returns 1.0 if event passed, 0.0 otherwiseDefinition MethodCuts.cxx:432; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(PDF *=nullptr, PDF *=nullptr) constcompute ""separation"" defined asDefinition MethodCuts.h:101; TMVA::MethodCuts::fCutMinDouble_t ** fCutMinminimum requirementDefinition MethodCuts.h:184; TMVA::MethodCuts::fVarHistSstd::vector< TH1 * > * fVarHistSreference histograms (signal)Definition MethodCuts.h:205; TMVA::MethodCuts::fRmsBstd::vector< Double_t > * fRmsBRMSs of variables (background)Definition MethodCuts.h:200; TMVA::MethodCuts::fEffSMaxDouble_t fEffSMaxused to test optimized signal efficiencyDefinition MethodCuts.h:174; TMVA::MethodCuts::EFitParametersEFitParametersDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMax@ kForceMaxDefinition MethodCuts.h:163; TMVA::MethodCuts::kNotEnforced@ kNotEnforcedDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMin@ kForceMinDefinition MethodCuts.h:162; TMVA::MethodCuts::kForceSmart@ kForceSmartDefinition MethodCuts.h:164; TMVA::MethodCuts::fVarHistB_smoothstd::vector< TH1 * > * fVarHistB_smoothsmoothed reference histograms (background)Definition MethodCuts.h:208; TMVA::MethodCuts::GetmuTransformDouble_t GetmuTransform(TTree *)Definition MethodCuts.h:103; TMVA::MethodCuts::fVarPdfSstd::vector< PDF * > * fVarPdfSreference PDFs (signal)Definition MethodCuts.h:209; TMVA::MethodCuts::GetEffsfromSelectionvoid GetEffsfromSelection(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20065,Modifiability,variab,variables,20065,"::~MethodCutsvirtual ~MethodCuts(void)destructorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMinuit@ kUseMinuitDefinition MethodCuts.h:149; TMVA::MethodCuts::kUseEventScan@ kUseEventScanDefinition MethodCuts.h:150; TMVA::MethodCuts::kUseSimulatedAnnealing@ kUseSimulatedAnnealingDefinition MethodCuts.h:148; TMVA::MethodCuts::kUseGeneticAlgorithm@ kUseGeneticAlgorithmDefinition MethodCuts.h:147; TMVA::MethodCuts::kUseMonteCarlo@ kUseMonteCarloDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMonteCarloEvents@ kUseMonteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCu",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20177,Modifiability,variab,variables,20177,"tionvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMinuit@ kUseMinuitDefinition MethodCuts.h:149; TMVA::MethodCuts::kUseEventScan@ kUseEventScanDefinition MethodCuts.h:150; TMVA::MethodCuts::kUseSimulatedAnnealing@ kUseSimulatedAnnealingDefinition MethodCuts.h:148; TMVA::MethodCuts::kUseGeneticAlgorithm@ kUseGeneticAlgorithmDefinition MethodCuts.h:147; TMVA::MethodCuts::kUseMonteCarlo@ kUseMonteCarloDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMonteCarloEvents@ kUseMonteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histogra",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20289,Modifiability,variab,variables,20289,"hodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMinuit@ kUseMinuitDefinition MethodCuts.h:149; TMVA::MethodCuts::kUseEventScan@ kUseEventScanDefinition MethodCuts.h:150; TMVA::MethodCuts::kUseSimulatedAnnealing@ kUseSimulatedAnnealingDefinition MethodCuts.h:148; TMVA::MethodCuts::kUseGeneticAlgorithm@ kUseGeneticAlgorithmDefinition MethodCuts.h:147; TMVA::MethodCuts::kUseMonteCarlo@ kUseMonteCarloDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMonteCarloEvents@ kUseMonteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20402,Modifiability,variab,variablesDefinition,20402,"A::MethodCuts::kUseEventScan@ kUseEventScanDefinition MethodCuts.h:150; TMVA::MethodCuts::kUseSimulatedAnnealing@ kUseSimulatedAnnealingDefinition MethodCuts.h:148; TMVA::MethodCuts::kUseGeneticAlgorithm@ kUseGeneticAlgorithmDefinition MethodCuts.h:147; TMVA::MethodCuts::kUseMonteCarlo@ kUseMonteCarloDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMonteCarloEvents@ kUseMonteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185;",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:22317,Modifiability,variab,variables,22317,"MaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::fEffBvsSLocalTH1 * fEffBvsSLocalintermediate eff. background versus eff signal histoDefinition MethodCuts.h:202; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCuts::GetCutsDouble_t GetCuts(Double_t effS, std::vector< Double_t > &cutMin, std::vector< Double_t > &cutMax) constretrieve cut values for given signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Types::ESBTypeESBTypeDefinition Types.h:134; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TRandomThis is the base class for the ROOT Random number generators.Definition TRandom.h:27; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; double; int; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22. tmvatmvaincTMVAMethodCuts.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:22788,Modifiability,variab,variable,22788,"MaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::fEffBvsSLocalTH1 * fEffBvsSLocalintermediate eff. background versus eff signal histoDefinition MethodCuts.h:202; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from PDFs for given cut sampleDefinition MethodCuts.cxx:1023; TMVA::MethodCuts::GetCutsDouble_t GetCuts(Double_t effS, std::vector< Double_t > &cutMin, std::vector< Double_t > &cutMax) constretrieve cut values for given signal efficiencyDefinition MethodCuts.cxx:551; TMVA::MethodCuts::PrintCutsvoid PrintCuts(Double_t effS) constprint cutsDefinition MethodCuts.cxx:465; TMVA::PDFPDF wrapper for histograms; uses user-defined spline interpolation.Definition PDF.h:63; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Types::ESBTypeESBTypeDefinition Types.h:134; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TRandomThis is the base class for the ROOT Random number generators.Definition TRandom.h:27; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; double; int; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22. tmvatmvaincTMVAMethodCuts.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6420,Performance,optimiz,optimized,6420,"eGeneticAlgorithm,; 148 kUseSimulatedAnnealing,; 149 kUseMinuit,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< us",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6490,Performance,optimiz,optimized,6490,"kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRan",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6560,Performance,optimiz,optimized,6560,"alculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 196 // ba",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:14045,Performance,optimiz,optimized,14045,"ts::SetTestSignalEfficiencyvoid SetTestSignalEfficiency(Double_t effS)Definition MethodCuts.h:116; TMVA::MethodCuts::fRangeSignstd::vector< Int_t > * fRangeSignused to match cuts to fit parameters (and vice versa)Definition MethodCuts.h:193; TMVA::MethodCuts::fNparInt_t fNparnumber of parameters in fit (default: 2*Nvar)Definition MethodCuts.h:191; TMVA::MethodCuts::DeclareOptionsvoid DeclareOptions()define the options (their key words) that can be set in the option string.Definition MethodCuts.cxx:319; TMVA::MethodCuts::fEffMethodSTString fEffMethodSchosen efficiency calculation method (string)Definition MethodCuts.h:169; TMVA::MethodCuts::fFitMethodEFitMethodType fFitMethodchosen fit methodDefinition MethodCuts.h:168; TMVA::MethodCuts::CreateRankingconst Ranking * CreateRanking()Definition MethodCuts.h:124; TMVA::MethodCuts::fNegEffWarningBool_t fNegEffWarningflag risen in case of negative efficiency warningDefinition MethodCuts.h:213; TMVA::MethodCuts::fEffSMinDouble_t fEffSMinused to test optimized signal efficiencyDefinition MethodCuts.h:173; TMVA::MethodCuts::fCutRangeMaxDouble_t * fCutRangeMaxmaximum of allowed cut rangeDefinition MethodCuts.h:176; TMVA::MethodCuts::GetSignificanceDouble_t GetSignificance(void) constcompute significance of mean differenceDefinition MethodCuts.h:102; TMVA::MethodCuts::MatchCutsToParsvoid MatchCutsToPars(std::vector< Double_t > &, Double_t *, Double_t *)translates cuts into parametersDefinition MethodCuts.cxx:1009; TMVA::MethodCuts::DynamicCaststatic MethodCuts * DynamicCast(IMethod *method)Definition MethodCuts.h:74; TMVA::MethodCuts::GetHelpMessagevoid GetHelpMessage() constget help message textDefinition MethodCuts.cxx:1719; TMVA::MethodCuts::Trainvoid Train(void)training method: here the cuts are optimised for the training sampleDefinition MethodCuts.cxx:578; TMVA::MethodCuts::GetRarityDouble_t GetRarity(Double_t, Types::ESBType) constcompute rarity:Definition MethodCuts.h:108; TMVA::MethodCuts::fgMaxAbsCutValstatic const Do",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:16419,Performance,optimiz,optimized,16419,"* > * fVarPdfBreference PDFs (background)Definition MethodCuts.h:210; TMVA::MethodCuts::fEffMethodEEffMethod fEffMethodchosen efficiency calculation methodDefinition MethodCuts.h:170; TMVA::MethodCuts::fTmpCutMinDouble_t * fTmpCutMintemporary minimum requirementDefinition MethodCuts.h:186; TMVA::MethodCuts::GetMvaValueDouble_t GetMvaValue(Double_t *err=nullptr, Double_t *errUpper=nullptr)cut evaluation: returns 1.0 if event passed, 0.0 otherwiseDefinition MethodCuts.cxx:432; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(PDF *=nullptr, PDF *=nullptr) constcompute ""separation"" defined asDefinition MethodCuts.h:101; TMVA::MethodCuts::fCutMinDouble_t ** fCutMinminimum requirementDefinition MethodCuts.h:184; TMVA::MethodCuts::fVarHistSstd::vector< TH1 * > * fVarHistSreference histograms (signal)Definition MethodCuts.h:205; TMVA::MethodCuts::fRmsBstd::vector< Double_t > * fRmsBRMSs of variables (background)Definition MethodCuts.h:200; TMVA::MethodCuts::fEffSMaxDouble_t fEffSMaxused to test optimized signal efficiencyDefinition MethodCuts.h:174; TMVA::MethodCuts::EFitParametersEFitParametersDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMax@ kForceMaxDefinition MethodCuts.h:163; TMVA::MethodCuts::kNotEnforced@ kNotEnforcedDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMin@ kForceMinDefinition MethodCuts.h:162; TMVA::MethodCuts::kForceSmart@ kForceSmartDefinition MethodCuts.h:164; TMVA::MethodCuts::fVarHistB_smoothstd::vector< TH1 * > * fVarHistB_smoothsmoothed reference histograms (background)Definition MethodCuts.h:208; TMVA::MethodCuts::GetmuTransformDouble_t GetmuTransform(TTree *)Definition MethodCuts.h:103; TMVA::MethodCuts::fVarPdfSstd::vector< PDF * > * fVarPdfSreference PDFs (signal)Definition MethodCuts.h:209; TMVA::MethodCuts::GetEffsfromSelectionvoid GetEffsfromSelection(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from event counting for given cut sampleDefinition MethodCuts.cx",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20772,Performance,optimiz,optimized,20772,"onteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::fEffBvsSLocalTH1 * fEffBvsSLocalintermediate eff. background versus eff signal histoDefinition MethodCuts.h:202; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:4125,Security,access,accessors,4125,"m & i );; 88 void ReadWeightsFromXML ( void* wghtnode );; 89 ; 90 // calculate the MVA value (for CUTs this is just a dummy); 91 Double_t GetMvaValue( Double_t* err = nullptr, Double_t* errUpper = nullptr );; 92 ; 93 // write method specific histos to target file; 94 void WriteMonitoringHistosToFile( void ) const;; 95 ; 96 // test the method; 97 void TestClassification();; 98 ; 99 // also overwrite --> not computed for cuts; 100 Double_t GetSeparation ( TH1*, TH1* ) const { return -1; }; 101 Double_t GetSeparation ( PDF* = nullptr, PDF* = nullptr ) const { return -1; }; 102 Double_t GetSignificance( void ) const { return -1; }; 103 Double_t GetmuTransform ( TTree *) { return -1; }; 104 Double_t GetEfficiency ( const TString&, Types::ETreeType, Double_t& );; 105 Double_t GetTrainingEfficiency(const TString& );; 106 ; 107 // rarity distributions (signal or background (default) is uniform in [0,1]); 108 Double_t GetRarity( Double_t, Types::ESBType ) const { return 0; }; 109 ; 110 // accessors for Minuit; 111 Double_t ComputeEstimator( std::vector<Double_t> & );; 112 ; 113 Double_t EstimatorFunction( std::vector<Double_t> & );; 114 Double_t EstimatorFunction( Int_t ievt1, Int_t ievt2 );; 115 ; 116 void SetTestSignalEfficiency( Double_t effS ) { fTestSignalEff = effS; }; 117 ; 118 // retrieve cut values for given signal efficiency; 119 void PrintCuts( Double_t effS ) const;; 120 Double_t GetCuts ( Double_t effS, std::vector<Double_t>& cutMin, std::vector<Double_t>& cutMax ) const;; 121 Double_t GetCuts ( Double_t effS, Double_t* cutMin, Double_t* cutMax ) const;; 122 ; 123 // ranking of input variables (not available for cuts); 124 const Ranking* CreateRanking() { return nullptr; }; 125 ; 126 void DeclareOptions();; 127 void ProcessOptions();; 128 ; 129 // maximum |cut| value; 130 static const Double_t fgMaxAbsCutVal;; 131 ; 132 // no check of options at this place; 133 void CheckSetup() {}; 134 ; 135 protected:; 136 ; 137 // make ROOT-independent C++ class for classifie",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:3458,Testability,test,test,3458,"& theOption = ""MC:150:10000:"");; 69 ; 70 MethodCuts( DataSetInfo& theData,; 71 const TString& theWeightFile);; 72 ; 73 // this is a workaround which is necessary since CINT is not capable of handling dynamic casts; 74 static MethodCuts* DynamicCast( IMethod* method ) { return dynamic_cast<MethodCuts*>(method); }; 75 ; 76 virtual ~MethodCuts( void );; 77 ; 78 virtual Bool_t HasAnalysisType( Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets );; 79 ; 80 // training method; 81 void Train( void );; 82 ; 83 using MethodBase::ReadWeightsFromStream;; 84 ; 85 void AddWeightsXMLTo ( void* parent ) const;; 86 ; 87 void ReadWeightsFromStream( std::istream & i );; 88 void ReadWeightsFromXML ( void* wghtnode );; 89 ; 90 // calculate the MVA value (for CUTs this is just a dummy); 91 Double_t GetMvaValue( Double_t* err = nullptr, Double_t* errUpper = nullptr );; 92 ; 93 // write method specific histos to target file; 94 void WriteMonitoringHistosToFile( void ) const;; 95 ; 96 // test the method; 97 void TestClassification();; 98 ; 99 // also overwrite --> not computed for cuts; 100 Double_t GetSeparation ( TH1*, TH1* ) const { return -1; }; 101 Double_t GetSeparation ( PDF* = nullptr, PDF* = nullptr ) const { return -1; }; 102 Double_t GetSignificance( void ) const { return -1; }; 103 Double_t GetmuTransform ( TTree *) { return -1; }; 104 Double_t GetEfficiency ( const TString&, Types::ETreeType, Double_t& );; 105 Double_t GetTrainingEfficiency(const TString& );; 106 ; 107 // rarity distributions (signal or background (default) is uniform in [0,1]); 108 Double_t GetRarity( Double_t, Types::ESBType ) const { return 0; }; 109 ; 110 // accessors for Minuit; 111 Double_t ComputeEstimator( std::vector<Double_t> & );; 112 ; 113 Double_t EstimatorFunction( std::vector<Double_t> & );; 114 Double_t EstimatorFunction( Int_t ievt1, Int_t ievt2 );; 115 ; 116 void SetTestSignalEfficiency( Double_t effS ) { fTestSignalEff = effS; }; 117 ; 118 // retrieve cut values for given s",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6415,Testability,test,test,6415," 147 kUseGeneticAlgorithm,; 148 kUseSimulatedAnnealing,; 149 kUseMinuit,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6485,Testability,test,test,6485,"t,; 150 kUseEventScan,; 151 kUseMonteCarloEvents };; 152 ; 153 // efficiency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRando",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:6555,Testability,test,test,6555,"ciency calculation method; 154 // - kUseEventSelection: computes efficiencies from given data sample; 155 // - kUsePDFs : creates smoothed PDFs from data samples, and; 156 // uses this to compute efficiencies; 157 enum EEffMethod { kUseEventSelection = 0,; 158 kUsePDFs };; 159 ; 160 // improve the Monte Carlo by providing some additional information; 161 enum EFitParameters { kNotEnforced = 0,; 162 kForceMin,; 163 kForceMax,; 164 kForceSmart };; 165 ; 166 // general; 167 TString fFitMethodS; ///< chosen fit method (string); 168 EFitMethodType fFitMethod; ///< chosen fit method; 169 TString fEffMethodS; ///< chosen efficiency calculation method (string); 170 EEffMethod fEffMethod; ///< chosen efficiency calculation method; 171 std::vector<EFitParameters>* fFitParams; ///< vector for series of fit methods; 172 Double_t fTestSignalEff; ///< used to test optimized signal efficiency; 173 Double_t fEffSMin; ///< used to test optimized signal efficiency; 174 Double_t fEffSMax; ///< used to test optimized signal efficiency; 175 Double_t* fCutRangeMin; ///< minimum of allowed cut range; 176 Double_t* fCutRangeMax; ///< maximum of allowed cut range; 177 std::vector<Interval*> fCutRange; ///< allowed ranges for cut optimisation; 178 ; 179 // for the use of the binary tree method; 180 BinarySearchTree* fBinaryTreeS;; 181 BinarySearchTree* fBinaryTreeB;; 182 ; 183 // MC method; 184 Double_t** fCutMin; ///< minimum requirement; 185 Double_t** fCutMax; ///< maximum requirement; 186 Double_t* fTmpCutMin; ///< temporary minimum requirement; 187 Double_t* fTmpCutMax; ///< temporary maximum requirement; 188 TString* fAllVarsI; ///< what to do with variables; 189 ; 190 // relevant for all methods; 191 Int_t fNpar; ///< number of parameters in fit (default: 2*Nvar); 192 Double_t fEffRef; ///< reference efficiency; 193 std::vector<Int_t>* fRangeSign; ///< used to match cuts to fit parameters (and vice versa); 194 TRandom* fRandom; ///< random generator for MC optimisation method; 195 ; 19",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:14040,Testability,test,test,14040,"ts::SetTestSignalEfficiencyvoid SetTestSignalEfficiency(Double_t effS)Definition MethodCuts.h:116; TMVA::MethodCuts::fRangeSignstd::vector< Int_t > * fRangeSignused to match cuts to fit parameters (and vice versa)Definition MethodCuts.h:193; TMVA::MethodCuts::fNparInt_t fNparnumber of parameters in fit (default: 2*Nvar)Definition MethodCuts.h:191; TMVA::MethodCuts::DeclareOptionsvoid DeclareOptions()define the options (their key words) that can be set in the option string.Definition MethodCuts.cxx:319; TMVA::MethodCuts::fEffMethodSTString fEffMethodSchosen efficiency calculation method (string)Definition MethodCuts.h:169; TMVA::MethodCuts::fFitMethodEFitMethodType fFitMethodchosen fit methodDefinition MethodCuts.h:168; TMVA::MethodCuts::CreateRankingconst Ranking * CreateRanking()Definition MethodCuts.h:124; TMVA::MethodCuts::fNegEffWarningBool_t fNegEffWarningflag risen in case of negative efficiency warningDefinition MethodCuts.h:213; TMVA::MethodCuts::fEffSMinDouble_t fEffSMinused to test optimized signal efficiencyDefinition MethodCuts.h:173; TMVA::MethodCuts::fCutRangeMaxDouble_t * fCutRangeMaxmaximum of allowed cut rangeDefinition MethodCuts.h:176; TMVA::MethodCuts::GetSignificanceDouble_t GetSignificance(void) constcompute significance of mean differenceDefinition MethodCuts.h:102; TMVA::MethodCuts::MatchCutsToParsvoid MatchCutsToPars(std::vector< Double_t > &, Double_t *, Double_t *)translates cuts into parametersDefinition MethodCuts.cxx:1009; TMVA::MethodCuts::DynamicCaststatic MethodCuts * DynamicCast(IMethod *method)Definition MethodCuts.h:74; TMVA::MethodCuts::GetHelpMessagevoid GetHelpMessage() constget help message textDefinition MethodCuts.cxx:1719; TMVA::MethodCuts::Trainvoid Train(void)training method: here the cuts are optimised for the training sampleDefinition MethodCuts.cxx:578; TMVA::MethodCuts::GetRarityDouble_t GetRarity(Double_t, Types::ESBType) constcompute rarity:Definition MethodCuts.h:108; TMVA::MethodCuts::fgMaxAbsCutValstatic const Do",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:16414,Testability,test,test,16414,"* > * fVarPdfBreference PDFs (background)Definition MethodCuts.h:210; TMVA::MethodCuts::fEffMethodEEffMethod fEffMethodchosen efficiency calculation methodDefinition MethodCuts.h:170; TMVA::MethodCuts::fTmpCutMinDouble_t * fTmpCutMintemporary minimum requirementDefinition MethodCuts.h:186; TMVA::MethodCuts::GetMvaValueDouble_t GetMvaValue(Double_t *err=nullptr, Double_t *errUpper=nullptr)cut evaluation: returns 1.0 if event passed, 0.0 otherwiseDefinition MethodCuts.cxx:432; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(PDF *=nullptr, PDF *=nullptr) constcompute ""separation"" defined asDefinition MethodCuts.h:101; TMVA::MethodCuts::fCutMinDouble_t ** fCutMinminimum requirementDefinition MethodCuts.h:184; TMVA::MethodCuts::fVarHistSstd::vector< TH1 * > * fVarHistSreference histograms (signal)Definition MethodCuts.h:205; TMVA::MethodCuts::fRmsBstd::vector< Double_t > * fRmsBRMSs of variables (background)Definition MethodCuts.h:200; TMVA::MethodCuts::fEffSMaxDouble_t fEffSMaxused to test optimized signal efficiencyDefinition MethodCuts.h:174; TMVA::MethodCuts::EFitParametersEFitParametersDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMax@ kForceMaxDefinition MethodCuts.h:163; TMVA::MethodCuts::kNotEnforced@ kNotEnforcedDefinition MethodCuts.h:161; TMVA::MethodCuts::kForceMin@ kForceMinDefinition MethodCuts.h:162; TMVA::MethodCuts::kForceSmart@ kForceSmartDefinition MethodCuts.h:164; TMVA::MethodCuts::fVarHistB_smoothstd::vector< TH1 * > * fVarHistB_smoothsmoothed reference histograms (background)Definition MethodCuts.h:208; TMVA::MethodCuts::GetmuTransformDouble_t GetmuTransform(TTree *)Definition MethodCuts.h:103; TMVA::MethodCuts::fVarPdfSstd::vector< PDF * > * fVarPdfSreference PDFs (signal)Definition MethodCuts.h:209; TMVA::MethodCuts::GetEffsfromSelectionvoid GetEffsfromSelection(Double_t *cutMin, Double_t *cutMax, Double_t &effS, Double_t &effB)compute signal and background efficiencies from event counting for given cut sampleDefinition MethodCuts.cx",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:19201,Testability,test,testDefinition,19201,"s.cxx:211; TMVA::MethodCuts::ProcessOptionsvoid ProcessOptions()process user options.Definition MethodCuts.cxx:363; TMVA::MethodCuts::WriteMonitoringHistosToFilevoid WriteMonitoringHistosToFile(void) constwrite histograms and PDFs to file for monitoring purposesDefinition MethodCuts.cxx:1411; TMVA::MethodCuts::EEffMethodEEffMethodDefinition MethodCuts.h:157; TMVA::MethodCuts::kUsePDFs@ kUsePDFsDefinition MethodCuts.h:158; TMVA::MethodCuts::kUseEventSelection@ kUseEventSelectionDefinition MethodCuts.h:157; TMVA::MethodCuts::CheckSetupvoid CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodCuts.h:133; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(const std::vector< Double_t > &, Double_t *, Double_t *)translates parameters into cutsDefinition MethodCuts.cxx:974; TMVA::MethodCuts::~MethodCutsvirtual ~MethodCuts(void)destructorDefinition MethodCuts.cxx:270; TMVA::MethodCuts::TestClassificationvoid TestClassification()nothing to testDefinition MethodCuts.cxx:827; TMVA::MethodCuts::EFitMethodTypeEFitMethodTypeDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMinuit@ kUseMinuitDefinition MethodCuts.h:149; TMVA::MethodCuts::kUseEventScan@ kUseEventScanDefinition MethodCuts.h:150; TMVA::MethodCuts::kUseSimulatedAnnealing@ kUseSimulatedAnnealingDefinition MethodCuts.h:148; TMVA::MethodCuts::kUseGeneticAlgorithm@ kUseGeneticAlgorithmDefinition MethodCuts.h:147; TMVA::MethodCuts::kUseMonteCarlo@ kUseMonteCarloDefinition MethodCuts.h:146; TMVA::MethodCuts::kUseMonteCarloEvents@ kUseMonteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:20767,Testability,test,test,20767,"onteCarloEventsDefinition MethodCuts.h:151; TMVA::MethodCuts::fCutRangeMinDouble_t * fCutRangeMinminimum of allowed cut rangeDefinition MethodCuts.h:175; TMVA::MethodCuts::fBinaryTreeBBinarySearchTree * fBinaryTreeBDefinition MethodCuts.h:181; TMVA::MethodCuts::fRmsSstd::vector< Double_t > * fRmsSRMSs of variables (signal)Definition MethodCuts.h:199; TMVA::MethodCuts::fMeanSstd::vector< Double_t > * fMeanSmeans of variables (signal)Definition MethodCuts.h:197; TMVA::MethodCuts::fMeanBstd::vector< Double_t > * fMeanBmeans of variables (background)Definition MethodCuts.h:198; TMVA::MethodCuts::fAllVarsITString * fAllVarsIwhat to do with variablesDefinition MethodCuts.h:188; TMVA::MethodCuts::fFitParamsstd::vector< EFitParameters > * fFitParamsvector for series of fit methodsDefinition MethodCuts.h:171; TMVA::MethodCuts::GetSeparationDouble_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodCuts.h:100; TMVA::MethodCuts::fTestSignalEffDouble_t fTestSignalEffused to test optimized signal efficiencyDefinition MethodCuts.h:172; TMVA::MethodCuts::fCutRangestd::vector< Interval * > fCutRangeallowed ranges for cut optimisationDefinition MethodCuts.h:177; TMVA::MethodCuts::fTmpCutMaxDouble_t * fTmpCutMaxtemporary maximum requirementDefinition MethodCuts.h:187; TMVA::MethodCuts::fVarHistS_smoothstd::vector< TH1 * > * fVarHistS_smoothsmoothed reference histograms (signal)Definition MethodCuts.h:207; TMVA::MethodCuts::MatchParsToCutsvoid MatchParsToCuts(Double_t *, Double_t *, Double_t *); TMVA::MethodCuts::fCutMaxDouble_t ** fCutMaxmaximum requirementDefinition MethodCuts.h:185; TMVA::MethodCuts::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)read coefficients from xml weight fileDefinition MethodCuts.cxx:1327; TMVA::MethodCuts::fEffBvsSLocalTH1 * fEffBvsSLocalintermediate eff. background versus eff signal histoDefinition MethodCuts.h:202; TMVA::MethodCuts::GetEffsfromPDFsvoid GetEffsfromPDFs(Double_t *cutMin, Double_t *cutMax, Double_t ",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodCuts_8h_source.html:10999,Usability,simpl,simple,10999,"Def(name, id)Definition Rtypes.h:342; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TMatrixDfwd.h; TH1TH1 is the base class of all histogram classes in ROOT.Definition TH1.h:59; TMVA::BinarySearchTreeA simple Binary search tree including a volume search method.Definition BinarySearchTree.h:65; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::IFitterTargetInterface for a fitter 'target'.Definition IFitterTarget.h:44; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::MethodCutsfriend class MethodCutsDefinition MethodBase.h:603; TMVA::MethodBase::ReadWeightsFromStreamvirtual void ReadWeightsFromStream(std::istream &)=0; TMVA::MethodCutsMultivariate optimisation of signal efficiency for given background efficiency, applying rectangular ...Definition MethodCuts.h:61; TMVA::MethodCuts::fRandomTRandom * fRandomrandom generator for MC optimisation methodDefinition MethodCuts.h:194; TMVA::MethodCuts::fEffRefDouble_t fEffRefreference efficiencyDefinition MethodCuts.h:192; TMVA::MethodCuts::fFitMethodSTString fFitMethodSc",MatchSource.WIKI,doc/master/MethodCuts_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodCuts_8h_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:6039,Availability,error,error,6039,"s = parseString.Tokenize(tokenDelim);; 154 TIter nextToken(tokenStrings);; 155 TObjString *tokenString = (TObjString *)nextToken();; 156 for (; tokenString != NULL; tokenString = (TObjString *)nextToken()) {; 157 std::stringstream sstr;; 158 double currentValue;; 159 sstr << tokenString->GetString().Data();; 160 sstr >> currentValue;; 161 values.push_back(currentValue);; 162 }; 163 return values;; 164}; 165 ; 166////////////////////////////////////////////////////////////////////////////////; 167void MethodDL::DeclareOptions(); 168{; 169 // Set default values for all option strings; 170 ; 171 DeclareOptionRef(fInputLayoutString = ""0|0|0"", ""InputLayout"", ""The Layout of the input"");; 172 ; 173 DeclareOptionRef(fBatchLayoutString = ""0|0|0"", ""BatchLayout"", ""The Layout of the batch"");; 174 ; 175 DeclareOptionRef(fLayoutString = ""DENSE|(N+100)*2|SOFTSIGN,DENSE|0|LINEAR"", ""Layout"", ""Layout of the network."");; 176 ; 177 DeclareOptionRef(fErrorStrategy = ""CROSSENTROPY"", ""ErrorStrategy"", ""Loss function: Mean squared error (regression)""; 178 "" or cross entropy (binary classification)."");; 179 AddPreDefVal(TString(""CROSSENTROPY""));; 180 AddPreDefVal(TString(""SUMOFSQUARES""));; 181 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 182 ; 183 DeclareOptionRef(fWeightInitializationString = ""XAVIER"", ""WeightInitialization"", ""Weight initialization strategy"");; 184 AddPreDefVal(TString(""XAVIER""));; 185 AddPreDefVal(TString(""XAVIERUNIFORM""));; 186 AddPreDefVal(TString(""GAUSS""));; 187 AddPreDefVal(TString(""UNIFORM""));; 188 AddPreDefVal(TString(""IDENTITY""));; 189 AddPreDefVal(TString(""ZERO""));; 190 ; 191 DeclareOptionRef(fRandomSeed = 0, ""RandomSeed"", ""Random seed used for weight initialization and batch shuffling"");; 192 ; 193 DeclareOptionRef(fNumValidationString = ""20%"", ""ValidationSize"", ""Part of the training data to use for validation. ""; 194 ""Specify as 0.2 or 20% to use a fifth of the data set as validation set. ""; 195 ""Specify as 100 to use exactly 100 events. (Default: 20%)"");; 196 ; 197",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:7426,Availability,avail,available,7426,"n strategy"");; 184 AddPreDefVal(TString(""XAVIER""));; 185 AddPreDefVal(TString(""XAVIERUNIFORM""));; 186 AddPreDefVal(TString(""GAUSS""));; 187 AddPreDefVal(TString(""UNIFORM""));; 188 AddPreDefVal(TString(""IDENTITY""));; 189 AddPreDefVal(TString(""ZERO""));; 190 ; 191 DeclareOptionRef(fRandomSeed = 0, ""RandomSeed"", ""Random seed used for weight initialization and batch shuffling"");; 192 ; 193 DeclareOptionRef(fNumValidationString = ""20%"", ""ValidationSize"", ""Part of the training data to use for validation. ""; 194 ""Specify as 0.2 or 20% to use a fifth of the data set as validation set. ""; 195 ""Specify as 100 to use exactly 100 events. (Default: 20%)"");; 196 ; 197 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 198 AddPreDefVal(TString(""STANDARD"")); // deprecated and not supported anymore; 199 AddPreDefVal(TString(""CPU""));; 200 AddPreDefVal(TString(""GPU""));; 201 AddPreDefVal(TString(""OPENCL"")); // not yet implemented; 202 AddPreDefVal(TString(""CUDNN"")); // not needed (by default GPU is now CUDNN if available); 203 ; 204 // define training strategy separated by a separator ""|""; 205 DeclareOptionRef(fTrainingStrategyString = ""LearningRate=1e-3,""; 206 ""Momentum=0.0,""; 207 ""ConvergenceSteps=100,""; 208 ""MaxEpochs=2000,""; 209 ""Optimizer=ADAM,""; 210 ""BatchSize=30,""; 211 ""TestRepetitions=1,""; 212 ""WeightDecay=0.0,""; 213 ""Regularization=None,""; 214 ""DropConfig=0.0"",; 215 ""TrainingStrategy"", ""Defines the training strategies."");; 216}; 217 ; 218////////////////////////////////////////////////////////////////////////////////; 219void MethodDL::ProcessOptions(); 220{; 221 ; 222 if (IgnoreEventsWithNegWeightsInTraining()) {; 223 Log() << kINFO << ""Will ignore negative events in training!"" << Endl;; 224 }; 225 ; 226 if (fArchitectureString == ""STANDARD"") {; 227 Log() << kWARNING << ""The STANDARD architecture is not supported anymore. ""; 228 ""Please use Architecture=CPU or Architecture=CPU.""; 229 ""See the TMVA Users' Guide for ins",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:8931,Availability,avail,available,8931,"ignore negative events in training!"" << Endl;; 224 }; 225 ; 226 if (fArchitectureString == ""STANDARD"") {; 227 Log() << kWARNING << ""The STANDARD architecture is not supported anymore. ""; 228 ""Please use Architecture=CPU or Architecture=CPU.""; 229 ""See the TMVA Users' Guide for instructions if you ""; 230 ""encounter problems.""; 231 << Endl;; 232 Log() << kINFO << ""We will use instead the CPU architecture"" << Endl;; 233 fArchitectureString = ""CPU"";; 234 }; 235 if (fArchitectureString == ""OPENCL"") {; 236 Log() << kERROR << ""The OPENCL architecture has not been implemented yet. ""; 237 ""Please use Architecture=CPU or Architecture=CPU for the ""; 238 ""time being. See the TMVA Users' Guide for instructions ""; 239 ""if you encounter problems.""; 240 << Endl;; 241 // use instead GPU; 242 Log() << kINFO << ""We will try using the GPU-CUDA architecture if available"" << Endl;; 243 fArchitectureString = ""GPU"";; 244 }; 245 ; 246 // the architecture can now be set at runtime as an option; 247 ; 248 ; 249 if (fArchitectureString == ""GPU"" || fArchitectureString == ""CUDNN"") {; 250#ifdef R__HAS_TMVAGPU; 251 Log() << kINFO << ""Will now use the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag i",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:10892,Availability,error,error,10892," (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCrossEntropy;; 286 }; 287 fOutputFunction = EOutputFunction::kSigmoid;; 288 } else if (fAnalysisType == Types::kRegression) {; 289 if (fErrorStrategy != ""SUMOFSQUARES"") {; 290 Log() << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 291 << "" neural net error function. Setting error function to ""; 292 << "" SUMOFSQUARES now."" << Endl;; 293 }; 294 ; 295 fLossFunction = ELossFunction::kMeanSquaredError;; 296 fOutputFunction = EOutputFunction::kIdentity;; 297 } else if (fAnalysisType == Types::kMulticlass) {; 298 if (fErrorStrategy == ""SUMOFSQUARES"") {; 299 fLossFunction = ELossFunction::kMeanSquaredError;; 300 }; 301 if (fErrorStrategy == ""CROSSENTROPY"") {; 302 fLossFunction = ELossFunction::kCrossEntropy;; 303 }; 304 if (fErrorStrategy == ""MUTUALEXCLUSIVE"") {; 305 fLossFunction = ELossFunction::kSoftmaxCrossEntropy;; 306 }; 307 fOutputFunction = EOutputFunction::kSoftmax;; 308 }; 309 ; 310 // Initialization; 311 // the biases will be always initialized to zero; 312 if (f",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:10916,Availability,error,error,10916,"erformances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCrossEntropy;; 286 }; 287 fOutputFunction = EOutputFunction::kSigmoid;; 288 } else if (fAnalysisType == Types::kRegression) {; 289 if (fErrorStrategy != ""SUMOFSQUARES"") {; 290 Log() << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 291 << "" neural net error function. Setting error function to ""; 292 << "" SUMOFSQUARES now."" << Endl;; 293 }; 294 ; 295 fLossFunction = ELossFunction::kMeanSquaredError;; 296 fOutputFunction = EOutputFunction::kIdentity;; 297 } else if (fAnalysisType == Types::kMulticlass) {; 298 if (fErrorStrategy == ""SUMOFSQUARES"") {; 299 fLossFunction = ELossFunction::kMeanSquaredError;; 300 }; 301 if (fErrorStrategy == ""CROSSENTROPY"") {; 302 fLossFunction = ELossFunction::kCrossEntropy;; 303 }; 304 if (fErrorStrategy == ""MUTUALEXCLUSIVE"") {; 305 fLossFunction = ELossFunction::kSoftmaxCrossEntropy;; 306 }; 307 fOutputFunction = EOutputFunction::kSoftmax;; 308 }; 309 ; 310 // Initialization; 311 // the biases will be always initialized to zero; 312 if (fWeightInitializationString == ""XAVIER"") {; 313 fWeightInitialization = DNN::EInitialization::kGlorotNormal;; 314 } else if (fWeightInitializationString == ""XAVIERUNIFORM"") {; 315 fWeightInitialization = DNN::EInitialization::kGlorotUniform;; 316 } else if (fWeightInitializationString == ""GAUSS"") {; 317 fWeightIni",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52453,Availability,error,error,52453,"; 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:56567,Availability,error,error,56567," logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << Endl;; 1425 if (!fInteractive) {; 1426 std::string separator(62, '-');; 1427 Log() << separator << Endl;; 1428 Log() << std::setw(10) << ""Epoch""; 1429 << "" | "" << std::setw(12) << ""Train Err."" << std::setw(12) << ""Val. Err."" << std::setw(12); 1430 << ""t(s)/epoch"" << std::setw(12) << ""t(s)/Loss"" << std::setw(12) << ""nEvents/s"" << std::setw(12); 1431 << ""Conv. Steps"" << Endl;; 1432 Log() << separator << Endl;; 1433 }; 1434 ; 1435 // set up generator for shuffling the batches; 1436 // if seed is zero we have always a different order in the batches; 1437 size_t shuffleSeed = 0;; 1438 if (fRandomSeed != 0) shuffleSeed = fRandomSeed + trainingPhase;; 1439 RandomGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1445 for (size_t l = 0; l < weights_tensor.size(); ++l); 1446 weights_tensor[l].Print();; 1447 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1448 bias_tensor[0].P",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:61187,Availability,error,error,61187,"::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62242,Availability,error,error,62242," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62397,Availability,error,error,62397," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:63251,Availability,error,error,63251,"um error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor = batch.GetInput();; 1595 auto outputMatrix = batch.GetOutput();; 1596 auto weights = batch.GetWeights();; 1597 trainingError += deepNet.Loss(inputTensor, outputMatrix, weights, false, false);; 1598 }; 1599 }; 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:64172,Availability,error,errors,64172,"or <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor = batch.GetInput();; 1595 auto outputMatrix = batch.GetOutput();; 1596 auto weights = batch.GetWeights();; 1597 trainingError += deepNet.Loss(inputTensor, outputMatrix, weights, false, false);; 1598 }; 1599 }; 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / settings.testInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_c",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:94382,Availability,error,error,94382,////////////////////////////////////////////////////////////; 2335const Ranking *TMVA::MethodDL::CreateRanking(); 2336{; 2337 // TODO; 2338 return NULL;; 2339}; 2340 ; 2341////////////////////////////////////////////////////////////////////////////////; 2342void MethodDL::GetHelpMessage() const; 2343{; 2344 // TODO; 2345}; 2346 ; 2347} // namespace TMVA; Adadelta.h; Adagrad.h; Adam.h; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cuda.h; DLMinimizers.h; IMethod.h; MethodDL.h; RMSProp.h; e#define e(i)Definition RSha256.hxx:103; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDefinition RtypesCore.h:69; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassImp#define ClassImp(name)Definition Rtypes.h:382; SGD.h; TCudnn.h; R__ASSERT#define R__ASSERT(e)Checks condition e and reports a fatal error if it's false.Definition TError.h:125; pwinID h TVirtualViewer3D TVirtualGLPainter pDefinition TGWin32VirtualGLProxy.cxx:51; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; widthOption_t Option_t widthDefinition TGWin32VirtualXProxy.cxx:56; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count,MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:113260,Availability,error,error,113260,"5; TMVA::MethodDL::GetMvaValueDouble_t GetMvaValue(Double_t *err=nullptr, Double_t *errUpper=nullptr)Definition MethodDL.cxx:1772; TMVA::MethodDL::ParseInputLayoutvoid ParseInputLayout()Parse the input layout.Definition MethodDL.cxx:439; TMVA::MethodDL::FillInputTensorvoid FillInputTensor()Get the input event tensor for evaluation Internal function to fill the fXInput tensor with the corre...Definition MethodDL.cxx:1704; TMVA::MethodDL::fBuildNetbool fBuildNetFlag to control whether to build fNet, the stored network used for the evaluation.Definition MethodDL.h:201; TMVA::MethodDL::SetInputHeightvoid SetInputHeight(int inputHeight)Definition MethodDL.h:287; TMVA::MethodDL::CreateDeepNetvoid CreateDeepNet(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets)After calling the ProcesOptions(), all of the options are parsed, so using the parsed options,...Definition MethodDL.cxx:529; TMVA::MethodDL::fErrorStrategyTString fErrorStrategyThe string defining the error strategy for training.Definition MethodDL.h:195; TMVA::MethodDL::DeclareOptionsvoid DeclareOptions()The option handling methods.Definition MethodDL.cxx:167; TMVA::MethodDL::fInputLayoutStringTString fInputLayoutStringThe string defining the layout of the input.Definition MethodDL.h:192; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::RandomGeneratorDefinition Tools.h:299; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::TimerTiming information for training and evaluation of MVA methods.Definition Timer.h:58; TMVA::Timer::GetElapsedTimeTString GetElapsedTime(Bool_t Scientific=kTRUE)returns pretty string with elapsed timeDefinition Timer.cxx:146; TMVA::Tools::xmlengineTXMLEngine & xmlengine()Definition Tools.h:262; TMVA::Tools::ReadAttrvoid ReadAttr(void *node, const char *, T &value)read attribute from xmlDefinition Tools.h:329; TMVA::Tools::GetChildvoid * GetChild(void *p",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:116079,Availability,error,error,116079,":Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::kFATAL@ kFATALDefinition Types.h:61; TMarker::Printvoid Print(Option_t *option="""") const overrideDump this marker with its attributes.Definition TMarker.cxx:339; TMatrixTBase::Printvoid Print(Option_t *name="""") const overridePrint the matrix as a table of elements.Definition TMatrixTBase.cxx:636; TMatrixTTMatrixT.Definition TMatrixT.h:40; TNamed::Printvoid Print(Option_t *option="""") const overridePrint TNamed name and title.Definition TNamed.cxx:128; TObjArrayAn array of TObjects.Definition TObjArray.h:31; TObjStringCollectable string class.Definition TObjString.h:28; TObjString::GetStringconst TString & GetString() constDefinition TObjString.h:46; TObject::Warningvirtual void Warning(const char *method, const char *msgfmt,...) constIssue warning message.Definition TObject.cxx:979; TObject::Errorvirtual void Error(const char *method, const char *msgfmt,...) constIssue error message.Definition TObject.cxx:993; TObject::Printvirtual void Print(Option_t *option="""") constThis method must be overridden when a class wants to print itself.Definition TObject.cxx:642; TStringBasic string class.Definition TString.h:139; TString::LengthSsiz_t Length() constDefinition TString.h:417; TString::AtoiInt_t Atoi() constReturn integer value of string.Definition TString.cxx:1988; TString::StripTSubString Strip(EStripType s=kTrailing, char c=' ') constReturn a substring of self stripped at beginning and/or end.Definition TString.cxx:1163; TString::IsFloatBool_t IsFloat() constReturns kTRUE if string contains a floating point or integer number.Definition TString.cxx:1858; TString::FirstSsiz_t First(char c) constFind first occurrence of a character c.Definition TString.cxx:538; TString::Dataconst char * Data() constDefinition TString.h:376; TString::ReplaceAllTString & ReplaceAll(const TString &s1, const TString &s2)Definition TString.h:704; TString::kTrailing@ kTrailingDe",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:119141,Availability,error,error,119141,"define I(x, y, z); TMVA::DNN::CNNDefinition ContextHandles.h:43; TMVA::DNNDefinition Adadelta.h:36; TMVA::DNN::EInitializationEInitializationDefinition Functions.h:72; TMVA::DNN::EInitialization::kGauss@ kGauss; TMVA::DNN::EInitialization::kGlorotNormal@ kGlorotNormal; TMVA::DNN::EInitialization::kUniform@ kUniform; TMVA::DNN::EInitialization::kGlorotUniform@ kGlorotUniform; TMVA::DNN::EInitialization::kZero@ kZero; TMVA::DNN::EInitialization::kIdentity@ kIdentity; TMVA::DNN::EOptimizerEOptimizerEnum representing the optimizer used for training.Definition Functions.h:82; TMVA::DNN::EOptimizer::kAdam@ kAdam; TMVA::DNN::EOptimizer::kRMSProp@ kRMSProp; TMVA::DNN::EOptimizer::kAdadelta@ kAdadelta; TMVA::DNN::EOptimizer::kSGD@ kSGD; TMVA::DNN::EOptimizer::kAdagrad@ kAdagrad; TMVA::DNN::EOutputFunctionEOutputFunctionEnum that represents output functions.Definition Functions.h:46; TMVA::DNN::weightDecaydouble weightDecay(double error, ItWeight itWeight, ItWeight itWeightEnd, double factorWeightDecay, EnumRegularization eRegularization)compute the weight decay for regularization (L1 or L2)Definition NeuralNet.icc:498; TMVA::DNN::regularizationauto regularization(const typename Architecture_t::Matrix_t &A, ERegularization R) -> decltype(Architecture_t::L1Regularization(A))Evaluate the regularization functional for a given weight matrix.Definition Functions.h:238; TMVA::DNN::ERegularizationERegularizationEnum representing the regularization type applied for a given layer.Definition Functions.h:65; TMVA::DNN::ERegularization::kL2@ kL2; TMVA::DNN::ERegularization::kL1@ kL1; TMVA::DNN::ERegularization::kNone@ kNone; TMVA::DNN::EActivationFunctionEActivationFunctionEnum that represents layer activation functions.Definition Functions.h:32; TMVA::DNN::EActivationFunction::kRelu@ kRelu; TMVA::DNN::EActivationFunction::kGauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kFastTanh@ kFastTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid;",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:406,Deployability,integrat,integrated,406,". ROOT: tmva/tmva/src/MethodDL.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodDL.cxx. Go to the documentation of this file. 1// @(#)root/tmva/tmva/cnn:$Id$Ndl; 2// Authors: Vladimir Ilievski, Lorenzo Moneta, Saurav Shekhar, Ravi Kiran; 3/**********************************************************************************; 4 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 5 * Package: TMVA *; 6 * Class : MethodDL *; 7 * *; 8 * *; 9 * Description: *; 10 * Deep Neural Network Method *; 11 * *; 12 * Authors (alphabetical): *; 13 * Vladimir Ilievski <ilievski.vladimir@live.com> - CERN, Switzerland *; 14 * Saurav Shekhar <sauravshekhar01@gmail.com> - ETH Zurich, Switzerland *; 15 * Ravi Kiran S <sravikiran0606@gmail.com> - CERN, Switzerland *; 16 * *; 17 * Copyright (c) 2005-2015: *; 18 * CERN, Switzerland *; 19 * U. of Victoria, Canada *; 20 * MPI-K Heidelberg, Germany *; 21 * U. of Bonn, Germany *; 22 * *; 23 * Redistribution and use in source and binary forms, with or without *; 24 * modification, are permitted according to the terms listed in LICENSE *; 25 * (see tmva/doc/LICENSE) *; 26 **********************************************************************************/; 27 ; 28#include ""TFormula.h""; 29#include ""TString.h""; 30#include ""TMath.h""; 31#include ""TObjString.h""; 32 ; 33#include ""TMVA/Tools.h""; 34#include ""TMVA/Configurable.h""; 35#include ""TMVA/IMethod.h""; 36#include ""TMVA/ClassifierFactory.h""; 37#include ""TMVA/MethodDL.h""; 38#include ""TMVA/Types.h""; 39#include ""TMVA/DNN/TensorDataLoader.h""; 40#include ""TMVA/DNN/Functions.h""; 41#include ""TMVA/DNN/DLMinimizers.h""; 42#include ""TMVA/DNN/SGD.h""; 43#include ""TMVA/DNN/Adam.h""; 44#include ""TMVA/DNN/Adagrad.h""; 45#include ""TMVA/DNN/RMSProp.h""; 46#include ""TMVA/DNN/Adadelta.h""; 47#include ""TMVA/Timer.h""; 48 ; 49#ifdef R__HAS_TMVAGPU; 50#include ""TMVA/DNN/Architectures/Cuda.h""; 51#ifdef R__HAS_CUDNN; 52#include ""TMVA/DNN/Architectures/TC",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:9379,Deployability,install,installed,9379,"as not been implemented yet. ""; 237 ""Please use Architecture=CPU or Architecture=CPU for the ""; 238 ""time being. See the TMVA Users' Guide for instructions ""; 239 ""if you encounter problems.""; 240 << Endl;; 241 // use instead GPU; 242 Log() << kINFO << ""We will try using the GPU-CUDA architecture if available"" << Endl;; 243 fArchitectureString = ""GPU"";; 244 }; 245 ; 246 // the architecture can now be set at runtime as an option; 247 ; 248 ; 249 if (fArchitectureString == ""GPU"" || fArchitectureString == ""CUDNN"") {; 250#ifdef R__HAS_TMVAGPU; 251 Log() << kINFO << ""Will now use the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCros",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60129,Deployability,update,update,60129," 1489 auto outputMatrix = my_batch.GetOutput();; 1490 auto weights = my_batch.GetWeights();; 1491 trainingError += deepNet.Loss(outputMatrix, weights, false);; 1492 }; 1493 ; 1494 if (debugFirstEpoch); 1495 std::cout << ""- doing backward \n"";; 1496 ; 1497#ifdef DEBUG; 1498 size_t nlayers = deepNet.GetLayers().size();; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono:",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62205,Deployability,configurat,configuration,62205," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62420,Deployability,configurat,configuration,62420," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:66411,Deployability,install,installed,66411,"ensor = deepNet.GetLayerAt(0)->GetBiases();; 1647 for (size_t l = 0; l < weights_tensor.size(); ++l); 1648 weights_tensor[l].Print();; 1649 bias_tensor[0].Print();; 1650 }; 1651 ; 1652 }; 1653 ; 1654 trainingPhase++;; 1655 } // end loop on training Phase; 1656}; 1657 ; 1658////////////////////////////////////////////////////////////////////////////////; 1659void MethodDL::Train(); 1660{; 1661 if (fInteractive) {; 1662 Log() << kFATAL << ""Not implemented yet"" << Endl;; 1663 return;; 1664 }; 1665 ; 1666 // using for training same scalar type defined for the prediction; 1667 if (this->GetArchitectureString() == ""GPU"") {; 1668#ifdef R__HAS_TMVAGPU; 1669 Log() << kINFO << ""Start of deep neural network training on GPU."" << Endl << Endl;; 1670#ifdef R__HAS_CUDNN; 1671 TrainDeepNet<DNN::TCudnn<ScalarImpl_t> >();; 1672#else; 1673 TrainDeepNet<DNN::TCuda<ScalarImpl_t>>();; 1674#endif; 1675#else; 1676 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1677 ""you have CUDA installed and it was successfully ""; 1678 ""detected by CMAKE.""; 1679 << Endl;; 1680 return;; 1681#endif; 1682 } else if (this->GetArchitectureString() == ""CPU"") {; 1683#ifdef R__HAS_TMVACPU; 1684 // note that number of threads used for BLAS might be different; 1685 // e.g use openblas_set_num_threads(num_threads) for OPENBLAS backend; 1686 Log() << kINFO << ""Start of deep neural network training on CPU using MT, nthreads = ""; 1687 << gConfig().GetNCpu() << Endl << Endl;; 1688#else; 1689 Log() << kINFO << ""Start of deep neural network training on single thread CPU (without ROOT-MT support) "" << Endl; 1690 << Endl;; 1691#endif; 1692 TrainDeepNet<DNN::TCpu<ScalarImpl_t> >();; 1693 return;; 1694 }; 1695 else {; 1696 Log() << kFATAL << this->GetArchitectureString() <<; 1697 "" is not a supported architecture for TMVA::MethodDL""; 1698 << Endl;; 1699 }; 1700 ; 1701}; 1702 ; 1703////////////////////////////////////////////////////////////////////////////////; 1704void TMVA::MethodDL::FillInputTensor(); 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:406,Integrability,integrat,integrated,406,". ROOT: tmva/tmva/src/MethodDL.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodDL.cxx. Go to the documentation of this file. 1// @(#)root/tmva/tmva/cnn:$Id$Ndl; 2// Authors: Vladimir Ilievski, Lorenzo Moneta, Saurav Shekhar, Ravi Kiran; 3/**********************************************************************************; 4 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 5 * Package: TMVA *; 6 * Class : MethodDL *; 7 * *; 8 * *; 9 * Description: *; 10 * Deep Neural Network Method *; 11 * *; 12 * Authors (alphabetical): *; 13 * Vladimir Ilievski <ilievski.vladimir@live.com> - CERN, Switzerland *; 14 * Saurav Shekhar <sauravshekhar01@gmail.com> - ETH Zurich, Switzerland *; 15 * Ravi Kiran S <sravikiran0606@gmail.com> - CERN, Switzerland *; 16 * *; 17 * Copyright (c) 2005-2015: *; 18 * CERN, Switzerland *; 19 * U. of Victoria, Canada *; 20 * MPI-K Heidelberg, Germany *; 21 * U. of Bonn, Germany *; 22 * *; 23 * Redistribution and use in source and binary forms, with or without *; 24 * modification, are permitted according to the terms listed in LICENSE *; 25 * (see tmva/doc/LICENSE) *; 26 **********************************************************************************/; 27 ; 28#include ""TFormula.h""; 29#include ""TString.h""; 30#include ""TMath.h""; 31#include ""TObjString.h""; 32 ; 33#include ""TMVA/Tools.h""; 34#include ""TMVA/Configurable.h""; 35#include ""TMVA/IMethod.h""; 36#include ""TMVA/ClassifierFactory.h""; 37#include ""TMVA/MethodDL.h""; 38#include ""TMVA/Types.h""; 39#include ""TMVA/DNN/TensorDataLoader.h""; 40#include ""TMVA/DNN/Functions.h""; 41#include ""TMVA/DNN/DLMinimizers.h""; 42#include ""TMVA/DNN/SGD.h""; 43#include ""TMVA/DNN/Adam.h""; 44#include ""TMVA/DNN/Adagrad.h""; 45#include ""TMVA/DNN/RMSProp.h""; 46#include ""TMVA/DNN/Adadelta.h""; 47#include ""TMVA/Timer.h""; 48 ; 49#ifdef R__HAS_TMVAGPU; 50#include ""TMVA/DNN/Architectures/Cuda.h""; 51#ifdef R__HAS_CUDNN; 52#include ""TMVA/DNN/Architectures/TC",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:22422,Integrability,interface,interface,22422,"Type == ""DENSE"") {; 558 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 559 } else if (strLayerType == ""CONV"") {; 560 ParseConvLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 561 } else if (strLayerType == ""MAXPOOL"") {; 562 ParseMaxPoolLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 563 } else if (strLayerType == ""RESHAPE"") {; 564 ParseReshapeLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 565 } else if (strLayerType == ""BNORM"") {; 566 ParseBatchNormLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 567 } else if (strLayerType == ""RNN"") {; 568 ParseRecurrentLayer(kLayerRNN, deepNet, nets, layerString->GetString(), subDelimiter);; 569 } else if (strLayerType == ""LSTM"") {; 570 ParseRecurrentLayer(kLayerLSTM, deepNet, nets, layerString->GetString(), subDelimiter);; 571 } else if (strLayerType == ""GRU"") {; 572 ParseRecurrentLayer(kLayerGRU, deepNet, nets, layerString->GetString(), subDelimiter);; 573 } else {; 574 // no type of layer specified - assume is dense layer as in old DNN interface; 575 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 576 }; 577 }; 578}; 579 ; 580////////////////////////////////////////////////////////////////////////////////; 581/// Pases the layer string and creates the appropriate dense layer; 582template <typename Architecture_t, typename Layer_t>; 583void MethodDL::ParseDenseLayer(DNN::TDeepNet<Architecture_t, Layer_t> &deepNet,; 584 std::vector<DNN::TDeepNet<Architecture_t, Layer_t>> & /*nets*/, TString layerString,; 585 TString delim); 586{; 587 int width = 0;; 588 EActivationFunction activationFunction = EActivationFunction::kTanh;; 589 ; 590 // this return number of input variables for the method; 591 // it can be used to deduce width of dense layer if specified as N+10; 592 // where N is the number of input variables; 593 const size_t inputSize = GetNvar();; 594 ; 595 // Split layer details; 596 TObjArray *subStrings = layerString.",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:67525,Integrability,depend,depending,67525,"inDeepNet<DNN::TCuda<ScalarImpl_t>>();; 1674#endif; 1675#else; 1676 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1677 ""you have CUDA installed and it was successfully ""; 1678 ""detected by CMAKE.""; 1679 << Endl;; 1680 return;; 1681#endif; 1682 } else if (this->GetArchitectureString() == ""CPU"") {; 1683#ifdef R__HAS_TMVACPU; 1684 // note that number of threads used for BLAS might be different; 1685 // e.g use openblas_set_num_threads(num_threads) for OPENBLAS backend; 1686 Log() << kINFO << ""Start of deep neural network training on CPU using MT, nthreads = ""; 1687 << gConfig().GetNCpu() << Endl << Endl;; 1688#else; 1689 Log() << kINFO << ""Start of deep neural network training on single thread CPU (without ROOT-MT support) "" << Endl; 1690 << Endl;; 1691#endif; 1692 TrainDeepNet<DNN::TCpu<ScalarImpl_t> >();; 1693 return;; 1694 }; 1695 else {; 1696 Log() << kFATAL << this->GetArchitectureString() <<; 1697 "" is not a supported architecture for TMVA::MethodDL""; 1698 << Endl;; 1699 }; 1700 ; 1701}; 1702 ; 1703////////////////////////////////////////////////////////////////////////////////; 1704void TMVA::MethodDL::FillInputTensor(); 1705{; 1706 // fill the input tensor fXInput from the current Event data; 1707 // with the correct shape depending on the model used; 1708 // The input tensor is used for network prediction after training ; 1709 // using a single event. The network batch size must be equal to 1. ; 1710 // The architecture specified at compile time in ArchitectureImpl_t; 1711 // is used. This should be the CPU architecture; 1712 ; 1713 if (!fNet || fNet->GetDepth() == 0) {; 1714 Log() << kFATAL << ""The network has not been trained and fNet is not built"" << Endl;; 1715 }; 1716 if (fNet->GetBatchSize() != 1) {; 1717 Log() << kFATAL << ""FillINputTensor::Network batch size must be equal to 1 when doing single event predicition"" << Endl;; 1718 }; 1719 ; 1720 // get current event; 1721 const std::vector<Float_t> &inputValues = GetEvent()->GetValues()",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:115955,Integrability,message,message,115955,"::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::kFATAL@ kFATALDefinition Types.h:61; TMarker::Printvoid Print(Option_t *option="""") const overrideDump this marker with its attributes.Definition TMarker.cxx:339; TMatrixTBase::Printvoid Print(Option_t *name="""") const overridePrint the matrix as a table of elements.Definition TMatrixTBase.cxx:636; TMatrixTTMatrixT.Definition TMatrixT.h:40; TNamed::Printvoid Print(Option_t *option="""") const overridePrint TNamed name and title.Definition TNamed.cxx:128; TObjArrayAn array of TObjects.Definition TObjArray.h:31; TObjStringCollectable string class.Definition TObjString.h:28; TObjString::GetStringconst TString & GetString() constDefinition TObjString.h:46; TObject::Warningvirtual void Warning(const char *method, const char *msgfmt,...) constIssue warning message.Definition TObject.cxx:979; TObject::Errorvirtual void Error(const char *method, const char *msgfmt,...) constIssue error message.Definition TObject.cxx:993; TObject::Printvirtual void Print(Option_t *option="""") constThis method must be overridden when a class wants to print itself.Definition TObject.cxx:642; TStringBasic string class.Definition TString.h:139; TString::LengthSsiz_t Length() constDefinition TString.h:417; TString::AtoiInt_t Atoi() constReturn integer value of string.Definition TString.cxx:1988; TString::StripTSubString Strip(EStripType s=kTrailing, char c=' ') constReturn a substring of self stripped at beginning and/or end.Definition TString.cxx:1163; TString::IsFloatBool_t IsFloat() constReturns kTRUE if string contains a floating point or integer number.Definition TString.cxx:1858; TString::FirstSsiz_t First(char c) constFind first occurrence of a character c.Definition TString.cxx:538; TString::Dataconst char * Data() constDefinition TString.h:376; TSt",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:116085,Integrability,message,message,116085,":Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::kFATAL@ kFATALDefinition Types.h:61; TMarker::Printvoid Print(Option_t *option="""") const overrideDump this marker with its attributes.Definition TMarker.cxx:339; TMatrixTBase::Printvoid Print(Option_t *name="""") const overridePrint the matrix as a table of elements.Definition TMatrixTBase.cxx:636; TMatrixTTMatrixT.Definition TMatrixT.h:40; TNamed::Printvoid Print(Option_t *option="""") const overridePrint TNamed name and title.Definition TNamed.cxx:128; TObjArrayAn array of TObjects.Definition TObjArray.h:31; TObjStringCollectable string class.Definition TObjString.h:28; TObjString::GetStringconst TString & GetString() constDefinition TObjString.h:46; TObject::Warningvirtual void Warning(const char *method, const char *msgfmt,...) constIssue warning message.Definition TObject.cxx:979; TObject::Errorvirtual void Error(const char *method, const char *msgfmt,...) constIssue error message.Definition TObject.cxx:993; TObject::Printvirtual void Print(Option_t *option="""") constThis method must be overridden when a class wants to print itself.Definition TObject.cxx:642; TStringBasic string class.Definition TString.h:139; TString::LengthSsiz_t Length() constDefinition TString.h:417; TString::AtoiInt_t Atoi() constReturn integer value of string.Definition TString.cxx:1988; TString::StripTSubString Strip(EStripType s=kTrailing, char c=' ') constReturn a substring of self stripped at beginning and/or end.Definition TString.cxx:1163; TString::IsFloatBool_t IsFloat() constReturns kTRUE if string contains a floating point or integer number.Definition TString.cxx:1858; TString::FirstSsiz_t First(char c) constFind first occurrence of a character c.Definition TString.cxx:538; TString::Dataconst char * Data() constDefinition TString.h:376; TString::ReplaceAllTString & ReplaceAll(const TString &s1, const TString &s2)Definition TString.h:704; TString::kTrailing@ kTrailingDe",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:20690,Modifiability,layers,layers,20690,"ng = (TObjString *)nextBatchDim();; 498 int idxToken = 0;; 499 ; 500 for (; batchDimString != nullptr; batchDimString = (TObjString *)nextBatchDim()) {; 501 switch (idxToken) {; 502 case 0: // input depth; 503 {; 504 TString strDepth(batchDimString->GetString());; 505 batchDepth = (size_t)strDepth.Atoi();; 506 } break;; 507 case 1: // input height; 508 {; 509 TString strHeight(batchDimString->GetString());; 510 batchHeight = (size_t)strHeight.Atoi();; 511 } break;; 512 case 2: // input width; 513 {; 514 TString strWidth(batchDimString->GetString());; 515 batchWidth = (size_t)strWidth.Atoi();; 516 } break;; 517 }; 518 ++idxToken;; 519 }; 520 ; 521 this->SetBatchDepth(batchDepth);; 522 this->SetBatchHeight(batchHeight);; 523 this->SetBatchWidth(batchWidth);; 524}; 525 ; 526////////////////////////////////////////////////////////////////////////////////; 527/// Create a deep net based on the layout string; 528template <typename Architecture_t, typename Layer_t>; 529void MethodDL::CreateDeepNet(DNN::TDeepNet<Architecture_t, Layer_t> &deepNet,; 530 std::vector<DNN::TDeepNet<Architecture_t, Layer_t>> &nets); 531{; 532 // Layer specification, layer details; 533 const TString layerDelimiter("","");; 534 const TString subDelimiter(""|"");; 535 ; 536 TString layoutString = this->GetLayoutString();; 537 ; 538 //std::cout << ""Create Deepnet - layout string "" << layoutString << ""\t layers : "" << deepNet.GetLayers().size() << std::endl;; 539 ; 540 // Split layers; 541 TObjArray *layerStrings = layoutString.Tokenize(layerDelimiter);; 542 TIter nextLayer(layerStrings);; 543 TObjString *layerString = (TObjString *)nextLayer();; 544 ; 545 ; 546 for (; layerString != nullptr; layerString = (TObjString *)nextLayer()) {; 547 ; 548 // Split layer details; 549 TObjArray *subStrings = layerString->GetString().Tokenize(subDelimiter);; 550 TIter nextToken(subStrings);; 551 TObjString *token = (TObjString *)nextToken();; 552 ; 553 // Determine the type of the layer; 554 TString strLayerType = toke",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:20765,Modifiability,layers,layers,20765,"; 512 case 2: // input width; 513 {; 514 TString strWidth(batchDimString->GetString());; 515 batchWidth = (size_t)strWidth.Atoi();; 516 } break;; 517 }; 518 ++idxToken;; 519 }; 520 ; 521 this->SetBatchDepth(batchDepth);; 522 this->SetBatchHeight(batchHeight);; 523 this->SetBatchWidth(batchWidth);; 524}; 525 ; 526////////////////////////////////////////////////////////////////////////////////; 527/// Create a deep net based on the layout string; 528template <typename Architecture_t, typename Layer_t>; 529void MethodDL::CreateDeepNet(DNN::TDeepNet<Architecture_t, Layer_t> &deepNet,; 530 std::vector<DNN::TDeepNet<Architecture_t, Layer_t>> &nets); 531{; 532 // Layer specification, layer details; 533 const TString layerDelimiter("","");; 534 const TString subDelimiter(""|"");; 535 ; 536 TString layoutString = this->GetLayoutString();; 537 ; 538 //std::cout << ""Create Deepnet - layout string "" << layoutString << ""\t layers : "" << deepNet.GetLayers().size() << std::endl;; 539 ; 540 // Split layers; 541 TObjArray *layerStrings = layoutString.Tokenize(layerDelimiter);; 542 TIter nextLayer(layerStrings);; 543 TObjString *layerString = (TObjString *)nextLayer();; 544 ; 545 ; 546 for (; layerString != nullptr; layerString = (TObjString *)nextLayer()) {; 547 ; 548 // Split layer details; 549 TObjArray *subStrings = layerString->GetString().Tokenize(subDelimiter);; 550 TIter nextToken(subStrings);; 551 TObjString *token = (TObjString *)nextToken();; 552 ; 553 // Determine the type of the layer; 554 TString strLayerType = token->GetString();; 555 ; 556 ; 557 if (strLayerType == ""DENSE"") {; 558 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 559 } else if (strLayerType == ""CONV"") {; 560 ParseConvLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 561 } else if (strLayerType == ""MAXPOOL"") {; 562 ParseMaxPoolLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 563 } else if (strLayerType == ""RESHAPE"") {; 564 ParseReshapeLayer(deepNet, nets, l",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:23084,Modifiability,variab,variables,23084,"Type == ""DENSE"") {; 558 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 559 } else if (strLayerType == ""CONV"") {; 560 ParseConvLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 561 } else if (strLayerType == ""MAXPOOL"") {; 562 ParseMaxPoolLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 563 } else if (strLayerType == ""RESHAPE"") {; 564 ParseReshapeLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 565 } else if (strLayerType == ""BNORM"") {; 566 ParseBatchNormLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 567 } else if (strLayerType == ""RNN"") {; 568 ParseRecurrentLayer(kLayerRNN, deepNet, nets, layerString->GetString(), subDelimiter);; 569 } else if (strLayerType == ""LSTM"") {; 570 ParseRecurrentLayer(kLayerLSTM, deepNet, nets, layerString->GetString(), subDelimiter);; 571 } else if (strLayerType == ""GRU"") {; 572 ParseRecurrentLayer(kLayerGRU, deepNet, nets, layerString->GetString(), subDelimiter);; 573 } else {; 574 // no type of layer specified - assume is dense layer as in old DNN interface; 575 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 576 }; 577 }; 578}; 579 ; 580////////////////////////////////////////////////////////////////////////////////; 581/// Pases the layer string and creates the appropriate dense layer; 582template <typename Architecture_t, typename Layer_t>; 583void MethodDL::ParseDenseLayer(DNN::TDeepNet<Architecture_t, Layer_t> &deepNet,; 584 std::vector<DNN::TDeepNet<Architecture_t, Layer_t>> & /*nets*/, TString layerString,; 585 TString delim); 586{; 587 int width = 0;; 588 EActivationFunction activationFunction = EActivationFunction::kTanh;; 589 ; 590 // this return number of input variables for the method; 591 // it can be used to deduce width of dense layer if specified as N+10; 592 // where N is the number of input variables; 593 const size_t inputSize = GetNvar();; 594 ; 595 // Split layer details; 596 TObjArray *subStrings = layerString.",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:23223,Modifiability,variab,variables,23223,"Type == ""DENSE"") {; 558 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 559 } else if (strLayerType == ""CONV"") {; 560 ParseConvLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 561 } else if (strLayerType == ""MAXPOOL"") {; 562 ParseMaxPoolLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 563 } else if (strLayerType == ""RESHAPE"") {; 564 ParseReshapeLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 565 } else if (strLayerType == ""BNORM"") {; 566 ParseBatchNormLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 567 } else if (strLayerType == ""RNN"") {; 568 ParseRecurrentLayer(kLayerRNN, deepNet, nets, layerString->GetString(), subDelimiter);; 569 } else if (strLayerType == ""LSTM"") {; 570 ParseRecurrentLayer(kLayerLSTM, deepNet, nets, layerString->GetString(), subDelimiter);; 571 } else if (strLayerType == ""GRU"") {; 572 ParseRecurrentLayer(kLayerGRU, deepNet, nets, layerString->GetString(), subDelimiter);; 573 } else {; 574 // no type of layer specified - assume is dense layer as in old DNN interface; 575 ParseDenseLayer(deepNet, nets, layerString->GetString(), subDelimiter);; 576 }; 577 }; 578}; 579 ; 580////////////////////////////////////////////////////////////////////////////////; 581/// Pases the layer string and creates the appropriate dense layer; 582template <typename Architecture_t, typename Layer_t>; 583void MethodDL::ParseDenseLayer(DNN::TDeepNet<Architecture_t, Layer_t> &deepNet,; 584 std::vector<DNN::TDeepNet<Architecture_t, Layer_t>> & /*nets*/, TString layerString,; 585 TString delim); 586{; 587 int width = 0;; 588 EActivationFunction activationFunction = EActivationFunction::kTanh;; 589 ; 590 // this return number of input variables for the method; 591 // it can be used to deduce width of dense layer if specified as N+10; 592 // where N is the number of input variables; 593 const size_t inputSize = GetNvar();; 594 ; 595 // Split layer details; 596 TObjArray *subStrings = layerString.",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:24821,Modifiability,variab,variable,24821,"ed as N+10; 592 // where N is the number of input variables; 593 const size_t inputSize = GetNvar();; 594 ; 595 // Split layer details; 596 TObjArray *subStrings = layerString.Tokenize(delim);; 597 TIter nextToken(subStrings);; 598 TObjString *token = (TObjString *)nextToken();; 599 ; 600 // loop on the tokens; 601 // order of sepcifying width and activation function is not relevant; 602 // both 100|TANH and TANH|100 are valid cases; 603 for (; token != nullptr; token = (TObjString *)nextToken()) {; 604 // try a match with the activation function; 605 TString strActFnc(token->GetString());; 606 // if first token defines the layer type- skip it; 607 if (strActFnc ==""DENSE"") continue;; 608 ; 609 if (strActFnc == ""RELU"") {; 610 activationFunction = DNN::EActivationFunction::kRelu;; 611 } else if (strActFnc == ""TANH"") {; 612 activationFunction = DNN::EActivationFunction::kTanh;; 613 } else if (strActFnc == ""FTANH"") {; 614 activationFunction = DNN::EActivationFunction::kFastTanh;; 615 } else if (strActFnc == ""SYMMRELU"") {; 616 activationFunction = DNN::EActivationFunction::kSymmRelu;; 617 } else if (strActFnc == ""SOFTSIGN"") {; 618 activationFunction = DNN::EActivationFunction::kSoftSign;; 619 } else if (strActFnc == ""SIGMOID"") {; 620 activationFunction = DNN::EActivationFunction::kSigmoid;; 621 } else if (strActFnc == ""LINEAR"") {; 622 activationFunction = DNN::EActivationFunction::kIdentity;; 623 } else if (strActFnc == ""GAUSS"") {; 624 activationFunction = DNN::EActivationFunction::kGauss;; 625 } else if (width == 0) {; 626 // no match found try to parse as text showing the width; 627 // support for input a formula where the variable 'x' is 'N' in the string; 628 // use TFormula for the evaluation; 629 TString strNumNodes = strActFnc;; 630 // number of nodes; 631 TString strN(""x"");; 632 strNumNodes.ReplaceAll(""N"", strN);; 633 strNumNodes.ReplaceAll(""n"", strN);; 634 TFormula fml(""tmp"", strNumNodes);; 635 width = fml.Eval(inputSize);; 636 }; 637 }; 638 // avoid zero width. ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:49706,Modifiability,layers,layers,49706,"least the same size as the smallest""; 1248 << "" of them."" << Endl;; 1249 }; 1250 ; 1251 DeepNet_t deepNet(batchSize, inputDepth, inputHeight, inputWidth, batchDepth, batchHeight, batchWidth, J, I, R, weightDecay);; 1252 ; 1253 // create a copy of DeepNet for evaluating but with batch size = 1; 1254 // fNet is the saved network and will be with CPU or Referrence architecture; 1255 if (trainingPhase == 1) {; 1256 fNet = std::unique_ptr<DeepNetImpl_t>(new DeepNetImpl_t(1, inputDepth, inputHeight, inputWidth, batchDepth,; 1257 batchHeight, batchWidth, J, I, R, weightDecay));; 1258 fBuildNet = true;; 1259 }; 1260 else; 1261 fBuildNet = false;; 1262 ; 1263 // Initialize the vector of slave nets; 1264 std::vector<DeepNet_t> nets{};; 1265 nets.reserve(nThreads);; 1266 for (size_t i = 0; i < nThreads; i++) {; 1267 // create a copies of the master deep net; 1268 nets.push_back(deepNet);; 1269 }; 1270 ; 1271 ; 1272 // Add all appropriate layers to deepNet and (if fBuildNet is true) also to fNet; 1273 CreateDeepNet(deepNet, nets);; 1274 ; 1275 ; 1276 // set droput probabilities; 1277 // use convention to store in the layer 1.- dropout probabilities; 1278 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1279 for (auto & p : dropoutVector) {; 1280 p = 1.0 - p;; 1281 }; 1282 deepNet.SetDropoutProbabilities(dropoutVector);; 1283 ; 1284 if (trainingPhase > 1) {; 1285 // copy initial weights from fNet to deepnet; 1286 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1287 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1288 }; 1289 }; 1290 ; 1291 // when fNet is built create also input matrix that will be used to evaluate it; 1292 if (fBuildNet) {; 1293 //int n1 = batchHeight;; 1294 //int n2 = batchWidth;; 1295 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1296 //if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) n1 = fNet->GetBatchSize();; 1297 //fXInput = ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:50595,Modifiability,layers,layers,50595,"ll appropriate layers to deepNet and (if fBuildNet is true) also to fNet; 1273 CreateDeepNet(deepNet, nets);; 1274 ; 1275 ; 1276 // set droput probabilities; 1277 // use convention to store in the layer 1.- dropout probabilities; 1278 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1279 for (auto & p : dropoutVector) {; 1280 p = 1.0 - p;; 1281 }; 1282 deepNet.SetDropoutProbabilities(dropoutVector);; 1283 ; 1284 if (trainingPhase > 1) {; 1285 // copy initial weights from fNet to deepnet; 1286 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1287 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1288 }; 1289 }; 1290 ; 1291 // when fNet is built create also input matrix that will be used to evaluate it; 1292 if (fBuildNet) {; 1293 //int n1 = batchHeight;; 1294 //int n2 = batchWidth;; 1295 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1296 //if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) n1 = fNet->GetBatchSize();; 1297 //fXInput = TensorImpl_t(1,n1,n2);; 1298 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 1299 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 1300 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth() );; 1301 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t tra",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62205,Modifiability,config,configuration,62205," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62420,Modifiability,config,configuration,62420," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:69006,Modifiability,variab,variable,69006,"tValues = GetEvent()->GetValues();; 1722 size_t nVariables = GetEvent()->GetNVariables();; 1723 ; 1724 // for Columnlayout tensor memory layout is HWC while for rowwise is CHW; 1725 if (fXInput.GetLayout() == TMVA::Experimental::MemoryLayout::ColumnMajor) {; 1726 R__ASSERT(fXInput.GetShape().size() < 4);; 1727 size_t nc, nhw = 0;; 1728 if (fXInput.GetShape().size() == 2) {; 1729 nc = fXInput.GetShape()[0];; 1730 if (nc != 1) {; 1731 ArchitectureImpl_t::PrintTensor(fXInput);; 1732 Log() << kFATAL << ""First tensor dimension should be equal to batch size, i.e. = 1"" << Endl;; 1733 }; 1734 nhw = fXInput.GetShape()[1];; 1735 } else {; 1736 nc = fXInput.GetCSize();; 1737 nhw = fXInput.GetWSize();; 1738 }; 1739 if (nVariables != nc * nhw) {; 1740 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1741 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nhw << Endl;; 1742 }; 1743 for (size_t j = 0; j < nc; j++) {; 1744 for (size_t k = 0; k < nhw; k++) {; 1745 // note that in TMVA events images are stored as C H W while in the buffer we stored as H W C; 1746 fXInputBuffer[k * nc + j] = inputValues[j * nhw + k]; // for column layout !!!; 1747 }; 1748 }; 1749 } else {; 1750 // row-wise layout; 1751 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuf",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:69101,Modifiability,variab,variables,69101,"tValues = GetEvent()->GetValues();; 1722 size_t nVariables = GetEvent()->GetNVariables();; 1723 ; 1724 // for Columnlayout tensor memory layout is HWC while for rowwise is CHW; 1725 if (fXInput.GetLayout() == TMVA::Experimental::MemoryLayout::ColumnMajor) {; 1726 R__ASSERT(fXInput.GetShape().size() < 4);; 1727 size_t nc, nhw = 0;; 1728 if (fXInput.GetShape().size() == 2) {; 1729 nc = fXInput.GetShape()[0];; 1730 if (nc != 1) {; 1731 ArchitectureImpl_t::PrintTensor(fXInput);; 1732 Log() << kFATAL << ""First tensor dimension should be equal to batch size, i.e. = 1"" << Endl;; 1733 }; 1734 nhw = fXInput.GetShape()[1];; 1735 } else {; 1736 nc = fXInput.GetCSize();; 1737 nhw = fXInput.GetWSize();; 1738 }; 1739 if (nVariables != nc * nhw) {; 1740 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1741 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nhw << Endl;; 1742 }; 1743 for (size_t j = 0; j < nc; j++) {; 1744 for (size_t k = 0; k < nhw; k++) {; 1745 // note that in TMVA events images are stored as C H W while in the buffer we stored as H W C; 1746 fXInputBuffer[k * nc + j] = inputValues[j * nhw + k]; // for column layout !!!; 1747 }; 1748 }; 1749 } else {; 1750 // row-wise layout; 1751 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuf",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:69775,Modifiability,variab,variable,69775,"f (nVariables != nc * nhw) {; 1740 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1741 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nhw << Endl;; 1742 }; 1743 for (size_t j = 0; j < nc; j++) {; 1744 for (size_t k = 0; k < nhw; k++) {; 1745 // note that in TMVA events images are stored as C H W while in the buffer we stored as H W C; 1746 fXInputBuffer[k * nc + j] = inputValues[j * nhw + k]; // for column layout !!!; 1747 }; 1748 }; 1749 } else {; 1750 // row-wise layout; 1751 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuffer().CopyFrom(fXInputBuffer);; 1768 return;; 1769}; 1770 ; 1771////////////////////////////////////////////////////////////////////////////////; 1772Double_t MethodDL::GetMvaValue(Double_t * /*errLower*/, Double_t * /*errUpper*/); 1773{; 1774 ; 1775 FillInputTensor();; 1776 ; 1777 // perform the prediction; 1778 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1779 ; 1780 // return value; 1781 double mvaValue = (*fYHat)(0, 0);; 1782 ; 1783 // for debugging; 1784#ifdef DEBUG_MVAVALUE; 1785 using Tensor_t = std::vector<MatrixImpl_t>;; 1786 TMatrixF xInput(n1,n2, inputValues.data() );; 1787 std::cout << ""Input data - class "" << GetEvent()->GetClass() << std::endl;; 1788 xInput.Print();; 1789 std::cout <",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:69870,Modifiability,variab,variables,69870,"f (nVariables != nc * nhw) {; 1740 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1741 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nhw << Endl;; 1742 }; 1743 for (size_t j = 0; j < nc; j++) {; 1744 for (size_t k = 0; k < nhw; k++) {; 1745 // note that in TMVA events images are stored as C H W while in the buffer we stored as H W C; 1746 fXInputBuffer[k * nc + j] = inputValues[j * nhw + k]; // for column layout !!!; 1747 }; 1748 }; 1749 } else {; 1750 // row-wise layout; 1751 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuffer().CopyFrom(fXInputBuffer);; 1768 return;; 1769}; 1770 ; 1771////////////////////////////////////////////////////////////////////////////////; 1772Double_t MethodDL::GetMvaValue(Double_t * /*errLower*/, Double_t * /*errUpper*/); 1773{; 1774 ; 1775 FillInputTensor();; 1776 ; 1777 // perform the prediction; 1778 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1779 ; 1780 // return value; 1781 double mvaValue = (*fYHat)(0, 0);; 1782 ; 1783 // for debugging; 1784#ifdef DEBUG_MVAVALUE; 1785 using Tensor_t = std::vector<MatrixImpl_t>;; 1786 TMatrixF xInput(n1,n2, inputValues.data() );; 1787 std::cout << ""Input data - class "" << GetEvent()->GetClass() << std::endl;; 1788 xInput.Print();; 1789 std::cout <",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:71046,Modifiability,layers,layers,71046," j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuffer().CopyFrom(fXInputBuffer);; 1768 return;; 1769}; 1770 ; 1771////////////////////////////////////////////////////////////////////////////////; 1772Double_t MethodDL::GetMvaValue(Double_t * /*errLower*/, Double_t * /*errUpper*/); 1773{; 1774 ; 1775 FillInputTensor();; 1776 ; 1777 // perform the prediction; 1778 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1779 ; 1780 // return value; 1781 double mvaValue = (*fYHat)(0, 0);; 1782 ; 1783 // for debugging; 1784#ifdef DEBUG_MVAVALUE; 1785 using Tensor_t = std::vector<MatrixImpl_t>;; 1786 TMatrixF xInput(n1,n2, inputValues.data() );; 1787 std::cout << ""Input data - class "" << GetEvent()->GetClass() << std::endl;; 1788 xInput.Print();; 1789 std::cout << ""Output of DeepNet "" << mvaValue << std::endl;; 1790 auto & deepnet = *fNet;; 1791 std::cout << ""Loop on layers "" << std::endl;; 1792 for (int l = 0; l < deepnet.GetDepth(); ++l) {; 1793 std::cout << ""Layer "" << l;; 1794 const auto * layer = deepnet.GetLayerAt(l);; 1795 const Tensor_t & layer_output = layer->GetOutput();; 1796 layer->Print();; 1797 std::cout << ""DNN output "" << layer_output.size() << std::endl;; 1798 for (size_t i = 0; i < layer_output.size(); ++i) {; 1799#ifdef R__HAS_TMVAGPU; 1800 //TMatrixD m(layer_output[i].GetNrows(), layer_output[i].GetNcols() , layer_output[i].GetDataPointer() );; 1801 TMatrixD m = layer_output[i];; 1802#else; 1803 TMatrixD m(layer_output[i].GetNrows(), layer_output[i].GetNcols() , layer_output[i].GetRawDataPointer() );; 1804#endif; 1805 m.Print();; 1806 }; 1807 const Tensor_t & layer_weights = layer->GetWeights();; 1808 std::cout << ""DNN weights "" << layer_weights.size() << std::endl;; 1809 if (layer_weights.size() > 0) {; 1810 int i = 0;; 1811#ifdef R__HAS_TMVAGPU; 1812 TMatrixD m = layer_weights[i];; 1813// ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:74778,Modifiability,layers,layers,74778," // create the deep neural network; 1855 DeepNet_t deepNet(batchSize, inputDepth, inputHeight, inputWidth, batchDepth, batchHeight, batchWidth, J, I, R, weightDecay);; 1856 std::vector<DeepNet_t> nets{};; 1857 fBuildNet = false;; 1858 CreateDeepNet(deepNet,nets);; 1859 ; 1860 // copy weights from the saved fNet to the built DeepNet; 1861 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1862 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1863 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1864 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), ""Inference: input weights"");; 1865 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), ""Inference: state weights"");; 1866 // }; 1867 }; 1868 ; 1869 size_t n1 = deepNet.GetBatchHeight();; 1870 size_t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCur",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:76450,Modifiability,variab,variable,76450,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:76545,Modifiability,variab,variables,76545,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:76749,Modifiability,variab,variable,76749,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:76844,Modifiability,variab,variables,76844,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:92372,Modifiability,layers,layers,92372,"te );; 2288 gTools().ReadAttr(layerXML, ""ReturnSequence"", returnSequence);; 2289 gTools().ReadAttr(layerXML, ""ResetGateAfter"", resetGateAfter);; 2290 ; 2291 if (!resetGateAfter && ArchitectureImpl_t::IsCudnn()); 2292 Warning(""ReadWeightsFromXML"",; 2293 ""Cannot use a reset gate after to false with CudNN - use implementation with resetgate=true"");; 2294 ; 2295 fNet->AddBasicGRULayer(stateSize, inputSize, timeSteps, rememberState, returnSequence, resetGateAfter);; 2296 }; 2297 // BatchNorm Layer; 2298 else if (layerName == ""BatchNormLayer"") {; 2299 // use some dammy value which will be overwrittem in BatchNormLayer::ReadWeightsFromXML; 2300 fNet->AddBatchNormLayer(0., 0.0);; 2301 }; 2302 // read weights and biases; 2303 fNet->GetLayers().back()->ReadWeightsFromXML(layerXML);; 2304 ; 2305 // read next layer; 2306 layerXML = gTools().GetNextChild(layerXML);; 2307 }; 2308 ; 2309 fBuildNet = false;; 2310 // create now the input and output matrices; 2311 //int n1 = batchHeight;; 2312 //int n2 = batchWidth;; 2313 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 2314 //if (fXInput.size() > 0) fXInput.clear();; 2315 //fXInput.emplace_back(MatrixImpl_t(n1,n2));; 2316 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 2317 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 2318 // make here a ColumnMajor tensor; 2319 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth(),TMVA::Experimental::MemoryLayout::ColumnMajor );; 2320 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize());; 2321 ; 2322 // create pointer to output matrix used for the predictions; 2323 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 2324 ; 2325 ; 2326}; 2327 ; 2328 ; 2329////////////////////////////////////////////////////////////////////////////////; 2330void MethodDL::ReadWeightsFromStr",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:96207,Modifiability,variab,variables,96207," dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; heightOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t heightDefinition TGWin32VirtualXProxy.cxx:164; TMath.h; TObjString.h; TString.h; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; TensorDataLoader.h; Timer.h; Tools.h; EventDefinition collection_proxies.C:172; R; TFormulaThe Formula class.Definition TFormula.h:89; TFormula::EvalDouble_t Eval(Args... args) constSet first 1, 2, 3 or 4 variables (e.g.Definition TFormula.h:324; TIterDefinition TCollection.h:235; TLine::Printvoid Print(Option_t *option="""") const overrideDump this line with its attributes.Definition TLine.cxx:419; TMVA::Config::GetNCpuUInt_t GetNCpu()Definition Config.h:70; TMVA::Configurable::DeclareOptionRefOptionBase * DeclareOptionRef(T &ref, const TString &name, const TString &desc=""""); TMVA::Configurable::AddPreDefValvoid AddPreDefVal(const T &)Definition Configurable.h:168; TMVA::Configurable::LogMsgLogger & Log() constDefinition Configurable.h:122; TMVA::DNN::CNN::TConvLayerDefinition ConvLayer.h:75; TMVA::DNN::TAdadeltaAdadelta Optimizer class.Definition Adadelta.h:45; TMVA::DNN::TAdagradAdagrad Optimizer class.Definition Adagrad.h:45; TMVA::DNN::TAdamAdam Optimizer class.Definition Adam.h:45; TMVA::DNN::TCpu::CreateTensorstatic Tensor_t CreateTensor(size_t n, size_t c, size_t h, size_t w)Definition Cpu.h:108; TMVA::DNN::TCpu::IsCudnnstatic bool IsCudnn()Definition Cpu.h:131; TMVA::D",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:101020,Modifiability,variab,variablesDefinition,101020,"aLoader.Definition TensorDataLoader.h:133; TMVA::DNN::VGeneralLayerGeneric General Layer class.Definition GeneralLayer.h:51; TMVA::DNN::VGeneralLayer::Initializevirtual void Initialize()Initialize the weights and biases according to the given initialization method.Definition GeneralLayer.h:395; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNClassesUInt_t GetNClasses() constDefinition DataSetInfo.h:155; TMVA::DataSet::GetCurrentTypeTypes::ETreeType GetCurrentType() constDefinition DataSet.h:194; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::SetCurrentEventvoid SetCurrentEvent(Long64_t ievt) constDefinition DataSet.h:88; TMVA::EventDefinition Event.h:51; TMVA::Event::SetTargetvoid SetTarget(UInt_t itgt, Float_t value)set the target value (dimension itgt) to valueDefinition Event.cxx:367; TMVA::Event::GetNVariablesUInt_t GetNVariables() constaccessor to the number of variablesDefinition Event.cxx:316; TMVA::Event::GetClassUInt_t GetClass() constDefinition Event.h:86; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::GetNameconst char * GetName() constDefinition MethodBase.h:334; TMVA::MethodBase::IgnoreEventsWithNegWeightsInTrainingBool_t IgnoreEventsWithNegWeightsInTraining() constDefinition MethodBase.h:686; TMVA::MethodBase::GetEventCollectionconst std::vector< TMVA::Event * > & GetEventCollection(Types::ETreeType type)returns the event collection (i.e.Definition MethodBase.cxx:3347; TMVA::MethodBase::GetNTargetsUInt_t GetNTargets() constDefinition MethodBase.h:346; TMVA::MethodBase::GetMethodNameconst TString & GetMethodName() constDefinition MethodBase.h:331; TMVA::MethodBase::GetEventconst Event * GetEvent() constDefinition MethodBase.h:751; TMVA::MethodBase::DataInfoDataSetInfo & DataInfo() constDefinition MethodBase.h:410; TMVA::MethodBase::GetNVariablesUInt_t",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:113705,Modifiability,variab,variables,113705,"er to build fNet, the stored network used for the evaluation.Definition MethodDL.h:201; TMVA::MethodDL::SetInputHeightvoid SetInputHeight(int inputHeight)Definition MethodDL.h:287; TMVA::MethodDL::CreateDeepNetvoid CreateDeepNet(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets)After calling the ProcesOptions(), all of the options are parsed, so using the parsed options,...Definition MethodDL.cxx:529; TMVA::MethodDL::fErrorStrategyTString fErrorStrategyThe string defining the error strategy for training.Definition MethodDL.h:195; TMVA::MethodDL::DeclareOptionsvoid DeclareOptions()The option handling methods.Definition MethodDL.cxx:167; TMVA::MethodDL::fInputLayoutStringTString fInputLayoutStringThe string defining the layout of the input.Definition MethodDL.h:192; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::RandomGeneratorDefinition Tools.h:299; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::TimerTiming information for training and evaluation of MVA methods.Definition Timer.h:58; TMVA::Timer::GetElapsedTimeTString GetElapsedTime(Bool_t Scientific=kTRUE)returns pretty string with elapsed timeDefinition Timer.cxx:146; TMVA::Tools::xmlengineTXMLEngine & xmlengine()Definition Tools.h:262; TMVA::Tools::ReadAttrvoid ReadAttr(void *node, const char *, T &value)read attribute from xmlDefinition Tools.h:329; TMVA::Tools::GetChildvoid * GetChild(void *parent, const char *childname=nullptr)get child nodeDefinition Tools.cxx:1150; TMVA::Tools::AddAttrvoid AddAttr(void *node, const char *, const T &value, Int_t precision=16)add attribute to xmlDefinition Tools.h:347; TMVA::Tools::StringFromIntTString StringFromInt(Long_t i)string toolsDefinition Tools.cxx:1223; TMVA::Tools::GetNextChildvoid * GetNextChild(void *prevchild, const char *childname=nullptr)XML helpers.Definition Tools.cxx:1162; TMVA::TrainingHistory::AddValuevoid AddVal",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:120629,Modifiability,variab,variable,120629,"regularization type applied for a given layer.Definition Functions.h:65; TMVA::DNN::ERegularization::kL2@ kL2; TMVA::DNN::ERegularization::kL1@ kL1; TMVA::DNN::ERegularization::kNone@ kNone; TMVA::DNN::EActivationFunctionEActivationFunctionEnum that represents layer activation functions.Definition Functions.h:32; TMVA::DNN::EActivationFunction::kRelu@ kRelu; TMVA::DNN::EActivationFunction::kGauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kFastTanh@ kFastTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid; TMVA::DNN::EActivationFunction::kIdentity@ kIdentity; TMVA::DNN::EActivationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; T",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:7104,Performance,perform,perform,7104,"; 178 "" or cross entropy (binary classification)."");; 179 AddPreDefVal(TString(""CROSSENTROPY""));; 180 AddPreDefVal(TString(""SUMOFSQUARES""));; 181 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 182 ; 183 DeclareOptionRef(fWeightInitializationString = ""XAVIER"", ""WeightInitialization"", ""Weight initialization strategy"");; 184 AddPreDefVal(TString(""XAVIER""));; 185 AddPreDefVal(TString(""XAVIERUNIFORM""));; 186 AddPreDefVal(TString(""GAUSS""));; 187 AddPreDefVal(TString(""UNIFORM""));; 188 AddPreDefVal(TString(""IDENTITY""));; 189 AddPreDefVal(TString(""ZERO""));; 190 ; 191 DeclareOptionRef(fRandomSeed = 0, ""RandomSeed"", ""Random seed used for weight initialization and batch shuffling"");; 192 ; 193 DeclareOptionRef(fNumValidationString = ""20%"", ""ValidationSize"", ""Part of the training data to use for validation. ""; 194 ""Specify as 0.2 or 20% to use a fifth of the data set as validation set. ""; 195 ""Specify as 100 to use exactly 100 events. (Default: 20%)"");; 196 ; 197 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 198 AddPreDefVal(TString(""STANDARD"")); // deprecated and not supported anymore; 199 AddPreDefVal(TString(""CPU""));; 200 AddPreDefVal(TString(""GPU""));; 201 AddPreDefVal(TString(""OPENCL"")); // not yet implemented; 202 AddPreDefVal(TString(""CUDNN"")); // not needed (by default GPU is now CUDNN if available); 203 ; 204 // define training strategy separated by a separator ""|""; 205 DeclareOptionRef(fTrainingStrategyString = ""LearningRate=1e-3,""; 206 ""Momentum=0.0,""; 207 ""ConvergenceSteps=100,""; 208 ""MaxEpochs=2000,""; 209 ""Optimizer=ADAM,""; 210 ""BatchSize=30,""; 211 ""TestRepetitions=1,""; 212 ""WeightDecay=0.0,""; 213 ""Regularization=None,""; 214 ""DropConfig=0.0"",; 215 ""TrainingStrategy"", ""Defines the training strategies."");; 216}; 217 ; 218////////////////////////////////////////////////////////////////////////////////; 219void MethodDL::ProcessOptions(); 220{; 221 ; 222 if (IgnoreEventsWithNegWeightsInTraining()) {; 223 Log(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:9934,Performance,perform,performances,9934,"/ the architecture can now be set at runtime as an option; 247 ; 248 ; 249 if (fArchitectureString == ""GPU"" || fArchitectureString == ""CUDNN"") {; 250#ifdef R__HAS_TMVAGPU; 251 Log() << kINFO << ""Will now use the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCrossEntropy;; 286 }; 287 fOutputFunction = EOutputFunction::kSigmoid;; 288 } else if (fAnalysisType == Types::kRegression) {; 289 if (fErrorStrategy != ""SUMOFSQUARES"") {; 290 Log() << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 291 << "" neural net error function. Setting error function to ""; 292 << "" SUMOFSQUARES now."" << Endl;; 293 }; 294 ; 295 fLossFunctio",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:10179,Performance,perform,performance,10179,"e the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCrossEntropy;; 286 }; 287 fOutputFunction = EOutputFunction::kSigmoid;; 288 } else if (fAnalysisType == Types::kRegression) {; 289 if (fErrorStrategy != ""SUMOFSQUARES"") {; 290 Log() << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 291 << "" neural net error function. Setting error function to ""; 292 << "" SUMOFSQUARES now."" << Endl;; 293 }; 294 ; 295 fLossFunction = ELossFunction::kMeanSquaredError;; 296 fOutputFunction = EOutputFunction::kIdentity;; 297 } else if (fAnalysisType == Types::kMulticlass) {; 298 if (fErrorStrategy == ""SUMOFSQUARES"") {; 299 fLossFunction",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13641,Performance,optimiz,optimizer,13641,"ings settings;; 333 ; 334 settings.convergenceSteps = fetchValueTmp(block, ""ConvergenceSteps"", 100);; 335 settings.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13719,Performance,optimiz,optimizerName,13719,"ngs.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSP",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13735,Performance,optimiz,optimizer,13735,"ngs.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSP",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13755,Performance,optimiz,optimizer,13755,"ngs.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSP",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13791,Performance,optimiz,optimizer,13791," fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamL",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13842,Performance,optimiz,optimizer,13842," fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamL",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13879,Performance,optimiz,optimizer,13879,", ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optim",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13931,Performance,optimiz,optimizer,13931,", ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optim",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13971,Performance,optimiz,optimizer,13971,";; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamNa",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14026,Performance,optimiz,optimizer,14026,";; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamNa",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14066,Performance,optimiz,optimizer,14066,"m = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTm",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14121,Performance,optimiz,optimizer,14121,"m = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTm",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14162,Performance,optimiz,optimizer,14162,", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.op",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14308,Performance,optimiz,optimizer,14308,"ring(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14358,Performance,optimiz,optimizerName,14358,"7 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14416,Performance,optimiz,optimizer,14416,"7 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14883,Performance,optimiz,optimizerName,14883,"= ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batch layout was not given. Use default then; 397 // (1, batchsize, nvariables); 398 // fInputShape[0] -> BatchSize; 399 // fInputShape[1] -> InputDepth; 400 // fInputShape[2] -> InputHeight; 401 // fInputShape[3] -> InputWidth; 402 if (fInputShape[3] == 0 && fInputShape[2] == 0 && fInputShape[1] == 0) {; 403 fInputShape[1] = 1;; 404 fInputShape[2] = 1;; 405 fInputShape[3] = GetNVariables();; 406 }; 407 // case when batch layout is not provided (all zero)",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:14920,Performance,optimiz,optimizer,14920,"= ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optimizer = DNN::EOptimizer::kRMSProp;; 362 } else if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batch layout was not given. Use default then; 397 // (1, batchsize, nvariables); 398 // fInputShape[0] -> BatchSize; 399 // fInputShape[1] -> InputDepth; 400 // fInputShape[2] -> InputHeight; 401 // fInputShape[3] -> InputWidth; 402 if (fInputShape[3] == 0 && fInputShape[2] == 0 && fInputShape[1] == 0) {; 403 fInputShape[1] = 1;; 404 fInputShape[2] = 1;; 405 fInputShape[3] = GetNVariables();; 406 }; 407 // case when batch layout is not provided (all zero)",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:15192,Performance,optimiz,optimizer,15192,"lse if (optimizer == ""ADADELTA"") {; 363 settings.optimizer = DNN::EOptimizer::kAdadelta;; 364 } else {; 365 // Make Adam as default choice if the input string is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batch layout was not given. Use default then; 397 // (1, batchsize, nvariables); 398 // fInputShape[0] -> BatchSize; 399 // fInputShape[1] -> InputDepth; 400 // fInputShape[2] -> InputHeight; 401 // fInputShape[3] -> InputWidth; 402 if (fInputShape[3] == 0 && fInputShape[2] == 0 && fInputShape[1] == 0) {; 403 fInputShape[1] = 1;; 404 fInputShape[2] = 1;; 405 fInputShape[3] = GetNVariables();; 406 }; 407 // case when batch layout is not provided (all zero); 408 // batch layout can be determined by the input layout + batch size; 409 // case DNN : { 1, B, W }; 410 // case CNN : { B, C, H*W}; 411 // case RNN : { B, T, H*W }; 41",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:15226,Performance,optimiz,optimizerParams,15226,"ring is; 366 // incorrect.; 367 settings.optimizer = DNN::EOptimizer::kAdam;; 368 settings.optimizerName = ""ADAM"";; 369 }; 370 // check for specific optimizer parameters; 371 std::vector<TString> optimParamLabels = {""_beta1"", ""_beta2"", ""_eps"", ""_rho""};; 372 //default values; 373 std::map<TString, double> defaultValues = {; 374 {""ADADELTA_eps"", 1.E-8}, {""ADADELTA_rho"", 0.95},; 375 {""ADAGRAD_eps"", 1.E-8},; 376 {""ADAM_beta1"", 0.9}, {""ADAM_beta2"", 0.999}, {""ADAM_eps"", 1.E-7},; 377 {""RMSPROP_eps"", 1.E-7}, {""RMSPROP_rho"", 0.9},; 378 };; 379 for (auto &pN : optimParamLabels) {; 380 TString optimParamName = settings.optimizerName + pN;; 381 // check if optimizer has default values for this specific parameters; 382 if (defaultValues.count(optimParamName) > 0) {; 383 double defValue = defaultValues[optimParamName];; 384 double val = fetchValueTmp(block, optimParamName, defValue);; 385 // create entry in settings for this optimizer parameter; 386 settings.optimizerParams[optimParamName] = val;; 387 }; 388 }; 389 ; 390 fTrainingSettings.push_back(settings);; 391 }; 392 ; 393 // this set fInputShape[0] = batchSize; 394 this->SetBatchSize(fTrainingSettings.front().batchSize);; 395 ; 396 // case inputlayout and batch layout was not given. Use default then; 397 // (1, batchsize, nvariables); 398 // fInputShape[0] -> BatchSize; 399 // fInputShape[1] -> InputDepth; 400 // fInputShape[2] -> InputHeight; 401 // fInputShape[3] -> InputWidth; 402 if (fInputShape[3] == 0 && fInputShape[2] == 0 && fInputShape[1] == 0) {; 403 fInputShape[1] = 1;; 404 fInputShape[2] = 1;; 405 fInputShape[3] = GetNVariables();; 406 }; 407 // case when batch layout is not provided (all zero); 408 // batch layout can be determined by the input layout + batch size; 409 // case DNN : { 1, B, W }; 410 // case CNN : { B, C, H*W}; 411 // case RNN : { B, T, H*W }; 412 ; 413 if (fBatchWidth == 0 && fBatchHeight == 0 && fBatchDepth == 0) {; 414 // case first layer is DENSE; 415 if (fInputShape[2] == 1 && fInputShape[1]",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:45850,Performance,multi-thread,multi-threading,45850,"t, Architecture_t>;; 1171 ; 1172 bool debug = Log().GetMinType() == kDEBUG;; 1173 ; 1174 ; 1175 // set the random seed for weight initialization; 1176 Architecture_t::SetRandomSeed(fRandomSeed);; 1177 ; 1178 ///split training data in training and validation data; 1179 // and determine the number of training and testing examples; 1180 ; 1181 size_t nValidationSamples = GetNumValidationSamples();; 1182 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1183 ; 1184 const std::vector<TMVA::Event *> &allData = GetEventCollection(Types::kTraining);; 1185 const std::vector<TMVA::Event *> eventCollectionTraining{allData.begin(), allData.begin() + nTrainingSamples};; 1186 const std::vector<TMVA::Event *> eventCollectionValidation{allData.begin() + nTrainingSamples, allData.end()};; 1187 ; 1188 size_t trainingPhase = 1;; 1189 ; 1190 for (TTrainingSettings &settings : this->GetTrainingSettings()) {; 1191 ; 1192 size_t nThreads = 1; // FIXME threads are hard coded to 1, no use of slave threads or multi-threading; 1193 ; 1194 ; 1195 // After the processing of the options, initialize the master deep net; 1196 size_t batchSize = settings.batchSize;; 1197 this->SetBatchSize(batchSize);; 1198 // Should be replaced by actual implementation. No support for this now.; 1199 size_t inputDepth = this->GetInputDepth();; 1200 size_t inputHeight = this->GetInputHeight();; 1201 size_t inputWidth = this->GetInputWidth();; 1202 size_t batchDepth = this->GetBatchDepth();; 1203 size_t batchHeight = this->GetBatchHeight();; 1204 size_t batchWidth = this->GetBatchWidth();; 1205 ELossFunction J = this->GetLossFunction();; 1206 EInitialization I = this->GetWeightInitialization();; 1207 ERegularization R = settings.regularization;; 1208 EOptimizer O = settings.optimizer;; 1209 Scalar_t weightDecay = settings.weightDecay;; 1210 ; 1211 //Batch size should be included in batch layout as well. There are two possibilities:; 1212 // 1. Batch depth = batch size one w",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:46605,Performance,optimiz,optimizer,46605,")};; 1187 ; 1188 size_t trainingPhase = 1;; 1189 ; 1190 for (TTrainingSettings &settings : this->GetTrainingSettings()) {; 1191 ; 1192 size_t nThreads = 1; // FIXME threads are hard coded to 1, no use of slave threads or multi-threading; 1193 ; 1194 ; 1195 // After the processing of the options, initialize the master deep net; 1196 size_t batchSize = settings.batchSize;; 1197 this->SetBatchSize(batchSize);; 1198 // Should be replaced by actual implementation. No support for this now.; 1199 size_t inputDepth = this->GetInputDepth();; 1200 size_t inputHeight = this->GetInputHeight();; 1201 size_t inputWidth = this->GetInputWidth();; 1202 size_t batchDepth = this->GetBatchDepth();; 1203 size_t batchHeight = this->GetBatchHeight();; 1204 size_t batchWidth = this->GetBatchWidth();; 1205 ELossFunction J = this->GetLossFunction();; 1206 EInitialization I = this->GetWeightInitialization();; 1207 ERegularization R = settings.regularization;; 1208 EOptimizer O = settings.optimizer;; 1209 Scalar_t weightDecay = settings.weightDecay;; 1210 ; 1211 //Batch size should be included in batch layout as well. There are two possibilities:; 1212 // 1. Batch depth = batch size one will input tensorsa as (batch_size x d1 x d2); 1213 // This is case for example if first layer is a conv layer and d1 = image depth, d2 = image width x image height; 1214 // 2. Batch depth = 1, batch height = batch size batxch width = dim of input features; 1215 // This should be case if first layer is a Dense 1 and input tensor must be ( 1 x batch_size x input_features ); 1216 ; 1217 if (batchDepth != batchSize && batchDepth > 1) {; 1218 Error(""Train"",""Given batch depth of %zu (specified in BatchLayout) should be equal to given batch size %zu"",batchDepth,batchSize);; 1219 return;; 1220 }; 1221 if (batchDepth == 1 && batchSize > 1 && batchSize != batchHeight ) {; 1222 Error(""Train"",""Given batch height of %zu (specified in BatchLayout) should be equal to given batch size %zu"",batchHeight,batchSize);; 1223 return;",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:53441,Performance,optimiz,optimizer,53441,"ion of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 setting",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:53608,Performance,optimiz,optimizer,53608,"dl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, se",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:53851,Performance,optimiz,optimizer,53851,"seCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architect",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54033,Performance,optimiz,optimizerParams,54033,"rization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADEL",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54079,Performance,optimiz,optimizerParams,54079,"ion term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""A",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54119,Performance,optimiz,optimizerParams,54119,"/ settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTe",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54217,Performance,optimiz,optimizer,54217,"/ settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTe",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54404,Performance,optimiz,optimizerParams,54404,"ved class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, ten",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54497,Performance,optimiz,optimizer,54497,"ved class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, ten",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54703,Performance,optimiz,optimizerParams,54703,"t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString op",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54750,Performance,optimiz,optimizerParams,54750,"r<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54844,Performance,optimiz,optimizer,54844,"r<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55033,Performance,optimiz,optimizerParams,55033,"ta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" (",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55081,Performance,optimiz,optimizerParams,55081,"N::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 141",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55628,Performance,optimiz,optimizer,55628,"rop<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << En",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55780,Performance,optimiz,optimizerParams,55780,"; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << Endl;; 1425 if (!fInteractive) {; 1426 std::string separator(62, '-');; 1427 Log() << separator << Endl;; 1428 Log() << std::setw(10) << ""Epoch""; 1429 << "" | "" << std::setw(12) << ""Train Err."" << std::setw(12)",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55864,Performance,optimiz,optimizerName,55864,"Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << Endl;; 1425 if (!fInteractive) {; 1426 std::string separator(62, '-');; 1427 Log() << separator << Endl;; 1428 Log() << std::setw(10) << ""Epoch""; 1429 << "" | "" << std::setw(12) << ""Train Err."" << std::setw(12) << ""Val. Err."" << std::setw(12); 1430 << ""t(s)/epoch"" << std::setw(12) << ""t(s)/Loss"" << std::setw(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:56392,Performance,optimiz,optimizerName,56392,"; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << Endl;; 1425 if (!fInteractive) {; 1426 std::string separator(62, '-');; 1427 Log() << separator << Endl;; 1428 Log() << std::setw(10) << ""Epoch""; 1429 << "" | "" << std::setw(12) << ""Train Err."" << std::setw(12) << ""Val. Err."" << std::setw(12); 1430 << ""t(s)/epoch"" << std::setw(12) << ""t(s)/Loss"" << std::setw(12) << ""nEvents/s"" << std::setw(12); 1431 << ""Conv. Steps"" << Endl;; 1432 Log() << separator << Endl;; 1433 }; 1434 ; 1435 // set up generator for shuffling the batches; 1436 // if seed is zero we have always a different order in the batches; 1437 size_t shuffleSeed = 0;; 1438 if (fRandomSeed != 0) shuffleSeed = fRandomSeed + trainingPhase;; 1439 RandomGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weigh",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:58182,Performance,load,load,58182," have always a different order in the batches; 1437 size_t shuffleSeed = 0;; 1438 if (fRandomSeed != 0) shuffleSeed = fRandomSeed + trainingPhase;; 1439 RandomGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1445 for (size_t l = 0; l < weights_tensor.size(); ++l); 1446 weights_tensor[l].Print();; 1447 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1448 bias_tensor[0].Print();; 1449 }; 1450 ; 1451 Log() << "" Start epoch iteration ..."" << Endl;; 1452 bool debugFirstEpoch = false;; 1453 bool computeLossInTraining = true; // compute loss in training or at test time; 1454 size_t nTrainEpochs = 0;; 1455 while (!converged) {; 1456 nTrainEpochs++;; 1457 trainingData.Shuffle(rng);; 1458 ; 1459 // execute all epochs; 1460 //for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1461 ; 1462 Double_t trainingError = 0;; 1463 for (size_t i = 0; i < batchesInEpoch; ++i ) {; 1464 // Clean and load new batches, one batch for one slave net; 1465 //batches.clear();; 1466 //batches.reserve(nThreads);; 1467 //for (size_t j = 0; j < nThreads; j++) {; 1468 // batches.push_back(trainingData.GetTensorBatch());; 1469 //}; 1470 if (debugFirstEpoch) std::cout << ""\n\n----- batch # "" << i << ""\n\n"";; 1471 ; 1472 auto my_batch = trainingData.GetTensorBatch();; 1473 ; 1474 if (debugFirstEpoch); 1475 std::cout << ""got batch data - doing forward \n"";; 1476 ; 1477#ifdef DEBUG; 1478 ; 1479 Architecture_t::PrintTensor(my_batch.GetInput(),""input tensor"",true);; 1480 typename Architecture_t::Tensor_t tOut(my_batch.GetOutput());; 1481 typename Architecture_t::Tensor_t tW(my_batch.GetWeights());; 1482 Architecture_t::PrintTensor(tOut,""label tensor"",true) ;; 1483 Architecture_t::PrintTensor(tW,""weight tensor"",true) ;; 1484#endif; 1485 ; 1486 deepNet.Forward(my_batch.GetInput(), true);; 1487 // compute also loss; 1488 ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60119,Performance,optimiz,optimizer,60119," 1489 auto outputMatrix = my_batch.GetOutput();; 1490 auto weights = my_batch.GetWeights();; 1491 trainingError += deepNet.Loss(outputMatrix, weights, false);; 1492 }; 1493 ; 1494 if (debugFirstEpoch); 1495 std::cout << ""- doing backward \n"";; 1496 ; 1497#ifdef DEBUG; 1498 size_t nlayers = deepNet.GetLayers().size();; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono:",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60167,Performance,optimiz,optimizer,60167," 1489 auto outputMatrix = my_batch.GetOutput();; 1490 auto weights = my_batch.GetWeights();; 1491 trainingError += deepNet.Loss(outputMatrix, weights, false);; 1492 }; 1493 ; 1494 if (debugFirstEpoch); 1495 std::cout << ""- doing backward \n"";; 1496 ; 1497#ifdef DEBUG; 1498 size_t nlayers = deepNet.GetLayers().size();; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono:",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60232,Performance,optimiz,optimizer,60232,"493 ; 1494 if (debugFirstEpoch); 1495 std::cout << ""- doing backward \n"";; 1496 ; 1497#ifdef DEBUG; 1498 size_t nlayers = deepNet.GetLayers().size();; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : vali",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60272,Performance,optimiz,optimizer,60272,"493 ; 1494 if (debugFirstEpoch); 1495 std::cout << ""- doing backward \n"";; 1496 ; 1497#ifdef DEBUG; 1498 size_t nlayers = deepNet.GetLayers().size();; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : vali",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60410,Performance,optimiz,optimizer,60410,";; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:63929,Performance,throughput,throughput,63929,"; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor = batch.GetInput();; 1595 auto outputMatrix = batch.GetOutput();; 1596 auto weights = batch.GetWeights();; 1597 trainingError += deepNet.Loss(inputTensor, outputMatrix, weights, false, false);; 1598 }; 1599 }; 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / sett",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:70511,Performance,perform,perform,70511,"51 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuffer().CopyFrom(fXInputBuffer);; 1768 return;; 1769}; 1770 ; 1771////////////////////////////////////////////////////////////////////////////////; 1772Double_t MethodDL::GetMvaValue(Double_t * /*errLower*/, Double_t * /*errUpper*/); 1773{; 1774 ; 1775 FillInputTensor();; 1776 ; 1777 // perform the prediction; 1778 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1779 ; 1780 // return value; 1781 double mvaValue = (*fYHat)(0, 0);; 1782 ; 1783 // for debugging; 1784#ifdef DEBUG_MVAVALUE; 1785 using Tensor_t = std::vector<MatrixImpl_t>;; 1786 TMatrixF xInput(n1,n2, inputValues.data() );; 1787 std::cout << ""Input data - class "" << GetEvent()->GetClass() << std::endl;; 1788 xInput.Print();; 1789 std::cout << ""Output of DeepNet "" << mvaValue << std::endl;; 1790 auto & deepnet = *fNet;; 1791 std::cout << ""Loop on layers "" << std::endl;; 1792 for (int l = 0; l < deepnet.GetDepth(); ++l) {; 1793 std::cout << ""Layer "" << l;; 1794 const auto * layer = deepnet.GetLayerAt(l);; 1795 const Tensor_t & layer_output = layer->GetOutput();; 1796 layer->Print();; 1797 std::cout << ""DNN output "" << layer_output.size() << std::endl;; 1798 for (size_t i = 0; i < layer_output.size(); ++i) {; 1799#ifdef R__HAS_TMVAGPU; 1800 //TMatrixD m(layer_output[i].GetNrows(), layer_output[i].GetNcols() , laye",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:78164,Performance,perform,perform,78164,"TensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }; 1940 else {; 1941 // case of remaining events: compute prediction by single event !; 1942 for (Long64_t i = ievt; i < lastEvt; ++i) {; 1943 Data()->SetCurrentEvent(i);; 1944 mvaValues[i] = GetMvaValue();; 1945 }; 1946 }; 1947 }; 1948 ; 1949 if (logProgress) {; 1950 Log() << kINFO; 1951 << ""Elapsed time for evaluation of "" << nEvents << "" events: ""; 1952 << timer.GetElapsedTime() << "" "" << Endl;; 1953 }; 1954 ; 1955 return mvaValues;; 1956}; 1957 ; 1958//////////////////////////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 FillInputTensor ();; 1965 ; 1966 // perform the network prediction; 1967 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1968 ; 1969 size_t nTargets = DataInfo().GetNTargets();; 1970 R__ASSERT(nTargets == fYHat->GetNcols());; 1971 ; 1972 std::vector<Float_t> output(nTargets);; 1973 for (size_t i = 0; i < nTargets; i++); 1974 output[i] = (*fYHat)(0, i);; 1975 ; 1976 // ned to transform back output values; 1977 if (fRegressionReturnVal == NULL); 1978 fRegressionReturnVal = new std::vector<Float_t>(nTargets);; 1979 R__ASSERT(fRegressionReturnVal->size() == nTargets);; 1980 ; 1981 // N.B. one should cache here temporary event class; 1982 Event *evT = new Event(*GetEvent());; 1983 for (size_t i = 0; i < nTargets; ++i) {; 1984 evT->SetTarget(i, output[i]);; 1985 }; 1986 const Event *evT2 = GetTransformationHandler().InverseTransform(evT);; 1987 for (size_t i = 0; i < nTarg",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:78736,Performance,cache,cache,78736,"///////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 FillInputTensor ();; 1965 ; 1966 // perform the network prediction; 1967 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1968 ; 1969 size_t nTargets = DataInfo().GetNTargets();; 1970 R__ASSERT(nTargets == fYHat->GetNcols());; 1971 ; 1972 std::vector<Float_t> output(nTargets);; 1973 for (size_t i = 0; i < nTargets; i++); 1974 output[i] = (*fYHat)(0, i);; 1975 ; 1976 // ned to transform back output values; 1977 if (fRegressionReturnVal == NULL); 1978 fRegressionReturnVal = new std::vector<Float_t>(nTargets);; 1979 R__ASSERT(fRegressionReturnVal->size() == nTargets);; 1980 ; 1981 // N.B. one should cache here temporary event class; 1982 Event *evT = new Event(*GetEvent());; 1983 for (size_t i = 0; i < nTargets; ++i) {; 1984 evT->SetTarget(i, output[i]);; 1985 }; 1986 const Event *evT2 = GetTransformationHandler().InverseTransform(evT);; 1987 for (size_t i = 0; i < nTargets; ++i) {; 1988 (*fRegressionReturnVal)[i] = evT2->GetTarget(i);; 1989 }; 1990 delete evT;; 1991 return *fRegressionReturnVal;; 1992}; 1993//////////////////////////////////////////////////////////////////////////; 1994/// Get the multi-class output values for a single event; 1995//////////////////////////////////////////////////////////////////////////; 1996const std::vector<Float_t> &TMVA::MethodDL::GetMulticlassValues(); 1997{; 1998 ; 1999 FillInputTensor();; 2000 ; 2001 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 2002 ; 2003 size_t nClasses = DataInfo().GetNClasses();; 2004 R__ASSERT(nClasses == fYHat->GetNcols());; 2005 ; 2006 if (fMulticlassReturnVal == NULL) {; 2007 fMulticlassReturnVal = new std::vector<Float_t>(nClasses);; 2008 }; 2009 R__ASSERT(fMulticlassReturnVal->size() == nClasses);; 2010 ; 2",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:111207,Performance,perform,perform,111207,"29; TMVA::MethodDL::GetRegressionValuesvirtual const std::vector< Float_t > & GetRegressionValues(); TMVA::MethodDL::fTrainingStrategyStringTString fTrainingStrategyStringThe string defining the training strategy.Definition MethodDL.h:196; TMVA::MethodDL::CreateRankingconst Ranking * CreateRanking()Definition MethodDL.cxx:2335; TMVA::MethodDL::HostBufferImpl_ttypename ArchitectureImpl_t::HostBuffer_t HostBufferImpl_tDefinition MethodDL.h:110; TMVA::MethodDL::SetBatchDepthvoid SetBatchDepth(size_t batchDepth)Definition MethodDL.h:292; TMVA::MethodDL::ParseKeyValueStringKeyValueVector_t ParseKeyValueString(TString parseString, TString blockDelim, TString tokenDelim)Function for parsing the training settings, provided as a string in a key-value form.Definition MethodDL.cxx:1052; TMVA::MethodDL::SetBatchWidthvoid SetBatchWidth(size_t batchWidth)Definition MethodDL.h:294; TMVA::MethodDL::PredictDeepNetstd::vector< Double_t > PredictDeepNet(Long64_t firstEvt, Long64_t lastEvt, size_t batchSize, Bool_t logProgress)perform prediction of the deep neural network using batches (called by GetMvaValues)Definition MethodDL.cxx:1828; TMVA::MethodDL::GetWeightInitializationDNN::EInitialization GetWeightInitialization() constDefinition MethodDL.h:268; TMVA::MethodDL::SetBatchSizevoid SetBatchSize(size_t batchSize)Definition MethodDL.h:291; TMVA::MethodDL::GetLayoutStringTString GetLayoutString() constDefinition MethodDL.h:274; TMVA::MethodDL::fBatchDepthsize_t fBatchDepthThe depth of the batch used to train the deep net.Definition MethodDL.h:182; TMVA::MethodDL::DeepNetImpl_tTMVA::DNN::TDeepNet< ArchitectureImpl_t > DeepNetImpl_tDefinition MethodDL.h:106; TMVA::MethodDL::GetBatchWidthsize_t GetBatchWidth() constDefinition MethodDL.h:264; TMVA::MethodDL::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constDefinition MethodDL.cxx:2051; TMVA::MethodDL::MatrixImpl_ttypename ArchitectureImpl_t::Matrix_t MatrixImpl_tDefinition MethodDL.h:107; TMVA::MethodDL::~MethodDLvirtual ~MethodDL(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:118729,Performance,optimiz,optimizer,118729,"intf style format descriptor and return a TString.Definition TString.cxx:2378; TXMLEngine::NewChildXMLNodePointer_t NewChild(XMLNodePointer_t parent, XMLNsPointer_t ns, const char *name, const char *content=nullptr)create new child element for parent nodeDefinition TXMLEngine.cxx:715; TXMLEngine::GetChildXMLNodePointer_t GetChild(XMLNodePointer_t xmlnode, Bool_t realnode=kTRUE)returns first child of xmlnodeDefinition TXMLEngine.cxx:1146; TXMLEngine::GetNodeNameconst char * GetNodeName(XMLNodePointer_t xmlnode)returns name of xmlnodeDefinition TXMLEngine.cxx:1075; bool; double; int; unsigned int; nconst Int_t nDefinition legend1.C:16; I#define I(x, y, z); TMVA::DNN::CNNDefinition ContextHandles.h:43; TMVA::DNNDefinition Adadelta.h:36; TMVA::DNN::EInitializationEInitializationDefinition Functions.h:72; TMVA::DNN::EInitialization::kGauss@ kGauss; TMVA::DNN::EInitialization::kGlorotNormal@ kGlorotNormal; TMVA::DNN::EInitialization::kUniform@ kUniform; TMVA::DNN::EInitialization::kGlorotUniform@ kGlorotUniform; TMVA::DNN::EInitialization::kZero@ kZero; TMVA::DNN::EInitialization::kIdentity@ kIdentity; TMVA::DNN::EOptimizerEOptimizerEnum representing the optimizer used for training.Definition Functions.h:82; TMVA::DNN::EOptimizer::kAdam@ kAdam; TMVA::DNN::EOptimizer::kRMSProp@ kRMSProp; TMVA::DNN::EOptimizer::kAdadelta@ kAdadelta; TMVA::DNN::EOptimizer::kSGD@ kSGD; TMVA::DNN::EOptimizer::kAdagrad@ kAdagrad; TMVA::DNN::EOutputFunctionEOutputFunctionEnum that represents output functions.Definition Functions.h:46; TMVA::DNN::weightDecaydouble weightDecay(double error, ItWeight itWeight, ItWeight itWeightEnd, double factorWeightDecay, EnumRegularization eRegularization)compute the weight decay for regularization (L1 or L2)Definition NeuralNet.icc:498; TMVA::DNN::regularizationauto regularization(const typename Architecture_t::Matrix_t &A, ERegularization R) -> decltype(Architecture_t::L1Regularization(A))Evaluate the regularization functional for a given weight matrix.Definiti",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121316,Performance,optimiz,optimizerParamsstd,121316,"nction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121359,Performance,optimiz,optimizerParamsDefinition,121359,"nction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121425,Performance,optimiz,optimizerNameTString,121425," functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmva",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121446,Performance,optimiz,optimizerNameDefinition,121446," functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmva",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121510,Performance,optimiz,optimizerDNN,121510,"le< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDL.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121535,Performance,optimiz,optimizerDefinition,121535,"le< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDL.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:9421,Safety,detect,detected,9421,"as not been implemented yet. ""; 237 ""Please use Architecture=CPU or Architecture=CPU for the ""; 238 ""time being. See the TMVA Users' Guide for instructions ""; 239 ""if you encounter problems.""; 240 << Endl;; 241 // use instead GPU; 242 Log() << kINFO << ""We will try using the GPU-CUDA architecture if available"" << Endl;; 243 fArchitectureString = ""GPU"";; 244 }; 245 ; 246 // the architecture can now be set at runtime as an option; 247 ; 248 ; 249 if (fArchitectureString == ""GPU"" || fArchitectureString == ""CUDNN"") {; 250#ifdef R__HAS_TMVAGPU; 251 Log() << kINFO << ""Will now use the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCros",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:10029,Safety,detect,detected,10029,"/ the architecture can now be set at runtime as an option; 247 ; 248 ; 249 if (fArchitectureString == ""GPU"" || fArchitectureString == ""CUDNN"") {; 250#ifdef R__HAS_TMVAGPU; 251 Log() << kINFO << ""Will now use the GPU architecture !"" << Endl;; 252#else // case TMVA does not support GPU; 253 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 254 ""you have CUDA installed and it was successfully ""; 255 ""detected by CMAKE by using -Dtmva-gpu=On ""; 256 << Endl;; 257 fArchitectureString = ""CPU"";; 258 Log() << kINFO << ""Will now use instead the CPU architecture !"" << Endl;; 259#endif; 260 }; 261 ; 262 if (fArchitectureString == ""CPU"") {; 263#ifdef R__HAS_TMVACPU // TMVA has CPU BLAS and IMT support; 264 Log() << kINFO << ""Will now use the CPU architecture with BLAS and IMT support !"" << Endl;; 265#else // TMVA has no CPU BLAS or IMT support; 266 Log() << kINFO << ""Multi-core CPU backend not enabled. For better performances, make sure ""; 267 ""you have a BLAS implementation and it was successfully ""; 268 ""detected by CMake as well that the imt CMake flag is set.""; 269 << Endl;; 270 Log() << kINFO << ""Will use anyway the CPU architecture but with slower performance"" << Endl;; 271#endif; 272 }; 273 ; 274 // Input Layout; 275 ParseInputLayout();; 276 ParseBatchLayout();; 277 ; 278 // Loss function and output.; 279 fOutputFunction = EOutputFunction::kSigmoid;; 280 if (fAnalysisType == Types::kClassification) {; 281 if (fErrorStrategy == ""SUMOFSQUARES"") {; 282 fLossFunction = ELossFunction::kMeanSquaredError;; 283 }; 284 if (fErrorStrategy == ""CROSSENTROPY"") {; 285 fLossFunction = ELossFunction::kCrossEntropy;; 286 }; 287 fOutputFunction = EOutputFunction::kSigmoid;; 288 } else if (fAnalysisType == Types::kRegression) {; 289 if (fErrorStrategy != ""SUMOFSQUARES"") {; 290 Log() << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 291 << "" neural net error function. Setting error function to ""; 292 << "" SUMOFSQUARES now."" << Endl;; 293 }; 294 ; 295 fLossFunctio",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:25156,Safety,avoid,avoid,25156,"ction::kFastTanh;; 615 } else if (strActFnc == ""SYMMRELU"") {; 616 activationFunction = DNN::EActivationFunction::kSymmRelu;; 617 } else if (strActFnc == ""SOFTSIGN"") {; 618 activationFunction = DNN::EActivationFunction::kSoftSign;; 619 } else if (strActFnc == ""SIGMOID"") {; 620 activationFunction = DNN::EActivationFunction::kSigmoid;; 621 } else if (strActFnc == ""LINEAR"") {; 622 activationFunction = DNN::EActivationFunction::kIdentity;; 623 } else if (strActFnc == ""GAUSS"") {; 624 activationFunction = DNN::EActivationFunction::kGauss;; 625 } else if (width == 0) {; 626 // no match found try to parse as text showing the width; 627 // support for input a formula where the variable 'x' is 'N' in the string; 628 // use TFormula for the evaluation; 629 TString strNumNodes = strActFnc;; 630 // number of nodes; 631 TString strN(""x"");; 632 strNumNodes.ReplaceAll(""N"", strN);; 633 strNumNodes.ReplaceAll(""n"", strN);; 634 TFormula fml(""tmp"", strNumNodes);; 635 width = fml.Eval(inputSize);; 636 }; 637 }; 638 // avoid zero width. assume is last layer and give width = output width; 639 // Determine the number of outputs; 640 size_t outputSize = 1;; 641 if (fAnalysisType == Types::kRegression && GetNTargets() != 0) {; 642 outputSize = GetNTargets();; 643 } else if (fAnalysisType == Types::kMulticlass && DataInfo().GetNClasses() >= 2) {; 644 outputSize = DataInfo().GetNClasses();; 645 }; 646 if (width == 0) width = outputSize;; 647 ; 648 // Add the dense layer, initialize the weights and biases and copy; 649 TDenseLayer<Architecture_t> *denseLayer = deepNet.AddDenseLayer(width, activationFunction);; 650 denseLayer->Initialize();; 651 ; 652 // add same layer to fNet; 653 if (fBuildNet) fNet->AddDenseLayer(width, activationFunction);; 654 ; 655 //TDenseLayer<Architecture_t> *copyDenseLayer = new TDenseLayer<Architecture_t>(*denseLayer);; 656 ; 657 // add the copy to all slave nets; 658 //for (size_t i = 0; i < nets.size(); i++) {; 659 // nets[i].AddDenseLayer(copyDenseLayer);; 660 //}; 6",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:51190,Safety,predict,predictions,51190,"et.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1288 }; 1289 }; 1290 ; 1291 // when fNet is built create also input matrix that will be used to evaluate it; 1292 if (fBuildNet) {; 1293 //int n1 = batchHeight;; 1294 //int n2 = batchWidth;; 1295 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1296 //if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) n1 = fNet->GetBatchSize();; 1297 //fXInput = TensorImpl_t(1,n1,n2);; 1298 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 1299 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 1300 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth() );; 1301 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:65983,Safety,predict,prediction,65983,"stInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_clock::now();; 1639 }; 1640 ; 1641 // if (stepCount % 10 == 0 || converged) {; 1642 if (converged && debug) {; 1643 Log() << ""Final Deep Net Weights for phase "" << trainingPhase << "" epoch "" << nTrainEpochs; 1644 << Endl;; 1645 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1646 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1647 for (size_t l = 0; l < weights_tensor.size(); ++l); 1648 weights_tensor[l].Print();; 1649 bias_tensor[0].Print();; 1650 }; 1651 ; 1652 }; 1653 ; 1654 trainingPhase++;; 1655 } // end loop on training Phase; 1656}; 1657 ; 1658////////////////////////////////////////////////////////////////////////////////; 1659void MethodDL::Train(); 1660{; 1661 if (fInteractive) {; 1662 Log() << kFATAL << ""Not implemented yet"" << Endl;; 1663 return;; 1664 }; 1665 ; 1666 // using for training same scalar type defined for the prediction; 1667 if (this->GetArchitectureString() == ""GPU"") {; 1668#ifdef R__HAS_TMVAGPU; 1669 Log() << kINFO << ""Start of deep neural network training on GPU."" << Endl << Endl;; 1670#ifdef R__HAS_CUDNN; 1671 TrainDeepNet<DNN::TCudnn<ScalarImpl_t> >();; 1672#else; 1673 TrainDeepNet<DNN::TCuda<ScalarImpl_t>>();; 1674#endif; 1675#else; 1676 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1677 ""you have CUDA installed and it was successfully ""; 1678 ""detected by CMAKE.""; 1679 << Endl;; 1680 return;; 1681#endif; 1682 } else if (this->GetArchitectureString() == ""CPU"") {; 1683#ifdef R__HAS_TMVACPU; 1684 // note that number of threads used for BLAS might be different; 1685 // e.g use openblas_set_num_threads(num_threads) for OPENBLAS backend; 1686 Log() << kINFO << ""Start of deep neural network training on CPU using MT, nthreads = ""; 1687 << gConfig().G",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:66454,Safety,detect,detected,66454,"ensor = deepNet.GetLayerAt(0)->GetBiases();; 1647 for (size_t l = 0; l < weights_tensor.size(); ++l); 1648 weights_tensor[l].Print();; 1649 bias_tensor[0].Print();; 1650 }; 1651 ; 1652 }; 1653 ; 1654 trainingPhase++;; 1655 } // end loop on training Phase; 1656}; 1657 ; 1658////////////////////////////////////////////////////////////////////////////////; 1659void MethodDL::Train(); 1660{; 1661 if (fInteractive) {; 1662 Log() << kFATAL << ""Not implemented yet"" << Endl;; 1663 return;; 1664 }; 1665 ; 1666 // using for training same scalar type defined for the prediction; 1667 if (this->GetArchitectureString() == ""GPU"") {; 1668#ifdef R__HAS_TMVAGPU; 1669 Log() << kINFO << ""Start of deep neural network training on GPU."" << Endl << Endl;; 1670#ifdef R__HAS_CUDNN; 1671 TrainDeepNet<DNN::TCudnn<ScalarImpl_t> >();; 1672#else; 1673 TrainDeepNet<DNN::TCuda<ScalarImpl_t>>();; 1674#endif; 1675#else; 1676 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1677 ""you have CUDA installed and it was successfully ""; 1678 ""detected by CMAKE.""; 1679 << Endl;; 1680 return;; 1681#endif; 1682 } else if (this->GetArchitectureString() == ""CPU"") {; 1683#ifdef R__HAS_TMVACPU; 1684 // note that number of threads used for BLAS might be different; 1685 // e.g use openblas_set_num_threads(num_threads) for OPENBLAS backend; 1686 Log() << kINFO << ""Start of deep neural network training on CPU using MT, nthreads = ""; 1687 << gConfig().GetNCpu() << Endl << Endl;; 1688#else; 1689 Log() << kINFO << ""Start of deep neural network training on single thread CPU (without ROOT-MT support) "" << Endl; 1690 << Endl;; 1691#endif; 1692 TrainDeepNet<DNN::TCpu<ScalarImpl_t> >();; 1693 return;; 1694 }; 1695 else {; 1696 Log() << kFATAL << this->GetArchitectureString() <<; 1697 "" is not a supported architecture for TMVA::MethodDL""; 1698 << Endl;; 1699 }; 1700 ; 1701}; 1702 ; 1703////////////////////////////////////////////////////////////////////////////////; 1704void TMVA::MethodDL::FillInputTensor(); 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:67599,Safety,predict,prediction,67599,"inDeepNet<DNN::TCuda<ScalarImpl_t>>();; 1674#endif; 1675#else; 1676 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1677 ""you have CUDA installed and it was successfully ""; 1678 ""detected by CMAKE.""; 1679 << Endl;; 1680 return;; 1681#endif; 1682 } else if (this->GetArchitectureString() == ""CPU"") {; 1683#ifdef R__HAS_TMVACPU; 1684 // note that number of threads used for BLAS might be different; 1685 // e.g use openblas_set_num_threads(num_threads) for OPENBLAS backend; 1686 Log() << kINFO << ""Start of deep neural network training on CPU using MT, nthreads = ""; 1687 << gConfig().GetNCpu() << Endl << Endl;; 1688#else; 1689 Log() << kINFO << ""Start of deep neural network training on single thread CPU (without ROOT-MT support) "" << Endl; 1690 << Endl;; 1691#endif; 1692 TrainDeepNet<DNN::TCpu<ScalarImpl_t> >();; 1693 return;; 1694 }; 1695 else {; 1696 Log() << kFATAL << this->GetArchitectureString() <<; 1697 "" is not a supported architecture for TMVA::MethodDL""; 1698 << Endl;; 1699 }; 1700 ; 1701}; 1702 ; 1703////////////////////////////////////////////////////////////////////////////////; 1704void TMVA::MethodDL::FillInputTensor(); 1705{; 1706 // fill the input tensor fXInput from the current Event data; 1707 // with the correct shape depending on the model used; 1708 // The input tensor is used for network prediction after training ; 1709 // using a single event. The network batch size must be equal to 1. ; 1710 // The architecture specified at compile time in ArchitectureImpl_t; 1711 // is used. This should be the CPU architecture; 1712 ; 1713 if (!fNet || fNet->GetDepth() == 0) {; 1714 Log() << kFATAL << ""The network has not been trained and fNet is not built"" << Endl;; 1715 }; 1716 if (fNet->GetBatchSize() != 1) {; 1717 Log() << kFATAL << ""FillINputTensor::Network batch size must be equal to 1 when doing single event predicition"" << Endl;; 1718 }; 1719 ; 1720 // get current event; 1721 const std::vector<Float_t> &inputValues = GetEvent()->GetValues()",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:70523,Safety,predict,prediction,70523,"51 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuffer().CopyFrom(fXInputBuffer);; 1768 return;; 1769}; 1770 ; 1771////////////////////////////////////////////////////////////////////////////////; 1772Double_t MethodDL::GetMvaValue(Double_t * /*errLower*/, Double_t * /*errUpper*/); 1773{; 1774 ; 1775 FillInputTensor();; 1776 ; 1777 // perform the prediction; 1778 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1779 ; 1780 // return value; 1781 double mvaValue = (*fYHat)(0, 0);; 1782 ; 1783 // for debugging; 1784#ifdef DEBUG_MVAVALUE; 1785 using Tensor_t = std::vector<MatrixImpl_t>;; 1786 TMatrixF xInput(n1,n2, inputValues.data() );; 1787 std::cout << ""Input data - class "" << GetEvent()->GetClass() << std::endl;; 1788 xInput.Print();; 1789 std::cout << ""Output of DeepNet "" << mvaValue << std::endl;; 1790 auto & deepnet = *fNet;; 1791 std::cout << ""Loop on layers "" << std::endl;; 1792 for (int l = 0; l < deepnet.GetDepth(); ++l) {; 1793 std::cout << ""Layer "" << l;; 1794 const auto * layer = deepnet.GetLayerAt(l);; 1795 const Tensor_t & layer_output = layer->GetOutput();; 1796 layer->Print();; 1797 std::cout << ""DNN output "" << layer_output.size() << std::endl;; 1798 for (size_t i = 0; i < layer_output.size(); ++i) {; 1799#ifdef R__HAS_TMVAGPU; 1800 //TMatrixD m(layer_output[i].GetNrows(), layer_output[i].GetNcols() , laye",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75474,Safety,predict,predictions,75474,"Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), ""Inference: state weights"");; 1866 // }; 1867 }; 1868 ; 1869 size_t n1 = deepNet.GetBatchHeight();; 1870 size_t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Even",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:76158,Safety,predict,prediction,76158,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:77133,Safety,predict,prediction,77133,"ize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }; 1940 else {; 1941 // case of remaining events: compute prediction by single event !; 1942 for (Long64_t i = ievt; i < lastEvt; ++i) {; 1943 Data()->SetCurrentEvent(i);; 1944 mvaValues[i] = GetMvaValue();; 1945 }; 1946 }; 1947 }; 1948 ; 1949 if (logProgress) {; 1950 Log() << kINFO; 1951 << ""Elapsed time for evaluation of "" << nEvents << "" events: ""; 1952 << timer.GetElapsedTime() << "" "" << Endl;; 1953 }; 1954 ; 1955 return mvaValues;; 1956}; 1957 ; 1958//////////////////////////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 Fil",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:77419,Safety,predict,prediction,77419,"< "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }; 1940 else {; 1941 // case of remaining events: compute prediction by single event !; 1942 for (Long64_t i = ievt; i < lastEvt; ++i) {; 1943 Data()->SetCurrentEvent(i);; 1944 mvaValues[i] = GetMvaValue();; 1945 }; 1946 }; 1947 }; 1948 ; 1949 if (logProgress) {; 1950 Log() << kINFO; 1951 << ""Elapsed time for evaluation of "" << nEvents << "" events: ""; 1952 << timer.GetElapsedTime() << "" "" << Endl;; 1953 }; 1954 ; 1955 return mvaValues;; 1956}; 1957 ; 1958//////////////////////////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 FillInputTensor ();; 1965 ; 1966 // perform the network prediction; 1967 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1968 ; 1969 size_t nTargets = DataInfo().GetNTargets();; 1970 R__ASSERT(nTargets == fYHat->GetNcols());; 1971 ; 1972 std::vector<Float_t> output(nTargets);; 1973 for (size_t i = 0; i < nTargets; i++); 1974 output[i] = (*fYHat)(0, i);; 1975 ; 1976 // ned to transform back output ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:78184,Safety,predict,prediction,78184,"TensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }; 1940 else {; 1941 // case of remaining events: compute prediction by single event !; 1942 for (Long64_t i = ievt; i < lastEvt; ++i) {; 1943 Data()->SetCurrentEvent(i);; 1944 mvaValues[i] = GetMvaValue();; 1945 }; 1946 }; 1947 }; 1948 ; 1949 if (logProgress) {; 1950 Log() << kINFO; 1951 << ""Elapsed time for evaluation of "" << nEvents << "" events: ""; 1952 << timer.GetElapsedTime() << "" "" << Endl;; 1953 }; 1954 ; 1955 return mvaValues;; 1956}; 1957 ; 1958//////////////////////////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 FillInputTensor ();; 1965 ; 1966 // perform the network prediction; 1967 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1968 ; 1969 size_t nTargets = DataInfo().GetNTargets();; 1970 R__ASSERT(nTargets == fYHat->GetNcols());; 1971 ; 1972 std::vector<Float_t> output(nTargets);; 1973 for (size_t i = 0; i < nTargets; i++); 1974 output[i] = (*fYHat)(0, i);; 1975 ; 1976 // ned to transform back output values; 1977 if (fRegressionReturnVal == NULL); 1978 fRegressionReturnVal = new std::vector<Float_t>(nTargets);; 1979 R__ASSERT(fRegressionReturnVal->size() == nTargets);; 1980 ; 1981 // N.B. one should cache here temporary event class; 1982 Event *evT = new Event(*GetEvent());; 1983 for (size_t i = 0; i < nTargets; ++i) {; 1984 evT->SetTarget(i, output[i]);; 1985 }; 1986 const Event *evT2 = GetTransformationHandler().InverseTransform(evT);; 1987 for (size_t i = 0; i < nTarg",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:81072,Safety,predict,prediction,81072,"2023{; 2024 ; 2025 Long64_t nEvents = Data()->GetNEvents();; 2026 if (firstEvt > lastEvt || lastEvt > nEvents) lastEvt = nEvents;; 2027 if (firstEvt < 0) firstEvt = 0;; 2028 nEvents = lastEvt-firstEvt;; 2029 ; 2030 // use same batch size as for training (from first strategy); 2031 size_t defaultEvalBatchSize = (fXInput.GetSize() > 1000) ? 100 : 1000;; 2032 size_t batchSize = (fTrainingSettings.empty()) ? defaultEvalBatchSize : fTrainingSettings.front().batchSize;; 2033 if ( size_t(nEvents) < batchSize ) batchSize = nEvents;; 2034 ; 2035 // using for training same scalar type defined for the prediction; 2036 if (this->GetArchitectureString() == ""GPU"") {; 2037#ifdef R__HAS_TMVAGPU; 2038 Log() << kINFO << ""Evaluate deep neural network on GPU using batches with size = "" << batchSize << Endl << Endl;; 2039#ifdef R__HAS_CUDNN; 2040 return PredictDeepNet<DNN::TCudnn<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2041#else; 2042 return PredictDeepNet<DNN::TCuda<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2043#endif; 2044 ; 2045#endif; 2046 }; 2047 Log() << kINFO << ""Evaluate deep neural network on CPU using batches with size = "" << batchSize << Endl << Endl;; 2048 return PredictDeepNet<DNN::TCpu<ScalarImpl_t> >(firstEvt, lastEvt, batchSize, logProgress);; 2049}; 2050////////////////////////////////////////////////////////////////////////////////; 2051void MethodDL::AddWeightsXMLTo(void * parent) const; 2052{; 2053 // Create the parent XML node with name ""Weights""; 2054 auto & xmlEngine = gTools().xmlengine();; 2055 void* nn = xmlEngine.NewChild(parent, 0, ""Weights"");; 2056 ; 2057 /*! Get all necessary information, in order to be able to reconstruct the net; 2058 * if we read the same XML file. */; 2059 ; 2060 // Deep Net specific info; 2061 Int_t depth = fNet->GetDepth();; 2062 ; 2063 Int_t inputDepth = fNet->GetInputDepth();; 2064 Int_t inputHeight = fNet->GetInputHeight();; 2065 Int_t inputWidth = fNet->GetInputWidth();; 2066 ; 2067 Int_t batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:92999,Safety,predict,predictions,92999,"hHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 2314 //if (fXInput.size() > 0) fXInput.clear();; 2315 //fXInput.emplace_back(MatrixImpl_t(n1,n2));; 2316 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 2317 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 2318 // make here a ColumnMajor tensor; 2319 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth(),TMVA::Experimental::MemoryLayout::ColumnMajor );; 2320 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize());; 2321 ; 2322 // create pointer to output matrix used for the predictions; 2323 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 2324 ; 2325 ; 2326}; 2327 ; 2328 ; 2329////////////////////////////////////////////////////////////////////////////////; 2330void MethodDL::ReadWeightsFromStream(std::istream & /*istr*/); 2331{; 2332}; 2333 ; 2334////////////////////////////////////////////////////////////////////////////////; 2335const Ranking *TMVA::MethodDL::CreateRanking(); 2336{; 2337 // TODO; 2338 return NULL;; 2339}; 2340 ; 2341////////////////////////////////////////////////////////////////////////////////; 2342void MethodDL::GetHelpMessage() const; 2343{; 2344 // TODO; 2345}; 2346 ; 2347} // namespace TMVA; Adadelta.h; Adagrad.h; Adam.h; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cuda.h; DLMinimizers.h; IMethod.h; MethodDL.h; RMSProp.h; e#define e(i)Definition RSha256.hxx:103; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; Long64_tlong long Long64_tDefinition RtypesCore.h:69; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassImp#define ClassImp(name)Definition Rtypes.h:382; SGD.h; TCudnn.h; R__ASSERT#",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:107948,Safety,predict,predictions,107948,"onstDefinition MethodDL.h:278; TMVA::MethodDL::ParseBatchLayoutvoid ParseBatchLayout()Parse the input layout.Definition MethodDL.cxx:482; TMVA::MethodDL::ParseBatchNormLayervoid ParseBatchNormLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate reshape layer.Definition MethodDL.cxx:890; TMVA::MethodDL::ReadWeightsFromStreamvoid ReadWeightsFromStream(std::istream &)Definition MethodDL.cxx:2330; TMVA::MethodDL::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)Definition MethodDL.cxx:2112; TMVA::MethodDL::fNumValidationStringTString fNumValidationStringThe string defining the number (or percentage) of training data used for validation.Definition MethodDL.h:199; TMVA::MethodDL::KeyValueVector_tstd::vector< std::map< TString, TString > > KeyValueVector_tDefinition MethodDL.h:93; TMVA::MethodDL::fOutputFunctionDNN::EOutputFunction fOutputFunctionThe output function for making the predictions.Definition MethodDL.h:189; TMVA::MethodDL::fWeightInitializationDNN::EInitialization fWeightInitializationThe initialization method.Definition MethodDL.h:188; TMVA::MethodDL::GetBatchDepthsize_t GetBatchDepth() constDefinition MethodDL.h:262; TMVA::MethodDL::ERecurrentLayerTypeERecurrentLayerTypeDefinition MethodDL.h:153; TMVA::MethodDL::kLayerLSTM@ kLayerLSTMDefinition MethodDL.h:153; TMVA::MethodDL::kLayerGRU@ kLayerGRUDefinition MethodDL.h:153; TMVA::MethodDL::kLayerRNN@ kLayerRNNDefinition MethodDL.h:153; TMVA::MethodDL::ParseRecurrentLayervoid ParseRecurrentLayer(ERecurrentLayerType type, DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate rnn layer.Definition MethodDL.cxx:931; TMVA::MethodDL::fTrainingSettingsstd::vector< TTrainingSettings > fTrainingSettingsThe vector defini",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:111215,Safety,predict,prediction,111215,"29; TMVA::MethodDL::GetRegressionValuesvirtual const std::vector< Float_t > & GetRegressionValues(); TMVA::MethodDL::fTrainingStrategyStringTString fTrainingStrategyStringThe string defining the training strategy.Definition MethodDL.h:196; TMVA::MethodDL::CreateRankingconst Ranking * CreateRanking()Definition MethodDL.cxx:2335; TMVA::MethodDL::HostBufferImpl_ttypename ArchitectureImpl_t::HostBuffer_t HostBufferImpl_tDefinition MethodDL.h:110; TMVA::MethodDL::SetBatchDepthvoid SetBatchDepth(size_t batchDepth)Definition MethodDL.h:292; TMVA::MethodDL::ParseKeyValueStringKeyValueVector_t ParseKeyValueString(TString parseString, TString blockDelim, TString tokenDelim)Function for parsing the training settings, provided as a string in a key-value form.Definition MethodDL.cxx:1052; TMVA::MethodDL::SetBatchWidthvoid SetBatchWidth(size_t batchWidth)Definition MethodDL.h:294; TMVA::MethodDL::PredictDeepNetstd::vector< Double_t > PredictDeepNet(Long64_t firstEvt, Long64_t lastEvt, size_t batchSize, Bool_t logProgress)perform prediction of the deep neural network using batches (called by GetMvaValues)Definition MethodDL.cxx:1828; TMVA::MethodDL::GetWeightInitializationDNN::EInitialization GetWeightInitialization() constDefinition MethodDL.h:268; TMVA::MethodDL::SetBatchSizevoid SetBatchSize(size_t batchSize)Definition MethodDL.h:291; TMVA::MethodDL::GetLayoutStringTString GetLayoutString() constDefinition MethodDL.h:274; TMVA::MethodDL::fBatchDepthsize_t fBatchDepthThe depth of the batch used to train the deep net.Definition MethodDL.h:182; TMVA::MethodDL::DeepNetImpl_tTMVA::DNN::TDeepNet< ArchitectureImpl_t > DeepNetImpl_tDefinition MethodDL.h:106; TMVA::MethodDL::GetBatchWidthsize_t GetBatchWidth() constDefinition MethodDL.h:264; TMVA::MethodDL::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constDefinition MethodDL.cxx:2051; TMVA::MethodDL::MatrixImpl_ttypename ArchitectureImpl_t::Matrix_t MatrixImpl_tDefinition MethodDL.h:107; TMVA::MethodDL::~MethodDLvirtual ~MethodDL(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:6848,Security,validat,validation,6848,"//////////////////////////////; 167void MethodDL::DeclareOptions(); 168{; 169 // Set default values for all option strings; 170 ; 171 DeclareOptionRef(fInputLayoutString = ""0|0|0"", ""InputLayout"", ""The Layout of the input"");; 172 ; 173 DeclareOptionRef(fBatchLayoutString = ""0|0|0"", ""BatchLayout"", ""The Layout of the batch"");; 174 ; 175 DeclareOptionRef(fLayoutString = ""DENSE|(N+100)*2|SOFTSIGN,DENSE|0|LINEAR"", ""Layout"", ""Layout of the network."");; 176 ; 177 DeclareOptionRef(fErrorStrategy = ""CROSSENTROPY"", ""ErrorStrategy"", ""Loss function: Mean squared error (regression)""; 178 "" or cross entropy (binary classification)."");; 179 AddPreDefVal(TString(""CROSSENTROPY""));; 180 AddPreDefVal(TString(""SUMOFSQUARES""));; 181 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 182 ; 183 DeclareOptionRef(fWeightInitializationString = ""XAVIER"", ""WeightInitialization"", ""Weight initialization strategy"");; 184 AddPreDefVal(TString(""XAVIER""));; 185 AddPreDefVal(TString(""XAVIERUNIFORM""));; 186 AddPreDefVal(TString(""GAUSS""));; 187 AddPreDefVal(TString(""UNIFORM""));; 188 AddPreDefVal(TString(""IDENTITY""));; 189 AddPreDefVal(TString(""ZERO""));; 190 ; 191 DeclareOptionRef(fRandomSeed = 0, ""RandomSeed"", ""Random seed used for weight initialization and batch shuffling"");; 192 ; 193 DeclareOptionRef(fNumValidationString = ""20%"", ""ValidationSize"", ""Part of the training data to use for validation. ""; 194 ""Specify as 0.2 or 20% to use a fifth of the data set as validation set. ""; 195 ""Specify as 100 to use exactly 100 events. (Default: 20%)"");; 196 ; 197 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 198 AddPreDefVal(TString(""STANDARD"")); // deprecated and not supported anymore; 199 AddPreDefVal(TString(""CPU""));; 200 AddPreDefVal(TString(""GPU""));; 201 AddPreDefVal(TString(""OPENCL"")); // not yet implemented; 202 AddPreDefVal(TString(""CUDNN"")); // not needed (by default GPU is now CUDNN if available); 203 ; 204 // define training strategy separat",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:6924,Security,validat,validation,6924,"ut of the network."");; 176 ; 177 DeclareOptionRef(fErrorStrategy = ""CROSSENTROPY"", ""ErrorStrategy"", ""Loss function: Mean squared error (regression)""; 178 "" or cross entropy (binary classification)."");; 179 AddPreDefVal(TString(""CROSSENTROPY""));; 180 AddPreDefVal(TString(""SUMOFSQUARES""));; 181 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 182 ; 183 DeclareOptionRef(fWeightInitializationString = ""XAVIER"", ""WeightInitialization"", ""Weight initialization strategy"");; 184 AddPreDefVal(TString(""XAVIER""));; 185 AddPreDefVal(TString(""XAVIERUNIFORM""));; 186 AddPreDefVal(TString(""GAUSS""));; 187 AddPreDefVal(TString(""UNIFORM""));; 188 AddPreDefVal(TString(""IDENTITY""));; 189 AddPreDefVal(TString(""ZERO""));; 190 ; 191 DeclareOptionRef(fRandomSeed = 0, ""RandomSeed"", ""Random seed used for weight initialization and batch shuffling"");; 192 ; 193 DeclareOptionRef(fNumValidationString = ""20%"", ""ValidationSize"", ""Part of the training data to use for validation. ""; 194 ""Specify as 0.2 or 20% to use a fifth of the data set as validation set. ""; 195 ""Specify as 100 to use exactly 100 events. (Default: 20%)"");; 196 ; 197 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 198 AddPreDefVal(TString(""STANDARD"")); // deprecated and not supported anymore; 199 AddPreDefVal(TString(""CPU""));; 200 AddPreDefVal(TString(""GPU""));; 201 AddPreDefVal(TString(""OPENCL"")); // not yet implemented; 202 AddPreDefVal(TString(""CUDNN"")); // not needed (by default GPU is now CUDNN if available); 203 ; 204 // define training strategy separated by a separator ""|""; 205 DeclareOptionRef(fTrainingStrategyString = ""LearningRate=1e-3,""; 206 ""Momentum=0.0,""; 207 ""ConvergenceSteps=100,""; 208 ""MaxEpochs=2000,""; 209 ""Optimizer=ADAM,""; 210 ""BatchSize=30,""; 211 ""TestRepetitions=1,""; 212 ""WeightDecay=0.0,""; 213 ""Regularization=None,""; 214 ""DropConfig=0.0"",; 215 ""TrainingStrategy"", ""Defines the training strategies."");; 216}; 217 ; 218////////////////////////////////////////",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:42172,Security,validat,validation,42172,"ue = TString(strKeyValue(delimPos + 1, strKeyValue.Length()));; 1079 ; 1080 strKey.Strip(TString::kBoth, ' ');; 1081 strValue.Strip(TString::kBoth, ' ');; 1082 ; 1083 currentBlock.insert(std::make_pair(strKey, strValue));; 1084 }; 1085 }; 1086 return blockKeyValues;; 1087}; 1088 ; 1089////////////////////////////////////////////////////////////////////////////////; 1090/// What kind of analysis type can handle the CNN; 1091Bool_t MethodDL::HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t /*numberTargets*/); 1092{; 1093 if (type == Types::kClassification && numberClasses == 2) return kTRUE;; 1094 if (type == Types::kMulticlass) return kTRUE;; 1095 if (type == Types::kRegression) return kTRUE;; 1096 ; 1097 return kFALSE;; 1098}; 1099 ; 1100////////////////////////////////////////////////////////////////////////////////; 1101/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 1102/// 100 etc.; 1103/// - 20% and 0.2 selects 20% of the training set as validation data.; 1104/// - 100 selects 100 events as the validation data.; 1105///; 1106/// @return number of samples in validation set; 1107///; 1108UInt_t TMVA::MethodDL::GetNumValidationSamples(); 1109{; 1110 Int_t nValidationSamples = 0;; 1111 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 1112 ; 1113 // Parsing + Validation; 1114 // --------------------; 1115 if (fNumValidationString.EndsWith(""%"")) {; 1116 // Relative spec. format 20%; 1117 TString intValStr = TString(fNumValidationString.Strip(TString::kTrailing, '%'));; 1118 ; 1119 if (intValStr.IsFloat()) {; 1120 Double_t valSizeAsDouble = fNumValidationString.Atof() / 100.0;; 1121 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 1122 } else {; 1123 Log() << kFATAL << ""Cannot parse number \"""" << fNumValidationString; 1124 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 1125 }; 1126 } else if (fNumValidationString.IsFloat()) {; 1127 Double_t val",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:42230,Security,validat,validation,42230,"h()));; 1079 ; 1080 strKey.Strip(TString::kBoth, ' ');; 1081 strValue.Strip(TString::kBoth, ' ');; 1082 ; 1083 currentBlock.insert(std::make_pair(strKey, strValue));; 1084 }; 1085 }; 1086 return blockKeyValues;; 1087}; 1088 ; 1089////////////////////////////////////////////////////////////////////////////////; 1090/// What kind of analysis type can handle the CNN; 1091Bool_t MethodDL::HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t /*numberTargets*/); 1092{; 1093 if (type == Types::kClassification && numberClasses == 2) return kTRUE;; 1094 if (type == Types::kMulticlass) return kTRUE;; 1095 if (type == Types::kRegression) return kTRUE;; 1096 ; 1097 return kFALSE;; 1098}; 1099 ; 1100////////////////////////////////////////////////////////////////////////////////; 1101/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 1102/// 100 etc.; 1103/// - 20% and 0.2 selects 20% of the training set as validation data.; 1104/// - 100 selects 100 events as the validation data.; 1105///; 1106/// @return number of samples in validation set; 1107///; 1108UInt_t TMVA::MethodDL::GetNumValidationSamples(); 1109{; 1110 Int_t nValidationSamples = 0;; 1111 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 1112 ; 1113 // Parsing + Validation; 1114 // --------------------; 1115 if (fNumValidationString.EndsWith(""%"")) {; 1116 // Relative spec. format 20%; 1117 TString intValStr = TString(fNumValidationString.Strip(TString::kTrailing, '%'));; 1118 ; 1119 if (intValStr.IsFloat()) {; 1120 Double_t valSizeAsDouble = fNumValidationString.Atof() / 100.0;; 1121 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 1122 } else {; 1123 Log() << kFATAL << ""Cannot parse number \"""" << fNumValidationString; 1124 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 1125 }; 1126 } else if (fNumValidationString.IsFloat()) {; 1127 Double_t valSizeAsDouble = fNumValidationString.Atof();; 1128 ; 112",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:42294,Security,validat,validation,42294,"trKey, strValue));; 1084 }; 1085 }; 1086 return blockKeyValues;; 1087}; 1088 ; 1089////////////////////////////////////////////////////////////////////////////////; 1090/// What kind of analysis type can handle the CNN; 1091Bool_t MethodDL::HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t /*numberTargets*/); 1092{; 1093 if (type == Types::kClassification && numberClasses == 2) return kTRUE;; 1094 if (type == Types::kMulticlass) return kTRUE;; 1095 if (type == Types::kRegression) return kTRUE;; 1096 ; 1097 return kFALSE;; 1098}; 1099 ; 1100////////////////////////////////////////////////////////////////////////////////; 1101/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 1102/// 100 etc.; 1103/// - 20% and 0.2 selects 20% of the training set as validation data.; 1104/// - 100 selects 100 events as the validation data.; 1105///; 1106/// @return number of samples in validation set; 1107///; 1108UInt_t TMVA::MethodDL::GetNumValidationSamples(); 1109{; 1110 Int_t nValidationSamples = 0;; 1111 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 1112 ; 1113 // Parsing + Validation; 1114 // --------------------; 1115 if (fNumValidationString.EndsWith(""%"")) {; 1116 // Relative spec. format 20%; 1117 TString intValStr = TString(fNumValidationString.Strip(TString::kTrailing, '%'));; 1118 ; 1119 if (intValStr.IsFloat()) {; 1120 Double_t valSizeAsDouble = fNumValidationString.Atof() / 100.0;; 1121 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 1122 } else {; 1123 Log() << kFATAL << ""Cannot parse number \"""" << fNumValidationString; 1124 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 1125 }; 1126 } else if (fNumValidationString.IsFloat()) {; 1127 Double_t valSizeAsDouble = fNumValidationString.Atof();; 1128 ; 1129 if (valSizeAsDouble < 1.0) {; 1130 // Relative spec. format 0.2; 1131 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAs",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:43667,Security,validat,validation,43667,"'));; 1118 ; 1119 if (intValStr.IsFloat()) {; 1120 Double_t valSizeAsDouble = fNumValidationString.Atof() / 100.0;; 1121 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 1122 } else {; 1123 Log() << kFATAL << ""Cannot parse number \"""" << fNumValidationString; 1124 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 1125 }; 1126 } else if (fNumValidationString.IsFloat()) {; 1127 Double_t valSizeAsDouble = fNumValidationString.Atof();; 1128 ; 1129 if (valSizeAsDouble < 1.0) {; 1130 // Relative spec. format 0.2; 1131 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 1132 } else {; 1133 // Absolute spec format 100 or 100.0; 1134 nValidationSamples = valSizeAsDouble;; 1135 }; 1136 } else {; 1137 Log() << kFATAL << ""Cannot parse number \"""" << fNumValidationString << ""\"". Expected string like \""0.2\"" or \""100\"".""; 1138 << Endl;; 1139 }; 1140 ; 1141 // Value validation; 1142 // ----------------; 1143 if (nValidationSamples < 0) {; 1144 Log() << kFATAL << ""Validation size \"""" << fNumValidationString << ""\"" is negative."" << Endl;; 1145 }; 1146 ; 1147 if (nValidationSamples == 0) {; 1148 Log() << kFATAL << ""Validation size \"""" << fNumValidationString << ""\"" is zero."" << Endl;; 1149 }; 1150 ; 1151 if (nValidationSamples >= (Int_t)trainingSetSize) {; 1152 Log() << kFATAL << ""Validation size \"""" << fNumValidationString; 1153 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 1154 }; 1155 ; 1156 return nValidationSamples;; 1157}; 1158 ; 1159 ; 1160////////////////////////////////////////////////////////////////////////////////; 1161/// Implementation of architecture specific train method; 1162///; 1163template <typename Architecture_t>; 1164void MethodDL::TrainDeepNet(); 1165{; 1166 ; 1167 using Scalar_t = typename Architecture_t::Scalar_t;; 1168 using Layer_t = TMVA::DNN::VGeneralLayer<Architecture_t>;; 1169 using DeepNet_t = TMVA::DNN::TDeepNet<Arch",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:45055,Security,validat,validation,45055,"Log() << kFATAL << ""Validation size \"""" << fNumValidationString; 1153 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 1154 }; 1155 ; 1156 return nValidationSamples;; 1157}; 1158 ; 1159 ; 1160////////////////////////////////////////////////////////////////////////////////; 1161/// Implementation of architecture specific train method; 1162///; 1163template <typename Architecture_t>; 1164void MethodDL::TrainDeepNet(); 1165{; 1166 ; 1167 using Scalar_t = typename Architecture_t::Scalar_t;; 1168 using Layer_t = TMVA::DNN::VGeneralLayer<Architecture_t>;; 1169 using DeepNet_t = TMVA::DNN::TDeepNet<Architecture_t, Layer_t>;; 1170 using TensorDataLoader_t = TTensorDataLoader<TMVAInput_t, Architecture_t>;; 1171 ; 1172 bool debug = Log().GetMinType() == kDEBUG;; 1173 ; 1174 ; 1175 // set the random seed for weight initialization; 1176 Architecture_t::SetRandomSeed(fRandomSeed);; 1177 ; 1178 ///split training data in training and validation data; 1179 // and determine the number of training and testing examples; 1180 ; 1181 size_t nValidationSamples = GetNumValidationSamples();; 1182 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1183 ; 1184 const std::vector<TMVA::Event *> &allData = GetEventCollection(Types::kTraining);; 1185 const std::vector<TMVA::Event *> eventCollectionTraining{allData.begin(), allData.begin() + nTrainingSamples};; 1186 const std::vector<TMVA::Event *> eventCollectionValidation{allData.begin() + nTrainingSamples, allData.end()};; 1187 ; 1188 size_t trainingPhase = 1;; 1189 ; 1190 for (TTrainingSettings &settings : this->GetTrainingSettings()) {; 1191 ; 1192 size_t nThreads = 1; // FIXME threads are hard coded to 1, no use of slave threads or multi-threading; 1193 ; 1194 ; 1195 // After the processing of the options, initialize the master deep net; 1196 size_t batchSize = settings.batchSize;; 1197 this->SetBatchSize(batchSize);; 1198 // Should be replaced b",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:51650,Security,validat,validation,51650,"utHeight() == 1 && GetInputDepth() == 1) n1 = fNet->GetBatchSize();; 1297 //fXInput = TensorImpl_t(1,n1,n2);; 1298 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 1299 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 1300 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth() );; 1301 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : valida",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52031,Security,validat,validationTuple,52031,"Size() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 134",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52123,Security,validat,validationData,52123,"Size() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 134",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52138,Security,validat,validationTuple,52138,"Size() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 134",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52624,Security,validat,validation,52624,"validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Archi",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52675,Security,validat,validationData,52675,"validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Archi",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60910,Security,validat,validation,60910,"re_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:61176,Security,validat,validation,61176,"::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:61294,Security,validat,validationData,61294,";; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:105829,Security,validat,validation,105829,"itializationsDefinition MethodDL.cxx:432; TMVA::MethodDL::MethodDLMethodDL(const TString &jobName, const TString &methodTitle, DataSetInfo &theData, const TString &theOption)Constructor.Definition MethodDL.cxx:1019; TMVA::MethodDL::TrainDeepNetvoid TrainDeepNet()train of deep neural network using the defined architectureDefinition MethodDL.cxx:1164; TMVA::MethodDL::GetTrainingSettingsconst std::vector< TTrainingSettings > & GetTrainingSettings() constDefinition MethodDL.h:280; TMVA::MethodDL::GetOutputFunctionDNN::EOutputFunction GetOutputFunction() constDefinition MethodDL.h:269; TMVA::MethodDL::ParseDenseLayervoid ParseDenseLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate dense layer.Definition MethodDL.cxx:583; TMVA::MethodDL::GetNumValidationSamplesUInt_t GetNumValidationSamples()parce the validation string and return the number of event data used for validation; TMVA::MethodDL::GetBatchLayoutStringTString GetBatchLayoutString() constDefinition MethodDL.h:273; TMVA::MethodDL::SetInputWidthvoid SetInputWidth(int inputWidth)Definition MethodDL.h:288; TMVA::MethodDL::ProcessOptionsvoid ProcessOptions()Definition MethodDL.cxx:219; TMVA::MethodDL::fXInputBufferHostBufferImpl_t fXInputBufferDefinition MethodDL.h:207; TMVA::MethodDL::fBatchWidthsize_t fBatchWidthThe width of the batch used to train the deep net.Definition MethodDL.h:184; TMVA::MethodDL::GetInputDepthsize_t GetInputDepth() constDefinition MethodDL.h:255; TMVA::MethodDL::fNetstd::unique_ptr< DeepNetImpl_t > fNetDefinition MethodDL.h:209; TMVA::MethodDL::GetInputLayoutStringTString GetInputLayoutString() constDefinition MethodDL.h:272; TMVA::MethodDL::SetBatchHeightvoid SetBatchHeight(size_t batchHeight)Definition MethodDL.h:293; TMVA::MethodDL::GetInputHeightsize_t GetInputHeight() constDefinition MethodDL.h:256; TMVA::MethodDL::GetArchitectureStringT",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:105892,Security,validat,validation,105892,"itializationsDefinition MethodDL.cxx:432; TMVA::MethodDL::MethodDLMethodDL(const TString &jobName, const TString &methodTitle, DataSetInfo &theData, const TString &theOption)Constructor.Definition MethodDL.cxx:1019; TMVA::MethodDL::TrainDeepNetvoid TrainDeepNet()train of deep neural network using the defined architectureDefinition MethodDL.cxx:1164; TMVA::MethodDL::GetTrainingSettingsconst std::vector< TTrainingSettings > & GetTrainingSettings() constDefinition MethodDL.h:280; TMVA::MethodDL::GetOutputFunctionDNN::EOutputFunction GetOutputFunction() constDefinition MethodDL.h:269; TMVA::MethodDL::ParseDenseLayervoid ParseDenseLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate dense layer.Definition MethodDL.cxx:583; TMVA::MethodDL::GetNumValidationSamplesUInt_t GetNumValidationSamples()parce the validation string and return the number of event data used for validation; TMVA::MethodDL::GetBatchLayoutStringTString GetBatchLayoutString() constDefinition MethodDL.h:273; TMVA::MethodDL::SetInputWidthvoid SetInputWidth(int inputWidth)Definition MethodDL.h:288; TMVA::MethodDL::ProcessOptionsvoid ProcessOptions()Definition MethodDL.cxx:219; TMVA::MethodDL::fXInputBufferHostBufferImpl_t fXInputBufferDefinition MethodDL.h:207; TMVA::MethodDL::fBatchWidthsize_t fBatchWidthThe width of the batch used to train the deep net.Definition MethodDL.h:184; TMVA::MethodDL::GetInputDepthsize_t GetInputDepth() constDefinition MethodDL.h:255; TMVA::MethodDL::fNetstd::unique_ptr< DeepNetImpl_t > fNetDefinition MethodDL.h:209; TMVA::MethodDL::GetInputLayoutStringTString GetInputLayoutString() constDefinition MethodDL.h:272; TMVA::MethodDL::SetBatchHeightvoid SetBatchHeight(size_t batchHeight)Definition MethodDL.h:293; TMVA::MethodDL::GetInputHeightsize_t GetInputHeight() constDefinition MethodDL.h:256; TMVA::MethodDL::GetArchitectureStringT",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:107690,Security,validat,validation,107690,"efinition MethodDL.h:272; TMVA::MethodDL::SetBatchHeightvoid SetBatchHeight(size_t batchHeight)Definition MethodDL.h:293; TMVA::MethodDL::GetInputHeightsize_t GetInputHeight() constDefinition MethodDL.h:256; TMVA::MethodDL::GetArchitectureStringTString GetArchitectureString() constDefinition MethodDL.h:278; TMVA::MethodDL::ParseBatchLayoutvoid ParseBatchLayout()Parse the input layout.Definition MethodDL.cxx:482; TMVA::MethodDL::ParseBatchNormLayervoid ParseBatchNormLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate reshape layer.Definition MethodDL.cxx:890; TMVA::MethodDL::ReadWeightsFromStreamvoid ReadWeightsFromStream(std::istream &)Definition MethodDL.cxx:2330; TMVA::MethodDL::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)Definition MethodDL.cxx:2112; TMVA::MethodDL::fNumValidationStringTString fNumValidationStringThe string defining the number (or percentage) of training data used for validation.Definition MethodDL.h:199; TMVA::MethodDL::KeyValueVector_tstd::vector< std::map< TString, TString > > KeyValueVector_tDefinition MethodDL.h:93; TMVA::MethodDL::fOutputFunctionDNN::EOutputFunction fOutputFunctionThe output function for making the predictions.Definition MethodDL.h:189; TMVA::MethodDL::fWeightInitializationDNN::EInitialization fWeightInitializationThe initialization method.Definition MethodDL.h:188; TMVA::MethodDL::GetBatchDepthsize_t GetBatchDepth() constDefinition MethodDL.h:262; TMVA::MethodDL::ERecurrentLayerTypeERecurrentLayerTypeDefinition MethodDL.h:153; TMVA::MethodDL::kLayerLSTM@ kLayerLSTMDefinition MethodDL.h:153; TMVA::MethodDL::kLayerGRU@ kLayerGRUDefinition MethodDL.h:153; TMVA::MethodDL::kLayerRNN@ kLayerRNNDefinition MethodDL.h:153; TMVA::MethodDL::ParseRecurrentLayervoid ParseRecurrentLayer(ERecurrentLayerType type, DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vecto",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:12891,Testability,test,testInterval,12891,"eightInitialization = DNN::EInitialization::kGauss;; 318 } else if (fWeightInitializationString == ""UNIFORM"") {; 319 fWeightInitialization = DNN::EInitialization::kUniform;; 320 } else if (fWeightInitializationString == ""ZERO"") {; 321 fWeightInitialization = DNN::EInitialization::kZero;; 322 } else if (fWeightInitializationString == ""IDENTITY"") {; 323 fWeightInitialization = DNN::EInitialization::kIdentity;; 324 } else {; 325 fWeightInitialization = DNN::EInitialization::kGlorotUniform;; 326 }; 327 ; 328 // Training settings.; 329 ; 330 KeyValueVector_t strategyKeyValues = ParseKeyValueString(fTrainingStrategyString, TString(""|""), TString("",""));; 331 for (auto &block : strategyKeyValues) {; 332 TTrainingSettings settings;; 333 ; 334 settings.convergenceSteps = fetchValueTmp(block, ""ConvergenceSteps"", 100);; 335 settings.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else i",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:45121,Testability,test,testing,45121,"Log() << kFATAL << ""Validation size \"""" << fNumValidationString; 1153 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 1154 }; 1155 ; 1156 return nValidationSamples;; 1157}; 1158 ; 1159 ; 1160////////////////////////////////////////////////////////////////////////////////; 1161/// Implementation of architecture specific train method; 1162///; 1163template <typename Architecture_t>; 1164void MethodDL::TrainDeepNet(); 1165{; 1166 ; 1167 using Scalar_t = typename Architecture_t::Scalar_t;; 1168 using Layer_t = TMVA::DNN::VGeneralLayer<Architecture_t>;; 1169 using DeepNet_t = TMVA::DNN::TDeepNet<Architecture_t, Layer_t>;; 1170 using TensorDataLoader_t = TTensorDataLoader<TMVAInput_t, Architecture_t>;; 1171 ; 1172 bool debug = Log().GetMinType() == kDEBUG;; 1173 ; 1174 ; 1175 // set the random seed for weight initialization; 1176 Architecture_t::SetRandomSeed(fRandomSeed);; 1177 ; 1178 ///split training data in training and validation data; 1179 // and determine the number of training and testing examples; 1180 ; 1181 size_t nValidationSamples = GetNumValidationSamples();; 1182 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1183 ; 1184 const std::vector<TMVA::Event *> &allData = GetEventCollection(Types::kTraining);; 1185 const std::vector<TMVA::Event *> eventCollectionTraining{allData.begin(), allData.begin() + nTrainingSamples};; 1186 const std::vector<TMVA::Event *> eventCollectionValidation{allData.begin() + nTrainingSamples, allData.end()};; 1187 ; 1188 size_t trainingPhase = 1;; 1189 ; 1190 for (TTrainingSettings &settings : this->GetTrainingSettings()) {; 1191 ; 1192 size_t nThreads = 1; // FIXME threads are hard coded to 1, no use of slave threads or multi-threading; 1193 ; 1194 ; 1195 // After the processing of the options, initialize the master deep net; 1196 size_t batchSize = settings.batchSize;; 1197 this->SetBatchSize(batchSize);; 1198 // Should be replaced b",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:48586,Testability,test,test,48586,"ut) should be equal to given batch size %zu"",batchHeight,batchSize);; 1223 return;; 1224 }; 1225 ; 1226 ; 1227 //check also that input layout compatible with batch layout; 1228 bool badLayout = false;; 1229 // case batch depth == batch size; 1230 if (batchDepth == batchSize); 1231 badLayout = ( inputDepth * inputHeight * inputWidth != batchHeight * batchWidth ) ;; 1232 // case batch Height is batch size; 1233 if (batchHeight == batchSize && batchDepth == 1); 1234 badLayout |= ( inputDepth * inputHeight * inputWidth != batchWidth);; 1235 if (badLayout) {; 1236 Error(""Train"",""Given input layout %zu x %zu x %zu is not compatible with batch layout %zu x %zu x %zu "",; 1237 inputDepth,inputHeight,inputWidth,batchDepth,batchHeight,batchWidth);; 1238 return;; 1239 }; 1240 ; 1241 // check batch size is compatible with number of events; 1242 if (nTrainingSamples < settings.batchSize || nValidationSamples < settings.batchSize) {; 1243 Log() << kFATAL << ""Number of samples in the datasets are train: (""; 1244 << nTrainingSamples << "") test: ("" << nValidationSamples; 1245 << ""). One of these is smaller than the batch size of ""; 1246 << settings.batchSize << "". Please increase the batch""; 1247 << "" size to be at least the same size as the smallest""; 1248 << "" of them."" << Endl;; 1249 }; 1250 ; 1251 DeepNet_t deepNet(batchSize, inputDepth, inputHeight, inputWidth, batchDepth, batchHeight, batchWidth, J, I, R, weightDecay);; 1252 ; 1253 // create a copy of DeepNet for evaluating but with batch size = 1; 1254 // fNet is the saved network and will be with CPU or Referrence architecture; 1255 if (trainingPhase == 1) {; 1256 fNet = std::unique_ptr<DeepNetImpl_t>(new DeepNetImpl_t(1, inputDepth, inputHeight, inputWidth, batchDepth,; 1257 batchHeight, batchWidth, J, I, R, weightDecay));; 1258 fBuildNet = true;; 1259 }; 1260 else; 1261 fBuildNet = false;; 1262 ; 1263 // Initialize the vector of slave nets; 1264 std::vector<DeepNet_t> nets{};; 1265 nets.reserve(nThreads);; 1266 for (size_t ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:51591,Testability,test,testing,51591,"utHeight() == 1 && GetInputDepth() == 1) n1 = fNet->GetBatchSize();; 1297 //fXInput = TensorImpl_t(1,n1,n2);; 1298 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 1299 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 1300 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth() );; 1301 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize() );; 1302 ; 1303 ; 1304 // create pointer to output matrix used for the predictions; 1305 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 1306 ; 1307 // print the created network; 1308 Log() << ""***** Deep Learning Network *****"" << Endl;; 1309 if (Log().GetMinType() <= kINFO); 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : valida",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:52448,Testability,test,test,52448,"; 1310 deepNet.Print();; 1311 }; 1312 Log() << ""Using "" << nTrainingSamples << "" events for training and "" << nValidationSamples << "" for testing"" << Endl;; 1313 ; 1314 // Loading the training and validation datasets; 1315 TMVAInput_t trainingTuple = std::tie(eventCollectionTraining, DataInfo());; 1316 TensorDataLoader_t trainingData(trainingTuple, nTrainingSamples, batchSize,; 1317 {inputDepth, inputHeight, inputWidth},; 1318 {deepNet.GetBatchDepth(), deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1319 deepNet.GetOutputWidth(), nThreads);; 1320 ; 1321 TMVAInput_t validationTuple = std::tie(eventCollectionValidation, DataInfo());; 1322 TensorDataLoader_t validationData(validationTuple, nValidationSamples, batchSize,; 1323 {inputDepth, inputHeight, inputWidth},; 1324 { deepNet.GetBatchDepth(),deepNet.GetBatchHeight(), deepNet.GetBatchWidth()} ,; 1325 deepNet.GetOutputWidth(), nThreads);; 1326 ; 1327 ; 1328 ; 1329 // do an evaluation of the network to compute initial minimum test error; 1330 ; 1331 Bool_t includeRegularization = (R != DNN::ERegularization::kNone);; 1332 ; 1333 Double_t minValError = 0.0;; 1334 Log() << ""Compute initial loss on the validation data "" << Endl;; 1335 for (auto batch : validationData) {; 1336 auto inputTensor = batch.GetInput();; 1337 auto outputMatrix = batch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55660,Testability,log,logging,55660,"rop<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << En",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:57847,Testability,test,test,57847,"30 << ""t(s)/epoch"" << std::setw(12) << ""t(s)/Loss"" << std::setw(12) << ""nEvents/s"" << std::setw(12); 1431 << ""Conv. Steps"" << Endl;; 1432 Log() << separator << Endl;; 1433 }; 1434 ; 1435 // set up generator for shuffling the batches; 1436 // if seed is zero we have always a different order in the batches; 1437 size_t shuffleSeed = 0;; 1438 if (fRandomSeed != 0) shuffleSeed = fRandomSeed + trainingPhase;; 1439 RandomGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1445 for (size_t l = 0; l < weights_tensor.size(); ++l); 1446 weights_tensor[l].Print();; 1447 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1448 bias_tensor[0].Print();; 1449 }; 1450 ; 1451 Log() << "" Start epoch iteration ..."" << Endl;; 1452 bool debugFirstEpoch = false;; 1453 bool computeLossInTraining = true; // compute loss in training or at test time; 1454 size_t nTrainEpochs = 0;; 1455 while (!converged) {; 1456 nTrainEpochs++;; 1457 trainingData.Shuffle(rng);; 1458 ; 1459 // execute all epochs; 1460 //for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1461 ; 1462 Double_t trainingError = 0;; 1463 for (size_t i = 0; i < batchesInEpoch; ++i ) {; 1464 // Clean and load new batches, one batch for one slave net; 1465 //batches.clear();; 1466 //batches.reserve(nThreads);; 1467 //for (size_t j = 0; j < nThreads; j++) {; 1468 // batches.push_back(trainingData.GetTensorBatch());; 1469 //}; 1470 if (debugFirstEpoch) std::cout << ""\n\n----- batch # "" << i << ""\n\n"";; 1471 ; 1472 auto my_batch = trainingData.GetTensorBatch();; 1473 ; 1474 if (debugFirstEpoch); 1475 std::cout << ""got batch data - doing forward \n"";; 1476 ; 1477#ifdef DEBUG; 1478 ; 1479 Architecture_t::PrintTensor(my_batch.GetInput(),""input tensor"",true);; 1480 typename Architecture_t::Tensor_t tOut(my_batch.GetOutput());; 1481 typename Architecture_t::Te",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:61007,Testability,test,testInterval,61007,"::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:62162,Testability,test,testInterval,62162," inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 // should we apply droput to the loss ??; 1551 valError += deepNet.Loss(inputTensor, outputMatrix, weights, inTraining, includeRegularization);; 1552 }; 1553 // normalize loss to number of batches and add regularization term; 1554 Double_t regTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1555 valError /= (Double_t)(nValidationSamples / settings.batchSize);; 1556 valError += regTerm;; 1557 ; 1558 //Log the loss value; 1559 fTrainHistory.AddValue(""valError"",nTrainEpochs,valError);; 1560 ; 1561 t2 = std::chrono::system_clock::now();; 1562 ; 1563 // checking for convergence; 1564 if (valError < minValError) {; 1565 convergenceCount = 0;; 1566 } else {; 1567 convergenceCount += settings.testInterval;; 1568 }; 1569 ; 1570 // copy configuration when reached a minimum error; 1571 if (valError < minValError ) {; 1572 // Copy weights from deepNet to fNet; 1573 Log() << std::setw(10) << nTrainEpochs; 1574 << "" Minimum Test error found - save the configuration "" << Endl;; 1575 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1576 fNet->GetLayerAt(i)->CopyParameters(*deepNet.GetLayerAt(i));; 1577 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1578 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), "" input weights"");; 1579 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), "" state weights"");; 1580 // }; 1581 }; 1582 // Architecture_t::PrintTensor(deepNet.GetLayerAt(1)->GetWeightsAt(0), "" cudnn weights"");; 1583 // ArchitectureImpl_t::PrintTensor(fNet->GetLayerAt(1)->GetWeightsAt(0), "" cpu weights"");; 1584 ; 1585 minValError = valError;; 1586 }; 1587 else if ( minValError <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor =",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:64167,Testability,test,test,64167,"or <= 0. ); 1588 minValError = valError;; 1589 ; 1590 if (!computeLossInTraining) {; 1591 trainingError = 0.0;; 1592 // Compute training error.; 1593 for (auto batch : trainingData) {; 1594 auto inputTensor = batch.GetInput();; 1595 auto outputMatrix = batch.GetOutput();; 1596 auto weights = batch.GetWeights();; 1597 trainingError += deepNet.Loss(inputTensor, outputMatrix, weights, false, false);; 1598 }; 1599 }; 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / settings.testInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_c",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:64341,Testability,test,testInterval,64341,"rix = batch.GetOutput();; 1596 auto weights = batch.GetWeights();; 1597 trainingError += deepNet.Loss(inputTensor, outputMatrix, weights, false, false);; 1598 }; 1599 }; 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / settings.testInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_clock::now();; 1639 }; 1640 ; 1641 // if (stepCount % 10 == 0 || converged) {; 1642 if (converged && debug) {; 1643 Log() << ""Final Deep Net Weights for phase "" << trainingPhase << "" epoch "" << nTrainEpochs; 1644 << Endl;; 1645 auto & weights_tensor",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:64518,Testability,test,testInterval,64518," 1600 // normalize loss to number of batches and add regularization term; 1601 trainingError /= (Double_t)(nTrainingSamples / settings.batchSize);; 1602 trainingError += regTerm;; 1603 ; 1604 //Log the loss value; 1605 fTrainHistory.AddValue(""trainingError"",nTrainEpochs,trainingError);; 1606 ; 1607 // stop measuring; 1608 tend = std::chrono::system_clock::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / settings.testInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_clock::now();; 1639 }; 1640 ; 1641 // if (stepCount % 10 == 0 || converged) {; 1642 if (converged && debug) {; 1643 Log() << ""Final Deep Net Weights for phase "" << trainingPhase << "" epoch "" << nTrainEpochs; 1644 << Endl;; 1645 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1646 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1647 for (size_t l = 0; l < weights_tensor.size(); ++l); 1648 we",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:64857,Testability,test,testInterval,64857,"ck::now();; 1609 ; 1610 // Compute numerical throughput.; 1611 std::chrono::duration<double> elapsed_seconds = tend - tstart;; 1612 std::chrono::duration<double> elapsed1 = t1-tstart;; 1613 // std::chrono::duration<double> elapsed2 = t2-tstart;; 1614 // time to compute training and test errors; 1615 std::chrono::duration<double> elapsed_testing = tend-t1;; 1616 ; 1617 double seconds = elapsed_seconds.count();; 1618 // double nGFlops = (double)(settings.testInterval * batchesInEpoch * settings.batchSize)*1.E-9;; 1619 // nGFlops *= deepnet.GetNFlops() * 1e-9;; 1620 double eventTime = elapsed1.count()/( batchesInEpoch * settings.testInterval * settings.batchSize);; 1621 ; 1622 converged =; 1623 convergenceCount > settings.convergenceSteps || nTrainEpochs >= settings.maxEpochs;; 1624 ; 1625 ; 1626 Log() << std::setw(10) << nTrainEpochs << "" | ""; 1627 << std::setw(12) << trainingError; 1628 << std::setw(12) << valError; 1629 << std::setw(12) << seconds / settings.testInterval; 1630 << std::setw(12) << elapsed_testing.count(); 1631 << std::setw(12) << 1. / eventTime; 1632 << std::setw(12) << convergenceCount; 1633 << Endl;; 1634 ; 1635 if (converged) {; 1636 Log() << Endl;; 1637 }; 1638 tstart = std::chrono::system_clock::now();; 1639 }; 1640 ; 1641 // if (stepCount % 10 == 0 || converged) {; 1642 if (converged && debug) {; 1643 Log() << ""Final Deep Net Weights for phase "" << trainingPhase << "" epoch "" << nTrainEpochs; 1644 << Endl;; 1645 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1646 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1647 for (size_t l = 0; l < weights_tensor.size(); ++l); 1648 weights_tensor[l].Print();; 1649 bias_tensor[0].Print();; 1650 }; 1651 ; 1652 }; 1653 ; 1654 trainingPhase++;; 1655 } // end loop on training Phase; 1656}; 1657 ; 1658////////////////////////////////////////////////////////////////////////////////; 1659void MethodDL::Train(); 1660{; 1661 if (fInteractive) {; 1662 Log() << kFATAL << ""Not implemented yet"" <",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:69523,Testability,assert,assert,69523,"tValues = GetEvent()->GetValues();; 1722 size_t nVariables = GetEvent()->GetNVariables();; 1723 ; 1724 // for Columnlayout tensor memory layout is HWC while for rowwise is CHW; 1725 if (fXInput.GetLayout() == TMVA::Experimental::MemoryLayout::ColumnMajor) {; 1726 R__ASSERT(fXInput.GetShape().size() < 4);; 1727 size_t nc, nhw = 0;; 1728 if (fXInput.GetShape().size() == 2) {; 1729 nc = fXInput.GetShape()[0];; 1730 if (nc != 1) {; 1731 ArchitectureImpl_t::PrintTensor(fXInput);; 1732 Log() << kFATAL << ""First tensor dimension should be equal to batch size, i.e. = 1"" << Endl;; 1733 }; 1734 nhw = fXInput.GetShape()[1];; 1735 } else {; 1736 nc = fXInput.GetCSize();; 1737 nhw = fXInput.GetWSize();; 1738 }; 1739 if (nVariables != nc * nhw) {; 1740 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1741 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nhw << Endl;; 1742 }; 1743 for (size_t j = 0; j < nc; j++) {; 1744 for (size_t k = 0; k < nhw; k++) {; 1745 // note that in TMVA events images are stored as C H W while in the buffer we stored as H W C; 1746 fXInputBuffer[k * nc + j] = inputValues[j * nhw + k]; // for column layout !!!; 1747 }; 1748 }; 1749 } else {; 1750 // row-wise layout; 1751 assert(fXInput.GetShape().size() >= 4);; 1752 size_t nc = fXInput.GetCSize();; 1753 size_t nh = fXInput.GetHSize();; 1754 size_t nw = fXInput.GetWSize();; 1755 size_t n = nc * nh * nw;; 1756 if (nVariables != n) {; 1757 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1758 << "" n-event variables "" << nVariables << "" expected input tensor "" << nc << "" x "" << nh << "" x "" << nw; 1759 << Endl;; 1760 }; 1761 for (size_t j = 0; j < n; j++) {; 1762 // in this case TMVA event has same order as input tensor; 1763 fXInputBuffer[j] = inputValues[j]; // for column layout !!!; 1764 }; 1765 }; 1766 // copy buffer in input; 1767 fXInput.GetDeviceBuf",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:72786,Testability,log,logProgress,72786,"Print();; 1818 }; 1819 }; 1820#endif; 1821 ; 1822 return (TMath::IsNaN(mvaValue)) ? -999. : mvaValue;; 1823}; 1824////////////////////////////////////////////////////////////////////////////////; 1825/// Evaluate the DeepNet on a vector of input values stored in the TMVA Event class; 1826////////////////////////////////////////////////////////////////////////////////; 1827template <typename Architecture_t>; 1828std::vector<Double_t> MethodDL::PredictDeepNet(Long64_t firstEvt, Long64_t lastEvt, size_t batchSize, Bool_t logProgress); 1829{; 1830 ; 1831 // Check whether the model is setup; 1832 if (!fNet || fNet->GetDepth() == 0) {; 1833 Log() << kFATAL << ""The network has not been trained and fNet is not built""; 1834 << Endl;; 1835 }; 1836 ; 1837 // rebuild the networks; 1838 this->SetBatchSize(batchSize);; 1839 size_t inputDepth = this->GetInputDepth();; 1840 size_t inputHeight = this->GetInputHeight();; 1841 size_t inputWidth = this->GetInputWidth();; 1842 size_t batchDepth = this->GetBatchDepth();; 1843 size_t batchHeight = this->GetBatchHeight();; 1844 size_t batchWidth = this->GetBatchWidth();; 1845 ELossFunction J = fNet->GetLossFunction();; 1846 EInitialization I = fNet->GetInitialization();; 1847 ERegularization R = fNet->GetRegularization();; 1848 Double_t weightDecay = fNet->GetWeightDecay();; 1849 ; 1850 using DeepNet_t = TMVA::DNN::TDeepNet<Architecture_t>;; 1851 using Matrix_t = typename Architecture_t::Matrix_t;; 1852 using TensorDataLoader_t = TTensorDataLoader<TMVAInput_t, Architecture_t>;; 1853 ; 1854 // create the deep neural network; 1855 DeepNet_t deepNet(batchSize, inputDepth, inputHeight, inputWidth, batchDepth, batchHeight, batchWidth, J, I, R, weightDecay);; 1856 std::vector<DeepNet_t> nets{};; 1857 fBuildNet = false;; 1858 CreateDeepNet(deepNet,nets);; 1859 ; 1860 // copy weights from the saved fNet to the built DeepNet; 1861 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1862 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 18",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75057,Testability,test,testTuple,75057,"ts from the saved fNet to the built DeepNet; 1861 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1862 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1863 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1864 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), ""Inference: input weights"");; 1865 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), ""Inference: state weights"");; 1866 // }; 1867 }; 1868 ; 1869 size_t n1 = deepNet.GetBatchHeight();; 1870 size_t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75162,Testability,test,testData,75162,"ts from the saved fNet to the built DeepNet; 1861 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1862 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1863 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1864 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), ""Inference: input weights"");; 1865 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), ""Inference: state weights"");; 1866 // }; 1867 }; 1868 ; 1869 size_t n1 = deepNet.GetBatchHeight();; 1870 size_t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75171,Testability,test,testTuple,75171,"ts from the saved fNet to the built DeepNet; 1861 for (size_t i = 0; i < deepNet.GetDepth(); ++i) {; 1862 deepNet.GetLayerAt(i)->CopyParameters(*fNet->GetLayerAt(i));; 1863 // if (i == 0 && deepNet.GetLayerAt(0)->GetWeights().size() > 1) {; 1864 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(0), ""Inference: input weights"");; 1865 // Architecture_t::PrintTensor(deepNet.GetLayerAt(0)->GetWeightsAt(1), ""Inference: state weights"");; 1866 // }; 1867 }; 1868 ; 1869 size_t n1 = deepNet.GetBatchHeight();; 1870 size_t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75649,Testability,log,logProgress,75649,"t n2 = deepNet.GetBatchWidth();; 1871 size_t n0 = deepNet.GetBatchSize();; 1872 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 1873 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1) {; 1874 n1 = deepNet.GetBatchSize();; 1875 n0 = 1;; 1876 }; 1877 //this->SetBatchDepth(n0);; 1878 Long64_t nEvents = lastEvt - firstEvt;; 1879 TMVAInput_t testTuple = std::tie(GetEventCollection(Data()->GetCurrentType()), DataInfo());; 1880 TensorDataLoader_t testData(testTuple, nEvents, batchSize, {inputDepth, inputHeight, inputWidth}, {n0, n1, n2}, deepNet.GetOutputWidth(), 1);; 1881 ; 1882 ; 1883 // Tensor_t xInput;; 1884 // for (size_t i = 0; i < n0; ++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:75851,Testability,test,testing,75851,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:77000,Testability,test,testData,77000,"++i); 1885 // xInput.emplace_back(Matrix_t(n1,n2));; 1886 ; 1887 // create pointer to output matrix used for the predictions; 1888 Matrix_t yHat(deepNet.GetBatchSize(), deepNet.GetOutputWidth() );; 1889 ; 1890 // use timer; 1891 Timer timer( nEvents, GetName(), kTRUE );; 1892 ; 1893 if (logProgress); 1894 Log() << kHEADER << Form(""[%s] : "",DataInfo().GetName()); 1895 << ""Evaluation of "" << GetMethodName() << "" on ""; 1896 << (Data()->GetCurrentType() == Types::kTraining ? ""training"" : ""testing""); 1897 << "" sample ("" << nEvents << "" events)"" << Endl;; 1898 ; 1899 ; 1900 // eventg loop; 1901 std::vector<double> mvaValues(nEvents);; 1902 ; 1903 ; 1904 for ( Long64_t ievt = firstEvt; ievt < lastEvt; ievt+=batchSize) {; 1905 ; 1906 Long64_t ievt_end = ievt + batchSize;; 1907 // case of batch prediction for; 1908 if (ievt_end <= lastEvt) {; 1909 ; 1910 if (ievt == firstEvt) {; 1911 Data()->SetCurrentEvent(ievt);; 1912 size_t nVariables = GetEvent()->GetNVariables();; 1913 ; 1914 if (n1 == batchSize && n0 == 1) {; 1915 if (n2 != nVariables) {; 1916 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1917 << "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:77609,Testability,log,logProgress,77609,"< "" n-event variables "" << nVariables << "" expected input matrix "" << n1 << "" x "" << n2; 1918 << Endl;; 1919 }; 1920 } else {; 1921 if (n1*n2 != nVariables || n0 != batchSize) {; 1922 Log() << kFATAL << ""Input Event variable dimensions are not compatible with the built network architecture""; 1923 << "" n-event variables "" << nVariables << "" expected input tensor "" << n0 << "" x "" << n1 << "" x "" << n2; 1924 << Endl;; 1925 }; 1926 }; 1927 }; 1928 ; 1929 auto batch = testData.GetTensorBatch();; 1930 auto inputTensor = batch.GetInput();; 1931 ; 1932 auto xInput = batch.GetInput();; 1933 // make the prediction; 1934 deepNet.Prediction(yHat, xInput, fOutputFunction);; 1935 for (size_t i = 0; i < batchSize; ++i) {; 1936 double value = yHat(i,0);; 1937 mvaValues[ievt + i] = (TMath::IsNaN(value)) ? -999. : value;; 1938 }; 1939 }; 1940 else {; 1941 // case of remaining events: compute prediction by single event !; 1942 for (Long64_t i = ievt; i < lastEvt; ++i) {; 1943 Data()->SetCurrentEvent(i);; 1944 mvaValues[i] = GetMvaValue();; 1945 }; 1946 }; 1947 }; 1948 ; 1949 if (logProgress) {; 1950 Log() << kINFO; 1951 << ""Elapsed time for evaluation of "" << nEvents << "" events: ""; 1952 << timer.GetElapsedTime() << "" "" << Endl;; 1953 }; 1954 ; 1955 return mvaValues;; 1956}; 1957 ; 1958//////////////////////////////////////////////////////////////////////////; 1959/// Get the regression output values for a single event; 1960//////////////////////////////////////////////////////////////////////////; 1961const std::vector<Float_t> & TMVA::MethodDL::GetRegressionValues(); 1962{; 1963 ; 1964 FillInputTensor ();; 1965 ; 1966 // perform the network prediction; 1967 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 1968 ; 1969 size_t nTargets = DataInfo().GetNTargets();; 1970 R__ASSERT(nTargets == fYHat->GetNcols());; 1971 ; 1972 std::vector<Float_t> output(nTargets);; 1973 for (size_t i = 0; i < nTargets; i++); 1974 output[i] = (*fYHat)(0, i);; 1975 ; 1976 // ned to transform back output ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:80460,Testability,log,logProgress,80460,"////////////////////////////////////; 1994/// Get the multi-class output values for a single event; 1995//////////////////////////////////////////////////////////////////////////; 1996const std::vector<Float_t> &TMVA::MethodDL::GetMulticlassValues(); 1997{; 1998 ; 1999 FillInputTensor();; 2000 ; 2001 fNet->Prediction(*fYHat, fXInput, fOutputFunction);; 2002 ; 2003 size_t nClasses = DataInfo().GetNClasses();; 2004 R__ASSERT(nClasses == fYHat->GetNcols());; 2005 ; 2006 if (fMulticlassReturnVal == NULL) {; 2007 fMulticlassReturnVal = new std::vector<Float_t>(nClasses);; 2008 }; 2009 R__ASSERT(fMulticlassReturnVal->size() == nClasses);; 2010 ; 2011 for (size_t i = 0; i < nClasses; i++) {; 2012 (*fMulticlassReturnVal)[i] = (*fYHat)(0, i);; 2013 }; 2014 return *fMulticlassReturnVal;; 2015}; 2016 ; 2017////////////////////////////////////////////////////////////////////////////////; 2018/// Evaluate the DeepNet on a vector of input values stored in the TMVA Event class; 2019/// Here we will evaluate using a default batch size and the same architecture used for ; 2020/// Training; 2021////////////////////////////////////////////////////////////////////////////////; 2022std::vector<Double_t> MethodDL::GetMvaValues(Long64_t firstEvt, Long64_t lastEvt, Bool_t logProgress); 2023{; 2024 ; 2025 Long64_t nEvents = Data()->GetNEvents();; 2026 if (firstEvt > lastEvt || lastEvt > nEvents) lastEvt = nEvents;; 2027 if (firstEvt < 0) firstEvt = 0;; 2028 nEvents = lastEvt-firstEvt;; 2029 ; 2030 // use same batch size as for training (from first strategy); 2031 size_t defaultEvalBatchSize = (fXInput.GetSize() > 1000) ? 100 : 1000;; 2032 size_t batchSize = (fTrainingSettings.empty()) ? defaultEvalBatchSize : fTrainingSettings.front().batchSize;; 2033 if ( size_t(nEvents) < batchSize ) batchSize = nEvents;; 2034 ; 2035 // using for training same scalar type defined for the prediction; 2036 if (this->GetArchitectureString() == ""GPU"") {; 2037#ifdef R__HAS_TMVAGPU; 2038 Log() << kINFO << ""Eval",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:81391,Testability,log,logProgress,81391,"2023{; 2024 ; 2025 Long64_t nEvents = Data()->GetNEvents();; 2026 if (firstEvt > lastEvt || lastEvt > nEvents) lastEvt = nEvents;; 2027 if (firstEvt < 0) firstEvt = 0;; 2028 nEvents = lastEvt-firstEvt;; 2029 ; 2030 // use same batch size as for training (from first strategy); 2031 size_t defaultEvalBatchSize = (fXInput.GetSize() > 1000) ? 100 : 1000;; 2032 size_t batchSize = (fTrainingSettings.empty()) ? defaultEvalBatchSize : fTrainingSettings.front().batchSize;; 2033 if ( size_t(nEvents) < batchSize ) batchSize = nEvents;; 2034 ; 2035 // using for training same scalar type defined for the prediction; 2036 if (this->GetArchitectureString() == ""GPU"") {; 2037#ifdef R__HAS_TMVAGPU; 2038 Log() << kINFO << ""Evaluate deep neural network on GPU using batches with size = "" << batchSize << Endl << Endl;; 2039#ifdef R__HAS_CUDNN; 2040 return PredictDeepNet<DNN::TCudnn<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2041#else; 2042 return PredictDeepNet<DNN::TCuda<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2043#endif; 2044 ; 2045#endif; 2046 }; 2047 Log() << kINFO << ""Evaluate deep neural network on CPU using batches with size = "" << batchSize << Endl << Endl;; 2048 return PredictDeepNet<DNN::TCpu<ScalarImpl_t> >(firstEvt, lastEvt, batchSize, logProgress);; 2049}; 2050////////////////////////////////////////////////////////////////////////////////; 2051void MethodDL::AddWeightsXMLTo(void * parent) const; 2052{; 2053 // Create the parent XML node with name ""Weights""; 2054 auto & xmlEngine = gTools().xmlengine();; 2055 void* nn = xmlEngine.NewChild(parent, 0, ""Weights"");; 2056 ; 2057 /*! Get all necessary information, in order to be able to reconstruct the net; 2058 * if we read the same XML file. */; 2059 ; 2060 // Deep Net specific info; 2061 Int_t depth = fNet->GetDepth();; 2062 ; 2063 Int_t inputDepth = fNet->GetInputDepth();; 2064 Int_t inputHeight = fNet->GetInputHeight();; 2065 Int_t inputWidth = fNet->GetInputWidth();; 2066 ; 2067 Int_t batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:81500,Testability,log,logProgress,81500,"2023{; 2024 ; 2025 Long64_t nEvents = Data()->GetNEvents();; 2026 if (firstEvt > lastEvt || lastEvt > nEvents) lastEvt = nEvents;; 2027 if (firstEvt < 0) firstEvt = 0;; 2028 nEvents = lastEvt-firstEvt;; 2029 ; 2030 // use same batch size as for training (from first strategy); 2031 size_t defaultEvalBatchSize = (fXInput.GetSize() > 1000) ? 100 : 1000;; 2032 size_t batchSize = (fTrainingSettings.empty()) ? defaultEvalBatchSize : fTrainingSettings.front().batchSize;; 2033 if ( size_t(nEvents) < batchSize ) batchSize = nEvents;; 2034 ; 2035 // using for training same scalar type defined for the prediction; 2036 if (this->GetArchitectureString() == ""GPU"") {; 2037#ifdef R__HAS_TMVAGPU; 2038 Log() << kINFO << ""Evaluate deep neural network on GPU using batches with size = "" << batchSize << Endl << Endl;; 2039#ifdef R__HAS_CUDNN; 2040 return PredictDeepNet<DNN::TCudnn<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2041#else; 2042 return PredictDeepNet<DNN::TCuda<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2043#endif; 2044 ; 2045#endif; 2046 }; 2047 Log() << kINFO << ""Evaluate deep neural network on CPU using batches with size = "" << batchSize << Endl << Endl;; 2048 return PredictDeepNet<DNN::TCpu<ScalarImpl_t> >(firstEvt, lastEvt, batchSize, logProgress);; 2049}; 2050////////////////////////////////////////////////////////////////////////////////; 2051void MethodDL::AddWeightsXMLTo(void * parent) const; 2052{; 2053 // Create the parent XML node with name ""Weights""; 2054 auto & xmlEngine = gTools().xmlengine();; 2055 void* nn = xmlEngine.NewChild(parent, 0, ""Weights"");; 2056 ; 2057 /*! Get all necessary information, in order to be able to reconstruct the net; 2058 * if we read the same XML file. */; 2059 ; 2060 // Deep Net specific info; 2061 Int_t depth = fNet->GetDepth();; 2062 ; 2063 Int_t inputDepth = fNet->GetInputDepth();; 2064 Int_t inputHeight = fNet->GetInputHeight();; 2065 Int_t inputWidth = fNet->GetInputWidth();; 2066 ; 2067 Int_t batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:81756,Testability,log,logProgress,81756,"2023{; 2024 ; 2025 Long64_t nEvents = Data()->GetNEvents();; 2026 if (firstEvt > lastEvt || lastEvt > nEvents) lastEvt = nEvents;; 2027 if (firstEvt < 0) firstEvt = 0;; 2028 nEvents = lastEvt-firstEvt;; 2029 ; 2030 // use same batch size as for training (from first strategy); 2031 size_t defaultEvalBatchSize = (fXInput.GetSize() > 1000) ? 100 : 1000;; 2032 size_t batchSize = (fTrainingSettings.empty()) ? defaultEvalBatchSize : fTrainingSettings.front().batchSize;; 2033 if ( size_t(nEvents) < batchSize ) batchSize = nEvents;; 2034 ; 2035 // using for training same scalar type defined for the prediction; 2036 if (this->GetArchitectureString() == ""GPU"") {; 2037#ifdef R__HAS_TMVAGPU; 2038 Log() << kINFO << ""Evaluate deep neural network on GPU using batches with size = "" << batchSize << Endl << Endl;; 2039#ifdef R__HAS_CUDNN; 2040 return PredictDeepNet<DNN::TCudnn<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2041#else; 2042 return PredictDeepNet<DNN::TCuda<ScalarImpl_t>>(firstEvt, lastEvt, batchSize, logProgress);; 2043#endif; 2044 ; 2045#endif; 2046 }; 2047 Log() << kINFO << ""Evaluate deep neural network on CPU using batches with size = "" << batchSize << Endl << Endl;; 2048 return PredictDeepNet<DNN::TCpu<ScalarImpl_t> >(firstEvt, lastEvt, batchSize, logProgress);; 2049}; 2050////////////////////////////////////////////////////////////////////////////////; 2051void MethodDL::AddWeightsXMLTo(void * parent) const; 2052{; 2053 // Create the parent XML node with name ""Weights""; 2054 auto & xmlEngine = gTools().xmlengine();; 2055 void* nn = xmlEngine.NewChild(parent, 0, ""Weights"");; 2056 ; 2057 /*! Get all necessary information, in order to be able to reconstruct the net; 2058 * if we read the same XML file. */; 2059 ; 2060 // Deep Net specific info; 2061 Int_t depth = fNet->GetDepth();; 2062 ; 2063 Int_t inputDepth = fNet->GetInputDepth();; 2064 Int_t inputHeight = fNet->GetInputHeight();; 2065 Int_t inputWidth = fNet->GetInputWidth();; 2066 ; 2067 Int_t batc",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:103759,Testability,log,logProgress,103759,"he height of the batch used to train the deep net.Definition MethodDL.h:183; TMVA::MethodDL::GetHelpMessagevoid GetHelpMessage() constDefinition MethodDL.cxx:2342; TMVA::MethodDL::fLossFunctionDNN::ELossFunction fLossFunctionThe loss function.Definition MethodDL.h:190; TMVA::MethodDL::fInputShapestd::vector< size_t > fInputShapeContains the batch size (no.Definition MethodDL.h:178; TMVA::MethodDL::fLayoutStringTString fLayoutStringThe string defining the layout of the deep net.Definition MethodDL.h:194; TMVA::MethodDL::SetInputDepthvoid SetInputDepth(int inputDepth)Setters.Definition MethodDL.h:286; TMVA::MethodDL::fYHatstd::unique_ptr< MatrixImpl_t > fYHatDefinition MethodDL.h:208; TMVA::MethodDL::Trainvoid Train()Methods for training the deep learning network.Definition MethodDL.cxx:1659; TMVA::MethodDL::GetBatchHeightsize_t GetBatchHeight() constDefinition MethodDL.h:263; TMVA::MethodDL::GetMvaValuesvirtual std::vector< Double_t > GetMvaValues(Long64_t firstEvt, Long64_t lastEvt, Bool_t logProgress)Evaluate the DeepNet on a vector of input values stored in the TMVA Event class Here we will evaluate...Definition MethodDL.cxx:2022; TMVA::MethodDL::fWeightInitializationStringTString fWeightInitializationStringThe string defining the weight initialization method.Definition MethodDL.h:197; TMVA::MethodDL::ParseMaxPoolLayervoid ParseMaxPoolLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate max pool layer.Definition MethodDL.cxx:768; TMVA::MethodDL::fXInputTensorImpl_t fXInputDefinition MethodDL.h:206; TMVA::MethodDL::fRandomSeedsize_t fRandomSeedThe random seed used to initialize the weights and shuffling batches (default is zero)Definition MethodDL.h:186; TMVA::MethodDL::GetMulticlassValuesvirtual const std::vector< Float_t > & GetMulticlassValues(); TMVA::MethodDL::fArchitectureStringTString fArchitectureStringThe ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:111195,Testability,log,logProgress,111195,"29; TMVA::MethodDL::GetRegressionValuesvirtual const std::vector< Float_t > & GetRegressionValues(); TMVA::MethodDL::fTrainingStrategyStringTString fTrainingStrategyStringThe string defining the training strategy.Definition MethodDL.h:196; TMVA::MethodDL::CreateRankingconst Ranking * CreateRanking()Definition MethodDL.cxx:2335; TMVA::MethodDL::HostBufferImpl_ttypename ArchitectureImpl_t::HostBuffer_t HostBufferImpl_tDefinition MethodDL.h:110; TMVA::MethodDL::SetBatchDepthvoid SetBatchDepth(size_t batchDepth)Definition MethodDL.h:292; TMVA::MethodDL::ParseKeyValueStringKeyValueVector_t ParseKeyValueString(TString parseString, TString blockDelim, TString tokenDelim)Function for parsing the training settings, provided as a string in a key-value form.Definition MethodDL.cxx:1052; TMVA::MethodDL::SetBatchWidthvoid SetBatchWidth(size_t batchWidth)Definition MethodDL.h:294; TMVA::MethodDL::PredictDeepNetstd::vector< Double_t > PredictDeepNet(Long64_t firstEvt, Long64_t lastEvt, size_t batchSize, Bool_t logProgress)perform prediction of the deep neural network using batches (called by GetMvaValues)Definition MethodDL.cxx:1828; TMVA::MethodDL::GetWeightInitializationDNN::EInitialization GetWeightInitialization() constDefinition MethodDL.h:268; TMVA::MethodDL::SetBatchSizevoid SetBatchSize(size_t batchSize)Definition MethodDL.h:291; TMVA::MethodDL::GetLayoutStringTString GetLayoutString() constDefinition MethodDL.h:274; TMVA::MethodDL::fBatchDepthsize_t fBatchDepthThe depth of the batch used to train the deep net.Definition MethodDL.h:182; TMVA::MethodDL::DeepNetImpl_tTMVA::DNN::TDeepNet< ArchitectureImpl_t > DeepNetImpl_tDefinition MethodDL.h:106; TMVA::MethodDL::GetBatchWidthsize_t GetBatchWidth() constDefinition MethodDL.h:264; TMVA::MethodDL::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constDefinition MethodDL.cxx:2051; TMVA::MethodDL::MatrixImpl_ttypename ArchitectureImpl_t::Matrix_t MatrixImpl_tDefinition MethodDL.h:107; TMVA::MethodDL::~MethodDLvirtual ~MethodDL(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121063,Testability,log,logarithm,121063,"Gauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kFastTanh@ kFastTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid; TMVA::DNN::EActivationFunction::kIdentity@ kIdentity; TMVA::DNN::EActivationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t conver",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:121848,Testability,test,testIntervalDefinition,121848," DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDL.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:00 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:13034,Usability,learn,learningRate,13034,"N::EInitialization::kUniform;; 320 } else if (fWeightInitializationString == ""ZERO"") {; 321 fWeightInitialization = DNN::EInitialization::kZero;; 322 } else if (fWeightInitializationString == ""IDENTITY"") {; 323 fWeightInitialization = DNN::EInitialization::kIdentity;; 324 } else {; 325 fWeightInitialization = DNN::EInitialization::kGlorotUniform;; 326 }; 327 ; 328 // Training settings.; 329 ; 330 KeyValueVector_t strategyKeyValues = ParseKeyValueString(fTrainingStrategyString, TString(""|""), TString("",""));; 331 for (auto &block : strategyKeyValues) {; 332 TTrainingSettings settings;; 333 ; 334 settings.convergenceSteps = fetchValueTmp(block, ""ConvergenceSteps"", 100);; 335 settings.batchSize = fetchValueTmp(block, ""BatchSize"", 30);; 336 settings.maxEpochs = fetchValueTmp(block, ""MaxEpochs"", 2000);; 337 settings.testInterval = fetchValueTmp(block, ""TestRepetitions"", 7);; 338 settings.weightDecay = fetchValueTmp(block, ""WeightDecay"", 0.0);; 339 settings.learningRate = fetchValueTmp(block, ""LearningRate"", 1e-5);; 340 settings.momentum = fetchValueTmp(block, ""Momentum"", 0.3);; 341 settings.dropoutProbabilities = fetchValueTmp(block, ""DropConfig"", std::vector<Double_t>());; 342 ; 343 TString regularization = fetchValueTmp(block, ""Regularization"", TString(""NONE""));; 344 if (regularization == ""L1"") {; 345 settings.regularization = DNN::ERegularization::kL1;; 346 } else if (regularization == ""L2"") {; 347 settings.regularization = DNN::ERegularization::kL2;; 348 } else {; 349 settings.regularization = DNN::ERegularization::kNone;; 350 }; 351 ; 352 TString optimizer = fetchValueTmp(block, ""Optimizer"", TString(""ADAM""));; 353 settings.optimizerName = optimizer;; 354 if (optimizer == ""SGD"") {; 355 settings.optimizer = DNN::EOptimizer::kSGD;; 356 } else if (optimizer == ""ADAM"") {; 357 settings.optimizer = DNN::EOptimizer::kAdam;; 358 } else if (optimizer == ""ADAGRAD"") {; 359 settings.optimizer = DNN::EOptimizer::kAdagrad;; 360 } else if (optimizer == ""RMSPROP"") {; 361 settings.optim",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:53749,Usability,learn,learningRate,53749,"ch.GetOutput();; 1338 auto weights = batch.GetWeights();; 1339 ; 1340 //std::cout << "" input use count "" << inputTensor.GetBufferUseCount() << std::endl;; 1341 // should we apply droput to the loss ??; 1342 minValError += deepNet.Loss(inputTensor, outputMatrix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54010,Usability,learn,learningRate,54010,"trix, weights, false, includeRegularization);; 1343 }; 1344 // add Regularization term; 1345 Double_t regzTerm = (includeRegularization) ? deepNet.RegularizationTerm() : 0.0;; 1346 minValError /= (Double_t)(nValidationSamples / settings.batchSize);; 1347 minValError += regzTerm;; 1348 ; 1349 ; 1350 // create a pointer to base class VOptimizer; 1351 std::unique_ptr<DNN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 138",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54375,Usability,learn,learningRate,54375,"NN::VOptimizer<Architecture_t, Layer_t, DeepNet_t>> optimizer;; 1352 ; 1353 // initialize the base class pointer with the corresponding derived class object.; 1354 switch (O) {; 1355 ; 1356 case EOptimizer::kSGD:; 1357 optimizer = std::unique_ptr<DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTr",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:54655,Usability,learn,learningRate,54655,"r_t, DeepNet_t>>(; 1358 new DNN::TSGD<Architecture_t, Layer_t, DeepNet_t>(settings.learningRate, deepNet, settings.momentum));; 1359 break;; 1360 ; 1361 case EOptimizer::kAdam: {; 1362 optimizer = std::unique_ptr<DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>>(; 1363 new DNN::TAdam<Architecture_t, Layer_t, DeepNet_t>(; 1364 deepNet, settings.learningRate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:55004,Usability,learn,learningRate,55004,"Rate, settings.optimizerParams[""ADAM_beta1""],; 1365 settings.optimizerParams[""ADAM_beta2""], settings.optimizerParams[""ADAM_eps""]));; 1366 break;; 1367 }; 1368 ; 1369 case EOptimizer::kAdagrad:; 1370 optimizer = std::unique_ptr<DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>>(; 1371 new DNN::TAdagrad<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1372 settings.optimizerParams[""ADAGRAD_eps""]));; 1373 break;; 1374 ; 1375 case EOptimizer::kRMSProp:; 1376 optimizer = std::unique_ptr<DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>>(; 1377 new DNN::TRMSProp<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate, settings.momentum,; 1378 settings.optimizerParams[""RMSPROP_rho""],; 1379 settings.optimizerParams[""RMSPROP_eps""]));; 1380 break;; 1381 ; 1382 case EOptimizer::kAdadelta:; 1383 optimizer = std::unique_ptr<DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>>(; 1384 new DNN::TAdadelta<Architecture_t, Layer_t, DeepNet_t>(deepNet, settings.learningRate,; 1385 settings.optimizerParams[""ADADELTA_rho""],; 1386 settings.optimizerParams[""ADADELTA_eps""]));; 1387 break;; 1388 }; 1389 ; 1390 ; 1391 // Initialize the vector of batches, one batch for one slave network; 1392 std::vector<TTensorBatch<Architecture_t>> batches{};; 1393 ; 1394 bool converged = false;; 1395 size_t convergenceCount = 0;; 1396 size_t batchesInEpoch = nTrainingSamples / deepNet.GetBatchSize();; 1397 ; 1398 // start measuring; 1399 std::chrono::time_point<std::chrono::system_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:56480,Usability,learn,learningRate,56480,"ystem_clock> tstart, tend;; 1400 tstart = std::chrono::system_clock::now();; 1401 ; 1402 // function building string with optimizer parameters values for logging; 1403 auto optimParametersString = [&]() {; 1404 TString optimParameters;; 1405 for ( auto & element : settings.optimizerParams) {; 1406 TString key = element.first;; 1407 key.ReplaceAll(settings.optimizerName + ""_"", """"); // strip optimizerName_; 1408 double value = element.second;; 1409 if (!optimParameters.IsNull()); 1410 optimParameters += "","";; 1411 else; 1412 optimParameters += "" ("";; 1413 optimParameters += TString::Format(""%s=%g"", key.Data(), value);; 1414 }; 1415 if (!optimParameters.IsNull()); 1416 optimParameters += "")"";; 1417 return optimParameters;; 1418 };; 1419 ; 1420 Log() << ""Training phase "" << trainingPhase << "" of "" << this->GetTrainingSettings().size() << "": ""; 1421 << "" Optimizer "" << settings.optimizerName; 1422 << optimParametersString(); 1423 << "" Learning rate = "" << settings.learningRate << "" regularization "" << (char)settings.regularization; 1424 << "" minimum error = "" << minValError << Endl;; 1425 if (!fInteractive) {; 1426 std::string separator(62, '-');; 1427 Log() << separator << Endl;; 1428 Log() << std::setw(10) << ""Epoch""; 1429 << "" | "" << std::setw(12) << ""Train Err."" << std::setw(12) << ""Val. Err."" << std::setw(12); 1430 << ""t(s)/epoch"" << std::setw(12) << ""t(s)/Loss"" << std::setw(12) << ""nEvents/s"" << std::setw(12); 1431 << ""Conv. Steps"" << Endl;; 1432 Log() << separator << Endl;; 1433 }; 1434 ; 1435 // set up generator for shuffling the batches; 1436 // if seed is zero we have always a different order in the batches; 1437 size_t shuffleSeed = 0;; 1438 if (fRandomSeed != 0) shuffleSeed = fRandomSeed + trainingPhase;; 1439 RandomGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1445 for (size_t l = ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:58244,Usability,clear,clear,58244,"domGenerator<TRandom3> rng(shuffleSeed);; 1440 ; 1441 // print weights before; 1442 if (fBuildNet && debug) {; 1443 Log() << ""Initial Deep Net Weights "" << Endl;; 1444 auto & weights_tensor = deepNet.GetLayerAt(0)->GetWeights();; 1445 for (size_t l = 0; l < weights_tensor.size(); ++l); 1446 weights_tensor[l].Print();; 1447 auto & bias_tensor = deepNet.GetLayerAt(0)->GetBiases();; 1448 bias_tensor[0].Print();; 1449 }; 1450 ; 1451 Log() << "" Start epoch iteration ..."" << Endl;; 1452 bool debugFirstEpoch = false;; 1453 bool computeLossInTraining = true; // compute loss in training or at test time; 1454 size_t nTrainEpochs = 0;; 1455 while (!converged) {; 1456 nTrainEpochs++;; 1457 trainingData.Shuffle(rng);; 1458 ; 1459 // execute all epochs; 1460 //for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1461 ; 1462 Double_t trainingError = 0;; 1463 for (size_t i = 0; i < batchesInEpoch; ++i ) {; 1464 // Clean and load new batches, one batch for one slave net; 1465 //batches.clear();; 1466 //batches.reserve(nThreads);; 1467 //for (size_t j = 0; j < nThreads; j++) {; 1468 // batches.push_back(trainingData.GetTensorBatch());; 1469 //}; 1470 if (debugFirstEpoch) std::cout << ""\n\n----- batch # "" << i << ""\n\n"";; 1471 ; 1472 auto my_batch = trainingData.GetTensorBatch();; 1473 ; 1474 if (debugFirstEpoch); 1475 std::cout << ""got batch data - doing forward \n"";; 1476 ; 1477#ifdef DEBUG; 1478 ; 1479 Architecture_t::PrintTensor(my_batch.GetInput(),""input tensor"",true);; 1480 typename Architecture_t::Tensor_t tOut(my_batch.GetOutput());; 1481 typename Architecture_t::Tensor_t tW(my_batch.GetWeights());; 1482 Architecture_t::PrintTensor(tOut,""label tensor"",true) ;; 1483 Architecture_t::PrintTensor(tW,""weight tensor"",true) ;; 1484#endif; 1485 ; 1486 deepNet.Forward(my_batch.GetInput(), true);; 1487 // compute also loss; 1488 if (computeLossInTraining) {; 1489 auto outputMatrix = my_batch.GetOutput();; 1490 auto weights = my_batch.GetWeights();; 1491 trainingError += deepNet.Loss(",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:60391,Usability,learn,learning,60391,";; 1499 for (size_t l = 0; l < nlayers; ++l) {; 1500 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0); 1501 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),; 1502 TString::Format(""initial weights layer %d"", l).Data());; 1503 ; 1504 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetOutput(),; 1505 TString::Format(""output tensor layer %d"", l).Data());; 1506 }; 1507#endif; 1508 ; 1509 //Architecture_t::PrintTensor(deepNet.GetLayerAt(nlayers-1)->GetOutput(),""output tensor last layer"" );; 1510 ; 1511 deepNet.Backward(my_batch.GetInput(), my_batch.GetOutput(), my_batch.GetWeights());; 1512 ; 1513 if (debugFirstEpoch); 1514 std::cout << ""- doing optimizer update \n"";; 1515 ; 1516 // increment optimizer step that is used in some algorithms (e.g. ADAM); 1517 optimizer->IncrementGlobalStep();; 1518 optimizer->Step();; 1519 ; 1520#ifdef DEBUG; 1521 std::cout << ""minmimizer step - momentum "" << settings.momentum << "" learning rate "" << optimizer->GetLearningRate() << std::endl;; 1522 for (size_t l = 0; l < nlayers; ++l) {; 1523 if (deepNet.GetLayerAt(l)->GetWeights().size() > 0) {; 1524 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightsAt(0),TString::Format(""weights after step layer %d"",l).Data());; 1525 Architecture_t::PrintTensor(deepNet.GetLayerAt(l)->GetWeightGradientsAt(0),""weight gradients"");; 1526 }; 1527 }; 1528#endif; 1529 ; 1530 }; 1531 ; 1532 if (debugFirstEpoch) std::cout << ""\n End batch loop - compute validation loss \n"";; 1533 //}; 1534 debugFirstEpoch = false;; 1535 if ((nTrainEpochs % settings.testInterval) == 0) {; 1536 ; 1537 std::chrono::time_point<std::chrono::system_clock> t1,t2;; 1538 ; 1539 t1 = std::chrono::system_clock::now();; 1540 ; 1541 // Compute validation error.; 1542 ; 1543 ; 1544 Double_t valError = 0.0;; 1545 bool inTraining = false;; 1546 for (auto batch : validationData) {; 1547 auto inputTensor = batch.GetInput();; 1548 auto outputMatrix = batch.GetOutput();; 1549 auto weights = batch.GetWeights();; 1550 ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:92460,Usability,clear,clear,92460,"Impl_t::IsCudnn()); 2292 Warning(""ReadWeightsFromXML"",; 2293 ""Cannot use a reset gate after to false with CudNN - use implementation with resetgate=true"");; 2294 ; 2295 fNet->AddBasicGRULayer(stateSize, inputSize, timeSteps, rememberState, returnSequence, resetGateAfter);; 2296 }; 2297 // BatchNorm Layer; 2298 else if (layerName == ""BatchNormLayer"") {; 2299 // use some dammy value which will be overwrittem in BatchNormLayer::ReadWeightsFromXML; 2300 fNet->AddBatchNormLayer(0., 0.0);; 2301 }; 2302 // read weights and biases; 2303 fNet->GetLayers().back()->ReadWeightsFromXML(layerXML);; 2304 ; 2305 // read next layer; 2306 layerXML = gTools().GetNextChild(layerXML);; 2307 }; 2308 ; 2309 fBuildNet = false;; 2310 // create now the input and output matrices; 2311 //int n1 = batchHeight;; 2312 //int n2 = batchWidth;; 2313 // treat case where batchHeight is the batchSize in case of first Dense layers (then we need to set to fNet batch size); 2314 //if (fXInput.size() > 0) fXInput.clear();; 2315 //fXInput.emplace_back(MatrixImpl_t(n1,n2));; 2316 fXInput = ArchitectureImpl_t::CreateTensor(fNet->GetBatchSize(), GetInputDepth(), GetInputHeight(), GetInputWidth() );; 2317 if (batchDepth == 1 && GetInputHeight() == 1 && GetInputDepth() == 1); 2318 // make here a ColumnMajor tensor; 2319 fXInput = TensorImpl_t( fNet->GetBatchSize(), GetInputWidth(),TMVA::Experimental::MemoryLayout::ColumnMajor );; 2320 fXInputBuffer = HostBufferImpl_t( fXInput.GetSize());; 2321 ; 2322 // create pointer to output matrix used for the predictions; 2323 fYHat = std::unique_ptr<MatrixImpl_t>(new MatrixImpl_t(fNet->GetBatchSize(), fNet->GetOutputWidth() ) );; 2324 ; 2325 ; 2326}; 2327 ; 2328 ; 2329////////////////////////////////////////////////////////////////////////////////; 2330void MethodDL::ReadWeightsFromStream(std::istream & /*istr*/); 2331{; 2332}; 2333 ; 2334////////////////////////////////////////////////////////////////////////////////; 2335const Ranking *TMVA::MethodDL::CreateRanking(); 233",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:103509,Usability,learn,learning,103509,"ferent data (used by MethodCategory::GetMvaValues)Definition MethodBase.h:448; TMVA::MethodDLDefinition MethodDL.h:89; TMVA::MethodDL::TensorImpl_ttypename ArchitectureImpl_t::Tensor_t TensorImpl_tDefinition MethodDL.h:108; TMVA::MethodDL::fBatchHeightsize_t fBatchHeightThe height of the batch used to train the deep net.Definition MethodDL.h:183; TMVA::MethodDL::GetHelpMessagevoid GetHelpMessage() constDefinition MethodDL.cxx:2342; TMVA::MethodDL::fLossFunctionDNN::ELossFunction fLossFunctionThe loss function.Definition MethodDL.h:190; TMVA::MethodDL::fInputShapestd::vector< size_t > fInputShapeContains the batch size (no.Definition MethodDL.h:178; TMVA::MethodDL::fLayoutStringTString fLayoutStringThe string defining the layout of the deep net.Definition MethodDL.h:194; TMVA::MethodDL::SetInputDepthvoid SetInputDepth(int inputDepth)Setters.Definition MethodDL.h:286; TMVA::MethodDL::fYHatstd::unique_ptr< MatrixImpl_t > fYHatDefinition MethodDL.h:208; TMVA::MethodDL::Trainvoid Train()Methods for training the deep learning network.Definition MethodDL.cxx:1659; TMVA::MethodDL::GetBatchHeightsize_t GetBatchHeight() constDefinition MethodDL.h:263; TMVA::MethodDL::GetMvaValuesvirtual std::vector< Double_t > GetMvaValues(Long64_t firstEvt, Long64_t lastEvt, Bool_t logProgress)Evaluate the DeepNet on a vector of input values stored in the TMVA Event class Here we will evaluate...Definition MethodDL.cxx:2022; TMVA::MethodDL::fWeightInitializationStringTString fWeightInitializationStringThe string defining the weight initialization method.Definition MethodDL.h:197; TMVA::MethodDL::ParseMaxPoolLayervoid ParseMaxPoolLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate max pool layer.Definition MethodDL.cxx:768; TMVA::MethodDL::fXInputTensorImpl_t fXInputDefinition MethodDL.h:206; TMVA::MethodDL::fRandomSeedsize_t fRandomSeedThe r",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:109536,Usability,learn,learning,109536,"dDL.h:153; TMVA::MethodDL::ParseRecurrentLayervoid ParseRecurrentLayer(ERecurrentLayerType type, DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate rnn layer.Definition MethodDL.cxx:931; TMVA::MethodDL::fTrainingSettingsstd::vector< TTrainingSettings > fTrainingSettingsThe vector defining each training strategy.Definition MethodDL.h:204; TMVA::MethodDL::GetInputWidthsize_t GetInputWidth() constDefinition MethodDL.h:257; TMVA::MethodDL::SetInputShapevoid SetInputShape(std::vector< size_t > inputShape)Definition MethodDL.h:289; TMVA::MethodDL::GetLossFunctionDNN::ELossFunction GetLossFunction() constDefinition MethodDL.h:270; TMVA::MethodDL::fBatchLayoutStringTString fBatchLayoutStringThe string defining the layout of the batch.Definition MethodDL.h:193; TMVA::MethodDL::HasAnalysisTypeBool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)Check the type of analysis the deep learning network can do.Definition MethodDL.cxx:1091; TMVA::MethodDL::ParseConvLayervoid ParseConvLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate convolutional layer.Definition MethodDL.cxx:669; TMVA::MethodDL::ParseReshapeLayervoid ParseReshapeLayer(DNN::TDeepNet< Architecture_t, Layer_t > &deepNet, std::vector< DNN::TDeepNet< Architecture_t, Layer_t > > &nets, TString layerString, TString delim)Pases the layer string and creates the appropriate reshape layer.Definition MethodDL.cxx:829; TMVA::MethodDL::GetRegressionValuesvirtual const std::vector< Float_t > & GetRegressionValues(); TMVA::MethodDL::fTrainingStrategyStringTString fTrainingStrategyStringThe string defining the training strategy.Definition MethodDL.h:196; TMVA::MethodDL::CreateRankingconst Rankin",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDL_8cxx_source.html:122237,Usability,learn,learningRateDefinition,122237," DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTmpTString fetchValueTmp(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDL.cxx:75; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::TTrainingSettingsAll of the options that can be specified in the training string.Definition MethodDL.h:72; TMVA::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDL.h:73; TMVA::TTrainingSettings::optimizerParamsstd::map< TString, double > optimizerParamsDefinition MethodDL.h:84; TMVA::TTrainingSettings::optimizerNameTString optimizerNameDefinition MethodDL.h:79; TMVA::TTrainingSettings::optimizerDNN::EOptimizer optimizerDefinition MethodDL.h:78; TMVA::TTrainingSettings::maxEpochssize_t maxEpochsDefinition MethodDL.h:76; TMVA::TTrainingSettings::momentumDouble_t momentumDefinition MethodDL.h:81; TMVA::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDL.h:82; TMVA::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDL.h:74; TMVA::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDL.h:77; TMVA::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDL.h:75; TMVA::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDL.h:83; TMVA::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDL.h:80; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; t1auto * t1Definition textangle.C:20; Functions.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDL.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:00 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodDL_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDL_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:4856,Availability,error,error,4856,"arget; 113 ; 114Bool_t TMVA::MethodDNN::HasAnalysisType(Types::EAnalysisType type,; 115 UInt_t numberClasses,; 116 UInt_t /*numberTargets*/ ); 117{; 118 if (type == Types::kClassification && numberClasses == 2 ) return kTRUE;; 119 if (type == Types::kMulticlass ) return kTRUE;; 120 if (type == Types::kRegression ) return kTRUE;; 121 ; 122 return kFALSE;; 123}; 124 ; 125////////////////////////////////////////////////////////////////////////////////; 126/// default initializations; 127 ; 128void TMVA::MethodDNN::Init() {; 129 Log() << kWARNING; 130 << ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSEN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:5734,Availability,error,error,5734,"ningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSENTROPY""));; 168 AddPreDefVal(TString(""SUMOFSQUARES""));; 169 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 170 ; 171 DeclareOptionRef(fWeightInitializationString=""XAVIER"",; 172 ""WeightInitialization"",; 173 ""Weight initialization strategy"");; 174 AddPreDefVal(TString(""XAVIER""));; 175 AddPreDefVal(TString(""XAVIERUNIFORM""));; 176 ; 177 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 178 AddPreDefVal(TString(""STANDARD""));; 179 AddPreDefVal(TString(""CPU""));; 180 AddPreDefVal(TString(""GPU""));; 181 AddPreDefVal(TString(""OPENCL""));; 182 ; 183 DeclareOptionRef(; 184 fTrainingStrategyString = ""LearningRate=1e-1,""; 185 ""Momentum=0.3,""; 186 ""Repetitions=3,""; 187 ""ConvergenceSteps=50,""; 188 ""BatchSize=30,""; 189 ""TestRepetitions=7,""; 190 ""WeightDecay=0.0,""; 191 ""Renormalize=L2,""; 192 ""DropConfig=0.0",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:17966,Availability,error,error,17966,"ize = GetNTargets();; 493 } else if (fAnalysisType == Types::kMulticlass && DataInfo().GetNClasses() >= 2) {; 494 outputSize = DataInfo().GetNClasses();; 495 }; 496 ; 497 fNet.SetBatchSize(1);; 498 fNet.SetInputWidth(inputSize);; 499 ; 500 auto itLayout = std::begin (fLayout);; 501 auto itLayoutEnd = std::end (fLayout)-1;; 502 for ( ; itLayout != itLayoutEnd; ++itLayout) {; 503 fNet.AddLayer((*itLayout).first, (*itLayout).second);; 504 }; 505 fNet.AddLayer(outputSize, EActivationFunction::kIdentity);; 506 ; 507 //; 508 // Loss function and output.; 509 //; 510 ; 511 fOutputFunction = EOutputFunction::kSigmoid;; 512 if (fAnalysisType == Types::kClassification); 513 {; 514 if (fErrorStrategy == ""SUMOFSQUARES"") {; 515 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 516 }; 517 if (fErrorStrategy == ""CROSSENTROPY"") {; 518 fNet.SetLossFunction(ELossFunction::kCrossEntropy);; 519 }; 520 fOutputFunction = EOutputFunction::kSigmoid;; 521 } else if (fAnalysisType == Types::kRegression) {; 522 if (fErrorStrategy != ""SUMOFSQUARES"") {; 523 Log () << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 524 << "" neural net error function. Setting error function to ""; 525 << "" SUMOFSQUARES now."" << Endl;; 526 }; 527 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 528 fOutputFunction = EOutputFunction::kIdentity;; 529 } else if (fAnalysisType == Types::kMulticlass) {; 530 if (fErrorStrategy == ""SUMOFSQUARES"") {; 531 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 532 }; 533 if (fErrorStrategy == ""CROSSENTROPY"") {; 534 fNet.SetLossFunction(ELossFunction::kCrossEntropy);; 535 }; 536 if (fErrorStrategy == ""MUTUALEXCLUSIVE"") {; 537 fNet.SetLossFunction(ELossFunction::kSoftmaxCrossEntropy);; 538 }; 539 fOutputFunction = EOutputFunction::kSoftmax;; 540 }; 541 ; 542 //; 543 // Initialization; 544 //; 545 ; 546 if (fWeightInitializationString == ""XAVIER"") {; 547 fWeightInitialization = DNN::EInitialization::kGauss;; 548 }; 549 else if (fWeightInitialization",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:17990,Availability,error,error,17990,"ize(1);; 498 fNet.SetInputWidth(inputSize);; 499 ; 500 auto itLayout = std::begin (fLayout);; 501 auto itLayoutEnd = std::end (fLayout)-1;; 502 for ( ; itLayout != itLayoutEnd; ++itLayout) {; 503 fNet.AddLayer((*itLayout).first, (*itLayout).second);; 504 }; 505 fNet.AddLayer(outputSize, EActivationFunction::kIdentity);; 506 ; 507 //; 508 // Loss function and output.; 509 //; 510 ; 511 fOutputFunction = EOutputFunction::kSigmoid;; 512 if (fAnalysisType == Types::kClassification); 513 {; 514 if (fErrorStrategy == ""SUMOFSQUARES"") {; 515 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 516 }; 517 if (fErrorStrategy == ""CROSSENTROPY"") {; 518 fNet.SetLossFunction(ELossFunction::kCrossEntropy);; 519 }; 520 fOutputFunction = EOutputFunction::kSigmoid;; 521 } else if (fAnalysisType == Types::kRegression) {; 522 if (fErrorStrategy != ""SUMOFSQUARES"") {; 523 Log () << kWARNING << ""For regression only SUMOFSQUARES is a valid ""; 524 << "" neural net error function. Setting error function to ""; 525 << "" SUMOFSQUARES now."" << Endl;; 526 }; 527 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 528 fOutputFunction = EOutputFunction::kIdentity;; 529 } else if (fAnalysisType == Types::kMulticlass) {; 530 if (fErrorStrategy == ""SUMOFSQUARES"") {; 531 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 532 }; 533 if (fErrorStrategy == ""CROSSENTROPY"") {; 534 fNet.SetLossFunction(ELossFunction::kCrossEntropy);; 535 }; 536 if (fErrorStrategy == ""MUTUALEXCLUSIVE"") {; 537 fNet.SetLossFunction(ELossFunction::kSoftmaxCrossEntropy);; 538 }; 539 fOutputFunction = EOutputFunction::kSoftmax;; 540 }; 541 ; 542 //; 543 // Initialization; 544 //; 545 ; 546 if (fWeightInitializationString == ""XAVIER"") {; 547 fWeightInitialization = DNN::EInitialization::kGauss;; 548 }; 549 else if (fWeightInitializationString == ""XAVIERUNIFORM"") {; 550 fWeightInitialization = DNN::EInitialization::kUniform;; 551 }; 552 else {; 553 fWeightInitialization = DNN::EInitialization::kGauss;; 554 }; 555 ; 556",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:36866,Availability,error,error,36866,", end;; 998 start = std::chrono::system_clock::now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37347,Availability,error,error,37347," 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.Get",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:43873,Availability,error,error,43873,"start, end;; 1187 start = std::chrono::system_clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddVal",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44414,Availability,error,error,44414,"trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterva",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55246,Availability,avail,available,55246,"ndl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 153",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55510,Availability,avail,available,55510,"\n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 1531 \n \; 1532 \""ErrorStrategy\"": \n \; 1533 - SUMOFSQUARES \n \; 1534 The error of the neural net is determined by a sum-of-squares error function \n \; 1535 For regression, this is the only possible choice. \n \; 1536 -",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:56358,Availability,error,error,56358,"ltithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 1531 \n \; 1532 \""ErrorStrategy\"": \n \; 1533 - SUMOFSQUARES \n \; 1534 The error of the neural net is determined by a sum-of-squares error function \n \; 1535 For regression, this is the only possible choice. \n \; 1536 - CROSSENTROPY \n \; 1537 The error of the neural net is determined by a cross entropy function. The \n \; 1538 output values are automatically (internally) transformed into probabilities \n \; 1539 using a sigmoid function. \n \; 1540 For signal/background classification this is the default choice. \n \; 1541 For multiclass using cross entropy more than one or no output classes \n \; 1542 can be equally true or false (e.g. Event 0: A and B are true, Event 1: \n \; 1543 A and C is true, Event 2: C is true, ...) \n \; 1544 - MUTUALEXCLUSIVE \n \; 1545 In multiclass settings, exactly one of the output classes can be true (e.g. either A or B or C) \n \; 1546 \n \; 1547 \""WeightInitialization\"" \n \; 1548 - XAVIER \n \; 1549 [recommended] \n \; 1550 \""Xavier Glorot & Yoshua Bengio\""-style of initializing the weights. The weights are chosen randomly \n \; 1551 such that th",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:56416,Availability,error,error,56416,"ltithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 1531 \n \; 1532 \""ErrorStrategy\"": \n \; 1533 - SUMOFSQUARES \n \; 1534 The error of the neural net is determined by a sum-of-squares error function \n \; 1535 For regression, this is the only possible choice. \n \; 1536 - CROSSENTROPY \n \; 1537 The error of the neural net is determined by a cross entropy function. The \n \; 1538 output values are automatically (internally) transformed into probabilities \n \; 1539 using a sigmoid function. \n \; 1540 For signal/background classification this is the default choice. \n \; 1541 For multiclass using cross entropy more than one or no output classes \n \; 1542 can be equally true or false (e.g. Event 0: A and B are true, Event 1: \n \; 1543 A and C is true, Event 2: C is true, ...) \n \; 1544 - MUTUALEXCLUSIVE \n \; 1545 In multiclass settings, exactly one of the output classes can be true (e.g. either A or B or C) \n \; 1546 \n \; 1547 \""WeightInitialization\"" \n \; 1548 - XAVIER \n \; 1549 [recommended] \n \; 1550 \""Xavier Glorot & Yoshua Bengio\""-style of initializing the weights. The weights are chosen randomly \n \; 1551 such that th",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:56533,Availability,error,error,56533,"orithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 1531 \n \; 1532 \""ErrorStrategy\"": \n \; 1533 - SUMOFSQUARES \n \; 1534 The error of the neural net is determined by a sum-of-squares error function \n \; 1535 For regression, this is the only possible choice. \n \; 1536 - CROSSENTROPY \n \; 1537 The error of the neural net is determined by a cross entropy function. The \n \; 1538 output values are automatically (internally) transformed into probabilities \n \; 1539 using a sigmoid function. \n \; 1540 For signal/background classification this is the default choice. \n \; 1541 For multiclass using cross entropy more than one or no output classes \n \; 1542 can be equally true or false (e.g. Event 0: A and B are true, Event 1: \n \; 1543 A and C is true, Event 2: C is true, ...) \n \; 1544 - MUTUALEXCLUSIVE \n \; 1545 In multiclass settings, exactly one of the output classes can be true (e.g. either A or B or C) \n \; 1546 \n \; 1547 \""WeightInitialization\"" \n \; 1548 - XAVIER \n \; 1549 [recommended] \n \; 1550 \""Xavier Glorot & Yoshua Bengio\""-style of initializing the weights. The weights are chosen randomly \n \; 1551 such that the variance of the values of the nodes is preserved for each layer. \n \; 1552 - XAVIERUNIFORM \n \; 1553 The same as XAVIER, but with uniformly distributed weight",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59025,Availability,error,error,59025,"DropFraction=0.0,DropRepetitions=5\"" \n \; 1559 - explanation: two stacked training settings separated by \""|\"" \n \; 1560 . first training setting: \""LearningRate=1e-1,Momentum=0.3,ConvergenceSteps=50,BatchSize=30,TestRepetitions=7,WeightDecay=0.0,Renormalize=L2,DropConfig=0.0,DropRepetitions=5\"" \n \; 1561 . second training setting : \""LearningRate=1e-4,Momentum=0.3,ConvergenceSteps=50,BatchSize=20,TestRepetitions=7,WeightDecay=0.001,Renormalize=L2,DropFractions=0.0,DropRepetitions=5\"" \n \; 1562 . LearningRate : \n \; 1563 - recommended for classification: 0.1 initially, 1e-4 later \n \; 1564 - recommended for regression: 1e-4 and less \n \; 1565 . Momentum : \n \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:63665,Availability,error,error,63665,"om_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TObjString.h; TString.h; Timer.h; Tools.h; PatternDefinition Pattern.h:8; TFormulaThe Formula class.Definition TFormula.h:89; TIterDefinition TCollection.h:235; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::DNN::LayerLayer defines the layout of a layer.Definition NeuralNet.h:673; TMVA::DNN::Netneural netDefinition NeuralNet.h:1062; TMVA::DNN::SettingsSettings for the training of the neural net.Definition NeuralNet.h:730; TMVA::DNN::SteepestSteepest Gradient Descent algorithm (SGD)Definition NeuralNet.h:334; TMVA::DNN::TCpu::Copystatic void Copy(Matrix_t &B, const Matrix_t &A)Definition Arithmetic.hxx:269; TMVA::DNN::TCuda::Copystatic void Copy(Matrix_t &B, const Matrix_t &A); TMVA::DNN::TDataLoaderTDataLoader.Definition DataLoader.h:129; TMVA::DNN::TGradientDescentDefinition Minimizers.h:56; TMVA::DNN::TGradientDescent::HasConvergedbool HasConverged()Increases the minimization step counter by the test error evaluation period and uses the current inte...Definition Minimizers.h:667; TMVA::DNN::TGradientDescent::Stepvoid Step(Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights)Perform a single optimization step on a given batch.Definition Minimizers.h:331; TMVA::DNN::TGradientDescent::GetTestIntervalsize_t GetTestInterval() constDefinition Minimizers.h:163; TMVA::DNN::TGradientDescent::StepMomentumvoid StepMomentum(Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum)Same as the Step(...) method for multiple batches but uses momentum.Definition Minimizers.h:438; TMVA::DNN::TGradientDescent::GetConvergenceCountsize_t GetConvergenceCount() constDefinition Minimizers.h:159; TMVA::DNN::TGradientDescent::GetConvergenceStepssize_t GetConvergenceSteps() constDefinition Minimizers.h:160; TMVA::DNN::TNetGeneric neural network class.Definition Net.h:49; TMVA::EventDefinition Event",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:354,Deployability,integrat,integrated,354,". ROOT: tmva/tmva/src/MethodDNN.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodDNN.cxx. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Peter Speckmayer; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodDNN *; 8 * *; 9 * *; 10 * Description: *; 11 * A neural network implementation *; 12 * *; 13 * Authors (alphabetical): *; 14 * Simon Pfreundschuh <s.pfreundschuh@gmail.com> - CERN, Switzerland *; 15 * Peter Speckmayer <peter.speckmayer@gmx.ch> - CERN, Switzerland *; 16 * *; 17 * Copyright (c) 2005-2015: *; 18 * CERN, Switzerland *; 19 * U. of Victoria, Canada *; 20 * MPI-K Heidelberg, Germany *; 21 * U. of Bonn, Germany *; 22 * *; 23 * Redistribution and use in source and binary forms, with or without *; 24 * modification, are permitted according to the terms listed in LICENSE *; 25 * (see tmva/doc/LICENSE) *; 26 **********************************************************************************/; 27 ; 28/*! \class TMVA::MethodDNN; 29\ingroup TMVA; 30Deep Neural Network Implementation.; 31*/; 32 ; 33#include ""TMVA/MethodDNN.h""; 34 ; 35#include ""TString.h""; 36#include ""TFormula.h""; 37#include ""TObjString.h""; 38 ; 39#include ""TMVA/ClassifierFactory.h""; 40#include ""TMVA/Configurable.h""; 41#include ""TMVA/IMethod.h""; 42#include ""TMVA/MsgLogger.h""; 43#include ""TMVA/MethodBase.h""; 44#include ""TMVA/Timer.h""; 45#include ""TMVA/Types.h""; 46#include ""TMVA/Tools.h""; 47#include ""TMVA/Config.h""; 48#include ""TMVA/Ranking.h""; 49 ; 50#include ""TMVA/DNN/Net.h""; 51#include ""TMVA/DNN/Architectures/Reference.h""; 52 ; 53#include ""TMVA/NeuralNet.h""; 54#include ""TMVA/Monitoring.h""; 55 ; 56#ifdef R__HAS_TMVACPU; 57#include ""TMVA/DNN/Architectures/Cpu.h""; 58#endif; 59#ifdef R__HAS_TMVAGPU; 60#include ""TMVA/DNN/Architectures/Cuda.h""; 61#endif; 62 ; 63#includ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:15691,Deployability,install,installed,15691,"L << ""The STANDARD architecture has been deprecated. ""; 439 ""Please use Architecture=CPU or Architecture=CPU.""; 440 ""See the TMVA Users' Guide for instructions if you ""; 441 ""encounter problems.""; 442 << Endl;; 443 }; 444 ; 445 if (fArchitectureString == ""OPENCL"") {; 446 Log() << kERROR << ""The OPENCL architecture has not been implemented yet. ""; 447 ""Please use Architecture=CPU or Architecture=CPU for the ""; 448 ""time being. See the TMVA Users' Guide for instructions ""; 449 ""if you encounter problems.""; 450 << Endl;; 451 Log() << kFATAL << ""The OPENCL architecture has not been implemented yet. ""; 452 ""Please use Architecture=CPU or Architecture=CPU for the ""; 453 ""time being. See the TMVA Users' Guide for instructions ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNV",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:15857,Deployability,install,installed,15857," ""; 441 ""encounter problems.""; 442 << Endl;; 443 }; 444 ; 445 if (fArchitectureString == ""OPENCL"") {; 446 Log() << kERROR << ""The OPENCL architecture has not been implemented yet. ""; 447 ""Please use Architecture=CPU or Architecture=CPU for the ""; 448 ""time being. See the TMVA Users' Guide for instructions ""; 449 ""if you encounter problems.""; 450 << Endl;; 451 Log() << kFATAL << ""The OPENCL architecture has not been implemented yet. ""; 452 ""Please use Architecture=CPU or Architecture=CPU for the ""; 453 ""time being. See the TMVA Users' Guide for instructions ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNVariables ();; 490 size_t outputSize = 1;; 491 if (fAnalysisType == Types::kRegression && GetNTargets() != 0) {; 492 outputSize = GetNTargets();; 493 } else if (fAnaly",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30926,Deployability,configurat,configuration,30926,"break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIndex = 0;; 882 for (size_t l = 0; l < fNet.GetDepth(); l++) {; 883 auto & layerWeights = fNet.GetLayer(l).GetWeights();; 884 for (Int_t j = 0; j < layerWeights.GetNcols(); j++) {; 885 for (Int_t i = 0; i < layerWeights.GetNrows(); i++) {; 886 layerWeights(i,j) = weights[weightIndex];; 887 weightIndex++;; 888 }; 889 }; 890 auto & layerBiases = fNet.GetLayer(l).GetBiases();; 891 if (l == 0) {; 892 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 893 layerBiases(i,0) = weights[weightIndex];; 894 weightIndex++;; 895 }; 896 } else {; 897 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:39097,Deployability,install,installed,39097,"std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;; 1093#endif // DNNCUDA; 1094}; 1095 ; 1096////////////////////////////////////////////////////////////////////////////////; 1097 ; 1098void TMVA::MethodDNN::TrainCpu(); 1099{; 1100 ; 1101#ifdef DNNCPU // Included only if DNNCPU flag is set.; 1102 Log() << kINFO << ""Start of neural network training on CPU."" << Endl << Endl;; 1103 ; 1104 size_t nValidationSamples = GetNumValidationSamples();; 1105 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1106 size_t nTestSamples = nValidationSamples;; 1107 ; 1108 Log() << kDEBUG << ""Using "" << nValidationSamples << "" validation samples."" << Endl;; 1109 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 1110 ; 1111 fNet.Initialize(fWeightInitialization);; 1112 ; 1113 size_t trainingPhase = 1;; 1114 for (TTrainingSettings & settings : fTrainingSettings) {; 1115 ; 1116 if (fInteractive){; 1117 fInteractive->ClearGraphs",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:60204,Deployability,configurat,configuration,60204,"\n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn on multithreading [recommended: True] \n \; 1595 \n"";; 1596 Log () << txt << Endl;; 1597}; 1598 ; 1599} // namespace TMVA; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cpu.h; Cuda.h; IMethod.h; MethodBase.h; MethodDNN.h; Monitoring.h; MsgLogger.h; Net.h; NeuralNet.h; f#define f(i)Definition RSha256.hxx:104; g#define g(i)Definition RSha256.hxx:105; h#define h(i)Definition RSha256.hxx:106; e#define e(i)Definition RSha256.hxx:103; Ranking.h; Reference.h; Bool_tbool Bool_tDefinition RtypesCore.h:63; Int_tint Int_tDefinition RtypesCore.h:45; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassImp#define ClassImp(name)Definition Rtypes",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54781,Energy Efficiency,reduce,reduced,54781,"TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TAN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55141,Energy Efficiency,reduce,reduces,55141,", i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the o",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55152,Energy Efficiency,adapt,adaptation,55152,", i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the o",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55183,Energy Efficiency,reduce,reduces,55183,", i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the o",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:354,Integrability,integrat,integrated,354,". ROOT: tmva/tmva/src/MethodDNN.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MethodDNN.cxx. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Peter Speckmayer; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MethodDNN *; 8 * *; 9 * *; 10 * Description: *; 11 * A neural network implementation *; 12 * *; 13 * Authors (alphabetical): *; 14 * Simon Pfreundschuh <s.pfreundschuh@gmail.com> - CERN, Switzerland *; 15 * Peter Speckmayer <peter.speckmayer@gmx.ch> - CERN, Switzerland *; 16 * *; 17 * Copyright (c) 2005-2015: *; 18 * CERN, Switzerland *; 19 * U. of Victoria, Canada *; 20 * MPI-K Heidelberg, Germany *; 21 * U. of Bonn, Germany *; 22 * *; 23 * Redistribution and use in source and binary forms, with or without *; 24 * modification, are permitted according to the terms listed in LICENSE *; 25 * (see tmva/doc/LICENSE) *; 26 **********************************************************************************/; 27 ; 28/*! \class TMVA::MethodDNN; 29\ingroup TMVA; 30Deep Neural Network Implementation.; 31*/; 32 ; 33#include ""TMVA/MethodDNN.h""; 34 ; 35#include ""TString.h""; 36#include ""TFormula.h""; 37#include ""TObjString.h""; 38 ; 39#include ""TMVA/ClassifierFactory.h""; 40#include ""TMVA/Configurable.h""; 41#include ""TMVA/IMethod.h""; 42#include ""TMVA/MsgLogger.h""; 43#include ""TMVA/MethodBase.h""; 44#include ""TMVA/Timer.h""; 45#include ""TMVA/Types.h""; 46#include ""TMVA/Tools.h""; 47#include ""TMVA/Config.h""; 48#include ""TMVA/Ranking.h""; 49 ; 50#include ""TMVA/DNN/Net.h""; 51#include ""TMVA/DNN/Architectures/Reference.h""; 52 ; 53#include ""TMVA/NeuralNet.h""; 54#include ""TMVA/Monitoring.h""; 55 ; 56#ifdef R__HAS_TMVACPU; 57#include ""TMVA/DNN/Architectures/Cpu.h""; 58#endif; 59#ifdef R__HAS_TMVAGPU; 60#include ""TMVA/DNN/Architectures/Cuda.h""; 61#endif; 62 ; 63#includ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:53410,Integrability,message,message,53410,"yer(i).GetWeights() = weights;; 1444 fNet.GetLayer(i).GetBiases() = biases;; 1445 ; 1446 layerXML = gTools().GetNextChild(layerXML);; 1447 previousWidth = width;; 1448 }; 1449}; 1450 ; 1451////////////////////////////////////////////////////////////////////////////////; 1452 ; 1453void TMVA::MethodDNN::ReadWeightsFromStream( std::istream & /*istr*/); 1454{; 1455}; 1456 ; 1457////////////////////////////////////////////////////////////////////////////////; 1458 ; 1459const TMVA::Ranking* TMVA::MethodDNN::CreateRanking(); 1460{; 1461 fRanking = new Ranking( GetName(), ""Importance"" );; 1462 for (UInt_t ivar=0; ivar<GetNvar(); ivar++) {; 1463 fRanking->AddRank( Rank( GetInputLabel(ivar), 1.0));; 1464 }; 1465 return fRanking;; 1466}; 1467 ; 1468////////////////////////////////////////////////////////////////////////////////; 1469 ; 1470void TMVA::MethodDNN::MakeClassSpecific( std::ostream& /*fout*/,; 1471 const TString& /*className*/ ) const; 1472{; 1473}; 1474 ; 1475////////////////////////////////////////////////////////////////////////////////; 1476 ; 1477void TMVA::MethodDNN::GetHelpMessage() const; 1478{; 1479 // get help message text; 1480 //; 1481 // typical length of text line:; 1482 // ""|--------------------------------------------------------------|""; 1483 TString col = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""bold"");; 1484 TString colres = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30926,Modifiability,config,configuration,30926,"break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIndex = 0;; 882 for (size_t l = 0; l < fNet.GetDepth(); l++) {; 883 auto & layerWeights = fNet.GetLayer(l).GetWeights();; 884 for (Int_t j = 0; j < layerWeights.GetNcols(); j++) {; 885 for (Int_t i = 0; i < layerWeights.GetNrows(); i++) {; 886 layerWeights(i,j) = weights[weightIndex];; 887 weightIndex++;; 888 }; 889 }; 890 auto & layerBiases = fNet.GetLayer(l).GetBiases();; 891 if (l == 0) {; 892 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 893 layerBiases(i,0) = weights[weightIndex];; 894 weightIndex++;; 895 }; 896 } else {; 897 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54144,Modifiability,variab,variables,54144,"//////////////////////////////; 1469 ; 1470void TMVA::MethodDNN::MakeClassSpecific( std::ostream& /*fout*/,; 1471 const TString& /*className*/ ) const; 1472{; 1473}; 1474 ; 1475////////////////////////////////////////////////////////////////////////////////; 1476 ; 1477void TMVA::MethodDNN::GetHelpMessage() const; 1478{; 1479 // get help message text; 1480 //; 1481 // typical length of text line:; 1482 // ""|--------------------------------------------------------------|""; 1483 TString col = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""bold"");; 1484 TString colres = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each trai",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54911,Modifiability,layers,layers,54911," Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation functio",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54971,Modifiability,layers,layers,54971,"-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55152,Modifiability,adapt,adaptation,55152,", i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the o",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:55761,Modifiability,layers,layers,55761,"an be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TANH|(N+30),LINEAR\"" \n \; 1523 - meaning: \n \; 1524 . two hidden layers (separated by \"",\"") \n \; 1525 . the activation function is TANH (other options: RELU, SOFTSIGN, LINEAR) \n \; 1526 . the activation function for the output layer is LINEAR \n \; 1527 . the first hidden layer has (N+30)*2 nodes where N is the number of input neurons \n \; 1528 . the second hidden layer has N+30 nodes, where N is the number of input neurons \n \; 1529 . the number of nodes in the output layer is determined by the number of output nodes \n \; 1530 and can therefore not be chosen freely. \n \; 1531 \n \; 1532 \""ErrorStrategy\"": \n \; 1533 - SUMOFSQUARES \n \; 1534 The error of the neural net is determined by a sum-of-squares error function \n \; 1535 For regression, this is the only possible choice. \n \; 1536 - CROSSENTROPY \n \; 1537 The error of the neural net is determined by a cross entropy function. The \n \; 1538 output values are automatically (internally) transformed into probabilities \n \; 1539 using a sigmoid function. \n \; 1540 For signal/background classification",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59653,Modifiability,layers,layers,59653," \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn on multithreading [recommended: True] \n \; 1595 \n"";; 1596 Log () << txt << Endl;; 1597}; 1598 ; 1599} // namespace TMVA; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cpu.h; Cuda.h; IMethod.h; MethodBase.h; MethodDNN.h; Monitoring.h; Ms",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:60027,Modifiability,layers,layers,60027,"nvergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn on multithreading [recommended: True] \n \; 1595 \n"";; 1596 Log () << txt << Endl;; 1597}; 1598 ; 1599} // namespace TMVA; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cpu.h; Cuda.h; IMethod.h; MethodBase.h; MethodDNN.h; Monitoring.h; MsgLogger.h; Net.h; NeuralNet.h; f#define f(i)Definition RSha256.hxx:104; g#define g(i)Definition RSha256.hxx:105; h#define h(i)Definition RSha256.hxx:106; e#define e(i)Definition RSha256.hxx:103; Ranking.h; Reference.h; Bool_tbool Bool_tDefinition RtypesCore.h:63; Int_tint Int_tDef",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:60204,Modifiability,config,configuration,60204,"\n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn on multithreading [recommended: True] \n \; 1595 \n"";; 1596 Log () << txt << Endl;; 1597}; 1598 ; 1599} // namespace TMVA; ClassifierFactory.h; REGISTER_METHOD#define REGISTER_METHOD(CLASS)for exampleDefinition ClassifierFactory.h:124; Configurable.h; Cpu.h; Cuda.h; IMethod.h; MethodBase.h; MethodDNN.h; Monitoring.h; MsgLogger.h; Net.h; NeuralNet.h; f#define f(i)Definition RSha256.hxx:104; g#define g(i)Definition RSha256.hxx:105; h#define h(i)Definition RSha256.hxx:106; e#define e(i)Definition RSha256.hxx:103; Ranking.h; Reference.h; Bool_tbool Bool_tDefinition RtypesCore.h:63; Int_tint Int_tDefinition RtypesCore.h:45; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; Double_tdouble Double_tDefinition RtypesCore.h:59; kTRUEconstexpr Bool_t kTRUEDefinition RtypesCore.h:93; ClassImp#define ClassImp(name)Definition Rtypes",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:64854,Modifiability,variab,variablesDefinition,64854,"t Matrix_t &output, const Matrix_t &weights)Perform a single optimization step on a given batch.Definition Minimizers.h:331; TMVA::DNN::TGradientDescent::GetTestIntervalsize_t GetTestInterval() constDefinition Minimizers.h:163; TMVA::DNN::TGradientDescent::StepMomentumvoid StepMomentum(Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum)Same as the Step(...) method for multiple batches but uses momentum.Definition Minimizers.h:438; TMVA::DNN::TGradientDescent::GetConvergenceCountsize_t GetConvergenceCount() constDefinition Minimizers.h:159; TMVA::DNN::TGradientDescent::GetConvergenceStepssize_t GetConvergenceSteps() constDefinition Minimizers.h:160; TMVA::DNN::TNetGeneric neural network class.Definition Net.h:49; TMVA::EventDefinition Event.h:51; TMVA::Event::SetTargetvoid SetTarget(UInt_t itgt, Float_t value)set the target value (dimension itgt) to valueDefinition Event.cxx:367; TMVA::Event::GetNVariablesUInt_t GetNVariables() constaccessor to the number of variablesDefinition Event.cxx:316; TMVA::Event::GetNTargetsUInt_t GetNTargets() constaccessor to the number of targetsDefinition Event.cxx:327; TMVA::Event::GetValuesstd::vector< Float_t > & GetValues()Definition Event.h:94; TMVA::Event::GetTargetFloat_t GetTarget(UInt_t itgt) constDefinition Event.h:102; TMVA::MethodDNNDeep Neural Network Implementation.Definition MethodDNN.h:77; TMVA::MethodDNN::TrainGpuvoid TrainGpu()Definition MethodDNN.cxx:908; TMVA::MethodDNN::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets); TMVA::MethodDNN::Initvoid Init(); TMVA::MethodDNN::GetMulticlassValuesvirtual const std::vector< Float_t > & GetMulticlassValues()Definition MethodDNN.cxx:1352; TMVA::MethodDNN::GetNumValidationSamplesUInt_t GetNumValidationSamples(); TMVA::MethodDNN::ReadWeightsFromXMLvoid ReadWeightsFromXML(void *wghtnode)Definition MethodDNN.cxx:1401; TMVA::MethodDNN::KeyValueVector_tstd::ve",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:67727,Modifiability,variab,variables,67727,"dDNN::fWeightInitializationDNN::EInitialization fWeightInitializationDefinition MethodDNN.h:112; TMVA::MethodDNN::TrainCpuvoid TrainCpu()Definition MethodDNN.cxx:1098; TMVA::MethodDNN::DeclareOptionsvoid DeclareOptions(); TMVA::MethodDNN::Trainvoid Train()Definition MethodDNN.cxx:665; TMVA::MethodDNN::CreateRankingconst Ranking * CreateRanking()Definition MethodDNN.cxx:1459; TMVA::MethodDNN::ParseKeyValueStringKeyValueVector_t ParseKeyValueString(TString parseString, TString blockDelim, TString tokenDelim); TMVA::MethodDNN::fOutputFunctionDNN::EOutputFunction fOutputFunctionDefinition MethodDNN.h:113; TMVA::MethodDNN::AddWeightsXMLTovoid AddWeightsXMLTo(void *parent) constDefinition MethodDNN.cxx:1375; TMVA::MethodDNN::GetHelpMessagevoid GetHelpMessage() constDefinition MethodDNN.cxx:1477; TMVA::MethodDNN::GetRegressionValuesvirtual const std::vector< Float_t > & GetRegressionValues()Definition MethodDNN.cxx:1314; TMVA::RankDefinition Ranking.h:76; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::xmlengineTXMLEngine & xmlengine()Definition Tools.h:262; TMVA::Tools::ReadAttrvoid ReadAttr(void *node, const char *, T &value)read attribute from xmlDefinition Tools.h:329; TMVA::Tools::GetChildvoid * GetChild(void *parent, const char *childname=nullptr)get child nodeDefinition Tools.cxx:1150; TMVA::Tools::GetNextChildvoid * GetNextChild(void *prevchild, const char *childname=nullptr)XML helpers.Definition Tools.cxx:1162; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::kWARNING@ kWARNINGDefinition Types.h:59; TMVA::kFATAL@ kFATALDefinition Types.h:61; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:72282,Modifiability,variab,variable,72282,"l for a given weight matrix.Definition Functions.h:238; TMVA::DNN::ERegularization::kL2@ kL2; TMVA::DNN::ERegularization::kL1@ kL1; TMVA::DNN::ERegularization::kNone@ kNone; TMVA::DNN::EActivationFunctionEActivationFunctionEnum that represents layer activation functions.Definition Functions.h:32; TMVA::DNN::EActivationFunction::kRelu@ kRelu; TMVA::DNN::EActivationFunction::kGauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid; TMVA::DNN::EActivationFunction::kIdentity@ kIdentity; TMVA::DNN::EActivationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::ModeOutputValuesModeOutputValuesDefinition NeuralNet.h:179; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTString fetchValue(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDNN.cxx:320; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::MethodDNN::TTrainingSettingsDefinition MethodDNN.h:90; TMVA::MethodDNN::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDNN.h:94; TMVA::MethodDNN::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDNN.h:93; TMVA::MethodDNN::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDNN.h:95; TMVA::MethodDNN::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDNN.h:92; TMVA::MethodDNN::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDNN.h:91; TMVA::MethodDNN::TTrainingSettings::dropoutProbabilities",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:6255,Performance,perform,perform,6255,"s that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSENTROPY""));; 168 AddPreDefVal(TString(""SUMOFSQUARES""));; 169 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 170 ; 171 DeclareOptionRef(fWeightInitializationString=""XAVIER"",; 172 ""WeightInitialization"",; 173 ""Weight initialization strategy"");; 174 AddPreDefVal(TString(""XAVIER""));; 175 AddPreDefVal(TString(""XAVIERUNIFORM""));; 176 ; 177 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 178 AddPreDefVal(TString(""STANDARD""));; 179 AddPreDefVal(TString(""CPU""));; 180 AddPreDefVal(TString(""GPU""));; 181 AddPreDefVal(TString(""OPENCL""));; 182 ; 183 DeclareOptionRef(; 184 fTrainingStrategyString = ""LearningRate=1e-1,""; 185 ""Momentum=0.3,""; 186 ""Repetitions=3,""; 187 ""ConvergenceSteps=50,""; 188 ""BatchSize=30,""; 189 ""TestRepetitions=7,""; 190 ""WeightDecay=0.0,""; 191 ""Renormalize=L2,""; 192 ""DropConfig=0.0,""; 193 ""DropRepetitions=5|LearningRate=1e-4,""; 194 ""Momentum=0.3,""; 195 ""Repetitions=3,""; 196 ""ConvergenceSteps=50,""; 197 ""BatchSize=20,""; 198 ""TestRepetitions=7,""; 199 ""WeightDecay=0.001,""; 200 ""Renormalize=L2,""; 201 ""DropConfig=0.0+0.5+0.5,""; 202 ""DropRepetitions=5,""; 203 ""Multithreading=True"",; 204 ""TrainingStrategy"",; 205 ""Defines the tr",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37793,Performance,throughput,throughput,37793,"026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:45174,Performance,throughput,throughput,45174,"utMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54460,Performance,perform,performance,54460,"; 1481 // typical length of text line:; 1482 // ""|--------------------------------------------------------------|""; 1483 TString col = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""bold"");; 1484 TString colres = gConfig().WriteOptionsReference() ? TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:63883,Performance,optimiz,optimization,63883,"finition TCollection.h:235; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::DNN::LayerLayer defines the layout of a layer.Definition NeuralNet.h:673; TMVA::DNN::Netneural netDefinition NeuralNet.h:1062; TMVA::DNN::SettingsSettings for the training of the neural net.Definition NeuralNet.h:730; TMVA::DNN::SteepestSteepest Gradient Descent algorithm (SGD)Definition NeuralNet.h:334; TMVA::DNN::TCpu::Copystatic void Copy(Matrix_t &B, const Matrix_t &A)Definition Arithmetic.hxx:269; TMVA::DNN::TCuda::Copystatic void Copy(Matrix_t &B, const Matrix_t &A); TMVA::DNN::TDataLoaderTDataLoader.Definition DataLoader.h:129; TMVA::DNN::TGradientDescentDefinition Minimizers.h:56; TMVA::DNN::TGradientDescent::HasConvergedbool HasConverged()Increases the minimization step counter by the test error evaluation period and uses the current inte...Definition Minimizers.h:667; TMVA::DNN::TGradientDescent::Stepvoid Step(Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights)Perform a single optimization step on a given batch.Definition Minimizers.h:331; TMVA::DNN::TGradientDescent::GetTestIntervalsize_t GetTestInterval() constDefinition Minimizers.h:163; TMVA::DNN::TGradientDescent::StepMomentumvoid StepMomentum(Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum)Same as the Step(...) method for multiple batches but uses momentum.Definition Minimizers.h:438; TMVA::DNN::TGradientDescent::GetConvergenceCountsize_t GetConvergenceCount() constDefinition Minimizers.h:159; TMVA::DNN::TGradientDescent::GetConvergenceStepssize_t GetConvergenceSteps() constDefinition Minimizers.h:160; TMVA::DNN::TNetGeneric neural network class.Definition Net.h:49; TMVA::EventDefinition Event.h:51; TMVA::Event::SetTargetvoid SetTarget(UInt_t itgt, Float_t value)set the target value (dimension itgt) to valueDefinition Event.cxx:367; TMVA::Event::GetNVariablesUInt_t GetNVariables() consta",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:15733,Safety,detect,detected,15733,"L << ""The STANDARD architecture has been deprecated. ""; 439 ""Please use Architecture=CPU or Architecture=CPU.""; 440 ""See the TMVA Users' Guide for instructions if you ""; 441 ""encounter problems.""; 442 << Endl;; 443 }; 444 ; 445 if (fArchitectureString == ""OPENCL"") {; 446 Log() << kERROR << ""The OPENCL architecture has not been implemented yet. ""; 447 ""Please use Architecture=CPU or Architecture=CPU for the ""; 448 ""time being. See the TMVA Users' Guide for instructions ""; 449 ""if you encounter problems.""; 450 << Endl;; 451 Log() << kFATAL << ""The OPENCL architecture has not been implemented yet. ""; 452 ""Please use Architecture=CPU or Architecture=CPU for the ""; 453 ""time being. See the TMVA Users' Guide for instructions ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNV",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:15899,Safety,detect,detected,15899," ""; 441 ""encounter problems.""; 442 << Endl;; 443 }; 444 ; 445 if (fArchitectureString == ""OPENCL"") {; 446 Log() << kERROR << ""The OPENCL architecture has not been implemented yet. ""; 447 ""Please use Architecture=CPU or Architecture=CPU for the ""; 448 ""time being. See the TMVA Users' Guide for instructions ""; 449 ""if you encounter problems.""; 450 << Endl;; 451 Log() << kFATAL << ""The OPENCL architecture has not been implemented yet. ""; 452 ""Please use Architecture=CPU or Architecture=CPU for the ""; 453 ""time being. See the TMVA Users' Guide for instructions ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNVariables ();; 490 size_t outputSize = 1;; 491 if (fAnalysisType == Types::kRegression && GetNTargets() != 0) {; 492 outputSize = GetNTargets();; 493 } else if (fAnaly",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:16223,Safety,detect,detected,16223,".""; 450 << Endl;; 451 Log() << kFATAL << ""The OPENCL architecture has not been implemented yet. ""; 452 ""Please use Architecture=CPU or Architecture=CPU for the ""; 453 ""time being. See the TMVA Users' Guide for instructions ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNVariables ();; 490 size_t outputSize = 1;; 491 if (fAnalysisType == Types::kRegression && GetNTargets() != 0) {; 492 outputSize = GetNTargets();; 493 } else if (fAnalysisType == Types::kMulticlass && DataInfo().GetNClasses() >= 2) {; 494 outputSize = DataInfo().GetNClasses();; 495 }; 496 ; 497 fNet.SetBatchSize(1);; 498 fNet.SetInputWidth(inputSize);; 499 ; 500 auto itLayout = std::begin (fLayout);; 501 auto itLayoutEnd = std::end (fLayout)-1;; 502 for ( ; itLayout != itLayoutEnd; ++itLayout) {; 503 fN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:16445,Safety,detect,detected,16445," ""; 454 ""if you encounter problems.""; 455 << Endl;; 456 }; 457 ; 458 if (fArchitectureString == ""GPU"") {; 459#ifndef DNNCUDA // Included only if DNNCUDA flag is _not_ set.; 460 Log() << kERROR << ""CUDA backend not enabled. Please make sure ""; 461 ""you have CUDA installed and it was successfully ""; 462 ""detected by CMAKE.""; 463 << Endl;; 464 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 465 ""you have CUDA installed and it was successfully ""; 466 ""detected by CMAKE.""; 467 << Endl;; 468#endif // DNNCUDA; 469 }; 470 ; 471 if (fArchitectureString == ""CPU"") {; 472#ifndef DNNCPU // Included only if DNNCPU flag is _not_ set.; 473 Log() << kERROR << ""Multi-core CPU backend not enabled. Please make sure ""; 474 ""you have a BLAS implementation and it was successfully ""; 475 ""detected by CMake as well that the imt CMake flag is set.""; 476 << Endl;; 477 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 478 ""you have a BLAS implementation and it was successfully ""; 479 ""detected by CMake as well that the imt CMake flag is set.""; 480 << Endl;; 481#endif // DNNCPU; 482 }; 483 ; 484 //; 485 // Set network structure.; 486 //; 487 ; 488 fLayout = TMVA::MethodDNN::ParseLayoutString (fLayoutString);; 489 size_t inputSize = GetNVariables ();; 490 size_t outputSize = 1;; 491 if (fAnalysisType == Types::kRegression && GetNTargets() != 0) {; 492 outputSize = GetNTargets();; 493 } else if (fAnalysisType == Types::kMulticlass && DataInfo().GetNClasses() >= 2) {; 494 outputSize = DataInfo().GetNClasses();; 495 }; 496 ; 497 fNet.SetBatchSize(1);; 498 fNet.SetInputWidth(inputSize);; 499 ; 500 auto itLayout = std::begin (fLayout);; 501 auto itLayoutEnd = std::end (fLayout)-1;; 502 for ( ; itLayout != itLayoutEnd; ++itLayout) {; 503 fNet.AddLayer((*itLayout).first, (*itLayout).second);; 504 }; 505 fNet.AddLayer(outputSize, EActivationFunction::kIdentity);; 506 ; 507 //; 508 // Loss function and output.; 509 //; 510 ; 511 fOutputFunction = EOutputFunctio",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:39140,Safety,detect,detected,39140,"std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;; 1093#endif // DNNCUDA; 1094}; 1095 ; 1096////////////////////////////////////////////////////////////////////////////////; 1097 ; 1098void TMVA::MethodDNN::TrainCpu(); 1099{; 1100 ; 1101#ifdef DNNCPU // Included only if DNNCPU flag is set.; 1102 Log() << kINFO << ""Start of neural network training on CPU."" << Endl << Endl;; 1103 ; 1104 size_t nValidationSamples = GetNumValidationSamples();; 1105 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1106 size_t nTestSamples = nValidationSamples;; 1107 ; 1108 Log() << kDEBUG << ""Using "" << nValidationSamples << "" validation samples."" << Endl;; 1109 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 1110 ; 1111 fNet.Initialize(fWeightInitialization);; 1112 ; 1113 size_t trainingPhase = 1;; 1114 for (TTrainingSettings & settings : fTrainingSettings) {; 1115 ; 1116 if (fInteractive){; 1117 fInteractive->ClearGraphs",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:46561,Safety,detect,detected,46561,"ractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l < net.GetDepth(); l++) {; 1282 auto & layer = fNet.GetLayer(l);; 1283 layer.GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1284 layer.GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1285 }; 1286 }; 1287 ; 1288#else // DNNCPU flag not set.; 1289 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 1290 ""you have a BLAS implementation and it was successfully ""; 1291 ""detected by CMake as well that the imt CMake flag is set."" << Endl;; 1292#endif // DNNCPU; 1293}; 1294 ; 1295////////////////////////////////////////////////////////////////////////////////; 1296 ; 1297Double_t TMVA::MethodDNN::GetMvaValue( Double_t* /*errLower*/, Double_t* /*errUpper*/ ); 1298{; 1299 size_t nVariables = GetEvent()->GetNVariables();; 1300 Matrix_t X(1, nVariables);; 1301 Matrix_t YHat(1, 1);; 1302 ; 1303 const std::vector<Float_t>& inputValues = GetEvent()->GetValues();; 1304 for (size_t i = 0; i < nVariables; i++) {; 1305 X(0,i) = inputValues[i];; 1306 }; 1307 ; 1308 fNet.Prediction(YHat, X, fOutputFunction);; 1309 return YHat(0,0);; 1310}; 1311 ; 1312////////////////////////////////////////////////////////////////////////////////; 1313 ; 1314const std::vector<Float_t> & TMVA::MethodDNN::GetRegressionValues(); 1315{; 1316 size_t nVariables = GetEvent()->GetNVariables();; 1317 Matrix_t X(1, nVariables);; 1318 ; 1319 const Event *ev = GetEvent();; 1320 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:4841,Security,validat,validation,4841,"arget; 113 ; 114Bool_t TMVA::MethodDNN::HasAnalysisType(Types::EAnalysisType type,; 115 UInt_t numberClasses,; 116 UInt_t /*numberTargets*/ ); 117{; 118 if (type == Types::kClassification && numberClasses == 2 ) return kTRUE;; 119 if (type == Types::kMulticlass ) return kTRUE;; 120 if (type == Types::kRegression ) return kTRUE;; 121 ; 122 return kFALSE;; 123}; 124 ; 125////////////////////////////////////////////////////////////////////////////////; 126/// default initializations; 127 ; 128void TMVA::MethodDNN::Init() {; 129 Log() << kWARNING; 130 << ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSEN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:4997,Security,validat,validation,4997,"erTargets*/ ); 117{; 118 if (type == Types::kClassification && numberClasses == 2 ) return kTRUE;; 119 if (type == Types::kMulticlass ) return kTRUE;; 120 if (type == Types::kRegression ) return kTRUE;; 121 ; 122 return kFALSE;; 123}; 124 ; 125////////////////////////////////////////////////////////////////////////////////; 126/// default initializations; 127 ; 128void TMVA::MethodDNN::Init() {; 129 Log() << kWARNING; 130 << ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSENTROPY""));; 168 AddPreDefVal(TString(""SUMOFSQUARES""));; 169 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 170 ; 171 DeclareOptionRef",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:5451,Security,validat,validation,5451,"<< ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSENTROPY""));; 168 AddPreDefVal(TString(""SUMOFSQUARES""));; 169 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 170 ; 171 DeclareOptionRef(fWeightInitializationString=""XAVIER"",; 172 ""WeightInitialization"",; 173 ""Weight initialization strategy"");; 174 AddPreDefVal(TString(""XAVIER""));; 175 AddPreDefVal(TString(""XAVIERUNIFORM""));; 176 ; 177 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 178 AddPreDefVal(TString(""STANDARD""));; 179 AddPreDefVal(TString(""CPU""));; 180 AddPreDefVal(TString(""GPU""));; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:5527,Security,validat,validation,5527,"; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entropy (binary classification)."");; 167 AddPreDefVal(TString(""CROSSENTROPY""));; 168 AddPreDefVal(TString(""SUMOFSQUARES""));; 169 AddPreDefVal(TString(""MUTUALEXCLUSIVE""));; 170 ; 171 DeclareOptionRef(fWeightInitializationString=""XAVIER"",; 172 ""WeightInitialization"",; 173 ""Weight initialization strategy"");; 174 AddPreDefVal(TString(""XAVIER""));; 175 AddPreDefVal(TString(""XAVIERUNIFORM""));; 176 ; 177 DeclareOptionRef(fArchitectureString = ""CPU"", ""Architecture"", ""Which architecture to perform the training on."");; 178 AddPreDefVal(TString(""STANDARD""));; 179 AddPreDefVal(TString(""CPU""));; 180 AddPreDefVal(TString(""GPU""));; 181 AddPreDefVal(TString(""OPENCL""));; 182 ; 183 DeclareOptionRef(; 184 fTrainingStrategyString = ""LearningRate=1e-1",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:19068,Security,validat,validation,19068,"rrorStrategy == ""SUMOFSQUARES"") {; 531 fNet.SetLossFunction(ELossFunction::kMeanSquaredError);; 532 }; 533 if (fErrorStrategy == ""CROSSENTROPY"") {; 534 fNet.SetLossFunction(ELossFunction::kCrossEntropy);; 535 }; 536 if (fErrorStrategy == ""MUTUALEXCLUSIVE"") {; 537 fNet.SetLossFunction(ELossFunction::kSoftmaxCrossEntropy);; 538 }; 539 fOutputFunction = EOutputFunction::kSoftmax;; 540 }; 541 ; 542 //; 543 // Initialization; 544 //; 545 ; 546 if (fWeightInitializationString == ""XAVIER"") {; 547 fWeightInitialization = DNN::EInitialization::kGauss;; 548 }; 549 else if (fWeightInitializationString == ""XAVIERUNIFORM"") {; 550 fWeightInitialization = DNN::EInitialization::kUniform;; 551 }; 552 else {; 553 fWeightInitialization = DNN::EInitialization::kGauss;; 554 }; 555 ; 556 //; 557 // Training settings.; 558 //; 559 ; 560 // Force validation of the ValidationSize option; 561 GetNumValidationSamples();; 562 ; 563 KeyValueVector_t strategyKeyValues = ParseKeyValueString(fTrainingStrategyString,; 564 TString (""|""),; 565 TString ("",""));; 566 ; 567 std::cout << ""Parsed Training DNN string "" << fTrainingStrategyString << std::endl;; 568 std::cout << ""STring has size "" << strategyKeyValues.size() << std::endl;; 569 for (auto& block : strategyKeyValues) {; 570 TTrainingSettings settings;; 571 ; 572 settings.convergenceSteps = fetchValue(block, ""ConvergenceSteps"", 100);; 573 settings.batchSize = fetchValue(block, ""BatchSize"", 30);; 574 settings.testInterval = fetchValue(block, ""TestRepetitions"", 7);; 575 settings.weightDecay = fetchValue(block, ""WeightDecay"", 0.0);; 576 settings.learningRate = fetchValue(block, ""LearningRate"", 1e-5);; 577 settings.momentum = fetchValue(block, ""Momentum"", 0.3);; 578 settings.dropoutProbabilities = fetchValue(block, ""DropConfig"",; 579 std::vector<Double_t>());; 580 ; 581 TString regularization = fetchValue(block, ""Regularization"",; 582 TString (""NONE""));; 583 if (regularization == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:20961,Security,validat,validation,20961,"gs.dropoutProbabilities = fetchValue(block, ""DropConfig"",; 579 std::vector<Double_t>());; 580 ; 581 TString regularization = fetchValue(block, ""Regularization"",; 582 TString (""NONE""));; 583 if (regularization == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 } else if (regularization == ""L2"") {; 586 settings.regularization = DNN::ERegularization::kL2;; 587 } else {; 588 settings.regularization = DNN::ERegularization::kNone;; 589 }; 590 ; 591 TString strMultithreading = fetchValue(block, ""Multithreading"",; 592 TString (""True""));; 593 if (strMultithreading.BeginsWith (""T"")) {; 594 settings.multithreading = true;; 595 } else {; 596 settings.multithreading = false;; 597 }; 598 ; 599 fTrainingSettings.push_back(settings);; 600 }; 601}; 602 ; 603////////////////////////////////////////////////////////////////////////////////; 604/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 605/// 100 etc.; 606/// - 20% and 0.2 selects 20% of the training set as validation data.; 607/// - 100 selects 100 events as the validation data.; 608///; 609/// @return number of samples in validation set; 610///; 611 ; 612UInt_t TMVA::MethodDNN::GetNumValidationSamples(); 613{; 614 Int_t nValidationSamples = 0;; 615 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 616 ; 617 // Parsing + Validation; 618 // --------------------; 619 if (fValidationSize.EndsWith(""%"")) {; 620 // Relative spec. format 20%; 621 TString intValStr = TString(fValidationSize.Strip(TString::kTrailing, '%'));; 622 ; 623 if (intValStr.IsFloat()) {; 624 Double_t valSizeAsDouble = fValidationSize.Atof() / 100.0;; 625 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 626 } else {; 627 Log() << kFATAL << ""Cannot parse number \"""" << fValidationSize; 628 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 629 }; 630 } else if (fValidationSize.IsFloat()) {; 631 Double_t valSizeAsDouble = fValidationSize.Atof();; 63",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:21018,Security,validat,validation,21018,""",; 579 std::vector<Double_t>());; 580 ; 581 TString regularization = fetchValue(block, ""Regularization"",; 582 TString (""NONE""));; 583 if (regularization == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 } else if (regularization == ""L2"") {; 586 settings.regularization = DNN::ERegularization::kL2;; 587 } else {; 588 settings.regularization = DNN::ERegularization::kNone;; 589 }; 590 ; 591 TString strMultithreading = fetchValue(block, ""Multithreading"",; 592 TString (""True""));; 593 if (strMultithreading.BeginsWith (""T"")) {; 594 settings.multithreading = true;; 595 } else {; 596 settings.multithreading = false;; 597 }; 598 ; 599 fTrainingSettings.push_back(settings);; 600 }; 601}; 602 ; 603////////////////////////////////////////////////////////////////////////////////; 604/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 605/// 100 etc.; 606/// - 20% and 0.2 selects 20% of the training set as validation data.; 607/// - 100 selects 100 events as the validation data.; 608///; 609/// @return number of samples in validation set; 610///; 611 ; 612UInt_t TMVA::MethodDNN::GetNumValidationSamples(); 613{; 614 Int_t nValidationSamples = 0;; 615 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 616 ; 617 // Parsing + Validation; 618 // --------------------; 619 if (fValidationSize.EndsWith(""%"")) {; 620 // Relative spec. format 20%; 621 TString intValStr = TString(fValidationSize.Strip(TString::kTrailing, '%'));; 622 ; 623 if (intValStr.IsFloat()) {; 624 Double_t valSizeAsDouble = fValidationSize.Atof() / 100.0;; 625 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 626 } else {; 627 Log() << kFATAL << ""Cannot parse number \"""" << fValidationSize; 628 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 629 }; 630 } else if (fValidationSize.IsFloat()) {; 631 Double_t valSizeAsDouble = fValidationSize.Atof();; 632 ; 633 if (valSizeAsDouble < 1.0) {; 634 // Relative s",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:21080,Security,validat,validation,21080,"zation == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 } else if (regularization == ""L2"") {; 586 settings.regularization = DNN::ERegularization::kL2;; 587 } else {; 588 settings.regularization = DNN::ERegularization::kNone;; 589 }; 590 ; 591 TString strMultithreading = fetchValue(block, ""Multithreading"",; 592 TString (""True""));; 593 if (strMultithreading.BeginsWith (""T"")) {; 594 settings.multithreading = true;; 595 } else {; 596 settings.multithreading = false;; 597 }; 598 ; 599 fTrainingSettings.push_back(settings);; 600 }; 601}; 602 ; 603////////////////////////////////////////////////////////////////////////////////; 604/// Validation of the ValidationSize option. Allowed formats are 20%, 0.2 and; 605/// 100 etc.; 606/// - 20% and 0.2 selects 20% of the training set as validation data.; 607/// - 100 selects 100 events as the validation data.; 608///; 609/// @return number of samples in validation set; 610///; 611 ; 612UInt_t TMVA::MethodDNN::GetNumValidationSamples(); 613{; 614 Int_t nValidationSamples = 0;; 615 UInt_t trainingSetSize = GetEventCollection(Types::kTraining).size();; 616 ; 617 // Parsing + Validation; 618 // --------------------; 619 if (fValidationSize.EndsWith(""%"")) {; 620 // Relative spec. format 20%; 621 TString intValStr = TString(fValidationSize.Strip(TString::kTrailing, '%'));; 622 ; 623 if (intValStr.IsFloat()) {; 624 Double_t valSizeAsDouble = fValidationSize.Atof() / 100.0;; 625 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 626 } else {; 627 Log() << kFATAL << ""Cannot parse number \"""" << fValidationSize; 628 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 629 }; 630 } else if (fValidationSize.IsFloat()) {; 631 Double_t valSizeAsDouble = fValidationSize.Atof();; 632 ; 633 if (valSizeAsDouble < 1.0) {; 634 // Relative spec. format 0.2; 635 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 636 } else {; 637 // Absolute spec format",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:22390,Security,validat,validation,22390,"String(fValidationSize.Strip(TString::kTrailing, '%'));; 622 ; 623 if (intValStr.IsFloat()) {; 624 Double_t valSizeAsDouble = fValidationSize.Atof() / 100.0;; 625 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 626 } else {; 627 Log() << kFATAL << ""Cannot parse number \"""" << fValidationSize; 628 << ""\"". Expected string like \""20%\"" or \""20.0%\""."" << Endl;; 629 }; 630 } else if (fValidationSize.IsFloat()) {; 631 Double_t valSizeAsDouble = fValidationSize.Atof();; 632 ; 633 if (valSizeAsDouble < 1.0) {; 634 // Relative spec. format 0.2; 635 nValidationSamples = GetEventCollection(Types::kTraining).size() * valSizeAsDouble;; 636 } else {; 637 // Absolute spec format 100 or 100.0; 638 nValidationSamples = valSizeAsDouble;; 639 }; 640 } else {; 641 Log() << kFATAL << ""Cannot parse number \"""" << fValidationSize << ""\"". Expected string like \""0.2\"" or \""100\"".""; 642 << Endl;; 643 }; 644 ; 645 // Value validation; 646 // ----------------; 647 if (nValidationSamples < 0) {; 648 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is negative."" << Endl;; 649 }; 650 ; 651 if (nValidationSamples == 0) {; 652 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is zero."" << Endl;; 653 }; 654 ; 655 if (nValidationSamples >= (Int_t)trainingSetSize) {; 656 Log() << kFATAL << ""Validation size \"""" << fValidationSize; 657 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 658 }; 659 ; 660 return nValidationSamples;; 661}; 662 ; 663////////////////////////////////////////////////////////////////////////////////; 664 ; 665void TMVA::MethodDNN::Train(); 666{; 667 if (fInteractive && fInteractive->NotInitialized()){; 668 std::vector<TString> titles = {""Error on training set"", ""Error on test set""};; 669 fInteractive->Init(titles);; 670 // JsMVA progress bar maximum (100%); 671 fIPyMaxIter = 100;; 672 }; 673 ; 674 for (TTrainingSettings & settings : fTrainingSettings) {; 675 si",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:32820,Security,validat,validation,32820,"6 layerWeights(i,j) = weights[weightIndex];; 887 weightIndex++;; 888 }; 889 }; 890 auto & layerBiases = fNet.GetLayer(l).GetBiases();; 891 if (l == 0) {; 892 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 893 layerBiases(i,0) = weights[weightIndex];; 894 weightIndex++;; 895 }; 896 } else {; 897 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 898 layerBiases(i,0) = 0.0;; 899 }; 900 }; 901 }; 902 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 903 ExitFromTraining();; 904}; 905 ; 906////////////////////////////////////////////////////////////////////////////////; 907 ; 908void TMVA::MethodDNN::TrainGpu(); 909{; 910 ; 911#ifdef DNNCUDA // Included only if DNNCUDA flag is set.; 912 Log() << kINFO << ""Start of neural network training on GPU."" << Endl << Endl;; 913 ; 914 size_t nValidationSamples = GetNumValidationSamples();; 915 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 916 size_t nTestSamples = nValidationSamples;; 917 ; 918 Log() << kDEBUG << ""Using "" << nValidationSamples << "" validation samples."" << Endl;; 919 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 920 ; 921 size_t trainingPhase = 1;; 922 fNet.Initialize(fWeightInitialization);; 923 for (TTrainingSettings & settings : fTrainingSettings) {; 924 ; 925 if (fInteractive){; 926 fInteractive->ClearGraphs();; 927 }; 928 ; 929 TNet<TCuda<>> net(settings.batchSize, fNet);; 930 net.SetWeightDecay(settings.weightDecay);; 931 net.SetRegularization(settings.regularization);; 932 ; 933 // Need to convert dropoutprobabilities to conventions used; 934 // by backend implementation.; 935 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 936 for (auto & p : dropoutVector) {; 937 p = 1.0 - p;; 938 }; 939 net.SetDropoutProbabilities(dropoutVector);; 940 ; 941 net.InitializeGradients();; 942 auto testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" o",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:33953,Security,validat,validation,33953,"e = 1;; 922 fNet.Initialize(fWeightInitialization);; 923 for (TTrainingSettings & settings : fTrainingSettings) {; 924 ; 925 if (fInteractive){; 926 fInteractive->ClearGraphs();; 927 }; 928 ; 929 TNet<TCuda<>> net(settings.batchSize, fNet);; 930 net.SetWeightDecay(settings.weightDecay);; 931 net.SetRegularization(settings.regularization);; 932 ; 933 // Need to convert dropoutprobabilities to conventions used; 934 // by backend implementation.; 935 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 936 for (auto & p : dropoutVector) {; 937 p = 1.0 - p;; 938 }; 939 net.SetDropoutProbabilities(dropoutVector);; 940 ; 941 net.InitializeGradients();; 942 auto testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:39776,Security,validat,validation,39776,"082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;; 1093#endif // DNNCUDA; 1094}; 1095 ; 1096////////////////////////////////////////////////////////////////////////////////; 1097 ; 1098void TMVA::MethodDNN::TrainCpu(); 1099{; 1100 ; 1101#ifdef DNNCPU // Included only if DNNCPU flag is set.; 1102 Log() << kINFO << ""Start of neural network training on CPU."" << Endl << Endl;; 1103 ; 1104 size_t nValidationSamples = GetNumValidationSamples();; 1105 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 1106 size_t nTestSamples = nValidationSamples;; 1107 ; 1108 Log() << kDEBUG << ""Using "" << nValidationSamples << "" validation samples."" << Endl;; 1109 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 1110 ; 1111 fNet.Initialize(fWeightInitialization);; 1112 ; 1113 size_t trainingPhase = 1;; 1114 for (TTrainingSettings & settings : fTrainingSettings) {; 1115 ; 1116 if (fInteractive){; 1117 fInteractive->ClearGraphs();; 1118 }; 1119 ; 1120 Log() << ""Training phase "" << trainingPhase << "" of ""; 1121 << fTrainingSettings.size() << "":"" << Endl;; 1122 trainingPhase++;; 1123 ; 1124 TNet<TCpu<>> net(settings.batchSize, fNet);; 1125 net.SetWeightDecay(settings.weightDecay);; 1126 net.SetRegularization(settings.regularization);; 1127 // Need to convert dropoutprobabilities to conventions used; 1128 // by backend implementation.; 1129 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1130 for (auto & p : dropoutVector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 11",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:40923,Security,validat,validation,40923,"; 1113 size_t trainingPhase = 1;; 1114 for (TTrainingSettings & settings : fTrainingSettings) {; 1115 ; 1116 if (fInteractive){; 1117 fInteractive->ClearGraphs();; 1118 }; 1119 ; 1120 Log() << ""Training phase "" << trainingPhase << "" of ""; 1121 << fTrainingSettings.size() << "":"" << Endl;; 1122 trainingPhase++;; 1123 ; 1124 TNet<TCpu<>> net(settings.batchSize, fNet);; 1125 net.SetWeightDecay(settings.weightDecay);; 1126 net.SetRegularization(settings.regularization);; 1127 // Need to convert dropoutprobabilities to conventions used; 1128 // by backend implementation.; 1129 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1130 for (auto & p : dropoutVector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:19685,Testability,test,testInterval,19685," {; 547 fWeightInitialization = DNN::EInitialization::kGauss;; 548 }; 549 else if (fWeightInitializationString == ""XAVIERUNIFORM"") {; 550 fWeightInitialization = DNN::EInitialization::kUniform;; 551 }; 552 else {; 553 fWeightInitialization = DNN::EInitialization::kGauss;; 554 }; 555 ; 556 //; 557 // Training settings.; 558 //; 559 ; 560 // Force validation of the ValidationSize option; 561 GetNumValidationSamples();; 562 ; 563 KeyValueVector_t strategyKeyValues = ParseKeyValueString(fTrainingStrategyString,; 564 TString (""|""),; 565 TString ("",""));; 566 ; 567 std::cout << ""Parsed Training DNN string "" << fTrainingStrategyString << std::endl;; 568 std::cout << ""STring has size "" << strategyKeyValues.size() << std::endl;; 569 for (auto& block : strategyKeyValues) {; 570 TTrainingSettings settings;; 571 ; 572 settings.convergenceSteps = fetchValue(block, ""ConvergenceSteps"", 100);; 573 settings.batchSize = fetchValue(block, ""BatchSize"", 30);; 574 settings.testInterval = fetchValue(block, ""TestRepetitions"", 7);; 575 settings.weightDecay = fetchValue(block, ""WeightDecay"", 0.0);; 576 settings.learningRate = fetchValue(block, ""LearningRate"", 1e-5);; 577 settings.momentum = fetchValue(block, ""Momentum"", 0.3);; 578 settings.dropoutProbabilities = fetchValue(block, ""DropConfig"",; 579 std::vector<Double_t>());; 580 ; 581 TString regularization = fetchValue(block, ""Regularization"",; 582 TString (""NONE""));; 583 if (regularization == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 } else if (regularization == ""L2"") {; 586 settings.regularization = DNN::ERegularization::kL2;; 587 } else {; 588 settings.regularization = DNN::ERegularization::kNone;; 589 }; 590 ; 591 TString strMultithreading = fetchValue(block, ""Multithreading"",; 592 TString (""True""));; 593 if (strMultithreading.BeginsWith (""T"")) {; 594 settings.multithreading = true;; 595 } else {; 596 settings.multithreading = false;; 597 }; 598 ; 599 fTrainingSettings.push_back(settings);; 600 }; 601}; 602 ;",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:23253,Testability,test,test,23253,"FATAL << ""Cannot parse number \"""" << fValidationSize << ""\"". Expected string like \""0.2\"" or \""100\"".""; 642 << Endl;; 643 }; 644 ; 645 // Value validation; 646 // ----------------; 647 if (nValidationSamples < 0) {; 648 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is negative."" << Endl;; 649 }; 650 ; 651 if (nValidationSamples == 0) {; 652 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is zero."" << Endl;; 653 }; 654 ; 655 if (nValidationSamples >= (Int_t)trainingSetSize) {; 656 Log() << kFATAL << ""Validation size \"""" << fValidationSize; 657 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 658 }; 659 ; 660 return nValidationSamples;; 661}; 662 ; 663////////////////////////////////////////////////////////////////////////////////; 664 ; 665void TMVA::MethodDNN::Train(); 666{; 667 if (fInteractive && fInteractive->NotInitialized()){; 668 std::vector<TString> titles = {""Error on training set"", ""Error on test set""};; 669 fInteractive->Init(titles);; 670 // JsMVA progress bar maximum (100%); 671 fIPyMaxIter = 100;; 672 }; 673 ; 674 for (TTrainingSettings & settings : fTrainingSettings) {; 675 size_t nValidationSamples = GetNumValidationSamples();; 676 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 677 size_t nTestSamples = nValidationSamples;; 678 ; 679 if (nTrainingSamples < settings.batchSize ||; 680 nValidationSamples < settings.batchSize ||; 681 nTestSamples < settings.batchSize) {; 682 Log() << kFATAL << ""Number of samples in the datasets are train: ""; 683 << nTrainingSamples << "" valid: "" << nValidationSamples; 684 << "" test: "" << nTestSamples << "". ""; 685 << ""One of these is smaller than the batch size of ""; 686 << settings.batchSize << "". Please increase the batch""; 687 << "" size to be at least the same size as the smallest""; 688 << "" of these values."" << Endl;; 689 }; 690 }; 691 ; 692 if (fArchitectureString == ""GPU"") {; 693 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:23932,Testability,test,test,23932,"ize to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 658 }; 659 ; 660 return nValidationSamples;; 661}; 662 ; 663////////////////////////////////////////////////////////////////////////////////; 664 ; 665void TMVA::MethodDNN::Train(); 666{; 667 if (fInteractive && fInteractive->NotInitialized()){; 668 std::vector<TString> titles = {""Error on training set"", ""Error on test set""};; 669 fInteractive->Init(titles);; 670 // JsMVA progress bar maximum (100%); 671 fIPyMaxIter = 100;; 672 }; 673 ; 674 for (TTrainingSettings & settings : fTrainingSettings) {; 675 size_t nValidationSamples = GetNumValidationSamples();; 676 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 677 size_t nTestSamples = nValidationSamples;; 678 ; 679 if (nTrainingSamples < settings.batchSize ||; 680 nValidationSamples < settings.batchSize ||; 681 nTestSamples < settings.batchSize) {; 682 Log() << kFATAL << ""Number of samples in the datasets are train: ""; 683 << nTrainingSamples << "" valid: "" << nValidationSamples; 684 << "" test: "" << nTestSamples << "". ""; 685 << ""One of these is smaller than the batch size of ""; 686 << settings.batchSize << "". Please increase the batch""; 687 << "" size to be at least the same size as the smallest""; 688 << "" of these values."" << Endl;; 689 }; 690 }; 691 ; 692 if (fArchitectureString == ""GPU"") {; 693 TrainGpu();; 694 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 695 ExitFromTraining();; 696 return;; 697 } else if (fArchitectureString == ""OpenCL"") {; 698 Log() << kFATAL << ""OpenCL backend not yet supported."" << Endl;; 699 return;; 700 } else if (fArchitectureString == ""CPU"") {; 701 TrainCpu();; 702 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 703 ExitFromTraining();; 704 return;; 705 }; 706 ; 707 Log() << kINFO << ""Using Standard Implementation."";; 708 ; 709 std::vector<Pattern> trainPattern;; 710 std::vector<Pattern> testPattern;; 711 ; 712 size_t nValidationSamples = GetNumValidationSamples",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:24794,Testability,test,testPattern,24794,"s in the datasets are train: ""; 683 << nTrainingSamples << "" valid: "" << nValidationSamples; 684 << "" test: "" << nTestSamples << "". ""; 685 << ""One of these is smaller than the batch size of ""; 686 << settings.batchSize << "". Please increase the batch""; 687 << "" size to be at least the same size as the smallest""; 688 << "" of these values."" << Endl;; 689 }; 690 }; 691 ; 692 if (fArchitectureString == ""GPU"") {; 693 TrainGpu();; 694 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 695 ExitFromTraining();; 696 return;; 697 } else if (fArchitectureString == ""OpenCL"") {; 698 Log() << kFATAL << ""OpenCL backend not yet supported."" << Endl;; 699 return;; 700 } else if (fArchitectureString == ""CPU"") {; 701 TrainCpu();; 702 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 703 ExitFromTraining();; 704 return;; 705 }; 706 ; 707 Log() << kINFO << ""Using Standard Implementation."";; 708 ; 709 std::vector<Pattern> trainPattern;; 710 std::vector<Pattern> testPattern;; 711 ; 712 size_t nValidationSamples = GetNumValidationSamples();; 713 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 714 ; 715 const std::vector<TMVA::Event *> &allData = GetEventCollection(Types::kTraining);; 716 const std::vector<TMVA::Event *> eventCollectionTraining{allData.begin(), allData.begin() + nTrainingSamples};; 717 const std::vector<TMVA::Event *> eventCollectionTesting{allData.begin() + nTrainingSamples, allData.end()};; 718 ; 719 for (auto &event : eventCollectionTraining) {; 720 const std::vector<Float_t>& values = event->GetValues();; 721 if (fAnalysisType == Types::kClassification) {; 722 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 723 trainPattern.push_back(Pattern (values.begin(),; 724 values.end(),; 725 outputValue,; 726 event->GetWeight()));; 727 trainPattern.back().addInput(1.0);; 728 } else if (fAnalysisType == Types::kMulticlass) {; 729 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 730 oneHot[event->GetClass",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:26539,Testability,test,testPattern,26539,"rn.push_back(Pattern (values.begin(),; 724 values.end(),; 725 outputValue,; 726 event->GetWeight()));; 727 trainPattern.back().addInput(1.0);; 728 } else if (fAnalysisType == Types::kMulticlass) {; 729 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 730 oneHot[event->GetClass()] = 1.0;; 731 trainPattern.push_back(Pattern (values.begin(), values.end(),; 732 oneHot.cbegin(), oneHot.cend(),; 733 event->GetWeight()));; 734 trainPattern.back().addInput(1.0);; 735 } else {; 736 const std::vector<Float_t>& targets = event->GetTargets ();; 737 trainPattern.push_back(Pattern(values.begin(),; 738 values.end(),; 739 targets.begin(),; 740 targets.end(),; 741 event->GetWeight ()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOu",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:26655,Testability,test,testPattern,26655,"eight()));; 727 trainPattern.back().addInput(1.0);; 728 } else if (fAnalysisType == Types::kMulticlass) {; 729 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 730 oneHot[event->GetClass()] = 1.0;; 731 trainPattern.push_back(Pattern (values.begin(), values.end(),; 732 oneHot.cbegin(), oneHot.cend(),; 733 event->GetWeight()));; 734 trainPattern.back().addInput(1.0);; 735 } else {; 736 const std::vector<Float_t>& targets = event->GetTargets ();; 737 trainPattern.push_back(Pattern(values.begin(),; 738 values.end(),; 739 targets.begin(),; 740 targets.end(),; 741 event->GetWeight ()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOutputSize(fNet.GetOutputWidth() + 1);; 780 ; 781 for (size_t i = 0; i < fNet.GetDepth(); i++)",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:26852,Testability,test,testPattern,26852,"tern.push_back(Pattern (values.begin(), values.end(),; 732 oneHot.cbegin(), oneHot.cend(),; 733 event->GetWeight()));; 734 trainPattern.back().addInput(1.0);; 735 } else {; 736 const std::vector<Float_t>& targets = event->GetTargets ();; 737 trainPattern.push_back(Pattern(values.begin(),; 738 values.end(),; 739 targets.begin(),; 740 targets.end(),; 741 event->GetWeight ()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOutputSize(fNet.GetOutputWidth() + 1);; 780 ; 781 for (size_t i = 0; i < fNet.GetDepth(); i++) {; 782 EActivationFunction f = fNet.GetLayer(i).GetActivationFunction();; 783 EnumFunction g = EnumFunction::LINEAR;; 784 switch(f) {; 785 case EActivationFunction::kIdentity: g = EnumFunction::LINEAR; break;; 786 case E",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:26982,Testability,test,testPattern,26982,"));; 734 trainPattern.back().addInput(1.0);; 735 } else {; 736 const std::vector<Float_t>& targets = event->GetTargets ();; 737 trainPattern.push_back(Pattern(values.begin(),; 738 values.end(),; 739 targets.begin(),; 740 targets.end(),; 741 event->GetWeight ()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOutputSize(fNet.GetOutputWidth() + 1);; 780 ; 781 for (size_t i = 0; i < fNet.GetDepth(); i++) {; 782 EActivationFunction f = fNet.GetLayer(i).GetActivationFunction();; 783 EnumFunction g = EnumFunction::LINEAR;; 784 switch(f) {; 785 case EActivationFunction::kIdentity: g = EnumFunction::LINEAR; break;; 786 case EActivationFunction::kRelu: g = EnumFunction::RELU; break;; 787 case EActivationFunction::kSigmoid: g = EnumFunctio",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:27100,Testability,test,testPattern,27100,"gets = event->GetTargets ();; 737 trainPattern.push_back(Pattern(values.begin(),; 738 values.end(),; 739 targets.begin(),; 740 targets.end(),; 741 event->GetWeight ()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOutputSize(fNet.GetOutputWidth() + 1);; 780 ; 781 for (size_t i = 0; i < fNet.GetDepth(); i++) {; 782 EActivationFunction f = fNet.GetLayer(i).GetActivationFunction();; 783 EnumFunction g = EnumFunction::LINEAR;; 784 switch(f) {; 785 case EActivationFunction::kIdentity: g = EnumFunction::LINEAR; break;; 786 case EActivationFunction::kRelu: g = EnumFunction::RELU; break;; 787 case EActivationFunction::kSigmoid: g = EnumFunction::SIGMOID; break;; 788 case EActivationFunction::kTanh: g = EnumFunction::TANH; break;; 789 c",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:27240,Testability,test,testPattern,27240,"()));; 742 trainPattern.back ().addInput (1.0); // bias node; 743 }; 744 }; 745 ; 746 for (auto &event : eventCollectionTesting) {; 747 const std::vector<Float_t>& values = event->GetValues();; 748 if (fAnalysisType == Types::kClassification) {; 749 double outputValue = event->GetClass () == 0 ? 0.9 : 0.1;; 750 testPattern.push_back(Pattern (values.begin(),; 751 values.end(),; 752 outputValue,; 753 event->GetWeight()));; 754 testPattern.back().addInput(1.0);; 755 } else if (fAnalysisType == Types::kMulticlass) {; 756 std::vector<Float_t> oneHot(DataInfo().GetNClasses(), 0.0);; 757 oneHot[event->GetClass()] = 1.0;; 758 testPattern.push_back(Pattern (values.begin(), values.end(),; 759 oneHot.cbegin(), oneHot.cend(),; 760 event->GetWeight()));; 761 testPattern.back().addInput(1.0);; 762 } else {; 763 const std::vector<Float_t>& targets = event->GetTargets ();; 764 testPattern.push_back(Pattern(values.begin(),; 765 values.end(),; 766 targets.begin(),; 767 targets.end(),; 768 event->GetWeight ()));; 769 testPattern.back ().addInput (1.0); // bias node; 770 }; 771 }; 772 ; 773 TMVA::DNN::Net net;; 774 std::vector<double> weights;; 775 ; 776 net.SetIpythonInteractive(fInteractive, &fExitFromTraining, &fIPyMaxIter, &fIPyCurrentIter);; 777 ; 778 net.setInputSize(fNet.GetInputWidth() + 1);; 779 net.setOutputSize(fNet.GetOutputWidth() + 1);; 780 ; 781 for (size_t i = 0; i < fNet.GetDepth(); i++) {; 782 EActivationFunction f = fNet.GetLayer(i).GetActivationFunction();; 783 EnumFunction g = EnumFunction::LINEAR;; 784 switch(f) {; 785 case EActivationFunction::kIdentity: g = EnumFunction::LINEAR; break;; 786 case EActivationFunction::kRelu: g = EnumFunction::RELU; break;; 787 case EActivationFunction::kSigmoid: g = EnumFunction::SIGMOID; break;; 788 case EActivationFunction::kTanh: g = EnumFunction::TANH; break;; 789 case EActivationFunction::kFastTanh: g = EnumFunction::TANH; break;; 790 case EActivationFunction::kSymmRelu: g = EnumFunction::SYMMRELU; break;; 791 case EActivatio",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30224,Testability,test,testInterval,30224,"OPY_MUTUALEXCLUSIVE);; 816 break;; 817 }; 818 ; 819 switch(fWeightInitialization) {; 820 case EInitialization::kGauss:; 821 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 822 std::back_inserter(weights));; 823 break;; 824 case EInitialization::kUniform:; 825 net.initializeWeights(WeightInitializationStrategy::XAVIERUNIFORM,; 826 std::back_inserter(weights));; 827 break;; 828 default:; 829 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 830 std::back_inserter(weights));; 831 break;; 832 }; 833 ; 834 int idxSetting = 0;; 835 for (auto s : fTrainingSettings) {; 836 ; 837 EnumRegularization r = EnumRegularization::NONE;; 838 switch(s.regularization) {; 839 case ERegularization::kNone: r = EnumRegularization::NONE; break;; 840 case ERegularization::kL1: r = EnumRegularization::L1; break;; 841 case ERegularization::kL2: r = EnumRegularization::L2; break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest mi",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:31371,Testability,test,testPattern,31371,"rSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIndex = 0;; 882 for (size_t l = 0; l < fNet.GetDepth(); l++) {; 883 auto & layerWeights = fNet.GetLayer(l).GetWeights();; 884 for (Int_t j = 0; j < layerWeights.GetNcols(); j++) {; 885 for (Int_t i = 0; i < layerWeights.GetNrows(); i++) {; 886 layerWeights(i,j) = weights[weightIndex];; 887 weightIndex++;; 888 }; 889 }; 890 auto & layerBiases = fNet.GetLayer(l).GetBiases();; 891 if (l == 0) {; 892 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 893 layerBiases(i,0) = weights[weightIndex];; 894 weightIndex++;; 895 }; 896 } else {; 897 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 898 layerBiases(i,0) = 0.0;; 899 }; 900 }; 901 }; 902 if (!fExitFromTraining) fIPyMaxIter = fIPyCurrentIter;; 903 ExitFromTraining();; 904}; 905 ; 906////////////////////////////////////////////////////////////////////////////////; 907 ; 908void TMVA::MethodDN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:33642,Testability,test,testNet,33642,"amples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 916 size_t nTestSamples = nValidationSamples;; 917 ; 918 Log() << kDEBUG << ""Using "" << nValidationSamples << "" validation samples."" << Endl;; 919 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 920 ; 921 size_t trainingPhase = 1;; 922 fNet.Initialize(fWeightInitialization);; 923 for (TTrainingSettings & settings : fTrainingSettings) {; 924 ; 925 if (fInteractive){; 926 fInteractive->ClearGraphs();; 927 }; 928 ; 929 TNet<TCuda<>> net(settings.batchSize, fNet);; 930 net.SetWeightDecay(settings.weightDecay);; 931 net.SetRegularization(settings.regularization);; 932 ; 933 // Need to convert dropoutprobabilities to conventions used; 934 // by backend implementation.; 935 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 936 for (auto & p : dropoutVector) {; 937 p = 1.0 - p;; 938 }; 939 net.SetDropoutProbabilities(dropoutVector);; 940 ; 941 net.InitializeGradients();; 942 auto testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = s",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34213,Testability,test,testInputData,34213," net.SetWeightDecay(settings.weightDecay);; 931 net.SetRegularization(settings.regularization);; 932 ; 933 // Need to convert dropoutprobabilities to conventions used; 934 // by backend implementation.; 935 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 936 for (auto & p : dropoutVector) {; 937 p = 1.0 - p;; 938 }; 939 net.SetDropoutProbabilities(dropoutVector);; 940 ; 941 net.InitializeGradients();; 942 auto testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34455,Testability,test,testInputData,34455,"ion.; 935 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 936 for (auto & p : dropoutVector) {; 937 p = 1.0 - p;; 938 }; 939 net.SetDropoutProbabilities(dropoutVector);; 940 ; 941 net.InitializeGradients();; 942 auto testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34533,Testability,test,test,34533,"o testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34690,Testability,test,testTuple,34690,"o testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34711,Testability,test,testInputData,34711,"o testNet = net.CreateClone(settings.batchSize);; 943 ; 944 Log() << kINFO << ""Training phase "" << trainingPhase << "" of ""; 945 << fTrainingSettings.size() << "":"" << Endl;; 946 trainingPhase++;; 947 ; 948 using DataLoader_t = TDataLoader<TMVAInput_t, TCuda<>>;; 949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34905,Testability,test,testData,34905,"949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34914,Testability,test,testTuple,34914,"949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:34939,Testability,test,testNet,34939,"949 ; 950 // Split training data into training and validation set; 951 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 952 const std::vector<Event *> trainingInputData =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:35142,Testability,test,testInterval,35142,"tor<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separato",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:36861,Testability,test,test,36861,", end;; 998 start = std::chrono::system_clock::now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:36888,Testability,test,testError,36888,":now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double se",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:36929,Testability,test,testData,36929,"() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlo",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37035,Testability,test,testError,37035,"2) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 10",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37048,Testability,test,testNet,37048,"2) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 10",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37103,Testability,test,testError,37103,"setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37228,Testability,test,testError,37228," 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.Get",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37249,Testability,test,testError,37249," 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.Get",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:37961,Testability,test,testInterval,37961,"35 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA fla",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:38081,Testability,test,testError,38081,"9 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;;",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:38229,Testability,test,testError,38229,"9 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;;",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:38515,Testability,test,testError,38515,"tInput();; 1049 auto outputMatrix = batch.GetOutput();; 1050 trainingError += net.Loss(inputMatrix, outputMatrix);; 1051 }; 1052 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1053 //Log the loss value; 1054 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1055 ; 1056 // Compute numerical throughput.; 1057 std::chrono::duration<double> elapsed_seconds = end - start;; 1058 double seconds = elapsed_seconds.count();; 1059 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1060 nFlops *= net.GetNFlops() * 1e-9;; 1061 ; 1062 converged = minimizer.HasConverged(testError);; 1063 start = std::chrono::system_clock::now();; 1064 ; 1065 if (fInteractive) {; 1066 fInteractive->AddPoint(stepCount, trainingError, testError);; 1067 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1068 / minimizer.GetConvergenceSteps ();; 1069 if (fExitFromTraining) break;; 1070 } else {; 1071 Log() << std::setw(10) << stepCount << "" | ""; 1072 << std::setw(12) << trainingError; 1073 << std::setw(12) << testError; 1074 << std::setw(12) << nFlops / seconds; 1075 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1076 if (converged) {; 1077 Log() << Endl;; 1078 }; 1079 }; 1080 }; 1081 }; 1082 for (size_t l = 0; l < net.GetDepth(); l++) {; 1083 fNet.GetLayer(l).GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1084 fNet.GetLayer(l).GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1085 }; 1086 }; 1087 ; 1088#else // DNNCUDA flag not set.; 1089 ; 1090 Log() << kFATAL << ""CUDA backend not enabled. Please make sure ""; 1091 ""you have CUDA installed and it was successfully ""; 1092 ""detected by CMAKE."" << Endl;; 1093#endif // DNNCUDA; 1094}; 1095 ; 1096////////////////////////////////////////////////////////////////////////////////; 1097 ; 1098void TMVA::MethodDNN::TrainCpu(); 1099{; 1100 ; 1101#ifdef DNNCPU // Included only if DNNCPU flag is set.; 1102 Log() << kINFO << ""Start of neural network train",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:40754,Testability,test,testNet,40754,"ng "" << nValidationSamples << "" validation samples."" << Endl;; 1109 Log() << kDEBUG << ""Using "" << nTestSamples << "" training samples."" << Endl;; 1110 ; 1111 fNet.Initialize(fWeightInitialization);; 1112 ; 1113 size_t trainingPhase = 1;; 1114 for (TTrainingSettings & settings : fTrainingSettings) {; 1115 ; 1116 if (fInteractive){; 1117 fInteractive->ClearGraphs();; 1118 }; 1119 ; 1120 Log() << ""Training phase "" << trainingPhase << "" of ""; 1121 << fTrainingSettings.size() << "":"" << Endl;; 1122 trainingPhase++;; 1123 ; 1124 TNet<TCpu<>> net(settings.batchSize, fNet);; 1125 net.SetWeightDecay(settings.weightDecay);; 1126 net.SetRegularization(settings.regularization);; 1127 // Need to convert dropoutprobabilities to conventions used; 1128 // by backend implementation.; 1129 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1130 for (auto & p : dropoutVector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41187,Testability,test,testInputData,41187,"of ""; 1121 << fTrainingSettings.size() << "":"" << Endl;; 1122 trainingPhase++;; 1123 ; 1124 TNet<TCpu<>> net(settings.batchSize, fNet);; 1125 net.SetWeightDecay(settings.weightDecay);; 1126 net.SetRegularization(settings.regularization);; 1127 // Need to convert dropoutprobabilities to conventions used; 1128 // by backend implementation.; 1129 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1130 for (auto & p : dropoutVector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<T",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41435,Testability,test,testInputData,41435,"rization(settings.regularization);; 1127 // Need to convert dropoutprobabilities to conventions used; 1128 // by backend implementation.; 1129 std::vector<Double_t> dropoutVector(settings.dropoutProbabilities);; 1130 for (auto & p : dropoutVector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetD",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41514,Testability,test,test,41514,"ector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBi",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41676,Testability,test,testTuple,41676,"ector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBi",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41697,Testability,test,testInputData,41697,"ector) {; 1131 p = 1.0 - p;; 1132 }; 1133 net.SetDropoutProbabilities(dropoutVector);; 1134 net.InitializeGradients();; 1135 auto testNet = net.CreateClone(settings.batchSize);; 1136 ; 1137 using DataLoader_t = TDataLoader<TMVAInput_t, TCpu<>>;; 1138 ; 1139 // Split training data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBi",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41895,Testability,test,testData,41895,"raining data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::syste",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41904,Testability,test,testTuple,41904,"raining data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::syste",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:41929,Testability,test,testNet,41929,"raining data into training and validation set; 1140 const std::vector<Event *> &allData = GetEventCollection(Types::kTraining);; 1141 const std::vector<Event *> trainingInputData =; 1142 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::syste",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:42136,Testability,test,testInterval,42136,"tData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::system_clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 119",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:43868,Testability,test,test,43868,"start, end;; 1187 start = std::chrono::system_clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddVal",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:43895,Testability,test,testError,43895,"clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:43936,Testability,test,testData,43936,"190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive-",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44088,Testability,test,testError,44088,"2) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergence",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44101,Testability,test,testNet,44101,"2) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergence",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44170,Testability,test,testError,44170,"string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44295,Testability,test,testError,44295,"trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterva",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44316,Testability,test,testError,44316,"trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterva",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:44980,Testability,test,testError,44980,"Data) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops /",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:45342,Testability,test,testInterval,45342,"lock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : trainingData) {; 1237 auto inputMatrix = batch.GetInput();; 1238 auto outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l < net.GetDepth(); l++) {; 1282 auto & layer = fNet.GetLayer(l);; 1283 layer.GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1284 layer.GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1285 }; 1286 }; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:45462,Testability,test,testError,45462,"o outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l < net.GetDepth(); l++) {; 1282 auto & layer = fNet.GetLayer(l);; 1283 layer.GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1284 layer.GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1285 }; 1286 }; 1287 ; 1288#else // DNNCPU flag not set.; 1289 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 1290 ""you have a BLAS implementation and it was successfull",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:45610,Testability,test,testError,45610,"o outputMatrix = batch.GetOutput();; 1239 auto weightMatrix = batch.GetWeights();; 1240 trainingError += net.Loss(inputMatrix, outputMatrix, weightMatrix);; 1241 }; 1242 trainingError /= (Double_t) (nTrainingSamples / settings.batchSize);; 1243 ; 1244 //Log the loss value; 1245 fTrainHistory.AddValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l < net.GetDepth(); l++) {; 1282 auto & layer = fNet.GetLayer(l);; 1283 layer.GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1284 layer.GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1285 }; 1286 }; 1287 ; 1288#else // DNNCPU flag not set.; 1289 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 1290 ""you have a BLAS implementation and it was successfull",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:45896,Testability,test,testError,45896,"dValue(""trainingError"",stepCount,trainingError);; 1246 ; 1247 if (fInteractive){; 1248 fInteractive->AddPoint(stepCount, trainingError, testError);; 1249 fIPyCurrentIter = 100*(double)minimizer.GetConvergenceCount() /(double)settings.convergenceSteps;; 1250 if (fExitFromTraining) break;; 1251 }; 1252 ; 1253 // Compute numerical throughput.; 1254 std::chrono::duration<double> elapsed_seconds = end - start;; 1255 double seconds = elapsed_seconds.count();; 1256 double nFlops = (double) (settings.testInterval * batchesInEpoch);; 1257 nFlops *= net.GetNFlops() * 1e-9;; 1258 ; 1259 converged = minimizer.HasConverged(testError);; 1260 start = std::chrono::system_clock::now();; 1261 ; 1262 if (fInteractive) {; 1263 fInteractive->AddPoint(stepCount, trainingError, testError);; 1264 fIPyCurrentIter = 100.0 * minimizer.GetConvergenceCount(); 1265 / minimizer.GetConvergenceSteps ();; 1266 if (fExitFromTraining) break;; 1267 } else {; 1268 Log() << std::setw(10) << stepCount << "" | ""; 1269 << std::setw(12) << trainingError; 1270 << std::setw(12) << testError; 1271 << std::setw(12) << nFlops / seconds; 1272 << std::setw(12) << minimizer.GetConvergenceCount() << Endl;; 1273 if (converged) {; 1274 Log() << Endl;; 1275 }; 1276 }; 1277 }; 1278 }; 1279 ; 1280 ; 1281 for (size_t l = 0; l < net.GetDepth(); l++) {; 1282 auto & layer = fNet.GetLayer(l);; 1283 layer.GetWeights() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetWeights();; 1284 layer.GetBiases() = (TMatrixT<Scalar_t>) net.GetLayer(l).GetBiases();; 1285 }; 1286 }; 1287 ; 1288#else // DNNCPU flag not set.; 1289 Log() << kFATAL << ""Multi-core CPU backend not enabled. Please make sure ""; 1290 ""you have a BLAS implementation and it was successfully ""; 1291 ""detected by CMake as well that the imt CMake flag is set."" << Endl;; 1292#endif // DNNCPU; 1293}; 1294 ; 1295////////////////////////////////////////////////////////////////////////////////; 1296 ; 1297Double_t TMVA::MethodDNN::GetMvaValue( Double_t* /*errLower*/, Double_t* /*errUppe",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59038,Testability,test,test,59038,"DropFraction=0.0,DropRepetitions=5\"" \n \; 1559 - explanation: two stacked training settings separated by \""|\"" \n \; 1560 . first training setting: \""LearningRate=1e-1,Momentum=0.3,ConvergenceSteps=50,BatchSize=30,TestRepetitions=7,WeightDecay=0.0,Renormalize=L2,DropConfig=0.0,DropRepetitions=5\"" \n \; 1561 . second training setting : \""LearningRate=1e-4,Momentum=0.3,ConvergenceSteps=50,BatchSize=20,TestRepetitions=7,WeightDecay=0.001,Renormalize=L2,DropFractions=0.0,DropRepetitions=5\"" \n \; 1562 . LearningRate : \n \; 1563 - recommended for classification: 0.1 initially, 1e-4 later \n \; 1564 - recommended for regression: 1e-4 and less \n \; 1565 . Momentum : \n \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59132,Testability,test,test,59132,"te=1e-1,Momentum=0.3,ConvergenceSteps=50,BatchSize=30,TestRepetitions=7,WeightDecay=0.0,Renormalize=L2,DropConfig=0.0,DropRepetitions=5\"" \n \; 1561 . second training setting : \""LearningRate=1e-4,Momentum=0.3,ConvergenceSteps=50,BatchSize=20,TestRepetitions=7,WeightDecay=0.001,Renormalize=L2,DropFractions=0.0,DropRepetitions=5\"" \n \; 1562 . LearningRate : \n \; 1563 - recommended for classification: 0.1 initially, 1e-4 later \n \; 1564 - recommended for regression: 1e-4 and less \n \; 1565 . Momentum : \n \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 15",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59304,Testability,test,testing,59304,"1e-4,Momentum=0.3,ConvergenceSteps=50,BatchSize=20,TestRepetitions=7,WeightDecay=0.001,Renormalize=L2,DropFractions=0.0,DropRepetitions=5\"" \n \; 1562 . LearningRate : \n \; 1563 - recommended for classification: 0.1 initially, 1e-4 later \n \; 1564 - recommended for regression: 1e-4 and less \n \; 1565 . Momentum : \n \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:59334,Testability,test,test,59334,"1e-4,Momentum=0.3,ConvergenceSteps=50,BatchSize=20,TestRepetitions=7,WeightDecay=0.001,Renormalize=L2,DropFractions=0.0,DropRepetitions=5\"" \n \; 1562 . LearningRate : \n \; 1563 - recommended for classification: 0.1 initially, 1e-4 later \n \; 1564 - recommended for regression: 1e-4 and less \n \; 1565 . Momentum : \n \; 1566 preserve a fraction of the momentum for the next training batch [fraction = 0.0 - 1.0] \n \; 1567 . Repetitions : \n \; 1568 train \""Repetitions\"" repetitions with the same minibatch before switching to the next one \n \; 1569 . ConvergenceSteps : \n \; 1570 Assume that convergence is reached after \""ConvergenceSteps\"" cycles where no improvement \n \; 1571 of the error on the test samples has been found. (Mind that only at each \""TestRepetitions\"" \n \; 1572 cycle the test samples are evaluated and thus the convergence is checked) \n \; 1573 . BatchSize \n \; 1574 Size of the mini-batches. \n \; 1575 . TestRepetitions \n \; 1576 Perform testing the neural net on the test samples each \""TestRepetitions\"" cycle \n \; 1577 . WeightDecay \n \; 1578 If \""Renormalize\"" is set to L1 or L2, \""WeightDecay\"" provides the renormalization factor \n \; 1579 . Renormalize \n \; 1580 NONE, L1 (|w|) or L2 (w^2) \n \; 1581 . DropConfig \n \; 1582 Drop a fraction of arbitrary nodes of each of the layers according to the values given \n \; 1583 in the DropConfig. \n \; 1584 [example: DropConfig=0.0+0.5+0.3 \n \; 1585 meaning: drop no nodes in layer 0 (input layer), half of the nodes in layer 1 and 30% of the nodes \n \; 1586 in layer 2 \n \; 1587 recommended: leave all the nodes turned on for the input layer (layer 0) \n \; 1588 turn off half of the nodes in later layers for the initial training; leave all nodes \n \; 1589 turned on (0.0) in later training stages] \n \; 1590 . DropRepetitions \n \; 1591 Each \""DropRepetitions\"" cycle the configuration of which nodes are dropped is changed \n \; 1592 [recommended : 1] \n \; 1593 . Multithreading \n \; 1594 turn ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:63660,Testability,test,test,63660,"om_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TObjString.h; TString.h; Timer.h; Tools.h; PatternDefinition Pattern.h:8; TFormulaThe Formula class.Definition TFormula.h:89; TIterDefinition TCollection.h:235; TMVA::Config::WriteOptionsReferenceBool_t WriteOptionsReference() constDefinition Config.h:65; TMVA::DNN::LayerLayer defines the layout of a layer.Definition NeuralNet.h:673; TMVA::DNN::Netneural netDefinition NeuralNet.h:1062; TMVA::DNN::SettingsSettings for the training of the neural net.Definition NeuralNet.h:730; TMVA::DNN::SteepestSteepest Gradient Descent algorithm (SGD)Definition NeuralNet.h:334; TMVA::DNN::TCpu::Copystatic void Copy(Matrix_t &B, const Matrix_t &A)Definition Arithmetic.hxx:269; TMVA::DNN::TCuda::Copystatic void Copy(Matrix_t &B, const Matrix_t &A); TMVA::DNN::TDataLoaderTDataLoader.Definition DataLoader.h:129; TMVA::DNN::TGradientDescentDefinition Minimizers.h:56; TMVA::DNN::TGradientDescent::HasConvergedbool HasConverged()Increases the minimization step counter by the test error evaluation period and uses the current inte...Definition Minimizers.h:667; TMVA::DNN::TGradientDescent::Stepvoid Step(Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights)Perform a single optimization step on a given batch.Definition Minimizers.h:331; TMVA::DNN::TGradientDescent::GetTestIntervalsize_t GetTestInterval() constDefinition Minimizers.h:163; TMVA::DNN::TGradientDescent::StepMomentumvoid StepMomentum(Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum)Same as the Step(...) method for multiple batches but uses momentum.Definition Minimizers.h:438; TMVA::DNN::TGradientDescent::GetConvergenceCountsize_t GetConvergenceCount() constDefinition Minimizers.h:159; TMVA::DNN::TGradientDescent::GetConvergenceStepssize_t GetConvergenceSteps() constDefinition Minimizers.h:160; TMVA::DNN::TNetGeneric neural network class.Definition Net.h:49; TMVA::EventDefinition Event",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:72652,Testability,log,logarithm,72652,"kRelu@ kRelu; TMVA::DNN::EActivationFunction::kGauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid; TMVA::DNN::EActivationFunction::kIdentity@ kIdentity; TMVA::DNN::EActivationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::ModeOutputValuesModeOutputValuesDefinition NeuralNet.h:179; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTString fetchValue(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDNN.cxx:320; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::MethodDNN::TTrainingSettingsDefinition MethodDNN.h:90; TMVA::MethodDNN::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDNN.h:94; TMVA::MethodDNN::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDNN.h:93; TMVA::MethodDNN::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDNN.h:95; TMVA::MethodDNN::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDNN.h:92; TMVA::MethodDNN::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDNN.h:91; TMVA::MethodDNN::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDNN.h:98; TMVA::MethodDNN::TTrainingSettings::multithreadingbool multithreadingDefinition MethodDNN.h:99; TMVA::MethodDNN::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDNN.h:97; TMVA::MethodDNN::TTrainingSettings::momentumDouble_t momentumDefinit",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:73117,Testability,test,testIntervalDefinition,73117,"ationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::ModeOutputValuesModeOutputValuesDefinition NeuralNet.h:179; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTString fetchValue(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDNN.cxx:320; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::MethodDNN::TTrainingSettingsDefinition MethodDNN.h:90; TMVA::MethodDNN::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDNN.h:94; TMVA::MethodDNN::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDNN.h:93; TMVA::MethodDNN::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDNN.h:95; TMVA::MethodDNN::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDNN.h:92; TMVA::MethodDNN::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDNN.h:91; TMVA::MethodDNN::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDNN.h:98; TMVA::MethodDNN::TTrainingSettings::multithreadingbool multithreadingDefinition MethodDNN.h:99; TMVA::MethodDNN::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDNN.h:97; TMVA::MethodDNN::TTrainingSettings::momentumDouble_t momentumDefinition MethodDNN.h:96; lTLine lDefinition textangle.C:4; Config.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDNN.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:00 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:4718,Usability,learn,learning,4718,"nction = DNN::EOutputFunction::kSigmoid;; 108}; 109 ; 110////////////////////////////////////////////////////////////////////////////////; 111/// MLP can handle classification with 2 classes and regression with; 112/// one regression-target; 113 ; 114Bool_t TMVA::MethodDNN::HasAnalysisType(Types::EAnalysisType type,; 115 UInt_t numberClasses,; 116 UInt_t /*numberTargets*/ ); 117{; 118 if (type == Types::kClassification && numberClasses == 2 ) return kTRUE;; 119 if (type == Types::kMulticlass ) return kTRUE;; 120 if (type == Types::kRegression ) return kTRUE;; 121 ; 122 return kFALSE;; 123}; 124 ; 125////////////////////////////////////////////////////////////////////////////////; 126/// default initializations; 127 ; 128void TMVA::MethodDNN::Init() {; 129 Log() << kWARNING; 130 << ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Def",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:4786,Usability,learn,learning,4786,"cation with 2 classes and regression with; 112/// one regression-target; 113 ; 114Bool_t TMVA::MethodDNN::HasAnalysisType(Types::EAnalysisType type,; 115 UInt_t numberClasses,; 116 UInt_t /*numberTargets*/ ); 117{; 118 if (type == Types::kClassification && numberClasses == 2 ) return kTRUE;; 119 if (type == Types::kMulticlass ) return kTRUE;; 120 if (type == Types::kRegression ) return kTRUE;; 121 ; 122 return kFALSE;; 123}; 124 ; 125////////////////////////////////////////////////////////////////////////////////; 126/// default initializations; 127 ; 128void TMVA::MethodDNN::Init() {; 129 Log() << kWARNING; 130 << ""MethodDNN is deprecated and it will be removed in future ROOT version. ""; 131 ""Please use MethodDL ( TMVA::kDL)""; 132 << Endl;; 133 ; 134}; 135 ; 136////////////////////////////////////////////////////////////////////////////////; 137/// Options to be set in the option string:; 138///; 139/// - LearningRate <float> DNN learning rate parameter.; 140/// - DecayRate <float> Decay rate for learning parameter.; 141/// - TestRate <int> Period of validation set error computation.; 142/// - BatchSize <int> Number of event per batch.; 143///; 144/// - ValidationSize <string> How many events to use for validation. ""0.2""; 145/// or ""20%"" indicates that a fifth of the; 146/// training data should be used. ""100""; 147/// indicates that 100 events should be used.; 148 ; 149void TMVA::MethodDNN::DeclareOptions(); 150{; 151 ; 152 DeclareOptionRef(fLayoutString=""SOFTSIGN|(N+100)*2,LINEAR"",; 153 ""Layout"",; 154 ""Layout of the network."");; 155 ; 156 DeclareOptionRef(fValidationSize = ""20%"", ""ValidationSize"",; 157 ""Part of the training data to use for ""; 158 ""validation. Specify as 0.2 or 20% to use a ""; 159 ""fifth of the data set as validation set. ""; 160 ""Specify as 100 to use exactly 100 events. ""; 161 ""(Default: 20%)"");; 162 ; 163 DeclareOptionRef(fErrorStrategy=""CROSSENTROPY"",; 164 ""ErrorStrategy"",; 165 ""Loss function: Mean squared error (regression)""; 166 "" or cross entr",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:19822,Usability,learn,learningRate,19822," fWeightInitialization = DNN::EInitialization::kUniform;; 551 }; 552 else {; 553 fWeightInitialization = DNN::EInitialization::kGauss;; 554 }; 555 ; 556 //; 557 // Training settings.; 558 //; 559 ; 560 // Force validation of the ValidationSize option; 561 GetNumValidationSamples();; 562 ; 563 KeyValueVector_t strategyKeyValues = ParseKeyValueString(fTrainingStrategyString,; 564 TString (""|""),; 565 TString ("",""));; 566 ; 567 std::cout << ""Parsed Training DNN string "" << fTrainingStrategyString << std::endl;; 568 std::cout << ""STring has size "" << strategyKeyValues.size() << std::endl;; 569 for (auto& block : strategyKeyValues) {; 570 TTrainingSettings settings;; 571 ; 572 settings.convergenceSteps = fetchValue(block, ""ConvergenceSteps"", 100);; 573 settings.batchSize = fetchValue(block, ""BatchSize"", 30);; 574 settings.testInterval = fetchValue(block, ""TestRepetitions"", 7);; 575 settings.weightDecay = fetchValue(block, ""WeightDecay"", 0.0);; 576 settings.learningRate = fetchValue(block, ""LearningRate"", 1e-5);; 577 settings.momentum = fetchValue(block, ""Momentum"", 0.3);; 578 settings.dropoutProbabilities = fetchValue(block, ""DropConfig"",; 579 std::vector<Double_t>());; 580 ; 581 TString regularization = fetchValue(block, ""Regularization"",; 582 TString (""NONE""));; 583 if (regularization == ""L1"") {; 584 settings.regularization = DNN::ERegularization::kL1;; 585 } else if (regularization == ""L2"") {; 586 settings.regularization = DNN::ERegularization::kL2;; 587 } else {; 588 settings.regularization = DNN::ERegularization::kNone;; 589 }; 590 ; 591 TString strMultithreading = fetchValue(block, ""Multithreading"",; 592 TString (""True""));; 593 if (strMultithreading.BeginsWith (""T"")) {; 594 settings.multithreading = true;; 595 } else {; 596 settings.multithreading = false;; 597 }; 598 ; 599 fTrainingSettings.push_back(settings);; 600 }; 601}; 602 ; 603////////////////////////////////////////////////////////////////////////////////; 604/// Validation of the ValidationSize option. All",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:23312,Usability,progress bar,progress bar,23312,"FATAL << ""Cannot parse number \"""" << fValidationSize << ""\"". Expected string like \""0.2\"" or \""100\"".""; 642 << Endl;; 643 }; 644 ; 645 // Value validation; 646 // ----------------; 647 if (nValidationSamples < 0) {; 648 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is negative."" << Endl;; 649 }; 650 ; 651 if (nValidationSamples == 0) {; 652 Log() << kFATAL << ""Validation size \"""" << fValidationSize << ""\"" is zero."" << Endl;; 653 }; 654 ; 655 if (nValidationSamples >= (Int_t)trainingSetSize) {; 656 Log() << kFATAL << ""Validation size \"""" << fValidationSize; 657 << ""\"" is larger than or equal in size to training set (size=\"""" << trainingSetSize << ""\"")."" << Endl;; 658 }; 659 ; 660 return nValidationSamples;; 661}; 662 ; 663////////////////////////////////////////////////////////////////////////////////; 664 ; 665void TMVA::MethodDNN::Train(); 666{; 667 if (fInteractive && fInteractive->NotInitialized()){; 668 std::vector<TString> titles = {""Error on training set"", ""Error on test set""};; 669 fInteractive->Init(titles);; 670 // JsMVA progress bar maximum (100%); 671 fIPyMaxIter = 100;; 672 }; 673 ; 674 for (TTrainingSettings & settings : fTrainingSettings) {; 675 size_t nValidationSamples = GetNumValidationSamples();; 676 size_t nTrainingSamples = GetEventCollection(Types::kTraining).size() - nValidationSamples;; 677 size_t nTestSamples = nValidationSamples;; 678 ; 679 if (nTrainingSamples < settings.batchSize ||; 680 nValidationSamples < settings.batchSize ||; 681 nTestSamples < settings.batchSize) {; 682 Log() << kFATAL << ""Number of samples in the datasets are train: ""; 683 << nTrainingSamples << "" valid: "" << nValidationSamples; 684 << "" test: "" << nTestSamples << "". ""; 685 << ""One of these is smaller than the batch size of ""; 686 << settings.batchSize << "". Please increase the batch""; 687 << "" size to be at least the same size as the smallest""; 688 << "" of these values."" << Endl;; 689 }; 690 }; 691 ; 692 if (fArchitectureString == ""GPU"") {; 693 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30289,Usability,learn,learningRate,30289,"nitialization) {; 820 case EInitialization::kGauss:; 821 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 822 std::back_inserter(weights));; 823 break;; 824 case EInitialization::kUniform:; 825 net.initializeWeights(WeightInitializationStrategy::XAVIERUNIFORM,; 826 std::back_inserter(weights));; 827 break;; 828 default:; 829 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 830 std::back_inserter(weights));; 831 break;; 832 }; 833 ; 834 int idxSetting = 0;; 835 for (auto s : fTrainingSettings) {; 836 ; 837 EnumRegularization r = EnumRegularization::NONE;; 838 switch(s.regularization) {; 839 case ERegularization::kNone: r = EnumRegularization::NONE; break;; 840 case ERegularization::kL1: r = EnumRegularization::L1; break;; 841 case ERegularization::kL2: r = EnumRegularization::L2; break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30476,Usability,learn,learning,30476,"zeWeights(WeightInitializationStrategy::XAVIERUNIFORM,; 826 std::back_inserter(weights));; 827 break;; 828 default:; 829 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 830 std::back_inserter(weights));; 831 break;; 832 }; 833 ; 834 int idxSetting = 0;; 835 for (auto s : fTrainingSettings) {; 836 ; 837 EnumRegularization r = EnumRegularization::NONE;; 838 switch(s.regularization) {; 839 case ERegularization::kNone: r = EnumRegularization::NONE; break;; 840 case ERegularization::kL1: r = EnumRegularization::L1; break;; 841 case ERegularization::kL2: r = EnumRegularization::L2; break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIn",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:30510,Usability,learn,learningRate,30510,"zeWeights(WeightInitializationStrategy::XAVIERUNIFORM,; 826 std::back_inserter(weights));; 827 break;; 828 default:; 829 net.initializeWeights(WeightInitializationStrategy::XAVIER,; 830 std::back_inserter(weights));; 831 break;; 832 }; 833 ; 834 int idxSetting = 0;; 835 for (auto s : fTrainingSettings) {; 836 ; 837 EnumRegularization r = EnumRegularization::NONE;; 838 switch(s.regularization) {; 839 case ERegularization::kNone: r = EnumRegularization::NONE; break;; 840 case ERegularization::kL1: r = EnumRegularization::L1; break;; 841 case ERegularization::kL2: r = EnumRegularization::L2; break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIn",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:31253,Usability,learn,learningRate,31253,"break;; 842 }; 843 ; 844 Settings * settings = new Settings(TString(), s.convergenceSteps, s.batchSize,; 845 s.testInterval, s.weightDecay, r,; 846 MinimizerType::fSteepest, s.learningRate,; 847 s.momentum, 1, s.multithreading);; 848 std::shared_ptr<Settings> ptrSettings(settings);; 849 ptrSettings->setMonitoring (0);; 850 Log() << kINFO; 851 << ""Training with learning rate = "" << ptrSettings->learningRate (); 852 << "", momentum = "" << ptrSettings->momentum (); 853 << "", repetitions = "" << ptrSettings->repetitions (); 854 << Endl;; 855 ; 856 ptrSettings->setProgressLimits ((idxSetting)*100.0/(fSettings.size ()),; 857 (idxSetting+1)*100.0/(fSettings.size ()));; 858 ; 859 const std::vector<double>& dropConfig = ptrSettings->dropFractions ();; 860 if (!dropConfig.empty ()) {; 861 Log () << kINFO << ""Drop configuration"" << Endl; 862 << "" drop repetitions = "" << ptrSettings->dropRepetitions(); 863 << Endl;; 864 }; 865 ; 866 int idx = 0;; 867 for (auto f : dropConfig) {; 868 Log () << kINFO << "" Layer "" << idx << "" = "" << f << Endl;; 869 ++idx;; 870 }; 871 Log () << kINFO << Endl;; 872 ; 873 DNN::Steepest minimizer(ptrSettings->learningRate(),; 874 ptrSettings->momentum(),; 875 ptrSettings->repetitions());; 876 net.train(weights, trainPattern, testPattern, minimizer, *ptrSettings.get());; 877 ptrSettings.reset();; 878 Log () << kINFO << Endl;; 879 idxSetting++;; 880 }; 881 size_t weightIndex = 0;; 882 for (size_t l = 0; l < fNet.GetDepth(); l++) {; 883 auto & layerWeights = fNet.GetLayer(l).GetWeights();; 884 for (Int_t j = 0; j < layerWeights.GetNcols(); j++) {; 885 for (Int_t i = 0; i < layerWeights.GetNrows(); i++) {; 886 layerWeights(i,j) = weights[weightIndex];; 887 weightIndex++;; 888 }; 889 }; 890 auto & layerBiases = fNet.GetLayer(l).GetBiases();; 891 if (l == 0) {; 892 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {; 893 layerBiases(i,0) = weights[weightIndex];; 894 weightIndex++;; 895 }; 896 } else {; 897 for (Int_t i = 0; i < layerBiases.GetNrows(); i++) {",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:35082,Usability,learn,learningRate,35082,"ta =; 953 std::vector<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 954 const std::vector<Event *> testInputData =; 955 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 956 ; 957 if (trainingInputData.size() != nTrainingSamples) {; 958 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 959 }; 960 if (testInputData.size() != nTestSamples) {; 961 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 962 }; 963 ; 964 size_t nThreads = 1;; 965 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 966 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 967 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 968 net.GetBatchSize(), net.GetInputWidth(),; 969 net.GetOutputWidth(), nThreads);; 970 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 971 net.GetInputWidth(), net.GetOutputWidth(),; 972 nThreads);; 973 DNN::TGradientDescent<TCuda<>> minimizer(settings.learningRate,; 974 settings.convergenceSteps,; 975 settings.testInterval);; 976 ; 977 std::vector<TNet<TCuda<>>> nets{};; 978 std::vector<TBatch<TCuda<>>> batches{};; 979 nets.reserve(nThreads);; 980 for (size_t i = 0; i < nThreads; i++) {; 981 nets.push_back(net);; 982 for (size_t j = 0; j < net.GetDepth(); j++); 983 {; 984 auto &masterLayer = net.GetLayer(j);; 985 auto &layer = nets.back().GetLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:36445,Usability,clear,clear,36445,"tLayer(j);; 986 TCuda<>::Copy(layer.GetWeights(),; 987 masterLayer.GetWeights());; 988 TCuda<>::Copy(layer.GetBiases(),; 989 masterLayer.GetBiases());; 990 }; 991 }; 992 ; 993 bool converged = false;; 994 size_t stepCount = 0;; 995 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 996 ; 997 std::chrono::time_point<std::chrono::system_clock> start, end;; 998 start = std::chrono::system_clock::now();; 999 ; 1000 if (!fInteractive) {; 1001 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1002 << std::setw(12) << ""Train Err.""; 1003 << std::setw(12) << ""Test Err.""; 1004 << std::setw(12) << ""GFLOP/s""; 1005 << std::setw(12) << ""Conv. Steps"" << Endl;; 1006 std::string separator(62, '-');; 1007 Log() << separator << Endl;; 1008 }; 1009 ; 1010 while (!converged); 1011 {; 1012 stepCount++;; 1013 ; 1014 // Perform minimization steps for a full epoch.; 1015 trainingData.Shuffle();; 1016 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1017 batches.clear();; 1018 for (size_t j = 0; j < nThreads; j++) {; 1019 batches.reserve(nThreads);; 1020 batches.push_back(trainingData.GetBatch());; 1021 }; 1022 if (settings.momentum > 0.0) {; 1023 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1024 } else {; 1025 minimizer.Step(net, nets, batches);; 1026 }; 1027 }; 1028 ; 1029 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1030 ; 1031 // Compute test error.; 1032 Double_t testError = 0.0;; 1033 for (auto batch : testData) {; 1034 auto inputMatrix = batch.GetInput();; 1035 auto outputMatrix = batch.GetOutput();; 1036 testError += testNet.Loss(inputMatrix, outputMatrix);; 1037 }; 1038 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1039 ; 1040 //Log the loss value; 1041 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1042 ; 1043 end = std::chrono::system_clock::now();; 1044 ; 1045 // Compute training error.; 1046 Double_t trainingError = 0.0;; 1047 for (auto batch : trainingData) {; 1048 auto inputMatrix = batch.GetInput();; 1049 a",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:42074,Usability,learn,learningRate,42074,"or<Event *>(allData.begin(), allData.begin() + nTrainingSamples);; 1143 const std::vector<Event *> testInputData =; 1144 std::vector<Event *>(allData.begin() + nTrainingSamples, allData.end());; 1145 ; 1146 if (trainingInputData.size() != nTrainingSamples) {; 1147 Log() << kFATAL << ""Inconsistent training sample size"" << Endl;; 1148 }; 1149 if (testInputData.size() != nTestSamples) {; 1150 Log() << kFATAL << ""Inconsistent test sample size"" << Endl;; 1151 }; 1152 ; 1153 size_t nThreads = 1;; 1154 TMVAInput_t trainingTuple = std::tie(trainingInputData, DataInfo());; 1155 TMVAInput_t testTuple = std::tie(testInputData, DataInfo());; 1156 DataLoader_t trainingData(trainingTuple, nTrainingSamples,; 1157 net.GetBatchSize(), net.GetInputWidth(),; 1158 net.GetOutputWidth(), nThreads);; 1159 DataLoader_t testData(testTuple, nTestSamples, testNet.GetBatchSize(),; 1160 net.GetInputWidth(), net.GetOutputWidth(),; 1161 nThreads);; 1162 DNN::TGradientDescent<TCpu<>> minimizer(settings.learningRate,; 1163 settings.convergenceSteps,; 1164 settings.testInterval);; 1165 ; 1166 std::vector<TNet<TCpu<>>> nets{};; 1167 std::vector<TBatch<TCpu<>>> batches{};; 1168 nets.reserve(nThreads);; 1169 for (size_t i = 0; i < nThreads; i++) {; 1170 nets.push_back(net);; 1171 for (size_t j = 0; j < net.GetDepth(); j++); 1172 {; 1173 auto &masterLayer = net.GetLayer(j);; 1174 auto &layer = nets.back().GetLayer(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::system_clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:43452,Usability,clear,clear,43452,"r(j);; 1175 TCpu<>::Copy(layer.GetWeights(),; 1176 masterLayer.GetWeights());; 1177 TCpu<>::Copy(layer.GetBiases(),; 1178 masterLayer.GetBiases());; 1179 }; 1180 }; 1181 ; 1182 bool converged = false;; 1183 size_t stepCount = 0;; 1184 size_t batchesInEpoch = nTrainingSamples / net.GetBatchSize();; 1185 ; 1186 std::chrono::time_point<std::chrono::system_clock> start, end;; 1187 start = std::chrono::system_clock::now();; 1188 ; 1189 if (!fInteractive) {; 1190 Log() << std::setw(10) << ""Epoch"" << "" | ""; 1191 << std::setw(12) << ""Train Err.""; 1192 << std::setw(12) << ""Test Err.""; 1193 << std::setw(12) << ""GFLOP/s""; 1194 << std::setw(12) << ""Conv. Steps"" << Endl;; 1195 std::string separator(62, '-');; 1196 Log() << separator << Endl;; 1197 }; 1198 ; 1199 while (!converged); 1200 {; 1201 stepCount++;; 1202 // Perform minimization steps for a full epoch.; 1203 trainingData.Shuffle();; 1204 for (size_t i = 0; i < batchesInEpoch; i += nThreads) {; 1205 batches.clear();; 1206 for (size_t j = 0; j < nThreads; j++) {; 1207 batches.reserve(nThreads);; 1208 batches.push_back(trainingData.GetBatch());; 1209 }; 1210 if (settings.momentum > 0.0) {; 1211 minimizer.StepMomentum(net, nets, batches, settings.momentum);; 1212 } else {; 1213 minimizer.Step(net, nets, batches);; 1214 }; 1215 }; 1216 ; 1217 if ((stepCount % minimizer.GetTestInterval()) == 0) {; 1218 ; 1219 // Compute test error.; 1220 Double_t testError = 0.0;; 1221 for (auto batch : testData) {; 1222 auto inputMatrix = batch.GetInput();; 1223 auto outputMatrix = batch.GetOutput();; 1224 auto weightMatrix = batch.GetWeights();; 1225 testError += testNet.Loss(inputMatrix, outputMatrix, weightMatrix);; 1226 }; 1227 testError /= (Double_t) (nTestSamples / settings.batchSize);; 1228 ; 1229 //Log the loss value; 1230 fTrainHistory.AddValue(""testError"",stepCount,testError);; 1231 ; 1232 end = std::chrono::system_clock::now();; 1233 ; 1234 // Compute training error.; 1235 Double_t trainingError = 0.0;; 1236 for (auto batch : traini",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:48142,Usability,clear,clear,48142,") = inputValues[i];; 1306 }; 1307 ; 1308 fNet.Prediction(YHat, X, fOutputFunction);; 1309 return YHat(0,0);; 1310}; 1311 ; 1312////////////////////////////////////////////////////////////////////////////////; 1313 ; 1314const std::vector<Float_t> & TMVA::MethodDNN::GetRegressionValues(); 1315{; 1316 size_t nVariables = GetEvent()->GetNVariables();; 1317 Matrix_t X(1, nVariables);; 1318 ; 1319 const Event *ev = GetEvent();; 1320 const std::vector<Float_t>& inputValues = ev->GetValues();; 1321 for (size_t i = 0; i < nVariables; i++) {; 1322 X(0,i) = inputValues[i];; 1323 }; 1324 ; 1325 size_t nTargets = std::max(1u, ev->GetNTargets());; 1326 Matrix_t YHat(1, nTargets);; 1327 std::vector<Float_t> output(nTargets);; 1328 auto net = fNet.CreateClone(1);; 1329 net.Prediction(YHat, X, fOutputFunction);; 1330 ; 1331 for (size_t i = 0; i < nTargets; i++); 1332 output[i] = YHat(0, i);; 1333 ; 1334 if (fRegressionReturnVal == NULL) {; 1335 fRegressionReturnVal = new std::vector<Float_t>();; 1336 }; 1337 fRegressionReturnVal->clear();; 1338 ; 1339 Event * evT = new Event(*ev);; 1340 for (size_t i = 0; i < nTargets; ++i) {; 1341 evT->SetTarget(i, output[i]);; 1342 }; 1343 ; 1344 const Event* evT2 = GetTransformationHandler().InverseTransform(evT);; 1345 for (size_t i = 0; i < nTargets; ++i) {; 1346 fRegressionReturnVal->push_back(evT2->GetTarget(i));; 1347 }; 1348 delete evT;; 1349 return *fRegressionReturnVal;; 1350}; 1351 ; 1352const std::vector<Float_t> & TMVA::MethodDNN::GetMulticlassValues(); 1353{; 1354 size_t nVariables = GetEvent()->GetNVariables();; 1355 Matrix_t X(1, nVariables);; 1356 Matrix_t YHat(1, DataInfo().GetNClasses());; 1357 if (fMulticlassReturnVal == NULL) {; 1358 fMulticlassReturnVal = new std::vector<Float_t>(DataInfo().GetNClasses());; 1359 }; 1360 ; 1361 const std::vector<Float_t>& inputValues = GetEvent()->GetValues();; 1362 for (size_t i = 0; i < nVariables; i++) {; 1363 X(0,i) = inputValues[i];; 1364 }; 1365 ; 1366 fNet.Prediction(YHat, X, fOutputFunc",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54668,Usability,learn,learning,54668,"TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TAN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:54747,Usability,learn,learning,54747,"TString() : gTools().Color(""reset"");; 1485 ; 1486 Log() << Endl;; 1487 Log() << col << ""--- Short description:"" << colres << Endl;; 1488 Log() << Endl;; 1489 Log() << ""The DNN neural network is a feedforward"" << Endl;; 1490 Log() << ""multilayer perceptron implementation. The DNN has a user-"" << Endl;; 1491 Log() << ""defined hidden layer architecture, where the number of input (output)"" << Endl;; 1492 Log() << ""nodes is determined by the input variables (output classes, i.e., "" << Endl;; 1493 Log() << ""signal and one background, regression or multiclass). "" << Endl;; 1494 Log() << Endl;; 1495 Log() << col << ""--- Performance optimisation:"" << colres << Endl;; 1496 Log() << Endl;; 1497 ; 1498 const char* txt = ""The DNN supports various options to improve performance in terms of training speed and \n \; 1499reduction of overfitting: \n \; 1500\n \; 1501 - different training settings can be stacked. Such that the initial training \n\; 1502 is done with a large learning rate and a large drop out fraction whilst \n \; 1503 in a later stage learning rate and drop out can be reduced. \n \; 1504 - drop out \n \; 1505 [recommended: \n \; 1506 initial training stage: 0.0 for the first layer, 0.5 for later layers. \n \; 1507 later training stage: 0.1 or 0.0 for all layers \n \; 1508 final training stage: 0.0] \n \; 1509 Drop out is a technique where a at each training cycle a fraction of arbitrary \n \; 1510 nodes is disabled. This reduces co-adaptation of weights and thus reduces overfitting. \n \; 1511 - L1 and L2 regularization are available \n \; 1512 - Minibatches \n \; 1513 [recommended 10 - 150] \n \; 1514 Arbitrary mini-batch sizes can be chosen. \n \; 1515 - Multithreading \n \; 1516 [recommended: True] \n \; 1517 Multithreading can be turned on. The minibatches are distributed to the available \n \; 1518 cores. The algorithm is lock-free (\""Hogwild!\""-style) for each cycle. \n \; 1519 \n \; 1520 Options: \n \; 1521 \""Layout\"": \n \; 1522 - example: \""TANH|(N+30)*2,TAN",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MethodDNN_8cxx_source.html:73023,Usability,learn,learningRateDefinition,73023,"ationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::ModeOutputValuesModeOutputValuesDefinition NeuralNet.h:179; TMVA::DNN::TMVAInput_tstd::tuple< const std::vector< Event * > &, const DataSetInfo & > TMVAInput_tDefinition DataLoader.h:40; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::fetchValueTString fetchValue(const std::map< TString, TString > &keyValueMap, TString key)Definition MethodDNN.cxx:320; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::LogDouble_t Log(Double_t x)Returns the natural logarithm of x.Definition TMath.h:756; TMVA::MethodDNN::TTrainingSettingsDefinition MethodDNN.h:90; TMVA::MethodDNN::TTrainingSettings::regularizationDNN::ERegularization regularizationDefinition MethodDNN.h:94; TMVA::MethodDNN::TTrainingSettings::convergenceStepssize_t convergenceStepsDefinition MethodDNN.h:93; TMVA::MethodDNN::TTrainingSettings::learningRateDouble_t learningRateDefinition MethodDNN.h:95; TMVA::MethodDNN::TTrainingSettings::testIntervalsize_t testIntervalDefinition MethodDNN.h:92; TMVA::MethodDNN::TTrainingSettings::batchSizesize_t batchSizeDefinition MethodDNN.h:91; TMVA::MethodDNN::TTrainingSettings::dropoutProbabilitiesstd::vector< Double_t > dropoutProbabilitiesDefinition MethodDNN.h:98; TMVA::MethodDNN::TTrainingSettings::multithreadingbool multithreadingDefinition MethodDNN.h:99; TMVA::MethodDNN::TTrainingSettings::weightDecayDouble_t weightDecayDefinition MethodDNN.h:97; TMVA::MethodDNN::TTrainingSettings::momentumDouble_t momentumDefinition MethodDNN.h:96; lTLine lDefinition textangle.C:4; Config.h; Types.h; outputstatic void output(). tmvatmvasrcMethodDNN.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:00 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MethodDNN_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MethodDNN_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:1956,Availability,error,error,1956,"empty() && ( Minim::gDefaultMinimizer == ""Minuit"" ||; 48 Minim::gDefaultMinimizer == ""Minuit2"") ); 49 Minim::gDefaultMinimAlgo = ""Migrad"";; 50}; 51void MinimizerOptions::SetDefaultErrorDef(double up) {; 52 // set the default error definition; 53 Minim::gDefaultErrorDef ",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:2095,Availability,toler,tolerance,2095,"empty() && ( Minim::gDefaultMinimizer == ""Minuit"" ||; 48 Minim::gDefaultMinimizer == ""Minuit2"") ); 49 Minim::gDefaultMinimAlgo = ""Migrad"";; 50}; 51void MinimizerOptions::SetDefaultErrorDef(double up) {; 52 // set the default error definition; 53 Minim::gDefaultErrorDef ",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:14172,Availability,error,errors,14172,"ing.Definition MinimizerOptions.cxx:239; ROOT::Math::MinimizerOptions::DefaultTolerancestatic double DefaultTolerance()Definition MinimizerOptions.cxx:92; ROOT::Math::MinimizerOptions::fMaxIterint fMaxItermaximum number of iterationsDefinition MinimizerOptions.h:244; ROOT::Math::MinimizerOptions::SetDefaultExtraOptionsstatic void SetDefaultExtraOptions(const IOptions *extraoptions)Set additional minimizer options as pair of (string,value).Definition MinimizerOptions.cxx:79; ROOT::Math::MinimizerOptions::DefaultExtraOptionsstatic IOptions * DefaultExtraOptions()Definition MinimizerOptions.cxx:98; ROOT::Math::MinimizerOptions::fAlgoTypestd::string fAlgoTypeMinimizer algorithmic specification (Migrad, Minimize, ...)Definition MinimizerOptions.h:250; ROOT::Math::MinimizerOptions::SetDefaultMaxIterationsstatic void SetDefaultMaxIterations(int maxiter)Set the maximum number of iterations.Definition MinimizerOptions.cxx:67; ROOT::Math::MinimizerOptions::SetDefaultErrorDefstatic void SetDefaultErrorDef(double up)Set the default level for computing the parameter errors.Definition MinimizerOptions.cxx:51; ROOT::Math::MinimizerOptions::SetDefaultMinimizerstatic void SetDefaultMinimizer(const char *type, const char *algo=nullptr)Set the default Minimizer type and corresponding algorithms.Definition MinimizerOptions.cxx:43; ROOT::Math::MinimizerOptions::SetDefaultStrategystatic void SetDefaultStrategy(int strat)Set the default strategy.Definition MinimizerOptions.cxx:71; ROOT::Math::MinimizerOptions::SetDefaultPrecisionstatic void SetDefaultPrecision(double prec)Set the default Minimizer precision.Definition MinimizerOptions.cxx:59; ROOT::Math::MinimizerOptions::DefaultMinimizerTypestatic const std::string & DefaultMinimizerType()Definition MinimizerOptions.cxx:100; ROOT::Math::MinimizerOptions::fTolerancedouble fToleranceminimize tolerance to reach solutionDefinition MinimizerOptions.h:247; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)se",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:14952,Availability,toler,tolerance,14952,"tions(int maxiter)Set the maximum number of iterations.Definition MinimizerOptions.cxx:67; ROOT::Math::MinimizerOptions::SetDefaultErrorDefstatic void SetDefaultErrorDef(double up)Set the default level for computing the parameter errors.Definition MinimizerOptions.cxx:51; ROOT::Math::MinimizerOptions::SetDefaultMinimizerstatic void SetDefaultMinimizer(const char *type, const char *algo=nullptr)Set the default Minimizer type and corresponding algorithms.Definition MinimizerOptions.cxx:43; ROOT::Math::MinimizerOptions::SetDefaultStrategystatic void SetDefaultStrategy(int strat)Set the default strategy.Definition MinimizerOptions.cxx:71; ROOT::Math::MinimizerOptions::SetDefaultPrecisionstatic void SetDefaultPrecision(double prec)Set the default Minimizer precision.Definition MinimizerOptions.cxx:59; ROOT::Math::MinimizerOptions::DefaultMinimizerTypestatic const std::string & DefaultMinimizerType()Definition MinimizerOptions.cxx:100; ROOT::Math::MinimizerOptions::fTolerancedouble fToleranceminimize tolerance to reach solutionDefinition MinimizerOptions.h:247; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)set extra options (in this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::fStrategyint fStrategyminimizer strategy (used by Minuit)Definition MinimizerOptions.h:245; ROOT::Math::MinimizerOptions::fLevelint fLeveldebug print levelDefinition MinimizerOptions.h:242; ROOT::Math::MinimizerOptions::~MinimizerOptions~MinimizerOptions()Definition MinimizerOptions.cxx:170; ROOT::Math::MinimizerOptions::PrintDefaultstatic void PrintDefault(const char *name=nullptr, std::ostream &os=std::cout)Print all the default options including the extra one specific for a given minimizer name.Definition MinimizerOptions.cxx:244; ROOT::Math::MinimizerOptions::fMaxCallsint fMaxCallsmaximum number of function callsDefinition MinimizerOptions.h:243; ROOT::Math::MinimizerOptions::DefaultMinimizerAlgostatic const std",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:16108,Availability,error,error,16108," this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::fStrategyint fStrategyminimizer strategy (used by Minuit)Definition MinimizerOptions.h:245; ROOT::Math::MinimizerOptions::fLevelint fLeveldebug print levelDefinition MinimizerOptions.h:242; ROOT::Math::MinimizerOptions::~MinimizerOptions~MinimizerOptions()Definition MinimizerOptions.cxx:170; ROOT::Math::MinimizerOptions::PrintDefaultstatic void PrintDefault(const char *name=nullptr, std::ostream &os=std::cout)Print all the default options including the extra one specific for a given minimizer name.Definition MinimizerOptions.cxx:244; ROOT::Math::MinimizerOptions::fMaxCallsint fMaxCallsmaximum number of function callsDefinition MinimizerOptions.h:243; ROOT::Math::MinimizerOptions::DefaultMinimizerAlgostatic const std::string & DefaultMinimizerAlgo()Definition MinimizerOptions.cxx:85; ROOT::Math::MinimizerOptions::fErrorDefdouble fErrorDeferror definition (=1. for getting 1 sigma error for chi2 fits)Definition MinimizerOptions.h:246; ROOT::Math::MinimizerOptions::SetDefaultPrintLevelstatic void SetDefaultPrintLevel(int level)Set the default Print Level.Definition MinimizerOptions.cxx:75; ROOT::Math::MinimizerOptions::DefaultMaxFunctionCallsstatic int DefaultMaxFunctionCalls()Definition MinimizerOptions.cxx:94; ROOT::Math::MinimizerOptions::fExtraOptionsROOT::Math::IOptions * fExtraOptionsDefinition MinimizerOptions.h:253; ROOT::Math::MinimizerOptions::DefaultStrategystatic int DefaultStrategy()Definition MinimizerOptions.cxx:96; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::MinimizerOptionsMinimizerOptions()Definition MinimizerOptions.cxx:137; ROOT::Math::MinimizerOptions::SetDefaultTolerancestatic void SetDefaultTolerance(double tol)Set the Minimization tolerance.Definition MinimizerOptions.cxx:55; ROOT::Math::MinimizerOptions::Printvoid ",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:17036,Availability,toler,tolerance,17036,"ition MinimizerOptions.cxx:85; ROOT::Math::MinimizerOptions::fErrorDefdouble fErrorDeferror definition (=1. for getting 1 sigma error for chi2 fits)Definition MinimizerOptions.h:246; ROOT::Math::MinimizerOptions::SetDefaultPrintLevelstatic void SetDefaultPrintLevel(int level)Set the default Print Level.Definition MinimizerOptions.cxx:75; ROOT::Math::MinimizerOptions::DefaultMaxFunctionCallsstatic int DefaultMaxFunctionCalls()Definition MinimizerOptions.cxx:94; ROOT::Math::MinimizerOptions::fExtraOptionsROOT::Math::IOptions * fExtraOptionsDefinition MinimizerOptions.h:253; ROOT::Math::MinimizerOptions::DefaultStrategystatic int DefaultStrategy()Definition MinimizerOptions.cxx:96; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::MinimizerOptionsMinimizerOptions()Definition MinimizerOptions.cxx:137; ROOT::Math::MinimizerOptions::SetDefaultTolerancestatic void SetDefaultTolerance(double tol)Set the Minimization tolerance.Definition MinimizerOptions.cxx:55; ROOT::Math::MinimizerOptions::Printvoid Print(std::ostream &os=std::cout) constprint all the optionsDefinition MinimizerOptions.cxx:216; ROOT::Math::MinimizerOptions::DefaultMaxIterationsstatic int DefaultMaxIterations()Definition MinimizerOptions.cxx:95; ROOT::Math::MinimizerOptions::DefaultErrorDefstatic double DefaultErrorDef()Definition MinimizerOptions.cxx:91; TEnv::GetValuevirtual Int_t GetValue(const char *name, Int_t dflt) constReturns the integer value for a resource.Definition TEnv.cxx:491; MathNamespace for new Math classes and functions.; ROOT::Math::Minim::gDefaultPrecisionstatic double gDefaultPrecisionDefinition MinimizerOptions.cxx:34; ROOT::Math::Minim::gDefaultPrintLevelstatic int gDefaultPrintLevelDefinition MinimizerOptions.cxx:38; ROOT::Math::Minim::gDefaultStrategystatic int gDefaultStrategyDefinition MinimizerOptions.cxx:37; ROOT::Math::Minim::gDefaultMinimAlgost",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:11383,Deployability,configurat,configuration,11383,winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; TVirtualRWMutex.h; R__WRITE_LOCKGUARD#define R__WRITE_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:157; R__READ_LOCKGUARD#define R__READ_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:154; ROOT::Math::GenAlgoOptions::PrintAllDefaultstatic void PrintAllDefault(std::ostream &os=std::cout)print all the default optionsDefinition GenAlgoOptions.cxx:67; ROOT::Math::GenAlgoOptions::Defaultstatic IOptions & Default(const char *algoname)Definition GenAlgoOptions.cxx:55; ROOT::Math::GenAlgoOptions::FindDefaultstatic IOptions * FindDefault(const char *algoname)Definition GenAlgoOptions.cxx:48; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::IOptions::Clonevirtual IOptions * Clone() const =0; ROOT::Math::IOptions::Printvirtual void Print(std::ostream &=std::cout) constprint optionsDefinition IOptions.cxx:56; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::Math::MinimizerOptions::SetDefaultMaxFunctionCallsstatic void SetDefaultMaxFunctionCalls(int m,MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:11360,Integrability,interface,interface,11360,winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; TVirtualRWMutex.h; R__WRITE_LOCKGUARD#define R__WRITE_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:157; R__READ_LOCKGUARD#define R__READ_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:154; ROOT::Math::GenAlgoOptions::PrintAllDefaultstatic void PrintAllDefault(std::ostream &os=std::cout)print all the default optionsDefinition GenAlgoOptions.cxx:67; ROOT::Math::GenAlgoOptions::Defaultstatic IOptions & Default(const char *algoname)Definition GenAlgoOptions.cxx:55; ROOT::Math::GenAlgoOptions::FindDefaultstatic IOptions * FindDefault(const char *algoname)Definition GenAlgoOptions.cxx:48; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::IOptions::Clonevirtual IOptions * Clone() const =0; ROOT::Math::IOptions::Printvirtual void Print(std::ostream &=std::cout) constprint optionsDefinition IOptions.cxx:56; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::Math::MinimizerOptions::SetDefaultMaxFunctionCallsstatic void SetDefaultMaxFunctionCalls(int m,MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:613,Modifiability,plug-in,plug-in,613,". ROOT: math/mathcore/src/MinimizerOptions.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MinimizerOptions.cxx. Go to the documentation of this file. 1// @(#)root/mathcore:$Id$; 2// Author: L. Moneta Fri Aug 15 2008; 3 ; 4/**********************************************************************; 5 * *; 6 * Copyright (c) 2008 LCG ROOT Math Team, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11#include ""Math/MinimizerOptions.h""; 12 ; 13#include ""Math/GenAlgoOptions.h""; 14 ; 15// case of using ROOT plug-in manager; 16#ifndef MATH_NO_PLUGIN_MANAGER; 17#include ""TEnv.h""; 18#include ""TVirtualRWMutex.h""; 19#endif; 20 ; 21 ; 22#include <iomanip>; 23 ; 24namespace ROOT {; 25 ; 26 ; 27namespace Math {; 28 ; 29 namespace Minim {; 30 static std::string gDefaultMinimizer; // take from /etc/system.rootrc in ROOT Fitter; 31 static std::string gDefaultMinimAlgo = ""Migrad"";; 32 static double gDefaultErrorDef = 1.;; 33 static double gDefaultTolerance = 1.E-2;; 34 static double gDefaultPrecision = -1; // value <= 0 means left to minimizer; 35 static int gDefaultMaxCalls = 0; // 0 means leave default values Deaf; 36 static int gDefaultMaxIter = 0;; 37 static int gDefaultStrategy = 1;; 38 static int gDefaultPrintLevel = 0;; 39 static IOptions * gDefaultExtraOptions = nullptr; // pointer to default extra options; 40 }; 41 ; 42 ; 43void MinimizerOptions::SetDefaultMinimizer(const char * type, const char * algo) {; 44 // set the default minimizer type and algorithm; 45 if (type) Minim::gDefaultMinimizer = std::string(type);; 46 if (algo) Minim::gDefaultMinimAlgo = std::string(algo);; 47 if (Minim::gDefaultMinimAlgo.empty() && ( Minim::gDefaultMinimizer == ""Minuit"" ||; 48 Minim::gDefaultMinimizer == ""Minuit2"") ); 49 Minim::gDefaultMinimAlgo = ""Migrad"";; 50}; 51void MinimizerOptions::SetDefaultErrorDef(double up) {; 52 // set the default error definition; 53 Minim::gDefaultErrorDef ",MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8cxx_source.html:11383,Modifiability,config,configuration,11383,winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; TVirtualRWMutex.h; R__WRITE_LOCKGUARD#define R__WRITE_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:157; R__READ_LOCKGUARD#define R__READ_LOCKGUARD(mutex)Definition TVirtualRWMutex.h:154; ROOT::Math::GenAlgoOptions::PrintAllDefaultstatic void PrintAllDefault(std::ostream &os=std::cout)print all the default optionsDefinition GenAlgoOptions.cxx:67; ROOT::Math::GenAlgoOptions::Defaultstatic IOptions & Default(const char *algoname)Definition GenAlgoOptions.cxx:55; ROOT::Math::GenAlgoOptions::FindDefaultstatic IOptions * FindDefault(const char *algoname)Definition GenAlgoOptions.cxx:48; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::IOptions::Clonevirtual IOptions * Clone() const =0; ROOT::Math::IOptions::Printvirtual void Print(std::ostream &=std::cout) constprint optionsDefinition IOptions.cxx:56; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::Math::MinimizerOptions::SetDefaultMaxFunctionCallsstatic void SetDefaultMaxFunctionCalls(int m,MatchSource.WIKI,doc/master/MinimizerOptions_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8cxx_source.html
https://root.cern/doc/master/MinimizerOptions_8h.html:254,Integrability,depend,dependency,254,". ROOT: math/mathcore/inc/Math/MinimizerOptions.h File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Namespaces ; MinimizerOptions.h File Reference. #include <string>; #include <iostream>. Include dependency graph for MinimizerOptions.h:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; class  ROOT::Math::MinimizerOptions;  Minimizer options. More...;  . Namespaces; namespace  ROOT;  tbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tbb::task_arena without forward declaring tbb::interface7 ;  ; namespace  ROOT::Math;  . mathmathcoreincMathMinimizerOptions.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:21 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MinimizerOptions_8h.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:1398,Availability,avail,available,1398,"*; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11#ifndef ROOT_Math_MinimizerOptions; 12#define ROOT_Math_MinimizerOptions; 13 ; 14#include <string>; 15 ; 16#include <iostream>; 17 ; 18namespace ROOT {; 19 ; 20 ; 21namespace Math {; 22 ; 23 ; 24class IOptions;; 25 ; 26//_______________________________________________________________________________; 27/**; 28 Minimizer options; 29 ; 30 @ingroup MultiMin; 31 ; 32 Class defining the options for the minimizer.; 33 It contains also static methods for setting the default Minimizer option values; 34 that will be used by default by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:1530,Availability,avail,available,1530,"zerOptions; 12#define ROOT_Math_MinimizerOptions; 13 ; 14#include <string>; 15 ; 16#include <iostream>; 17 ; 18namespace ROOT {; 19 ; 20 ; 21namespace Math {; 22 ; 23 ; 24class IOptions;; 25 ; 26//_______________________________________________________________________________; 27/**; 28 Minimizer options; 29 ; 30 @ingroup MultiMin; 31 ; 32 Class defining the options for the minimizer.; 33 It contains also static methods for setting the default Minimizer option values; 34 that will be used by default by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:1745,Availability,avail,available,1745,"t by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorit",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:3312,Availability,error,errors,3312,"Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorithm using only function values and not the gradient.; 73 /// - Genetic Genetic minimization algorithms (see TMVA::Genetic); 74 ///; 75 static void SetDefaultMinimizer(const char *type, const char *algo = nullptr);; 76 ; 77 /// Set the default level for computing the parameter errors.; 78 /// For example for 1-sigma parameter errors; 79 /// - up = 1 for a chi-squared function; 80 /// - up = 0.5 for a negative log-likelihood function; 81 ///; 82 /// The value will be used also by Minos when computing the confidence interval; 83 static void SetDefaultErrorDef(double up);; 84 ; 85 /// Set the Minimization tolerance.; 86 /// The Default value for Minuit and Minuit2 is 0.01; 87 static void SetDefaultTolerance(double tol);; 88 ; 89 /// Set the default Minimizer precision.; 90 /// (used only by MInuit and Minuit2); 91 /// It is used to specify the numerical precision used for computing the; 92 /// objective function. It should be left to the default value found by the Minimizer; 93 /// (typically double precision); 94 static void SetDefaultPrecision(double prec);; 95 ; 96 /// Set the maximum number of function calls.; 97 static void SetDefaultMaxFunctionCall",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:3362,Availability,error,errors,3362,"kelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorithm using only function values and not the gradient.; 73 /// - Genetic Genetic minimization algorithms (see TMVA::Genetic); 74 ///; 75 static void SetDefaultMinimizer(const char *type, const char *algo = nullptr);; 76 ; 77 /// Set the default level for computing the parameter errors.; 78 /// For example for 1-sigma parameter errors; 79 /// - up = 1 for a chi-squared function; 80 /// - up = 0.5 for a negative log-likelihood function; 81 ///; 82 /// The value will be used also by Minos when computing the confidence interval; 83 static void SetDefaultErrorDef(double up);; 84 ; 85 /// Set the Minimization tolerance.; 86 /// The Default value for Minuit and Minuit2 is 0.01; 87 static void SetDefaultTolerance(double tol);; 88 ; 89 /// Set the default Minimizer precision.; 90 /// (used only by MInuit and Minuit2); 91 /// It is used to specify the numerical precision used for computing the; 92 /// objective function. It should be left to the default value found by the Minimizer; 93 /// (typically double precision); 94 static void SetDefaultPrecision(double prec);; 95 ; 96 /// Set the maximum number of function calls.; 97 static void SetDefaultMaxFunctionCalls(int maxcall);; 98 ; 99 /// Set the maximum number of iterations.; 100 /// Used by the GSL minimizers and Genetic. Not used by Minuit,Minuit2.; 101 static void SetDefaultM",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:3644,Availability,toler,tolerance,3644,"r from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorithm using only function values and not the gradient.; 73 /// - Genetic Genetic minimization algorithms (see TMVA::Genetic); 74 ///; 75 static void SetDefaultMinimizer(const char *type, const char *algo = nullptr);; 76 ; 77 /// Set the default level for computing the parameter errors.; 78 /// For example for 1-sigma parameter errors; 79 /// - up = 1 for a chi-squared function; 80 /// - up = 0.5 for a negative log-likelihood function; 81 ///; 82 /// The value will be used also by Minos when computing the confidence interval; 83 static void SetDefaultErrorDef(double up);; 84 ; 85 /// Set the Minimization tolerance.; 86 /// The Default value for Minuit and Minuit2 is 0.01; 87 static void SetDefaultTolerance(double tol);; 88 ; 89 /// Set the default Minimizer precision.; 90 /// (used only by MInuit and Minuit2); 91 /// It is used to specify the numerical precision used for computing the; 92 /// objective function. It should be left to the default value found by the Minimizer; 93 /// (typically double precision); 94 static void SetDefaultPrecision(double prec);; 95 ; 96 /// Set the maximum number of function calls.; 97 static void SetDefaultMaxFunctionCalls(int maxcall);; 98 ; 99 /// Set the maximum number of iterations.; 100 /// Used by the GSL minimizers and Genetic. Not used by Minuit,Minuit2.; 101 static void SetDefaultMaxIterations(int maxiter);; 102 ; 103 /// Set the default strategy.; 104 /// The strategy is a parameter used only by Minuit and Minuit2.; 105 /// Possible values are:",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:8063,Availability,toler,tolerance,8063,,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:8291,Availability,error,error,8291,t);; 156 ; 157public:; 158 ; 159 // constructor using the default options; 160 MinimizerOptions();; 161 ; 162 // destructor; 163 ~MinimizerOptions();; 164 ; 165 // copy constructor; 166 MinimizerOptions(const MinimizerOptions & opt);; 167 ; 168 /// assignment operators; 169 MinimizerOptions & operator=(const MinimizerOptions & opt);; 170 ; 171 /** non-static methods for retrieving options */; 172 ; 173 /// set print level; 174 int PrintLevel() const { return fLevel; }; 175 ; 176 /// max number of function calls; 177 unsigned int MaxFunctionCalls() const { return fMaxCalls; }; 178 ; 179 /// max iterations; 180 unsigned int MaxIterations() const { return fMaxIter; }; 181 ; 182 /// strategy; 183 int Strategy() const { return fStrategy; }; 184 ; 185 /// absolute tolerance; 186 double Tolerance() const { return fTolerance; }; 187 ; 188 /// precision in the objective function calculation (value <=0 means left to default); 189 double Precision() const { return fPrecision; }; 190 ; 191 /// error definition; 192 double ErrorDef() const { return fErrorDef; }; 193 ; 194 /// return extra options (NULL pointer if they are not present); 195 const IOptions * ExtraOptions() const { return fExtraOptions; }; 196 ; 197 /// type of minimizer; 198 const std::string & MinimizerType() const { return fMinimType; }; 199 ; 200 /// type of algorithm; 201 const std::string & MinimizerAlgorithm() const { return fAlgoType; }; 202 ; 203 /// print all the options; 204 void Print(std::ostream & os = std::cout) const;; 205 ; 206 /** non-static methods for setting options */; 207 void ResetToDefaultOptions();; 208 ; 209 /// set print level; 210 void SetPrintLevel(int level) { fLevel = level; }; 211 ; 212 ///set maximum of function calls; 213 void SetMaxFunctionCalls(unsigned int maxfcn) { fMaxCalls = maxfcn; }; 214 ; 215 /// set maximum iterations (one iteration can have many function calls); 216 void SetMaxIterations(unsigned int maxiter) { fMaxIter = maxiter; }; 217 ; 218 /// set the tolerance; 219,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:9280,Availability,toler,tolerance,9280,ouble Precision() const { return fPrecision; }; 190 ; 191 /// error definition; 192 double ErrorDef() const { return fErrorDef; }; 193 ; 194 /// return extra options (NULL pointer if they are not present); 195 const IOptions * ExtraOptions() const { return fExtraOptions; }; 196 ; 197 /// type of minimizer; 198 const std::string & MinimizerType() const { return fMinimType; }; 199 ; 200 /// type of algorithm; 201 const std::string & MinimizerAlgorithm() const { return fAlgoType; }; 202 ; 203 /// print all the options; 204 void Print(std::ostream & os = std::cout) const;; 205 ; 206 /** non-static methods for setting options */; 207 void ResetToDefaultOptions();; 208 ; 209 /// set print level; 210 void SetPrintLevel(int level) { fLevel = level; }; 211 ; 212 ///set maximum of function calls; 213 void SetMaxFunctionCalls(unsigned int maxfcn) { fMaxCalls = maxfcn; }; 214 ; 215 /// set maximum iterations (one iteration can have many function calls); 216 void SetMaxIterations(unsigned int maxiter) { fMaxIter = maxiter; }; 217 ; 218 /// set the tolerance; 219 void SetTolerance(double tol) { fTolerance = tol; }; 220 ; 221 /// set the precision; 222 void SetPrecision(double prec) { fPrecision = prec; }; 223 ; 224 /// set the strategy; 225 void SetStrategy(int stra) { fStrategy = stra; }; 226 ; 227 /// set error def; 228 void SetErrorDef(double err) { fErrorDef = err; }; 229 ; 230 /// set minimizer type; 231 void SetMinimizerType(const char * type) { fMinimType = type; }; 232 ; 233 /// set minimizer algorithm; 234 void SetMinimizerAlgorithm(const char *type) { fAlgoType = type; }; 235 ; 236 /// set extra options (in this case pointer is cloned); 237 void SetExtraOptions(const IOptions & opt);; 238 ; 239 ; 240private:; 241 ; 242 int fLevel; ///< debug print level; 243 int fMaxCalls; ///< maximum number of function calls; 244 int fMaxIter; ///< maximum number of iterations; 245 int fStrategy; ///< minimizer strategy (used by Minuit); 246 double fErrorDef; ///< error definition (=1.,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:9544,Availability,error,error,9544,ouble Precision() const { return fPrecision; }; 190 ; 191 /// error definition; 192 double ErrorDef() const { return fErrorDef; }; 193 ; 194 /// return extra options (NULL pointer if they are not present); 195 const IOptions * ExtraOptions() const { return fExtraOptions; }; 196 ; 197 /// type of minimizer; 198 const std::string & MinimizerType() const { return fMinimType; }; 199 ; 200 /// type of algorithm; 201 const std::string & MinimizerAlgorithm() const { return fAlgoType; }; 202 ; 203 /// print all the options; 204 void Print(std::ostream & os = std::cout) const;; 205 ; 206 /** non-static methods for setting options */; 207 void ResetToDefaultOptions();; 208 ; 209 /// set print level; 210 void SetPrintLevel(int level) { fLevel = level; }; 211 ; 212 ///set maximum of function calls; 213 void SetMaxFunctionCalls(unsigned int maxfcn) { fMaxCalls = maxfcn; }; 214 ; 215 /// set maximum iterations (one iteration can have many function calls); 216 void SetMaxIterations(unsigned int maxiter) { fMaxIter = maxiter; }; 217 ; 218 /// set the tolerance; 219 void SetTolerance(double tol) { fTolerance = tol; }; 220 ; 221 /// set the precision; 222 void SetPrecision(double prec) { fPrecision = prec; }; 223 ; 224 /// set the strategy; 225 void SetStrategy(int stra) { fStrategy = stra; }; 226 ; 227 /// set error def; 228 void SetErrorDef(double err) { fErrorDef = err; }; 229 ; 230 /// set minimizer type; 231 void SetMinimizerType(const char * type) { fMinimType = type; }; 232 ; 233 /// set minimizer algorithm; 234 void SetMinimizerAlgorithm(const char *type) { fAlgoType = type; }; 235 ; 236 /// set extra options (in this case pointer is cloned); 237 void SetExtraOptions(const IOptions & opt);; 238 ; 239 ; 240private:; 241 ; 242 int fLevel; ///< debug print level; 243 int fMaxCalls; ///< maximum number of function calls; 244 int fMaxIter; ///< maximum number of iterations; 245 int fStrategy; ///< minimizer strategy (used by Minuit); 246 double fErrorDef; ///< error definition (=1.,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:10209,Availability,error,error,10209,ouble Precision() const { return fPrecision; }; 190 ; 191 /// error definition; 192 double ErrorDef() const { return fErrorDef; }; 193 ; 194 /// return extra options (NULL pointer if they are not present); 195 const IOptions * ExtraOptions() const { return fExtraOptions; }; 196 ; 197 /// type of minimizer; 198 const std::string & MinimizerType() const { return fMinimType; }; 199 ; 200 /// type of algorithm; 201 const std::string & MinimizerAlgorithm() const { return fAlgoType; }; 202 ; 203 /// print all the options; 204 void Print(std::ostream & os = std::cout) const;; 205 ; 206 /** non-static methods for setting options */; 207 void ResetToDefaultOptions();; 208 ; 209 /// set print level; 210 void SetPrintLevel(int level) { fLevel = level; }; 211 ; 212 ///set maximum of function calls; 213 void SetMaxFunctionCalls(unsigned int maxfcn) { fMaxCalls = maxfcn; }; 214 ; 215 /// set maximum iterations (one iteration can have many function calls); 216 void SetMaxIterations(unsigned int maxiter) { fMaxIter = maxiter; }; 217 ; 218 /// set the tolerance; 219 void SetTolerance(double tol) { fTolerance = tol; }; 220 ; 221 /// set the precision; 222 void SetPrecision(double prec) { fPrecision = prec; }; 223 ; 224 /// set the strategy; 225 void SetStrategy(int stra) { fStrategy = stra; }; 226 ; 227 /// set error def; 228 void SetErrorDef(double err) { fErrorDef = err; }; 229 ; 230 /// set minimizer type; 231 void SetMinimizerType(const char * type) { fMinimType = type; }; 232 ; 233 /// set minimizer algorithm; 234 void SetMinimizerAlgorithm(const char *type) { fAlgoType = type; }; 235 ; 236 /// set extra options (in this case pointer is cloned); 237 void SetExtraOptions(const IOptions & opt);; 238 ; 239 ; 240private:; 241 ; 242 int fLevel; ///< debug print level; 243 int fMaxCalls; ///< maximum number of function calls; 244 int fMaxIter; ///< maximum number of iterations; 245 int fStrategy; ///< minimizer strategy (used by Minuit); 246 double fErrorDef; ///< error definition (=1.,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:10251,Availability,error,error,10251,"ision; 222 void SetPrecision(double prec) { fPrecision = prec; }; 223 ; 224 /// set the strategy; 225 void SetStrategy(int stra) { fStrategy = stra; }; 226 ; 227 /// set error def; 228 void SetErrorDef(double err) { fErrorDef = err; }; 229 ; 230 /// set minimizer type; 231 void SetMinimizerType(const char * type) { fMinimType = type; }; 232 ; 233 /// set minimizer algorithm; 234 void SetMinimizerAlgorithm(const char *type) { fAlgoType = type; }; 235 ; 236 /// set extra options (in this case pointer is cloned); 237 void SetExtraOptions(const IOptions & opt);; 238 ; 239 ; 240private:; 241 ; 242 int fLevel; ///< debug print level; 243 int fMaxCalls; ///< maximum number of function calls; 244 int fMaxIter; ///< maximum number of iterations; 245 int fStrategy; ///< minimizer strategy (used by Minuit); 246 double fErrorDef; ///< error definition (=1. for getting 1 sigma error for chi2 fits); 247 double fTolerance; ///< minimize tolerance to reach solution; 248 double fPrecision; ///< precision of the objective function evaluation (value <=0 means left to default); 249 std::string fMinimType; ///< Minimizer type (Minuit, Minuit2, etc..; 250 std::string fAlgoType; ///< Minimizer algorithmic specification (Migrad, Minimize, ...); 251 ; 252 // extra options; 253 ROOT::Math::IOptions * fExtraOptions; // extra options; 254 ; 255};; 256 ; 257 } // end namespace Math; 258 ; 259} // end namespace ROOT; 260 ; 261#endif; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count cons",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:10310,Availability,toler,tolerance,10310,"ision; 222 void SetPrecision(double prec) { fPrecision = prec; }; 223 ; 224 /// set the strategy; 225 void SetStrategy(int stra) { fStrategy = stra; }; 226 ; 227 /// set error def; 228 void SetErrorDef(double err) { fErrorDef = err; }; 229 ; 230 /// set minimizer type; 231 void SetMinimizerType(const char * type) { fMinimType = type; }; 232 ; 233 /// set minimizer algorithm; 234 void SetMinimizerAlgorithm(const char *type) { fAlgoType = type; }; 235 ; 236 /// set extra options (in this case pointer is cloned); 237 void SetExtraOptions(const IOptions & opt);; 238 ; 239 ; 240private:; 241 ; 242 int fLevel; ///< debug print level; 243 int fMaxCalls; ///< maximum number of function calls; 244 int fMaxIter; ///< maximum number of iterations; 245 int fStrategy; ///< minimizer strategy (used by Minuit); 246 double fErrorDef; ///< error definition (=1. for getting 1 sigma error for chi2 fits); 247 double fTolerance; ///< minimize tolerance to reach solution; 248 double fPrecision; ///< precision of the objective function evaluation (value <=0 means left to default); 249 std::string fMinimType; ///< Minimizer type (Minuit, Minuit2, etc..; 250 std::string fAlgoType; ///< Minimizer algorithmic specification (Migrad, Minimize, ...); 251 ; 252 // extra options; 253 ROOT::Math::IOptions * fExtraOptions; // extra options; 254 ; 255};; 256 ; 257 } // end namespace Math; 258 ; 259} // end namespace ROOT; 260 ; 261#endif; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count cons",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:13944,Availability,toler,toleranceDefinition,13944,"ions::fPrecisiondouble fPrecisionprecision of the objective function evaluation (value <=0 means left to default)Definition MinimizerOptions.h:248; ROOT::Math::MinimizerOptions::fMinimTypestd::string fMinimTypeMinimizer type (Minuit, Minuit2, etc..Definition MinimizerOptions.h:249; ROOT::Math::MinimizerOptions::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition MinimizerOptions.h:216; ROOT::Math::MinimizerOptions::Strategyint Strategy() conststrategyDefinition MinimizerOptions.h:183; ROOT::Math::MinimizerOptions::ExtraOptionsconst IOptions * ExtraOptions() constreturn extra options (NULL pointer if they are not present)Definition MinimizerOptions.h:195; ROOT::Math::MinimizerOptions::FindDefaultstatic ROOT::Math::IOptions * FindDefault(const char *name)Find an extra options and return a nullptr if it is not existing.Definition MinimizerOptions.cxx:239; ROOT::Math::MinimizerOptions::Tolerancedouble Tolerance() constabsolute toleranceDefinition MinimizerOptions.h:186; ROOT::Math::MinimizerOptions::Precisiondouble Precision() constprecision in the objective function calculation (value <=0 means left to default)Definition MinimizerOptions.h:189; ROOT::Math::MinimizerOptions::SetMinimizerTypevoid SetMinimizerType(const char *type)set minimizer typeDefinition MinimizerOptions.h:231; ROOT::Math::MinimizerOptions::MinimizerAlgorithmconst std::string & MinimizerAlgorithm() consttype of algorithmDefinition MinimizerOptions.h:201; ROOT::Math::MinimizerOptions::DefaultTolerancestatic double DefaultTolerance()Definition MinimizerOptions.cxx:92; ROOT::Math::MinimizerOptions::fMaxIterint fMaxItermaximum number of iterationsDefinition MinimizerOptions.h:244; ROOT::Math::MinimizerOptions::ErrorDefdouble ErrorDef() consterror definitionDefinition MinimizerOptions.h:192; ROOT::Math::MinimizerOptions::SetDefaultExtraOptionsstatic void SetDefaultExtraOptions(const IOptions *extraoptions)Set additional m",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:15592,Availability,error,errors,15592,"rance()Definition MinimizerOptions.cxx:92; ROOT::Math::MinimizerOptions::fMaxIterint fMaxItermaximum number of iterationsDefinition MinimizerOptions.h:244; ROOT::Math::MinimizerOptions::ErrorDefdouble ErrorDef() consterror definitionDefinition MinimizerOptions.h:192; ROOT::Math::MinimizerOptions::SetDefaultExtraOptionsstatic void SetDefaultExtraOptions(const IOptions *extraoptions)Set additional minimizer options as pair of (string,value).Definition MinimizerOptions.cxx:79; ROOT::Math::MinimizerOptions::DefaultExtraOptionsstatic IOptions * DefaultExtraOptions()Definition MinimizerOptions.cxx:98; ROOT::Math::MinimizerOptions::fAlgoTypestd::string fAlgoTypeMinimizer algorithmic specification (Migrad, Minimize, ...)Definition MinimizerOptions.h:250; ROOT::Math::MinimizerOptions::SetDefaultMaxIterationsstatic void SetDefaultMaxIterations(int maxiter)Set the maximum number of iterations.Definition MinimizerOptions.cxx:67; ROOT::Math::MinimizerOptions::SetDefaultErrorDefstatic void SetDefaultErrorDef(double up)Set the default level for computing the parameter errors.Definition MinimizerOptions.cxx:51; ROOT::Math::MinimizerOptions::SetDefaultMinimizerstatic void SetDefaultMinimizer(const char *type, const char *algo=nullptr)Set the default Minimizer type and corresponding algorithms.Definition MinimizerOptions.cxx:43; ROOT::Math::MinimizerOptions::SetDefaultStrategystatic void SetDefaultStrategy(int strat)Set the default strategy.Definition MinimizerOptions.cxx:71; ROOT::Math::MinimizerOptions::SetDefaultPrecisionstatic void SetDefaultPrecision(double prec)Set the default Minimizer precision.Definition MinimizerOptions.cxx:59; ROOT::Math::MinimizerOptions::DefaultMinimizerTypestatic const std::string & DefaultMinimizerType()Definition MinimizerOptions.cxx:100; ROOT::Math::MinimizerOptions::fTolerancedouble fToleranceminimize tolerance to reach solutionDefinition MinimizerOptions.h:247; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)se",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:16372,Availability,toler,tolerance,16372,"tions(int maxiter)Set the maximum number of iterations.Definition MinimizerOptions.cxx:67; ROOT::Math::MinimizerOptions::SetDefaultErrorDefstatic void SetDefaultErrorDef(double up)Set the default level for computing the parameter errors.Definition MinimizerOptions.cxx:51; ROOT::Math::MinimizerOptions::SetDefaultMinimizerstatic void SetDefaultMinimizer(const char *type, const char *algo=nullptr)Set the default Minimizer type and corresponding algorithms.Definition MinimizerOptions.cxx:43; ROOT::Math::MinimizerOptions::SetDefaultStrategystatic void SetDefaultStrategy(int strat)Set the default strategy.Definition MinimizerOptions.cxx:71; ROOT::Math::MinimizerOptions::SetDefaultPrecisionstatic void SetDefaultPrecision(double prec)Set the default Minimizer precision.Definition MinimizerOptions.cxx:59; ROOT::Math::MinimizerOptions::DefaultMinimizerTypestatic const std::string & DefaultMinimizerType()Definition MinimizerOptions.cxx:100; ROOT::Math::MinimizerOptions::fTolerancedouble fToleranceminimize tolerance to reach solutionDefinition MinimizerOptions.h:247; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)set extra options (in this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::fStrategyint fStrategyminimizer strategy (used by Minuit)Definition MinimizerOptions.h:245; ROOT::Math::MinimizerOptions::fLevelint fLeveldebug print levelDefinition MinimizerOptions.h:242; ROOT::Math::MinimizerOptions::~MinimizerOptions~MinimizerOptions()Definition MinimizerOptions.cxx:170; ROOT::Math::MinimizerOptions::PrintDefaultstatic void PrintDefault(const char *name=nullptr, std::ostream &os=std::cout)Print all the default options including the extra one specific for a given minimizer name.Definition MinimizerOptions.cxx:244; ROOT::Math::MinimizerOptions::fMaxCallsint fMaxCallsmaximum number of function callsDefinition MinimizerOptions.h:243; ROOT::Math::MinimizerOptions::DefaultMinimizerAlgostatic const std",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:17528,Availability,error,error,17528," this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::fStrategyint fStrategyminimizer strategy (used by Minuit)Definition MinimizerOptions.h:245; ROOT::Math::MinimizerOptions::fLevelint fLeveldebug print levelDefinition MinimizerOptions.h:242; ROOT::Math::MinimizerOptions::~MinimizerOptions~MinimizerOptions()Definition MinimizerOptions.cxx:170; ROOT::Math::MinimizerOptions::PrintDefaultstatic void PrintDefault(const char *name=nullptr, std::ostream &os=std::cout)Print all the default options including the extra one specific for a given minimizer name.Definition MinimizerOptions.cxx:244; ROOT::Math::MinimizerOptions::fMaxCallsint fMaxCallsmaximum number of function callsDefinition MinimizerOptions.h:243; ROOT::Math::MinimizerOptions::DefaultMinimizerAlgostatic const std::string & DefaultMinimizerAlgo()Definition MinimizerOptions.cxx:85; ROOT::Math::MinimizerOptions::fErrorDefdouble fErrorDeferror definition (=1. for getting 1 sigma error for chi2 fits)Definition MinimizerOptions.h:246; ROOT::Math::MinimizerOptions::MinimizerTypeconst std::string & MinimizerType() consttype of minimizerDefinition MinimizerOptions.h:198; ROOT::Math::MinimizerOptions::SetDefaultPrintLevelstatic void SetDefaultPrintLevel(int level)Set the default Print Level.Definition MinimizerOptions.cxx:75; ROOT::Math::MinimizerOptions::DefaultMaxFunctionCallsstatic int DefaultMaxFunctionCalls()Definition MinimizerOptions.cxx:94; ROOT::Math::MinimizerOptions::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::fExtraOptionsROOT::Math::IOptions * fExtraOptionsDefinition MinimizerOptions.h:253; ROOT::Math::MinimizerOptions::DefaultStrategystatic int DefaultStrategy()Definition MinimizerOptions.cxx:96; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFuncti",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:18988,Availability,toler,tolerance,18988,tic int DefaultMaxFunctionCalls()Definition MinimizerOptions.cxx:94; ROOT::Math::MinimizerOptions::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::fExtraOptionsROOT::Math::IOptions * fExtraOptionsDefinition MinimizerOptions.h:253; ROOT::Math::MinimizerOptions::DefaultStrategystatic int DefaultStrategy()Definition MinimizerOptions.cxx:96; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::MinimizerOptionsMinimizerOptions()Definition MinimizerOptions.cxx:137; ROOT::Math::MinimizerOptions::SetDefaultTolerancestatic void SetDefaultTolerance(double tol)Set the Minimization tolerance.Definition MinimizerOptions.cxx:55; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::Printvoid Print(std::ostream &os=std::cout) constprint all the optionsDefinition MinimizerOptions.cxx:216; ROOT::Math::MinimizerOptions::DefaultMaxIterationsstatic int DefaultMaxIterations()Definition MinimizerOptions.cxx:95; ROOT::Math::MinimizerOptions::DefaultErrorDefstatic double DefaultErrorDef()Definition MinimizerOptions.cxx:91; ROOT::Math::MinimizerOptions::SetMinimizerAlgorithmvoid SetMinimizerAlgorithm(const char *type)set minimizer algorithmDefinition MinimizerOptions.h:234; ,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:19245,Availability,error,error,19245,"zerOptions.h:253; ROOT::Math::MinimizerOptions::DefaultStrategystatic int DefaultStrategy()Definition MinimizerOptions.cxx:96; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::MinimizerOptionsMinimizerOptions()Definition MinimizerOptions.cxx:137; ROOT::Math::MinimizerOptions::SetDefaultTolerancestatic void SetDefaultTolerance(double tol)Set the Minimization tolerance.Definition MinimizerOptions.cxx:55; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::Printvoid Print(std::ostream &os=std::cout) constprint all the optionsDefinition MinimizerOptions.cxx:216; ROOT::Math::MinimizerOptions::DefaultMaxIterationsstatic int DefaultMaxIterations()Definition MinimizerOptions.cxx:95; ROOT::Math::MinimizerOptions::DefaultErrorDefstatic double DefaultErrorDef()Definition MinimizerOptions.cxx:91; ROOT::Math::MinimizerOptions::SetMinimizerAlgorithmvoid SetMinimizerAlgorithm(const char *type)set minimizer algorithmDefinition MinimizerOptions.h:234; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:20011,Availability,toler,toleranceDefinition,20011,"void SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::MinimizerOptionsMinimizerOptions()Definition MinimizerOptions.cxx:137; ROOT::Math::MinimizerOptions::SetDefaultTolerancestatic void SetDefaultTolerance(double tol)Set the Minimization tolerance.Definition MinimizerOptions.cxx:55; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::Printvoid Print(std::ostream &os=std::cout) constprint all the optionsDefinition MinimizerOptions.cxx:216; ROOT::Math::MinimizerOptions::DefaultMaxIterationsstatic int DefaultMaxIterations()Definition MinimizerOptions.cxx:95; ROOT::Math::MinimizerOptions::DefaultErrorDefstatic double DefaultErrorDef()Definition MinimizerOptions.cxx:91; ROOT::Math::MinimizerOptions::SetMinimizerAlgorithmvoid SetMinimizerAlgorithm(const char *type)set minimizer algorithmDefinition MinimizerOptions.h:234; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimizerOptions.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:2189,Deployability,integrat,integrated,2189,"t by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorit",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:11726,Deployability,configurat,configuration,11726, ; 257 } // end namespace Math; 258 ; 259} // end namespace ROOT; 260 ; 261#endif; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:2189,Integrability,integrat,integrated,2189,"t by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorit",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:11703,Integrability,interface,interface,11703, ; 257 } // end namespace Math; 258 ; 259} // end namespace ROOT; 260 ; 261#endif; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:1815,Modifiability,variab,variable,1815,"t by all Minimizer instances.; 35 To see the current default options do:; 36 ; 37 ROOT::Math::MinimizerOptions::PrintDefault();; 38 ; 39*/; 40class MinimizerOptions {; 41 ; 42public:; 43 ; 44 // static methods for setting and retrieving the default options; 45 ; 46 /// Set the default Minimizer type and corresponding algorithms.; 47 /// Here is the list of the available minimizers and their corresponding algorithms.; 48 /// For some minimizers (e.g. Fumili) there are no specific algorithms available, then there is no need to specify it.; 49 ///; 50 /// \anchor ROOTMinimizers; 51 /// ### ROOT Minimizers; 52 ///; 53 /// - Minuit Minimizer based on TMinuit, the legacy Minuit implementation. Here are the available algorithms:; 54 /// - Migrad default algorithm based on the variable metric minimizer; 55 /// - Minimize combination of Simplex and Migrad; 56 /// - Simplex minimization algorithm not using the gradient information; 57 /// - Scan brute function scan; 58 /// - Minuit2 New C++ implementation of Minuit (the recommended one); 59 /// - Migrad (default); 60 /// - Minimize; 61 /// - Simplex; 62 /// - Fumili2 new implementation of Fumili integrated in Minuit2; 63 /// - Fumili Minimizer using an approximation for the Hessian based on first derivatives of the model function (see TFumili). Works only for chi-squared and likelihood functions.; 64 /// - Linear Linear minimizer (fitter) working only for linear functions (see TLinearFitter and TLinearMinimizer); 65 /// - GSLMultiMin Minimizer from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorit",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:5280,Modifiability,config,configured,5280,"; 101 static void SetDefaultMaxIterations(int maxiter);; 102 ; 103 /// Set the default strategy.; 104 /// The strategy is a parameter used only by Minuit and Minuit2.; 105 /// Possible values are:; 106 /// - `strat = 0` : rough approximation of Hessian using the gradient. Avoid computing the full Hessian matrix; 107 /// - `strat = 1` (default and recommended one) - Use Hessian approximation but compute full Hessian at the end of minimization if needed.; 108 /// - `strat = 2` Perform several full Hessian computations during the minimization. Slower and not always working better than `strat=1`.; 109 static void SetDefaultStrategy(int strat);; 110 ; 111 /// Set the default Print Level.; 112 /// Possible levels are from 0 (minimal printing) to 3 (maximum printing); 113 static void SetDefaultPrintLevel(int level);; 114 ; 115 /// Set additional minimizer options as pair of (string,value).; 116 /// Extra option defaults can be configured for a specific algorithm and; 117 /// then if a matching with the correct option name exists it will be used; 118 /// whenever creating a new minimizer instance.; 119 /// For example for changing the default number of steps of the Genetic minimizer from 100 to 500 do; 120 ///; 121 /// auto extraOpt = ROOT::Math::MinimizerOptions::Default(""Genetic""); 122 /// extraOpts.SetValue(""Steps"",500);; 123 ///; 124 /// and when creating the Genetic minimizer you will have the new value for the option:; 125 ///; 126 /// auto gmin = ROOT::Math::Factory::CreateMinimizer(""Genetic"");; 127 /// gmin->Options().Print();; 128 ///; 129 static void SetDefaultExtraOptions(const IOptions * extraoptions);; 130 ; 131 ; 132 static const std::string & DefaultMinimizerType();; 133 static const std::string & DefaultMinimizerAlgo();; 134 static double DefaultErrorDef();; 135 static double DefaultTolerance();; 136 static double DefaultPrecision();; 137 static int DefaultMaxFunctionCalls();; 138 static int DefaultMaxIterations();; 139 static int DefaultStrategy();; 140 stat",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:7180,Modifiability,config,configured,7180,,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:11726,Modifiability,config,configuration,11726, ; 257 } // end namespace Math; 258 ; 259} // end namespace ROOT; 260 ; 261#endif; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; namechar name[80]Definition TGX11.cxx:110; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::operator=MinimizerOptions & operator=(const MinimizerOptions &opt)assignment operatorsDefinition MinimizerOptions.cxx:151; ROOT::Math::MinimizerOptions::DefaultPrintLevelstatic int DefaultPrintLevel()Definition MinimizerOptions.cxx:97; ROOT::Math::MinimizerOptions::DefaultPrecisionstatic double DefaultPrecision()Definition MinimizerOptions.cxx:93; ROOT::Math::MinimizerOptions::Defaultstatic ROOT::Math::IOptions & Default(const char *name)Retrieve extra options for a given minimizer name.Definition MinimizerOptions.cxx:234; ROOT::,MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/MinimizerOptions_8h_source.html:3447,Testability,log,log-likelihood,3447,"r from GSL based on the ROOT::Math::GSLMinimizer. Available algorithms are:; 66 /// - BFGS2 (default); 67 /// - BFGS; 68 /// - ConjugateFR; 69 /// - ConjugatePR; 70 /// - SteepestDescent; 71 /// - GSLMultiFit Minimizer based on GSL for minimizing only non linear least-squared functions (using an approximation similar to Fumili). See ROOT::Math::GSLMultiFit.; 72 /// - GSLSimAn Simulated annealing minimizer from GSL (see ROOT::Math::GSLSimAnMinimizer). It is a stochastic minimization algorithm using only function values and not the gradient.; 73 /// - Genetic Genetic minimization algorithms (see TMVA::Genetic); 74 ///; 75 static void SetDefaultMinimizer(const char *type, const char *algo = nullptr);; 76 ; 77 /// Set the default level for computing the parameter errors.; 78 /// For example for 1-sigma parameter errors; 79 /// - up = 1 for a chi-squared function; 80 /// - up = 0.5 for a negative log-likelihood function; 81 ///; 82 /// The value will be used also by Minos when computing the confidence interval; 83 static void SetDefaultErrorDef(double up);; 84 ; 85 /// Set the Minimization tolerance.; 86 /// The Default value for Minuit and Minuit2 is 0.01; 87 static void SetDefaultTolerance(double tol);; 88 ; 89 /// Set the default Minimizer precision.; 90 /// (used only by MInuit and Minuit2); 91 /// It is used to specify the numerical precision used for computing the; 92 /// objective function. It should be left to the default value found by the Minimizer; 93 /// (typically double precision); 94 static void SetDefaultPrecision(double prec);; 95 ; 96 /// Set the maximum number of function calls.; 97 static void SetDefaultMaxFunctionCalls(int maxcall);; 98 ; 99 /// Set the maximum number of iterations.; 100 /// Used by the GSL minimizers and Genetic. Not used by Minuit,Minuit2.; 101 static void SetDefaultMaxIterations(int maxiter);; 102 ; 103 /// Set the default strategy.; 104 /// The strategy is a parameter used only by Minuit and Minuit2.; 105 /// Possible values are:",MatchSource.WIKI,doc/master/MinimizerOptions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimizerOptions_8h_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:606,Availability,error,error,606,"h>; 9 ; 10namespace ROOT {; 11namespace Math {; 12 ; 13/** set initial second derivatives; 14 */; 15bool Minimizer::SetCovarianceDiag(std::span<const double> g2, unsigned int n); 16{; 17 MATH_UNUSED(g2);; 18 MATH_UNUSED(n);; 19 return false;; 20}; 21 ; 22/** set initial values for covariance/error matrix; 23 The covariance matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4397,Availability,error,errors,4397,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4808,Availability,error,errors,4808,"ot implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most s",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:5989,Availability,error,error,5989,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6086,Availability,error,errors,6086,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6210,Availability,error,error,6210,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6622,Availability,error,error,6622,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6906,Availability,error,error,6906,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12923,Availability,error,error,12923,"int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar a",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13650,Availability,error,error,13650,"e step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoresrcMinimizer.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:41 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3235,Deployability,release,release,3235,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13492,Deployability,release,release,13492,"e step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoresrcMinimizer.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:41 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4875,Integrability,interface,interface,4875,"xisting variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors ar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:925,Modifiability,variab,variable,925,"h>; 9 ; 10namespace ROOT {; 11namespace Math {; 12 ; 13/** set initial second derivatives; 14 */; 15bool Minimizer::SetCovarianceDiag(std::span<const double> g2, unsigned int n); 16{; 17 MATH_UNUSED(g2);; 18 MATH_UNUSED(n);; 19 return false;; 20}; 21 ; 22/** set initial values for covariance/error matrix; 23 The covariance matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1018,Modifiability,variab,variable,1018,"h>; 9 ; 10namespace ROOT {; 11namespace Math {; 12 ; 13/** set initial second derivatives; 14 */; 15bool Minimizer::SetCovarianceDiag(std::span<const double> g2, unsigned int n); 16{; 17 MATH_UNUSED(g2);; 18 MATH_UNUSED(n);; 19 return false;; 20}; 21 ; 22/** set initial values for covariance/error matrix; 23 The covariance matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1242,Modifiability,variab,variable,1242,"h>; 9 ; 10namespace ROOT {; 11namespace Math {; 12 ; 13/** set initial second derivatives; 14 */; 15bool Minimizer::SetCovarianceDiag(std::span<const double> g2, unsigned int n); 16{; 17 MATH_UNUSED(g2);; 18 MATH_UNUSED(n);; 19 return false;; 20}; 21 ; 22/** set initial values for covariance/error matrix; 23 The covariance matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1417,Modifiability,variab,variable,1417,"imizer::SetCovarianceDiag(std::span<const double> g2, unsigned int n); 16{; 17 MATH_UNUSED(g2);; 18 MATH_UNUSED(n);; 19 return false;; 20}; 21 ; 22/** set initial values for covariance/error matrix; 23 The covariance matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 7",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1630,Modifiability,variab,variable,1630,"nce matrix must be provided in compressed form (row-major ordered upper traingular part); 24*/; 25bool Minimizer::SetCovariance(std::span<const double> cov, unsigned int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1793,Modifiability,variab,variable,1793,"ed int nrow); 26{; 27 MATH_UNUSED(cov);; 28 MATH_UNUSED(nrow);; 29 return false;; 30}; 31 ; 32/// set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:1936,Modifiability,variab,variable,1936,"ide if minimizer supports them ) otherwise as default set an unlimited; 33/// variable; 34bool Minimizer::SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSE",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2093,Modifiability,variab,variable,2093,"ring &name, double val, double step, double lower,; 35 double upper); 36{; 37 MATH_WARN_MSG(""Minimizer::SetLimitedVariable"", ""Setting of limited variable not implemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2253,Modifiability,variab,variable,2253,"plemented - set as unlimited"");; 38 MATH_UNUSED(lower);; 39 MATH_UNUSED(upper);; 40 return SetVariable(ivar, name, val, step);; 41}; 42 ; 43/// set a new fixed variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing var",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2411,Modifiability,variab,variable,2411,"d variable (override if minimizer supports them ); 44bool Minimizer::SetFixedVariable(unsigned int ivar, const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemen",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2575,Modifiability,variab,variable,2575,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2729,Modifiability,variab,variable,2729,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:2893,Modifiability,variab,variable,2893,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3025,Modifiability,variab,variable,3025,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3154,Modifiability,variab,variable,3154,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3255,Modifiability,variab,variable,3255,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3395,Modifiability,variab,variable,3395,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3498,Modifiability,variab,variable,3498,", const std::string &name, double val); 45{; 46 MATH_ERROR_MSG(""Minimizer::SetFixedVariable"", ""Setting of fixed variable not implemented"");; 47 MATH_UNUSED(ivar);; 48 MATH_UNUSED(name);; 49 MATH_UNUSED(val);; 50 return false;; 51}; 52/// set the value of an already existing variable; 53bool Minimizer::SetVariableValue(unsigned int ivar, double value); 54{; 55 MATH_ERROR_MSG(""Minimizer::SetVariableValue"", ""Set of a variable value not implemented"");; 56 MATH_UNUSED(ivar);; 57 MATH_UNUSED(value);; 58 return false;; 59}; 60 ; 61/// set the step size of an already existing variable; 62bool Minimizer::SetVariableStepSize(unsigned int ivar, double value); 63{; 64 MATH_ERROR_MSG(""Minimizer::SetVariableStepSize"", ""Setting an existing variable step size not implemented"");; 65 MATH_UNUSED(ivar);; 66 MATH_UNUSED(value);; 67 return false;; 68}; 69/// set the lower-limit of an already existing variable; 70bool Minimizer::SetVariableLowerLimit(unsigned int ivar, double lower); 71{; 72 MATH_ERROR_MSG(""Minimizer::SetVariableLowerLimit"", ""Setting an existing variable limit not implemented"");; 73 MATH_UNUSED(ivar);; 74 MATH_UNUSED(lower);; 75 return false;; 76}; 77/// set the upper-limit of an already existing variable; 78bool Minimizer::SetVariableUpperLimit(unsigned int ivar, double upper); 79{; 80 MATH_ERROR_MSG(""Minimizer::SetVariableUpperLimit"", ""Setting an existing variable limit not implemented"");; 81 MATH_UNUSED(ivar);; 82 MATH_UNUSED(upper);; 83 return false;; 84}; 85 ; 86/// fix an existing variable; 87bool Minimizer::FixVariable(unsigned int ivar); 88{; 89 MATH_ERROR_MSG(""Minimizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3596,Modifiability,variab,variables,3596,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3759,Modifiability,variab,variable,3759,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3848,Modifiability,variab,variable,3848,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:3871,Modifiability,variab,variable,3871,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4103,Modifiability,variab,variable,4103,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4251,Modifiability,variab,variables,4251,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4283,Modifiability,variab,variable,4283,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4351,Modifiability,variab,variables,4351,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4659,Modifiability,variab,variable,4659,"imizer::FixVariable"", ""Fixing an existing variable not implemented"");; 90 MATH_UNUSED(ivar);; 91 return false;; 92}; 93/// release an existing variable; 94bool Minimizer::ReleaseVariable(unsigned int ivar); 95{; 96 MATH_ERROR_MSG(""Minimizer::ReleaseVariable"", ""Releasing an existing variable not implemented"");; 97 MATH_UNUSED(ivar);; 98 return false;; 99}; 100/// query if an existing variable is fixed (i.e. considered constant in the minimization); 101/// note that by default all variables are not fixed; 102bool Minimizer::IsFixedVariable(unsigned int ivar) const; 103{; 104 MATH_ERROR_MSG(""Minimizer::IsFixedVariable"", ""Querying an existing variable not implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4780,Modifiability,variab,variables,4780,"ot implemented"");; 105 MATH_UNUSED(ivar);; 106 return false;; 107}; 108/// get variable settings in a variable object (like ROOT::Fit::ParamsSettings); 109bool Minimizer::GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) const; 110{; 111 MATH_ERROR_MSG(""Minimizer::GetVariableSettings"", ""Querying an existing variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most s",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:4951,Modifiability,variab,variable,4951,"xisting variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors ar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:5289,Modifiability,variab,variable,5289,"xisting variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors ar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:5336,Modifiability,variab,variables,5336,"xisting variable not implemented"");; 112 MATH_UNUSED(ivar);; 113 MATH_UNUSED(pars);; 114 return false;; 115}; 116/** return covariance matrices element for variables ivar,jvar; 117 if the variable is fixed the return value is zero; 118 The ordering of the variables is the same as in the parameter and errors vectors; 119*/; 120double Minimizer::CovMatrix(unsigned int ivar, unsigned int jvar) const; 121{; 122 MATH_UNUSED(ivar);; 123 MATH_UNUSED(jvar);; 124 return 0;; 125}; 126 ; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors ar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:5584,Modifiability,variab,variable,5584,"; 127/**; 128 Fill the passed array with the covariance matrix elements; 129 if the variable is fixed or const the value is zero.; 130 The array will be filled as cov[i *ndim + j]; 131 The ordering of the variables is the same as in errors and parameter value.; 132 This is different from the direct interface of Minuit2 or TMinuit where the; 133 values were obtained only to variable parameters; 134*/; 135bool Minimizer::GetCovMatrix(double *covMat) const; 136{; 137 MATH_UNUSED(covMat);; 138 return false;; 139}; 140 ; 141/**; 142 Fill the passed array with the Hessian matrix elements; 143 The Hessian matrix is the matrix of the second derivatives; 144 and is the inverse of the covariance matrix; 145 If the variable is fixed or const the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:5999,Modifiability,variab,variable,5999,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6818,Modifiability,variab,variable,6818,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:7876,Modifiability,variab,variables,7876,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:7928,Modifiability,variab,variable,7928,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:7978,Modifiability,variab,variable,7978,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:8178,Modifiability,variab,variable,8178,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:8195,Modifiability,variab,variable,8195,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:8238,Modifiability,variab,variable,8238,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:8387,Modifiability,variab,variable,8387,"187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;; 205}; 206 ; 207/**; 208 find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum; 209 The contour will be find for value of the function = Min + ErrorUp();; 210 */; 211bool Minimizer::Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj); 212{; 213 MATH_ERROR_MSG(""Minimizer::Contour"", ""Contour not implemented"");; 214 MATH_UNUSED(ivar);; 215 MATH_UNUSED(jvar);; 216 MATH_UNUSED(npoints);; 217 MATH_UNUSED(xi);; 218 MATH_UNUSED(xj);; 219 return false;; 220}; 221 ; 222/// get name of variables (override if minimizer support storing of variable names); 223/// return an empty string if variable is not found; 224std::string Minimizer::VariableName(unsigned int ivar) const; 225{; 226 MATH_UNUSED(ivar);; 227 return std::string(); // return empty string; 228}; 229 ; 230/// get index of variable given a variable given a name; 231/// return -1 if variable is not found; 232int Minimizer::VariableIndex(const std::string &name) const; 233{; 234 MATH_ERROR_MSG(""Minimizer::VariableIndex"", ""Getting variable index from name not implemented"");; 235 MATH_UNUSED(name);; 236 return -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:9530,Modifiability,variab,variable,9530,"urn -1;; 237}; 238 ; 239} // namespace Math; 240} // namespace ROOT; Error.h; MATH_ERROR_MSG#define MATH_ERROR_MSG(loc, str)Definition Error.h:83; MATH_WARN_MSG#define MATH_WARN_MSG(loc, str)Definition Error.h:80; Minimizer.h; optionOption_t Option_t optionDefinition TGWin32VirtualXProxy.cxx:44; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; Util.h; MATH_UNUSED#define MATH_UNUSED(var)Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtu",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:9738,Modifiability,variab,variable,9738,"on TGWin32VirtualXProxy.cxx:44; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; Util.h; MATH_UNUSED#define MATH_UNUSED(var)Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:9755,Modifiability,variab,variable,9755,"on TGWin32VirtualXProxy.cxx:44; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; Util.h; MATH_UNUSED#define MATH_UNUSED(var)Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:9790,Modifiability,variab,variable,9790,"on TGWin32VirtualXProxy.cxx:44; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; Util.h; MATH_UNUSED#define MATH_UNUSED(var)Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:9988,Modifiability,variab,variable,9988,"efinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; Util.h; MATH_UNUSED#define MATH_UNUSED(var)Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVari",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:10205,Modifiability,variab,variableDefinition,10205,")Definition Util.h:34; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:10556,Modifiability,variab,variable,10556,"d variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > c",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:10579,Modifiability,variab,variable,10579,"d variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > c",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:10778,Modifiability,variab,variable,10778,"ble given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimiz",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11008,Modifiability,variab,variableDefinition,11008," the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covaria",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11207,Modifiability,variab,variable,11207,"Size(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHess",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11402,Modifiability,variab,variableDefinition,11402,"e > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bo",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11713,Modifiability,variab,variables,11713," GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11765,Modifiability,variab,variable,11765," GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:11974,Modifiability,variab,variables,11974,"leValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12001,Modifiability,variab,variable,12001,"leValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12435,Modifiability,variab,variable,12435,"imizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimi",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12538,Modifiability,variab,variableDefinition,12538,"imizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimi",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12715,Modifiability,variab,variable,12715,"e(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefiniti",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:12933,Modifiability,variab,variable,12933,"int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar a",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13205,Modifiability,variab,variableDefinition,13205,"double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOT",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13354,Modifiability,variab,variable,13354,"inimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoresrcMinimizer.cxx",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13512,Modifiability,variab,variableDefinition,13512,"e step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoresrcMinimizer.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:41 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:6569,Performance,perform,perform,6569,"nst the values for that variables are zero.; 146 The array will be filled as h[i *ndim + j]; 147*/; 148bool Minimizer::GetHessianMatrix(double *hMat) const; 149{; 150 MATH_UNUSED(hMat);; 151 return false;; 152}; 153 ; 154/**; 155 return global correlation coefficient for variable i; 156 This is a number between zero and one which gives; 157 the correlation between the i-th parameter and that linear combination of all; 158 other parameters which is most strongly correlated with i.; 159 Minimizer must overload method if implemented; 160 */; 161double Minimizer::GlobalCC(unsigned int ivar) const; 162{; 163 MATH_UNUSED(ivar);; 164 return -1;; 165}; 166 ; 167/**; 168 minos error for variable i, return false if Minos failed or not supported; 169 and the lower and upper errors are returned in errLow and errUp; 170 An extra flag specifies if only the lower (option=-1) or the upper (option=+1) error calculation is run; 171*/; 172bool Minimizer::GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option); 173{; 174 MATH_ERROR_MSG(""Minimizer::GetMinosError"", ""Minos Error not implemented"");; 175 MATH_UNUSED(ivar);; 176 MATH_UNUSED(errLow);; 177 MATH_UNUSED(errUp);; 178 MATH_UNUSED(option);; 179 return false;; 180}; 181 ; 182/**; 183 perform a full calculation of the Hessian matrix for error calculation; 184 */; 185bool Minimizer::Hesse(); 186{; 187 MATH_ERROR_MSG(""Minimizer::Hesse"", ""Hesse not implemented"");; 188 return false;; 189}; 190 ; 191/**; 192 scan function minimum for variable i. Variable and function must be set before using Scan; 193 Return false if an error or if minimizer does not support this functionality; 194 */; 195bool Minimizer::Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin, double xmax); 196{; 197 MATH_ERROR_MSG(""Minimizer::Scan"", ""Scan not implemented"");; 198 MATH_UNUSED(ivar);; 199 MATH_UNUSED(nstep);; 200 MATH_UNUSED(x);; 201 MATH_UNUSED(y);; 202 MATH_UNUSED(xmin);; 203 MATH_UNUSED(xmax);; 204 return false;;",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8cxx_source.html:13597,Performance,perform,perform,13597,"e step)=0set a new free variable; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoresrcMinimizer.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:41 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8cxx_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:2393,Availability,avail,available,2393,"inimizers and their respective methods (algorithms) that can be instantiated:; 55 The name shown below can be used to create them. More documentation can be found in the respective class; 56 ; 57 - Minuit (class TMinuitMinimizer); 58 - Migrad (default); 59 - MigradImproved (Migrad with adding a method to improve minimization when ends-up in a local minimum, see par. 6.3 of [Minuit tutorial on Function Minimization](https://seal.web.cern.ch/documents/minuit/mntutorial.pdf)); 60 - Simplex; 61 - Minimize (a combination of Simplex + Migrad); 62 - Minimize; 63 - Scan; 64 - Seek; 65 ; 66 - Minuit2 (class ROOT::Minuit2::Minuit2Minimizer); 67 - Migrad (default); 68 - Simplex; 69 - Minimize; 70 - Fumili (Fumili2); 71 - Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunct",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:2595,Availability,avail,available,2595,"inimizers and their respective methods (algorithms) that can be instantiated:; 55 The name shown below can be used to create them. More documentation can be found in the respective class; 56 ; 57 - Minuit (class TMinuitMinimizer); 58 - Migrad (default); 59 - MigradImproved (Migrad with adding a method to improve minimization when ends-up in a local minimum, see par. 6.3 of [Minuit tutorial on Function Minimization](https://seal.web.cern.ch/documents/minuit/mntutorial.pdf)); 60 - Simplex; 61 - Minimize (a combination of Simplex + Migrad); 62 - Minimize; 63 - Scan; 64 - Seek; 65 ; 66 - Minuit2 (class ROOT::Minuit2::Minuit2Minimizer); 67 - Migrad (default); 68 - Simplex; 69 - Minimize; 70 - Fumili (Fumili2); 71 - Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunct",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:2701,Availability,avail,available,2701,"inimizers and their respective methods (algorithms) that can be instantiated:; 55 The name shown below can be used to create them. More documentation can be found in the respective class; 56 ; 57 - Minuit (class TMinuitMinimizer); 58 - Migrad (default); 59 - MigradImproved (Migrad with adding a method to improve minimization when ends-up in a local minimum, see par. 6.3 of [Minuit tutorial on Function Minimization](https://seal.web.cern.ch/documents/minuit/mntutorial.pdf)); 60 - Simplex; 61 - Minimize (a combination of Simplex + Migrad); 62 - Minimize; 63 - Scan; 64 - Seek; 65 ; 66 - Minuit2 (class ROOT::Minuit2::Minuit2Minimizer); 67 - Migrad (default); 68 - Simplex; 69 - Minimize; 70 - Fumili (Fumili2); 71 - Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunct",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:2863,Availability,avail,available,2863,"inimizers and their respective methods (algorithms) that can be instantiated:; 55 The name shown below can be used to create them. More documentation can be found in the respective class; 56 ; 57 - Minuit (class TMinuitMinimizer); 58 - Migrad (default); 59 - MigradImproved (Migrad with adding a method to improve minimization when ends-up in a local minimum, see par. 6.3 of [Minuit tutorial on Function Minimization](https://seal.web.cern.ch/documents/minuit/mntutorial.pdf)); 60 - Simplex; 61 - Minimize (a combination of Simplex + Migrad); 62 - Minimize; 63 - Scan; 64 - Seek; 65 ; 66 - Minuit2 (class ROOT::Minuit2::Minuit2Minimizer); 67 - Migrad (default); 68 - Simplex; 69 - Minimize; 70 - Fumili (Fumili2); 71 - Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunct",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4797,Availability,avail,available,4797,"nctions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:5001,Availability,error,error,5001,"ctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:7178,Availability,error,error,7178,,MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:11141,Availability,error,error,11141," { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nst",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:11151,Availability,error,error,11151," { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nst",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:11244,Availability,error,errors,11244," { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nst",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:13456,Availability,toler,tolerance,13456,"; 274 double xmin = 0, double xmax = 0);; 275 virtual bool Contour(unsigned int ivar , unsigned int jvar, unsigned int & npoints,; 276 double * xi , double * xj );; 277 ; 278 /// return reference to the objective function; 279 ///virtual const ROOT::Math::IGenFunction & Function() const = 0;; 280 ; 281 /// print the result according to set level (implemented for TMinuit for maintaining Minuit-style printing); 282 virtual void PrintResults() {}; 283 ; 284 virtual std::string VariableName(unsigned int ivar) const;; 285 ; 286 virtual int VariableIndex(const std::string & name) const;; 287 ; 288 /** minimizer configuration parameters **/; 289 ; 290 /// set print level; 291 int PrintLevel() const { return fOptions.PrintLevel(); }; 292 ; 293 /// max number of function calls; 294 unsigned int MaxFunctionCalls() const { return fOptions.MaxFunctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// precision of minimizer in the evaluation of the objective function; 303 /// ( a value <=0 corresponds to the let the minimizer choose its default one); 304 double Precision() const { return fOptions.Precision(); }; 305 ; 306 /// strategy; 307 int Strategy() const { return fOptions.Strategy(); }; 308 ; 309 /// status code of minimizer; 310 int Status() const { return fStatus; }; 311 ; 312 /// status code of Minos (to be re-implemented by the minimizers supporting Minos); 313 virtual int MinosStatus() const { return -1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:14137,Availability,error,error,14137,VariableIndex(const std::string & name) const;; 287 ; 288 /** minimizer configuration parameters **/; 289 ; 290 /// set print level; 291 int PrintLevel() const { return fOptions.PrintLevel(); }; 292 ; 293 /// max number of function calls; 294 unsigned int MaxFunctionCalls() const { return fOptions.MaxFunctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// precision of minimizer in the evaluation of the objective function; 303 /// ( a value <=0 corresponds to the let the minimizer choose its default one); 304 double Precision() const { return fOptions.Precision(); }; 305 ; 306 /// strategy; 307 int Strategy() const { return fOptions.Strategy(); }; 308 ; 309 /// status code of minimizer; 310 int Status() const { return fStatus; }; 311 ; 312 /// status code of Minos (to be re-implemented by the minimizers supporting Minos); 313 virtual int MinosStatus() const { return -1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the minimizer options (implement derived class if needed); 323 virtual MinimizerOptions Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOp,MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:14337,Availability,error,error,14337,"nctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// precision of minimizer in the evaluation of the objective function; 303 /// ( a value <=0 corresponds to the let the minimizer choose its default one); 304 double Precision() const { return fOptions.Precision(); }; 305 ; 306 /// strategy; 307 int Strategy() const { return fOptions.Strategy(); }; 308 ; 309 /// status code of minimizer; 310 int Status() const { return fStatus; }; 311 ; 312 /// status code of Minos (to be re-implemented by the minimizers supporting Minos); 313 virtual int MinosStatus() const { return -1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the minimizer options (implement derived class if needed); 323 virtual MinimizerOptions Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:15062,Availability,toler,tolerance,15062,"-1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the minimizer options (implement derived class if needed); 323 virtual MinimizerOptions Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i.e. default case); 341 void SetPrecision(double prec) { fOptions.SetPrecision(prec); }; 342 ; 343 ///set the strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:15553,Availability,error,errors,15553,"Options Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i.e. default case); 341 void SetPrecision(double prec) { fOptions.SetPrecision(prec); }; 342 ; 343 ///set the strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:15691,Availability,error,error,15691,"evel) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i.e. default case); 341 void SetPrecision(double prec) { fOptions.SetPrecision(prec); }; 342 ; 343 ///set the strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 } // end namespace Math; 375 ; 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunctio",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:16354,Availability,error,errors,16354,"e strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 } // end namespace Math; 375 ; 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IB",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:18369,Availability,toler,toleranceDefinition,18369, also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IBaseFunctionMultiDimTemplDocumentation for the abstract class IBaseFunctionMultiDim.Definition IFunction.h:61; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition MinimizerOptions.h:216; ROOT::Math::MinimizerOptions::Strategyint Strategy() conststrategyDefinition MinimizerOptions.h:183; ROOT::Math::MinimizerOptions::Tolerancedouble Tolerance() constabsolute toleranceDefinition MinimizerOptions.h:186; ROOT::Math::MinimizerOptions::Precisiondouble Precision() constprecision in the objective function calculation (value <=0 means left to default)Definition MinimizerOptions.h:189; ROOT::Math::MinimizerOptions::ErrorDefdouble ErrorDef() consterror definitionDefinition MinimizerOptions.h:192; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)set extra options (in this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptio,MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:19638,Availability,error,error,19638,"ions::ErrorDefdouble ErrorDef() consterror definitionDefinition MinimizerOptions.h:192; ROOT::Math::MinimizerOptions::SetExtraOptionsvoid SetExtraOptions(const IOptions &opt)set extra options (in this case pointer is cloned)Definition MinimizerOptions.cxx:210; ROOT::Math::MinimizerOptions::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:19883,Availability,toler,toleranceDefinition,19883,"erOptions.cxx:210; ROOT::Math::MinimizerOptions::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFu",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20435,Availability,toler,toleranceDefinition,20435,"ions.cxx:174; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; RO",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20545,Availability,error,errors,20545,"ionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:22905,Availability,error,errorsDefinition,22905,"on< bool(std::span< const double >, double *)>)set the function implementing Hessian computation (re-implemented by Minimizer using it)Definition Minimizer.h:142; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer t",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23295,Availability,error,error,23295,"nst double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:24921,Availability,error,errors,24921,"25; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefi",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:25448,Availability,toler,toleranceDefinition,25448,"l bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer:",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:28110,Availability,error,error,28110,"h the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0..",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:28120,Availability,error,error,28120,"h the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0..",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:29047,Availability,error,error,29047,"mizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROO",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:29766,Availability,error,error,29766,"ned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neede",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:30138,Availability,error,error,30138,"OT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:32246,Availability,error,error,32246,"tion virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimizer.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:13076,Deployability,configurat,configuration,13076,"eturn status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nstep , double * x , double * y ,; 274 double xmin = 0, double xmax = 0);; 275 virtual bool Contour(unsigned int ivar , unsigned int jvar, unsigned int & npoints,; 276 double * xi , double * xj );; 277 ; 278 /// return reference to the objective function; 279 ///virtual const ROOT::Math::IGenFunction & Function() const = 0;; 280 ; 281 /// print the result according to set level (implemented for TMinuit for maintaining Minuit-style printing); 282 virtual void PrintResults() {}; 283 ; 284 virtual std::string VariableName(unsigned int ivar) const;; 285 ; 286 virtual int VariableIndex(const std::string & name) const;; 287 ; 288 /** minimizer configuration parameters **/; 289 ; 290 /// set print level; 291 int PrintLevel() const { return fOptions.PrintLevel(); }; 292 ; 293 /// max number of function calls; 294 unsigned int MaxFunctionCalls() const { return fOptions.MaxFunctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// p",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:17576,Deployability,configurat,configuration,17576," 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IBaseFunctionMultiDimTemplDocumentation for the abstract class IBaseFunctionMultiDim.Definition IFunction.h:61; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition MinimizerOptions.h:216; ROOT::Math::MinimizerOptions::Strategyint Strategy() conststrategyDefinition MinimizerOptions.h:183; ROOT::Math::MinimizerOptions::Tolerancedouble Tolerance() constabsolute toleranceDefinition MinimizerOptions.h:186; ROOT::Math::MinimizerOptions::Precisiondouble Precision() constprecision in the objective function calculation (value <=0 means left to default)Definition Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31050,Deployability,configurat,configuration,31050," Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minim",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:32088,Deployability,release,release,32088,"arametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:1272,Integrability,interface,interface,1272,"Math/Minimizer.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Minimizer.h. Go to the documentation of this file. 1// @(#)root/mathcore:$Id$; 2// Author: L. Moneta Fri Sep 22 15:06:47 2006; 3 ; 4/**********************************************************************; 5 * *; 6 * Copyright (c) 2006 LCG ROOT Math Team, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// Header file for class Minimizer; 12 ; 13#ifndef ROOT_Math_Minimizer; 14#define ROOT_Math_Minimizer; 15 ; 16#include ""Math/IFunction.h""; 17#include ""Math/MinimizerOptions.h""; 18 ; 19#include <ROOT/RSpan.hxx>; 20 ; 21#include <string>; 22#include <limits>; 23#include <cmath>; 24#include <vector>; 25#include <functional>; 26 ; 27 ; 28 ; 29namespace ROOT {; 30 ; 31 namespace Fit {; 32 class ParameterSettings;; 33 }; 34 ; 35 ; 36 namespace Math {; 37 ; 38/**; 39 @defgroup MultiMin Multi-dimensional Minimization; 40 @ingroup NumAlgo; 41 ; 42 Classes implementing algorithms for multi-dimensional minimization; 43 */; 44 ; 45 ; 46 ; 47//_______________________________________________________________________________; 48/**; 49 Abstract Minimizer class, defining the interface for the various minimizer; 50 (like Minuit2, Minuit, GSL, etc..) in ROOT.; 51 Plug-in's exist in ROOT to be able to instantiate the derived classes without linking the library; 52 using the static function ROOT::Math::Factory::CreateMinimizer.; 53 ; 54 Here is the list of all possible minimizers and their respective methods (algorithms) that can be instantiated:; 55 The name shown below can be used to create them. More documentation can be found in the respective class; 56 ; 57 - Minuit (class TMinuitMinimizer); 58 - Migrad (default); 59 - MigradImproved (Migrad with adding a method to improve minimization when ends-up in a local minimum, see par. 6.3 of [Minuit tutorial on Function Minimization](https://seal.web.cern.ch/documents/min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3191,Integrability,interface,interface,3191,"Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3389,Integrability,interface,interface,3389,"Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3533,Integrability,interface,interface,3533,"ePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3690,Integrability,interface,interface,3690,"available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3815,Integrability,wrap,wrap,3815,"mizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter on",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3857,Integrability,interface,interface,3857,"mizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter on",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3976,Integrability,wrap,wrapping,3976,"ead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( mi",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4272,Integrability,interface,interface,4272,"ation including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutoria",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4294,Integrability,interface,interface,4294,"00 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// D",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4490,Integrability,interface,interface,4490,"imensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usuall",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4906,Integrability,interface,interface,4906,"ctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:5042,Integrability,interface,interface,5042,"Fit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func) = 0;; 140 ; 141 /// set the function implementing Hessian computation (re-implemented by Minimizer using it); 142 virtua",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:5200,Integrability,interface,interface,5200,"given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func) = 0;; 140 ; 141 /// set the function implementing Hessian computation (re-implemented by Minimizer using it); 142 virtual void SetHessianFunction(std::function<bool(std::span<const double>, double *)> ) {}; 143 ; 144 /// add varia",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:7184,Integrability,message,message,7184,,MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:17553,Integrability,interface,interface,17553," 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IBaseFunctionMultiDimTemplDocumentation for the abstract class IBaseFunctionMultiDim.Definition IFunction.h:61; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition MinimizerOptions.h:216; ROOT::Math::MinimizerOptions::Strategyint Strategy() conststrategyDefinition MinimizerOptions.h:183; ROOT::Math::MinimizerOptions::Tolerancedouble Tolerance() constabsolute toleranceDefinition MinimizerOptions.h:186; ROOT::Math::MinimizerOptions::Precisiondouble Precision() constprecision in the objective function calculation (value <=0 means left to default)Definition Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:19987,Integrability,interface,interface,19987,"MinimizerOptions.h:180; ROOT::Math::MinimizerOptions::SetPrecisionvoid SetPrecision(double prec)set the precisionDefinition MinimizerOptions.h:222; ROOT::Math::MinimizerOptions::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bo",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:4352,Modifiability,variab,variables,4352,"00 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// D",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:6188,Modifiability,variab,variables,6188,"t look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func) = 0;; 140 ; 141 /// set the function implementing Hessian computation (re-implemented by Minimizer using it); 142 virtual void SetHessianFunction(std::function<bool(std::span<const double>, double *)> ) {}; 143 ; 144 /// add variables . Return number of variables successfully added; 145 template<class VariableIterator>; 146 int SetVariables(const VariableIterator & begin, const VariableIterator & end) {; 147 unsigned int ivar = 0;; 148 for ( VariableIterator vitr = begin; vitr != end; ++vitr) {; 149 bool iret = false;; 150 if (vitr->IsFixed() ); 151 iret = SetFixedVariable(ivar, vitr->Name(), vitr->Value() );; 152 else if (vitr->IsDoubleBound() ); 153 iret = SetLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->LowerLimit(), vitr->UpperLimit() );; 154 else if (vitr->HasLowerLimit() ); 155 iret = SetLowerLimitedVariable(ivar, vitr->N",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:6217,Modifiability,variab,variables,6217,,MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:7323,Modifiability,variab,variable,7323,"r & begin, const VariableIterator & end) {; 147 unsigned int ivar = 0;; 148 for ( VariableIterator vitr = begin; vitr != end; ++vitr) {; 149 bool iret = false;; 150 if (vitr->IsFixed() ); 151 iret = SetFixedVariable(ivar, vitr->Name(), vitr->Value() );; 152 else if (vitr->IsDoubleBound() ); 153 iret = SetLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->LowerLimit(), vitr->UpperLimit() );; 154 else if (vitr->HasLowerLimit() ); 155 iret = SetLowerLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->LowerLimit() );; 156 else if (vitr->HasUpperLimit() ); 157 iret = SetUpperLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->UpperLimit() );; 158 else; 159 iret = SetVariable( ivar, vitr->Name(), vitr->Value(), vitr->StepSize() );; 160 ; 161 if (iret) ivar++;; 162 ; 163 // an error message should be eventually be reported in the virtual single SetVariable methods; 164 }; 165 return ivar;; 166 }; 167 /// set a new free variable; 168 virtual bool SetVariable(unsigned int ivar, const std::string & name, double val, double step) = 0;; 169 /// set initial second derivatives; 170 virtual bool SetCovarianceDiag(std::span<const double> d2, unsigned int n);; 171 /// set initial covariance matrix; 172 virtual bool SetCovariance(std::span<const double> cov, unsigned int nrow);; 173 ; 174 /// set a new lower limit variable (override if minimizer supports them ); 175 virtual bool SetLowerLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double lower ) {; 176 return SetLimitedVariable(ivar, name, val, step, lower, std::numeric_limits<double>::infinity() );; 177 }; 178 /// set a new upper limit variable (override if minimizer supports them ); 179 virtual bool SetUpperLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double upper ) {; 180 return SetLimitedVariable(ivar, name, val, step, - std::numeric_limits<double>::infinity(), upper );; 18",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:7715,Modifiability,variab,variable,7715,"), vitr->UpperLimit() );; 154 else if (vitr->HasLowerLimit() ); 155 iret = SetLowerLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->LowerLimit() );; 156 else if (vitr->HasUpperLimit() ); 157 iret = SetUpperLimitedVariable(ivar, vitr->Name(), vitr->Value(), vitr->StepSize(), vitr->UpperLimit() );; 158 else; 159 iret = SetVariable( ivar, vitr->Name(), vitr->Value(), vitr->StepSize() );; 160 ; 161 if (iret) ivar++;; 162 ; 163 // an error message should be eventually be reported in the virtual single SetVariable methods; 164 }; 165 return ivar;; 166 }; 167 /// set a new free variable; 168 virtual bool SetVariable(unsigned int ivar, const std::string & name, double val, double step) = 0;; 169 /// set initial second derivatives; 170 virtual bool SetCovarianceDiag(std::span<const double> d2, unsigned int n);; 171 /// set initial covariance matrix; 172 virtual bool SetCovariance(std::span<const double> cov, unsigned int nrow);; 173 ; 174 /// set a new lower limit variable (override if minimizer supports them ); 175 virtual bool SetLowerLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double lower ) {; 176 return SetLimitedVariable(ivar, name, val, step, lower, std::numeric_limits<double>::infinity() );; 177 }; 178 /// set a new upper limit variable (override if minimizer supports them ); 179 virtual bool SetUpperLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double upper ) {; 180 return SetLimitedVariable(ivar, name, val, step, - std::numeric_limits<double>::infinity(), upper );; 181 }; 182 virtual bool SetLimitedVariable(unsigned int ivar , const std::string & name , double val , double step ,; 183 double lower , double upper );; 184 virtual bool SetFixedVariable(unsigned int ivar , const std::string & name , double val );; 185 virtual bool SetVariableValue(unsigned int ivar , double value);; 186 /// set the values of all existing variables (array must be dimensione",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:8038,Modifiability,variab,variable,8038," 158 else; 159 iret = SetVariable( ivar, vitr->Name(), vitr->Value(), vitr->StepSize() );; 160 ; 161 if (iret) ivar++;; 162 ; 163 // an error message should be eventually be reported in the virtual single SetVariable methods; 164 }; 165 return ivar;; 166 }; 167 /// set a new free variable; 168 virtual bool SetVariable(unsigned int ivar, const std::string & name, double val, double step) = 0;; 169 /// set initial second derivatives; 170 virtual bool SetCovarianceDiag(std::span<const double> d2, unsigned int n);; 171 /// set initial covariance matrix; 172 virtual bool SetCovariance(std::span<const double> cov, unsigned int nrow);; 173 ; 174 /// set a new lower limit variable (override if minimizer supports them ); 175 virtual bool SetLowerLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double lower ) {; 176 return SetLimitedVariable(ivar, name, val, step, lower, std::numeric_limits<double>::infinity() );; 177 }; 178 /// set a new upper limit variable (override if minimizer supports them ); 179 virtual bool SetUpperLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double upper ) {; 180 return SetLimitedVariable(ivar, name, val, step, - std::numeric_limits<double>::infinity(), upper );; 181 }; 182 virtual bool SetLimitedVariable(unsigned int ivar , const std::string & name , double val , double step ,; 183 double lower , double upper );; 184 virtual bool SetFixedVariable(unsigned int ivar , const std::string & name , double val );; 185 virtual bool SetVariableValue(unsigned int ivar , double value);; 186 /// set the values of all existing variables (array must be dimensioned to the size of the existing parameters); 187 virtual bool SetVariableValues(const double * x) {; 188 bool ret = true;; 189 unsigned int i = 0;; 190 while ( i <= NDim() && ret) {; 191 ret &= SetVariableValue(i,x[i] ); i++;; 192 }; 193 return ret;; 194 }; 195 virtual bool SetVariableStepSize(unsigned int ivar, double valu",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:8685,Modifiability,variab,variables,8685,"// set a new lower limit variable (override if minimizer supports them ); 175 virtual bool SetLowerLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double lower ) {; 176 return SetLimitedVariable(ivar, name, val, step, lower, std::numeric_limits<double>::infinity() );; 177 }; 178 /// set a new upper limit variable (override if minimizer supports them ); 179 virtual bool SetUpperLimitedVariable(unsigned int ivar , const std::string & name , double val , double step , double upper ) {; 180 return SetLimitedVariable(ivar, name, val, step, - std::numeric_limits<double>::infinity(), upper );; 181 }; 182 virtual bool SetLimitedVariable(unsigned int ivar , const std::string & name , double val , double step ,; 183 double lower , double upper );; 184 virtual bool SetFixedVariable(unsigned int ivar , const std::string & name , double val );; 185 virtual bool SetVariableValue(unsigned int ivar , double value);; 186 /// set the values of all existing variables (array must be dimensioned to the size of the existing parameters); 187 virtual bool SetVariableValues(const double * x) {; 188 bool ret = true;; 189 unsigned int i = 0;; 190 while ( i <= NDim() && ret) {; 191 ret &= SetVariableValue(i,x[i] ); i++;; 192 }; 193 return ret;; 194 }; 195 virtual bool SetVariableStepSize(unsigned int ivar, double value );; 196 virtual bool SetVariableLowerLimit(unsigned int ivar, double lower);; 197 virtual bool SetVariableUpperLimit(unsigned int ivar, double upper);; 198 /// set the limits of an already existing variable; 199 virtual bool SetVariableLimits(unsigned int ivar, double lower, double upper) {; 200 return SetVariableLowerLimit(ivar,lower) && SetVariableUpperLimit(ivar,upper);; 201 }; 202 virtual bool FixVariable(unsigned int ivar);; 203 virtual bool ReleaseVariable(unsigned int ivar);; 204 virtual bool IsFixedVariable(unsigned int ivar) const;; 205 virtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings & pars) cons",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:9243,Modifiability,variab,variable,9243,"le(ivar, name, val, step, - std::numeric_limits<double>::infinity(), upper );; 181 }; 182 virtual bool SetLimitedVariable(unsigned int ivar , const std::string & name , double val , double step ,; 183 double lower , double upper );; 184 virtual bool SetFixedVariable(unsigned int ivar , const std::string & name , double val );; 185 virtual bool SetVariableValue(unsigned int ivar , double value);; 186 /// set the values of all existing variables (array must be dimensioned to the size of the existing parameters); 187 virtual bool SetVariableValues(const double * x) {; 188 bool ret = true;; 189 unsigned int i = 0;; 190 while ( i <= NDim() && ret) {; 191 ret &= SetVariableValue(i,x[i] ); i++;; 192 }; 193 return ret;; 194 }; 195 virtual bool SetVariableStepSize(unsigned int ivar, double value );; 196 virtual bool SetVariableLowerLimit(unsigned int ivar, double lower);; 197 virtual bool SetVariableUpperLimit(unsigned int ivar, double upper);; 198 /// set the limits of an already existing variable; 199 virtual bool SetVariableLimits(unsigned int ivar, double lower, double upper) {; 200 return SetVariableLowerLimit(ivar,lower) && SetVariableUpperLimit(ivar,upper);; 201 }; 202 virtual bool FixVariable(unsigned int ivar);; 203 virtual bool ReleaseVariable(unsigned int ivar);; 204 virtual bool IsFixedVariable(unsigned int ivar) const;; 205 virtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings & pars) const;; 206 ; 207 /// set the initial range of an existing variable; 208 virtual bool SetVariableInitialRange(unsigned int /* ivar */, double /* mininitial */, double /* maxinitial */) {; 209 return false;; 210 }; 211 ; 212 /// method to perform the minimization; 213 virtual bool Minimize() = 0;; 214 ; 215 /// return minimum function value; 216 virtual double MinValue() const = 0;; 217 ; 218 /// return pointer to X values at the minimum; 219 virtual const double * X() const = 0;; 220 ; 221 /// return expected distance reached from the minimum (re-implement",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:9745,Modifiability,variab,variable,9745,"size of the existing parameters); 187 virtual bool SetVariableValues(const double * x) {; 188 bool ret = true;; 189 unsigned int i = 0;; 190 while ( i <= NDim() && ret) {; 191 ret &= SetVariableValue(i,x[i] ); i++;; 192 }; 193 return ret;; 194 }; 195 virtual bool SetVariableStepSize(unsigned int ivar, double value );; 196 virtual bool SetVariableLowerLimit(unsigned int ivar, double lower);; 197 virtual bool SetVariableUpperLimit(unsigned int ivar, double upper);; 198 /// set the limits of an already existing variable; 199 virtual bool SetVariableLimits(unsigned int ivar, double lower, double upper) {; 200 return SetVariableLowerLimit(ivar,lower) && SetVariableUpperLimit(ivar,upper);; 201 }; 202 virtual bool FixVariable(unsigned int ivar);; 203 virtual bool ReleaseVariable(unsigned int ivar);; 204 virtual bool IsFixedVariable(unsigned int ivar) const;; 205 virtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings & pars) const;; 206 ; 207 /// set the initial range of an existing variable; 208 virtual bool SetVariableInitialRange(unsigned int /* ivar */, double /* mininitial */, double /* maxinitial */) {; 209 return false;; 210 }; 211 ; 212 /// method to perform the minimization; 213 virtual bool Minimize() = 0;; 214 ; 215 /// return minimum function value; 216 virtual double MinValue() const = 0;; 217 ; 218 /// return pointer to X values at the minimum; 219 virtual const double * X() const = 0;; 220 ; 221 /// return expected distance reached from the minimum (re-implement if minimizer provides it; 222 virtual double Edm() const { return -1; }; 223 ; 224 /// return pointer to gradient values at the minimum; 225 virtual const double * MinGradient() const { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:10775,Modifiability,variab,variables,10775,"/, double /* maxinitial */) {; 209 return false;; 210 }; 211 ; 212 /// method to perform the minimization; 213 virtual bool Minimize() = 0;; 214 ; 215 /// return minimum function value; 216 virtual double MinValue() const = 0;; 217 ; 218 /// return pointer to X values at the minimum; 219 virtual const double * X() const = 0;; 220 ; 221 /// return expected distance reached from the minimum (re-implement if minimizer provides it; 222 virtual double Edm() const { return -1; }; 223 ; 224 /// return pointer to gradient values at the minimum; 225 virtual const double * MinGradient() const { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:10884,Modifiability,variab,variables,10884,"/, double /* maxinitial */) {; 209 return false;; 210 }; 211 ; 212 /// method to perform the minimization; 213 virtual bool Minimize() = 0;; 214 ; 215 /// return minimum function value; 216 virtual double MinValue() const = 0;; 217 ; 218 /// return pointer to X values at the minimum; 219 virtual const double * X() const = 0;; 220 ; 221 /// return expected distance reached from the minimum (re-implement if minimizer provides it; 222 virtual double Edm() const { return -1; }; 223 ; 224 /// return pointer to gradient values at the minimum; 225 virtual const double * MinGradient() const { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:11891,Modifiability,variab,variable,11891," { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().NDim() which is the total; 234 /// number of variables (free+ constrained ones); 235 virtual unsigned int NDim() const = 0;; 236 ; 237 /// number of free variables (real dimension of the problem); 238 /// this is <= Function().NDim() which is the total; 239 /// (re-implement if minimizer supports bounded parameters); 240 virtual unsigned int NFree() const { return NDim(); }; 241 ; 242 /// minimizer provides error and error matrix; 243 virtual bool ProvidesError() const { return false; }; 244 ; 245 /// return errors at the minimum; 246 virtual const double * Errors() const { return nullptr; }; 247 ; 248 virtual double CovMatrix(unsigned int ivar , unsigned int jvar ) const;; 249 virtual bool GetCovMatrix(double * covMat) const;; 250 virtual bool GetHessianMatrix(double * hMat) const;; 251 ; 252 ; 253 ///return status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nst",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:11921,Modifiability,variab,variable,11921,"eturn status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nstep , double * x , double * y ,; 274 double xmin = 0, double xmax = 0);; 275 virtual bool Contour(unsigned int ivar , unsigned int jvar, unsigned int & npoints,; 276 double * xi , double * xj );; 277 ; 278 /// return reference to the objective function; 279 ///virtual const ROOT::Math::IGenFunction & Function() const = 0;; 280 ; 281 /// print the result according to set level (implemented for TMinuit for maintaining Minuit-style printing); 282 virtual void PrintResults() {}; 283 ; 284 virtual std::string VariableName(unsigned int ivar) const;; 285 ; 286 virtual int VariableIndex(const std::string & name) const;; 287 ; 288 /** minimizer configuration parameters **/; 289 ; 290 /// set print level; 291 int PrintLevel() const { return fOptions.PrintLevel(); }; 292 ; 293 /// max number of function calls; 294 unsigned int MaxFunctionCalls() const { return fOptions.MaxFunctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// p",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:13076,Modifiability,config,configuration,13076,"eturn status of covariance matrix; 254 /// using Minuit convention {0 not calculated 1 approximated 2 made pos def , 3 accurate}; 255 /// Minimizer who implements covariance matrix calculation will re-implement the method; 256 virtual int CovMatrixStatus() const {; 257 return 0;; 258 }; 259 ; 260 /**; 261 return correlation coefficient between variable i and j.; 262 If the variable is fixed or const the return value is zero; 263 */; 264 virtual double Correlation(unsigned int i, unsigned int j ) const {; 265 double tmp = CovMatrix(i,i) * CovMatrix(j,j);; 266 return ( tmp < 0) ? 0 : CovMatrix(i,j) / std::sqrt( tmp );; 267 }; 268 ; 269 virtual double GlobalCC(unsigned int ivar) const;; 270 ; 271 virtual bool GetMinosError(unsigned int ivar , double & errLow, double & errUp, int option = 0);; 272 virtual bool Hesse();; 273 virtual bool Scan(unsigned int ivar , unsigned int & nstep , double * x , double * y ,; 274 double xmin = 0, double xmax = 0);; 275 virtual bool Contour(unsigned int ivar , unsigned int jvar, unsigned int & npoints,; 276 double * xi , double * xj );; 277 ; 278 /// return reference to the objective function; 279 ///virtual const ROOT::Math::IGenFunction & Function() const = 0;; 280 ; 281 /// print the result according to set level (implemented for TMinuit for maintaining Minuit-style printing); 282 virtual void PrintResults() {}; 283 ; 284 virtual std::string VariableName(unsigned int ivar) const;; 285 ; 286 virtual int VariableIndex(const std::string & name) const;; 287 ; 288 /** minimizer configuration parameters **/; 289 ; 290 /// set print level; 291 int PrintLevel() const { return fOptions.PrintLevel(); }; 292 ; 293 /// max number of function calls; 294 unsigned int MaxFunctionCalls() const { return fOptions.MaxFunctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// p",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:17576,Modifiability,config,configuration,17576," 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IBaseFunctionMultiDimTemplDocumentation for the abstract class IBaseFunctionMultiDim.Definition IFunction.h:61; ROOT::Math::IOptionsGeneric interface for defining configuration options of a numerical algorithm.Definition IOptions.h:28; ROOT::Math::MinimizerOptionsMinimizer options.Definition MinimizerOptions.h:40; ROOT::Math::MinimizerOptions::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition MinimizerOptions.h:213; ROOT::Math::MinimizerOptions::SetStrategyvoid SetStrategy(int stra)set the strategyDefinition MinimizerOptions.h:225; ROOT::Math::MinimizerOptions::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition MinimizerOptions.h:216; ROOT::Math::MinimizerOptions::Strategyint Strategy() conststrategyDefinition MinimizerOptions.h:183; ROOT::Math::MinimizerOptions::Tolerancedouble Tolerance() constabsolute toleranceDefinition MinimizerOptions.h:186; ROOT::Math::MinimizerOptions::Precisiondouble Precision() constprecision in the objective function calculation (value <=0 means left to default)Definition Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20267,Modifiability,variab,variable,20267,"ls() constmax number of function callsDefinition MinimizerOptions.h:177; ROOT::Math::MinimizerOptions::ResetToDefaultOptionsvoid ResetToDefaultOptions()non-static methods for setting optionsDefinition MinimizerOptions.cxx:174; ROOT::Math::MinimizerOptions::PrintLevelint PrintLevel() constnon-static methods for retrieving optionsDefinition MinimizerOptions.h:174; ROOT::Math::MinimizerOptions::SetErrorDefvoid SetErrorDef(double err)set error defDefinition MinimizerOptions.h:228; ROOT::Math::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirt",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20699,Modifiability,variab,variable,20699,"th::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluat",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20716,Modifiability,variab,variable,20716,"th::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluat",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:20751,Modifiability,variab,variable,20751,"th::MinimizerOptions::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition MinimizerOptions.h:210; ROOT::Math::MinimizerOptions::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition MinimizerOptions.h:219; ROOT::Math::MinimizerAbstract Minimizer class, defining the interface for the various minimizer (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluat",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:21081,Modifiability,variab,variable,21081," (like Minuit2,...Definition Minimizer.h:119; ROOT::Math::Minimizer::SetLimitedVariablevirtual bool SetLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower, double upper)set a new upper/lower limited variable (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluation of the objective function ( a value <=0 corresponds to the l...Definition Minimizer.h:304; ROOT::Math::Minimizer::SetHessianFunctionvirtual void SetHessianFunction(std::function< bool(std::span< const double >, double *)>)set the function implementing Hessian computation (re-implemented by Minimizer using it)Definition Minimize",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:21337,Modifiability,variab,variable,21337,"le (override if minimizer supports them ) otherwise as default se...Definition Minimizer.cxx:34; ROOT::Math::Minimizer::Tolerancedouble Tolerance() constabsolute toleranceDefinition Minimizer.h:300; ROOT::Math::Minimizer::Errorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluation of the objective function ( a value <=0 corresponds to the l...Definition Minimizer.h:304; ROOT::Math::Minimizer::SetHessianFunctionvirtual void SetHessianFunction(std::function< bool(std::span< const double >, double *)>)set the function implementing Hessian computation (re-implemented by Minimizer using it)Definition Minimizer.h:142; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:21560,Modifiability,variab,variableDefinition,21560,"rorsvirtual const double * Errors() constreturn errors at the minimumDefinition Minimizer.h:246; ROOT::Math::Minimizer::VariableIndexvirtual int VariableIndex(const std::string &name) constget index of variable given a variable given a name return -1 if variable is not foundDefinition Minimizer.cxx:232; ROOT::Math::Minimizer::MaxFunctionCallsunsigned int MaxFunctionCalls() constmax number of function callsDefinition Minimizer.h:294; ROOT::Math::Minimizer::GetCovMatrixvirtual bool GetCovMatrix(double *covMat) constFill the passed array with the covariance matrix elements if the variable is fixed or const the value...Definition Minimizer.cxx:135; ROOT::Math::Minimizer::SetLowerLimitedVariablevirtual bool SetLowerLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double lower)set a new lower limit variable (override if minimizer supports them )Definition Minimizer.h:175; ROOT::Math::Minimizer::SetVariableStepSizevirtual bool SetVariableStepSize(unsigned int ivar, double value)set the step size of an already existing variableDefinition Minimizer.cxx:62; ROOT::Math::Minimizer::Precisiondouble Precision() constprecision of minimizer in the evaluation of the objective function ( a value <=0 corresponds to the l...Definition Minimizer.h:304; ROOT::Math::Minimizer::SetHessianFunctionvirtual void SetHessianFunction(std::function< bool(std::span< const double >, double *)>)set the function implementing Hessian computation (re-implemented by Minimizer using it)Definition Minimizer.h:142; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid S",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:22778,Modifiability,variab,variableDefinition,22778,"ction ( a value <=0 corresponds to the l...Definition Minimizer.h:304; ROOT::Math::Minimizer::SetHessianFunctionvirtual void SetHessianFunction(std::function< bool(std::span< const double >, double *)>)set the function implementing Hessian computation (re-implemented by Minimizer using it)Definition Minimizer.h:142; ROOT::Math::Minimizer::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number b",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23078,Modifiability,variab,variable,23078,"er::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimi",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23101,Modifiability,variab,variable,23101,"er::SetCovarianceDiagvirtual bool SetCovarianceDiag(std::span< const double > d2, unsigned int n)set initial second derivativesDefinition Minimizer.cxx:15; ROOT::Math::Minimizer::Xvirtual const double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimi",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23480,Modifiability,variab,variables,23480,"Definition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23509,Modifiability,variab,variables,23509,"t maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23686,Modifiability,variab,variable,23686,"SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterati",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:24220,Modifiability,variab,variableDefinition,24220,"nimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Defin",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:24552,Modifiability,variab,variable,24552,"tVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTole",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:25340,Modifiability,variab,variableDefinition,25340,"mizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covarianc",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:26104,Modifiability,variab,variableDefinition,26104,"inosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:26506,Modifiability,variab,variables,26506,"::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariab",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:26558,Modifiability,variab,variable,26558,"::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariab",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:26875,Modifiability,variab,variables,26875,"the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, co",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:26902,Modifiability,variab,variable,26902,"the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, co",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:27603,Modifiability,variab,variable,27603,"upport storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOption",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:27898,Modifiability,variab,variable,27898," covariance matrices element for variables ivar,jvar if the variable is fixed the return value ...Definition Minimizer.cxx:120; ROOT::Math::Minimizer::GetHessianMatrixvirtual bool GetHessianMatrix(double *hMat) constFill the passed array with the Hessian matrix elements The Hessian matrix is the matrix of the second...Definition Minimizer.cxx:148; ROOT::Math::Minimizer::Strategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:28253,Modifiability,variab,variableDefinition,28253,"rategyint Strategy() conststrategyDefinition Minimizer.h:307; ROOT::Math::Minimizer::NCallsvirtual unsigned int NCalls() constnumber of function calls to reach the minimumDefinition Minimizer.h:228; ROOT::Math::Minimizer::SetUpperLimitedVariablevirtual bool SetUpperLimitedVariable(unsigned int ivar, const std::string &name, double val, double step, double upper)set a new upper limit variable (override if minimizer supports them )Definition Minimizer.h:179; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROO",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:28788,Modifiability,variab,variable,28788,"erator=(Minimizer &&)=delete; ROOT::Math::Minimizer::SetVariablevirtual bool SetVariable(unsigned int ivar, const std::string &name, double val, double step)=0set a new free variable; ROOT::Math::Minimizer::SetStrategyvoid SetStrategy(int strategyLevel)set the strategyDefinition Minimizer.h:344; ROOT::Math::Minimizer::ProvidesErrorvirtual bool ProvidesError() constminimizer provides error and error matrixDefinition Minimizer.h:243; ROOT::Math::Minimizer::FixVariablevirtual bool FixVariable(unsigned int ivar)fix an existing variableDefinition Minimizer.cxx:87; ROOT::Math::Minimizer::SetPrecisionvoid SetPrecision(double prec)set in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn tr",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:29436,Modifiability,variab,variable,29436,"et in the minimizer the objective function evaluation precision ( a value <=0 means the minimizer wi...Definition Minimizer.h:341; ROOT::Math::Minimizer::Optionsvirtual MinimizerOptions Options() constretrieve the minimizer options (implement derived class if needed)Definition Minimizer.h:323; ROOT::Math::Minimizer::Correlationvirtual double Correlation(unsigned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all optio",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:30148,Modifiability,variab,variable,30148,"OT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:30514,Modifiability,variab,variables,30514,"nition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtua",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31050,Modifiability,config,configuration,31050," Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minim",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31417,Modifiability,variab,variables,31417,"the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for paramet",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31530,Modifiability,variab,variables,31530,"the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neededDefinition Minimizer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for paramet",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31801,Modifiability,variab,variableDefinition,31801,"izer.h:136; ROOT::Math::Minimizer::SetExtraOptionsvoid SetExtraOptions(const IOptions &extraOptions)set only the extra optionsDefinition Minimizer.h:358; ROOT::Math::Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Fopti",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:31950,Modifiability,variab,variable,31950,":Minimizer::MinValuevirtual double MinValue() const =0return minimum function value; ROOT::Math::Minimizer::PrintLevelint PrintLevel() constminimizer configuration parametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; MathNamespace for new Math cl",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:32108,Modifiability,variab,variableDefinition,32108,"arametersDefinition Minimizer.h:291; ROOT::Math::Minimizer::PrintResultsvirtual void PrintResults()return reference to the objective function virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:3204,Performance,perform,perform,3204,"Scan; 72 ; 73 - Fumili (class TFumiliMinimizer); 74 ; 75 - GSLMultiMin (class ROOT::Math::GSLMinimizer) available when ROOT is built with `mathmore` support; 76 - BFGS2 (Default); 77 - BFGS; 78 - ConjugateFR; 79 - ConjugatePR; 80 - SteepestDescent; 81 ; 82 - GSLMultiFit (class ROOT::Math::GSLNLMinimizer) available when ROOT is built `mathmore` support; 83 ; 84 - GSLSimAn (class ROOT::Math::GSLSimAnMinimizer) available when ROOT is built with `mathmore` support; 85 ; 86 - Genetic (class ROOT::Math::GeneticMinimizer); 87 ; 88 - RMinimizer (class ROOT::Math::RMinimizer) available when ROOT is built with `r` support; 89 - BFGS (default); 90 - L-BFGS-S; 91 - Nelder-Mead; 92 - CG; 93 - and more methods, see the Details in the documentation of the function `optimix` of the [optmix R package](https://cran.r-project.org/web/packages/optimx/optimx.pdf); 94 ; 95 ; 96 The Minimizer class provides the interface to perform the minimization including; 97 ; 98 ; 99 In addition to provide the API for function minimization (via ROOT::Math::Minimizer::Minimize) the Minimizer class provides:; 100 - the interface for setting the function to be minimized. The objective function passed to the Minimizer must implement the multi-dimensional generic interface; 101 ROOT::Math::IBaseFunctionMultiDim. If the function provides gradient calculation (e.g. implementing the ROOT::Math::IGradientFunctionMultiDim interface); 102 the gradient will be used by the Minimizer class, when needed. There are convenient classes for the users to wrap their own functions in this required interface for minimization.; 103 These are the `ROOT::Math::Functor` class and the `ROOT::Math::GradFunctor` class for wrapping functions providing both evaluation and gradient. Some methods, like Fumili, Fumili2 and GSLMultiFit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:5055,Performance,perform,perform,5055,"Fit are; 104 specialized method for least-square and also likelihood minimizations. They require then that the given function implements in addition; 105 the `ROOT::Math::FitMethodFunction` interface.; 106 - The interface for setting the initial values for the function variables (which are the parameters in; 107 of the model function in case of solving for fitting) and specifying their limits.; 108 - The interface to set and retrieve basic minimization parameters. These parameter are controlled by the class `ROOT::Math::MinimizerOptions`.; 109 When no parameters are specified the default ones are used. Specific Minimizer options can also be passed via the `MinimizerOptions` class.; 110 For the list of the available option parameter one must look at the documentation of the corresponding derived class.; 111 - The interface to retrieve the result of minimization ( minimum X values, function value, gradient, error on the minimum, etc...); 112 - The interface to perform a Scan, Hesse or a Contour plot (for the minimizers that support this, i.e. Minuit and Minuit2); 113 ; 114 An example on how to use this interface is the tutorial NumericalMinimization.C in the tutorials/fit directory.; 115 ; 116 @ingroup MultiMin; 117*/; 118 ; 119class Minimizer {; 120 ; 121public:; 122 ; 123 /// Default constructor.; 124 Minimizer () {}; 125 ; 126 /// Destructor (no operations).; 127 virtual ~Minimizer () {}; 128 ; 129 // usually copying is non trivial, so we delete this; 130 Minimizer(Minimizer const&) = delete;; 131 Minimizer &operator=(Minimizer const&) = delete;; 132 Minimizer(Minimizer &&) = delete;; 133 Minimizer &operator=(Minimizer &&) = delete;; 134 ; 135 /// reset for consecutive minimization - implement if needed; 136 virtual void Clear() {}; 137 ; 138 /// set the function to minimize; 139 virtual void SetFunction(const ROOT::Math::IMultiGenFunction & func) = 0;; 140 ; 141 /// set the function implementing Hessian computation (re-implemented by Minimizer using it); 142 virtua",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:9924,Performance,perform,perform,9924,"size of the existing parameters); 187 virtual bool SetVariableValues(const double * x) {; 188 bool ret = true;; 189 unsigned int i = 0;; 190 while ( i <= NDim() && ret) {; 191 ret &= SetVariableValue(i,x[i] ); i++;; 192 }; 193 return ret;; 194 }; 195 virtual bool SetVariableStepSize(unsigned int ivar, double value );; 196 virtual bool SetVariableLowerLimit(unsigned int ivar, double lower);; 197 virtual bool SetVariableUpperLimit(unsigned int ivar, double upper);; 198 /// set the limits of an already existing variable; 199 virtual bool SetVariableLimits(unsigned int ivar, double lower, double upper) {; 200 return SetVariableLowerLimit(ivar,lower) && SetVariableUpperLimit(ivar,upper);; 201 }; 202 virtual bool FixVariable(unsigned int ivar);; 203 virtual bool ReleaseVariable(unsigned int ivar);; 204 virtual bool IsFixedVariable(unsigned int ivar) const;; 205 virtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings & pars) const;; 206 ; 207 /// set the initial range of an existing variable; 208 virtual bool SetVariableInitialRange(unsigned int /* ivar */, double /* mininitial */, double /* maxinitial */) {; 209 return false;; 210 }; 211 ; 212 /// method to perform the minimization; 213 virtual bool Minimize() = 0;; 214 ; 215 /// return minimum function value; 216 virtual double MinValue() const = 0;; 217 ; 218 /// return pointer to X values at the minimum; 219 virtual const double * X() const = 0;; 220 ; 221 /// return expected distance reached from the minimum (re-implement if minimizer provides it; 222 virtual double Edm() const { return -1; }; 223 ; 224 /// return pointer to gradient values at the minimum; 225 virtual const double * MinGradient() const { return nullptr; }; 226 ; 227 /// number of function calls to reach the minimum; 228 virtual unsigned int NCalls() const { return 0; }; 229 ; 230 /// number of iterations to reach the minimum; 231 virtual unsigned int NIterations() const { return NCalls(); }; 232 ; 233 /// this is <= Function().",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:14316,Performance,perform,performed,14316,"nctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// precision of minimizer in the evaluation of the objective function; 303 /// ( a value <=0 corresponds to the let the minimizer choose its default one); 304 double Precision() const { return fOptions.Precision(); }; 305 ; 306 /// strategy; 307 int Strategy() const { return fOptions.Strategy(); }; 308 ; 309 /// status code of minimizer; 310 int Status() const { return fStatus; }; 311 ; 312 /// status code of Minos (to be re-implemented by the minimizers supporting Minos); 313 virtual int MinosStatus() const { return -1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the minimizer options (implement derived class if needed); 323 virtual MinimizerOptions Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:15674,Performance,perform,perform,15674,"evel) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i.e. default case); 341 void SetPrecision(double prec) { fOptions.SetPrecision(prec); }; 342 ; 343 ///set the strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 } // end namespace Math; 375 ; 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunctio",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:23278,Performance,perform,perform,23278,"nst double * X() const =0return pointer to X values at the minimum; ROOT::Math::Minimizer::NIterationsvirtual unsigned int NIterations() constnumber of iterations to reach the minimumDefinition Minimizer.h:231; ROOT::Math::Minimizer::SetMaxIterationsvoid SetMaxIterations(unsigned int maxiter)set maximum iterations (one iteration can have many function calls)Definition Minimizer.h:334; ROOT::Math::Minimizer::SetVariableInitialRangevirtual bool SetVariableInitialRange(unsigned int, double, double)set the initial range of an existing variableDefinition Minimizer.h:208; ROOT::Math::Minimizer::SetErrorDefvoid SetErrorDef(double up)set scale for calculating the errorsDefinition Minimizer.h:347; ROOT::Math::Minimizer::GetVariableSettingsvirtual bool GetVariableSettings(unsigned int ivar, ROOT::Fit::ParameterSettings &pars) constget variable settings in a variable object (like ROOT::Fit::ParamsSettings)Definition Minimizer.cxx:109; ROOT::Math::Minimizer::SetValidErrorvoid SetValidError(bool on)flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit)Definition Minimizer.h:350; ROOT::Math::Minimizer::SetVariablesint SetVariables(const VariableIterator &begin, const VariableIterator &end)add variables . Return number of variables successfully addedDefinition Minimizer.h:146; ROOT::Math::Minimizer::GlobalCCvirtual double GlobalCC(unsigned int ivar) constreturn global correlation coefficient for variable i This is a number between zero and one which give...Definition Minimizer.cxx:161; ROOT::Math::Minimizer::MinGradientvirtual const double * MinGradient() constreturn pointer to gradient values at the minimumDefinition Minimizer.h:225; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Min",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:25825,Performance,perform,perform,25825,"361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefinition Minimizer.h:310; ROOT::Math::Minimizer::SetVariableUpperLimitvirtual bool SetVariableUpperLimit(unsigned int ivar, double upper)set the upper-limit of an already existing variableDefinition Minimizer.cxx:78; ROOT::Math::Minimizer::SetCovariancevirtual bool SetCovariance(std::span< const double > cov, unsigned int nrow)set initial covariance matrixDefinition Minimizer.cxx:25; ROOT::Math::Minimizer::MinimizerMinimizer()Default constructor.Definition Minimizer.h:124; ROOT::Math::Minimizer::VariableNamevirtual std::string VariableName(unsigned int ivar) constget name of variables (override if minimizer support storing of variable names) return an empty strin...Definition Minimizer.cxx:224; ROOT::Math::Minimizer::SetPrintLevelvoid SetPrintLevel(int level)set print levelDefinition Minimizer.h:328; ROOT::Math::Minimizer::CovMatrixvirtual double CovMatrix(unsigned int ivar, unsigned int jvar) constreturn covar",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:29745,Performance,perform,performed,29745,"ned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neede",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:32193,Performance,perform,perform,32193,"tion virtual const ROOT::Math::IGenFunction & Function() const ...Definition Minimizer.h:282; ROOT::Math::Minimizer::NDimvirtual unsigned int NDim() const =0this is <= Function().NDim() which is the total number of variables (free+ constrained ones); ROOT::Math::Minimizer::NFreevirtual unsigned int NFree() constnumber of free variables (real dimension of the problem) this is <= Function().NDim() which is the to...Definition Minimizer.h:240; ROOT::Math::Minimizer::SetVariableLowerLimitvirtual bool SetVariableLowerLimit(unsigned int ivar, double lower)set the lower-limit of an already existing variableDefinition Minimizer.cxx:70; ROOT::Math::Minimizer::IsFixedVariablevirtual bool IsFixedVariable(unsigned int ivar) constquery if an existing variable is fixed (i.e.Definition Minimizer.cxx:102; ROOT::Math::Minimizer::ReleaseVariablevirtual bool ReleaseVariable(unsigned int ivar)release an existing variableDefinition Minimizer.cxx:94; ROOT::Math::Minimizer::Hessevirtual bool Hesse()perform a full calculation of the Hessian matrix for error calculationDefinition Minimizer.cxx:185; ROOT::Math::Minimizer::Contourvirtual bool Contour(unsigned int ivar, unsigned int jvar, unsigned int &npoints, double *xi, double *xj)find the contour points (xi, xj) of the function for parameter ivar and jvar around the minimum The c...Definition Minimizer.cxx:211; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; HFit::FitTFitResultPtr Fit(FitObject *h1, TF1 *f1, Foption_t &option, const ROOT::Math::MinimizerOptions &moption, const char *goption, ROOT::Fit::DataRange &range)Definition HFitImpl.cxx:133; MathNamespace for new Math classes and functions.; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimizer.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:14343,Security,validat,validation,14343,"nctionCalls(); }; 295 ; 296 /// max iterations; 297 unsigned int MaxIterations() const { return fOptions.MaxIterations(); }; 298 ; 299 /// absolute tolerance; 300 double Tolerance() const { return fOptions.Tolerance(); }; 301 ; 302 /// precision of minimizer in the evaluation of the objective function; 303 /// ( a value <=0 corresponds to the let the minimizer choose its default one); 304 double Precision() const { return fOptions.Precision(); }; 305 ; 306 /// strategy; 307 int Strategy() const { return fOptions.Strategy(); }; 308 ; 309 /// status code of minimizer; 310 int Status() const { return fStatus; }; 311 ; 312 /// status code of Minos (to be re-implemented by the minimizers supporting Minos); 313 virtual int MinosStatus() const { return -1; }; 314 ; 315 /// return the statistical scale used for calculate the error; 316 /// is typically 1 for Chi2 and 0.5 for likelihood minimization; 317 double ErrorDef() const { return fOptions.ErrorDef(); }; 318 ; 319 ///return true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit); 320 bool IsValidError() const { return fValidError; }; 321 ; 322 /// retrieve the minimizer options (implement derived class if needed); 323 virtual MinimizerOptions Options() const {; 324 return fOptions;; 325 }; 326 ; 327 /// set print level; 328 void SetPrintLevel(int level) { fOptions.SetPrintLevel(level); }; 329 ; 330 ///set maximum of function calls; 331 void SetMaxFunctionCalls(unsigned int maxfcn) { if (maxfcn > 0) fOptions.SetMaxFunctionCalls(maxfcn); }; 332 ; 333 /// set maximum iterations (one iteration can have many function calls); 334 void SetMaxIterations(unsigned int maxiter) { if (maxiter > 0) fOptions.SetMaxIterations(maxiter); }; 335 ; 336 /// set the tolerance; 337 void SetTolerance(double tol) { fOptions.SetTolerance(tol); }; 338 ; 339 /// set in the minimizer the objective function evaluation precision; 340 /// ( a value <=0 means the minimizer will choose its optimal value automatically, i",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:16259,Security,access,accessible,16259,"e strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 } // end namespace Math; 375 ; 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IB",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:16371,Security,validat,validated,16371,"e strategy; 344 void SetStrategy(int strategyLevel) { fOptions.SetStrategy(strategyLevel); }; 345 ; 346 /// set scale for calculating the errors; 347 void SetErrorDef(double up) { fOptions.SetErrorDef(up); }; 348 ; 349 /// flag to check if minimizer needs to perform accurate error analysis (e.g. run Hesse for Minuit); 350 void SetValidError(bool on) { fValidError = on; }; 351 ; 352 /// set all options in one go; 353 void SetOptions(const MinimizerOptions & opt) {; 354 fOptions = opt;; 355 }; 356 ; 357 /// set only the extra options; 358 void SetExtraOptions(const IOptions & extraOptions) { fOptions.SetExtraOptions(extraOptions); }; 359 ; 360 /// reset the default options (defined in MinimizerOptions); 361 void SetDefaultOptions() {; 362 fOptions.ResetToDefaultOptions();; 363 }; 364 ; 365protected:; 366 ; 367 // keep protected to be accessible by the derived classes; 368 ; 369 bool fValidError = false; ///< flag to control if errors have been validated (Hesse has been run in case of Minuit); 370 MinimizerOptions fOptions; ///< minimizer options; 371 int fStatus = -1; ///< status of minimizer; 372};; 373 ; 374 } // end namespace Math; 375 ; 376} // end namespace ROOT; 377 ; 378 ; 379#endif /* ROOT_Math_Minimizer */; IFunction.h; MinimizerOptions.h; RSpan.hxx; onOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void onDefinition TGWin32VirtualXProxy.cxx:106; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; namechar name[80]Definition TGX11.cxx:110; xminfloat xminDefinition THbookFile.cxx:95; xmaxfloat xmaxDefinition THbookFile.cxx:95; ROOT::Fit::ParameterSettingsClass, describing value, limits and step size of the parameters Provides functionality also to set/re...Definition ParameterSettings.h:33; ROOT::Math::IB",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:24938,Security,validat,validated,24938,"25; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer &&)=delete; ROOT::Math::Minimizer::fStatusint fStatusstatus of minimizerDefinition Minimizer.h:371; ROOT::Math::Minimizer::SetVariableValuevirtual bool SetVariableValue(unsigned int ivar, double value)set the value of an already existing variableDefinition Minimizer.cxx:53; ROOT::Math::Minimizer::SetFunctionvirtual void SetFunction(const ROOT::Math::IMultiGenFunction &func)=0set the function to minimize; ROOT::Math::Minimizer::Scanvirtual bool Scan(unsigned int ivar, unsigned int &nstep, double *x, double *y, double xmin=0, double xmax=0)scan function minimum for variable i.Definition Minimizer.cxx:195; ROOT::Math::Minimizer::MaxIterationsunsigned int MaxIterations() constmax iterationsDefinition Minimizer.h:297; ROOT::Math::Minimizer::SetDefaultOptionsvoid SetDefaultOptions()reset the default options (defined in MinimizerOptions)Definition Minimizer.h:361; ROOT::Math::Minimizer::fValidErrorbool fValidErrorflag to control if errors have been validated (Hesse has been run in case of Minuit)Definition Minimizer.h:369; ROOT::Math::Minimizer::MinosStatusvirtual int MinosStatus() conststatus code of Minos (to be re-implemented by the minimizers supporting Minos)Definition Minimizer.h:313; ROOT::Math::Minimizer::SetVariableLimitsvirtual bool SetVariableLimits(unsigned int ivar, double lower, double upper)set the limits of an already existing variableDefinition Minimizer.h:199; ROOT::Math::Minimizer::SetTolerancevoid SetTolerance(double tol)set the toleranceDefinition Minimizer.h:337; ROOT::Math::Minimizer::MinimizerMinimizer(Minimizer const &)=delete; ROOT::Math::Minimizer::CovMatrixStatusvirtual int CovMatrixStatus() constreturn status of covariance matrix using Minuit convention {0 not calculated 1 approximated 2 made po...Definition Minimizer.h:256; ROOT::Math::Minimizer::Minimizevirtual bool Minimize()=0method to perform the minimization; ROOT::Math::Minimizer::Statusint Status() conststatus code of minimizerDefi",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/Minimizer_8h_source.html:29772,Security,validat,validation,29772,"ned int i, unsigned int j) constreturn correlation coefficient between variable i and j.Definition Minimizer.h:264; ROOT::Math::Minimizer::~Minimizervirtual ~Minimizer()Destructor (no operations).Definition Minimizer.h:127; ROOT::Math::Minimizer::ErrorDefdouble ErrorDef() constreturn the statistical scale used for calculate the error is typically 1 for Chi2 and 0....Definition Minimizer.h:317; ROOT::Math::Minimizer::fOptionsMinimizerOptions fOptionsminimizer optionsDefinition Minimizer.h:370; ROOT::Math::Minimizer::operator=Minimizer & operator=(Minimizer const &)=delete; ROOT::Math::Minimizer::SetFixedVariablevirtual bool SetFixedVariable(unsigned int ivar, const std::string &name, double val)set a new fixed variable (override if minimizer supports them )Definition Minimizer.cxx:44; ROOT::Math::Minimizer::SetMaxFunctionCallsvoid SetMaxFunctionCalls(unsigned int maxfcn)set maximum of function callsDefinition Minimizer.h:331; ROOT::Math::Minimizer::IsValidErrorbool IsValidError() constreturn true if Minimizer has performed a detailed error validation (e.g. run Hesse for Minuit)Definition Minimizer.h:320; ROOT::Math::Minimizer::Edmvirtual double Edm() constreturn expected distance reached from the minimum (re-implement if minimizer provides itDefinition Minimizer.h:222; ROOT::Math::Minimizer::GetMinosErrorvirtual bool GetMinosError(unsigned int ivar, double &errLow, double &errUp, int option=0)minos error for variable i, return false if Minos failed or not supported and the lower and upper err...Definition Minimizer.cxx:172; ROOT::Math::Minimizer::SetOptionsvoid SetOptions(const MinimizerOptions &opt)set all options in one goDefinition Minimizer.h:353; ROOT::Math::Minimizer::SetVariableValuesvirtual bool SetVariableValues(const double *x)set the values of all existing variables (array must be dimensioned to the size of the existing param...Definition Minimizer.h:187; ROOT::Math::Minimizer::Clearvirtual void Clear()reset for consecutive minimization - implement if neede",MatchSource.WIKI,doc/master/Minimizer_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/Minimizer_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1403,Availability,error,error,1403,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:818,Modifiability,variab,variable,818,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:899,Modifiability,variab,variables,899,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:970,Modifiability,variab,variable,970,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1012,Modifiability,variab,variable,1012,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1039,Modifiability,variab,variable,1039,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1083,Modifiability,variab,variable,1083,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1128,Modifiability,variab,variable,1128,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1256,Modifiability,variab,variables,1256,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1333,Modifiability,variab,variable,1333,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1417,Modifiability,variab,variable,1417,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1644,Modifiability,variab,variable,1644,"h""; 16 ; 17#include <memory>; 18 ; 19namespace ROOT {; 20 ; 21 namespace Math {; 22 ; 23 /**; 24 Enumeration describing the status of the variable; 25 The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // con",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:1826,Modifiability,variab,variable,1826," The enumeration are used in the minimizer classes to categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // constructor for double bound variable; 67 MinimTransformVariable (double lower, double upper, SinVariableTransformation * trafo) :; 68 fFix(false), fLowBound(false), fUpBound(false), fBounds(true),; 69 fTransform(trafo),; 70 fLower(lower), fUpper(upper); 71 { }; 72 ; 73 // constructor for lower bound variable; 74 MinimTransformVariable (double lower, SqrtLowVariableTransformation * trafo) :; 75 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 76 fTransform(trafo), fLower(lower), fUpper(lower); 77 {}; 78 ; 79 // constructor for upper bound variable; 80 MinimTransformVariable (double upper, SqrtUpVariableTransformation * trafo) :; 81 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 82 fTransform(trafo), fLower(upper), fUpper(upper); 83 {}; 84 ; 85 // copy constructor; 86 MinimTransformVariable ",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:2027,Modifiability,variab,variable,2027,"o categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // constructor for double bound variable; 67 MinimTransformVariable (double lower, double upper, SinVariableTransformation * trafo) :; 68 fFix(false), fLowBound(false), fUpBound(false), fBounds(true),; 69 fTransform(trafo),; 70 fLower(lower), fUpper(upper); 71 { }; 72 ; 73 // constructor for lower bound variable; 74 MinimTransformVariable (double lower, SqrtLowVariableTransformation * trafo) :; 75 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 76 fTransform(trafo), fLower(lower), fUpper(lower); 77 {}; 78 ; 79 // constructor for upper bound variable; 80 MinimTransformVariable (double upper, SqrtUpVariableTransformation * trafo) :; 81 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 82 fTransform(trafo), fLower(upper), fUpper(upper); 83 {}; 84 ; 85 // copy constructor; 86 MinimTransformVariable (const MinimTransformVariable & rhs) :; 87 fFix(rhs.",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:2300,Modifiability,variab,variable,2300,"o categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // constructor for double bound variable; 67 MinimTransformVariable (double lower, double upper, SinVariableTransformation * trafo) :; 68 fFix(false), fLowBound(false), fUpBound(false), fBounds(true),; 69 fTransform(trafo),; 70 fLower(lower), fUpper(upper); 71 { }; 72 ; 73 // constructor for lower bound variable; 74 MinimTransformVariable (double lower, SqrtLowVariableTransformation * trafo) :; 75 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 76 fTransform(trafo), fLower(lower), fUpper(lower); 77 {}; 78 ; 79 // constructor for upper bound variable; 80 MinimTransformVariable (double upper, SqrtUpVariableTransformation * trafo) :; 81 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 82 fTransform(trafo), fLower(upper), fUpper(upper); 83 {}; 84 ; 85 // copy constructor; 86 MinimTransformVariable (const MinimTransformVariable & rhs) :; 87 fFix(rhs.",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:2558,Modifiability,variab,variable,2558,"o categorize the variables; 26 */; 27 enum EMinimVariableType {; 28 kDefault, ///< free variable (unlimited); 29 kFix, ///< fixed variable; 30 kBounds, ///< variable has two bounds; 31 kLowBound, ///< variable has a lower bound; 32 kUpBound ///< variable has an upper bounds; 33 };; 34 ; 35 ; 36 ; 37/**; 38 MinimTransformVariable class; 39 Contains meta information of the variables such as bounds, fix flags and; 40 deals with transformation of the variable; 41 The class does not contain the values and the step size (error) of the variable; 42 This is an internal class used by the MinimTransformFunction class; 43 ; 44 @ingroup MultiMin; 45*/; 46 ; 47 ; 48class MinimTransformVariable {; 49 ; 50public:; 51 ; 52 /**; 53 Default Constructor for an unlimited variable; 54 */; 55 MinimTransformVariable () :; 56 fFix(false), fLowBound(false), fUpBound(false), fBounds(false),; 57 fLower(1), fUpper(0); 58 {}; 59 ; 60 // constructor for fixed variable; 61 MinimTransformVariable (double value) :; 62 fFix(true), fLowBound(false), fUpBound(false), fBounds(false),; 63 fLower(value), fUpper(value); 64 {}; 65 ; 66 // constructor for double bound variable; 67 MinimTransformVariable (double lower, double upper, SinVariableTransformation * trafo) :; 68 fFix(false), fLowBound(false), fUpBound(false), fBounds(true),; 69 fTransform(trafo),; 70 fLower(lower), fUpper(upper); 71 { }; 72 ; 73 // constructor for lower bound variable; 74 MinimTransformVariable (double lower, SqrtLowVariableTransformation * trafo) :; 75 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 76 fTransform(trafo), fLower(lower), fUpper(lower); 77 {}; 78 ; 79 // constructor for upper bound variable; 80 MinimTransformVariable (double upper, SqrtUpVariableTransformation * trafo) :; 81 fFix(false), fLowBound(true), fUpBound(false), fBounds(false),; 82 fTransform(trafo), fLower(upper), fUpper(upper); 83 {}; 84 ; 85 // copy constructor; 86 MinimTransformVariable (const MinimTransformVariable & rhs) :; 87 fFix(rhs.",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:4636,Modifiability,variab,variable,4636,"nd() const { return fLower; }; 118 ; 119 double UpperBound() const { return fUpper; }; 120 ; 121 double FixValue() const { return fLower; }; 122 ; 123 // internal to external transformation; 124 double InternalToExternal( double x) const {; 125 return (fTransform.get() ) ? fTransform->Int2ext(x, fLower, fUpper) : x;; 126 }; 127 ; 128 // derivative of the internal to external transformation ( d Int2Ext / d int ); 129 double DerivativeIntToExt ( double x) const {; 130 return (fTransform.get() ) ? fTransform->DInt2Ext( x, fLower, fUpper) : 1.0;; 131 }; 132 ; 133 // external to internal transformation; 134 double ExternalToInternal(double x) const {; 135 return (fTransform.get() ) ? fTransform->Ext2int(x, fLower, fUpper) : x;; 136 }; 137 ; 138private:; 139 ; 140 bool fFix; ///< fix variable; 141 bool fLowBound; ///< has lower bound; 142 bool fUpBound; ///< has upper bound param; 143 bool fBounds; ///< has double bound; 144 std::unique_ptr< MinimizerVariableTransformation> fTransform; ///< pointer to the minimizer transformation; 145 double fLower; ///< lower parameter limit; 146 double fUpper; ///< upper parameter limit; 147 ; 148};; 149 ; 150 } // end namespace Math; 151 ; 152} // end namespace ROOT; 153 ; 154 ; 155#endif /* ROOT_Math_MinimTransformVariable */; 156 ; 157 ; MinimizerVariableTransformation.h; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; ROOT::Math::MinimTransformVariableMinimTransformVariable class Contains meta information of the variables such as bounds,...Definition MinimTransformVariable.h:48; ROOT::Math::MinimTransformVariable::InternalToExternaldouble InternalToExternal(double x) constDefinition MinimTransformVariable.h:124; ROOT::Math::MinimTransformVariable::fTransformstd::unique_ptr< MinimizerVariableTransformation > fTransformpointer to the minimizer transformationDefinition MinimTrans",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:5477,Modifiability,variab,variables,5477,"transformation; 134 double ExternalToInternal(double x) const {; 135 return (fTransform.get() ) ? fTransform->Ext2int(x, fLower, fUpper) : x;; 136 }; 137 ; 138private:; 139 ; 140 bool fFix; ///< fix variable; 141 bool fLowBound; ///< has lower bound; 142 bool fUpBound; ///< has upper bound param; 143 bool fBounds; ///< has double bound; 144 std::unique_ptr< MinimizerVariableTransformation> fTransform; ///< pointer to the minimizer transformation; 145 double fLower; ///< lower parameter limit; 146 double fUpper; ///< upper parameter limit; 147 ; 148};; 149 ; 150 } // end namespace Math; 151 ; 152} // end namespace ROOT; 153 ; 154 ; 155#endif /* ROOT_Math_MinimTransformVariable */; 156 ; 157 ; MinimizerVariableTransformation.h; valueOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void valueDefinition TGWin32VirtualXProxy.cxx:142; ROOT::Math::MinimTransformVariableMinimTransformVariable class Contains meta information of the variables such as bounds,...Definition MinimTransformVariable.h:48; ROOT::Math::MinimTransformVariable::InternalToExternaldouble InternalToExternal(double x) constDefinition MinimTransformVariable.h:124; ROOT::Math::MinimTransformVariable::fTransformstd::unique_ptr< MinimizerVariableTransformation > fTransformpointer to the minimizer transformationDefinition MinimTransformVariable.h:144; ROOT::Math::MinimTransformVariable::LowerBounddouble LowerBound() constDefinition MinimTransformVariable.h:117; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double lower, double upper, SinVariableTransformation *trafo)Definition MinimTransformVariable.h:67; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(const MinimTransformVariable &rhs)Definition MinimTransformVariable.h:86; ROOT::Math::MinimTransformVariable::fLowBoundbool fLowBoundhas lower boundDefinition MinimTransformVariable.h:141; ROOT::",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:7384,Modifiability,variab,variableDefinition,7384,"dhas lower boundDefinition MinimTransformVariable.h:141; ROOT::Math::MinimTransformVariable::HasUpperBoundbool HasUpperBound() constDefinition MinimTransformVariable.h:115; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double value)Definition MinimTransformVariable.h:61; ROOT::Math::MinimTransformVariable::UpperBounddouble UpperBound() constDefinition MinimTransformVariable.h:119; ROOT::Math::MinimTransformVariable::FixValuedouble FixValue() constDefinition MinimTransformVariable.h:121; ROOT::Math::MinimTransformVariable::IsFixedbool IsFixed() constDefinition MinimTransformVariable.h:109; ROOT::Math::MinimTransformVariable::operator=MinimTransformVariable & operator=(const MinimTransformVariable &rhs)Definition MinimTransformVariable.h:95; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double upper, SqrtUpVariableTransformation *trafo)Definition MinimTransformVariable.h:80; ROOT::Math::MinimTransformVariable::fFixbool fFixfix variableDefinition MinimTransformVariable.h:140; ROOT::Math::MinimTransformVariable::ExternalToInternaldouble ExternalToInternal(double x) constDefinition MinimTransformVariable.h:134; ROOT::Math::MinimTransformVariable::HasLowerBoundbool HasLowerBound() constDefinition MinimTransformVariable.h:113; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable()Default Constructor for an unlimited variable.Definition MinimTransformVariable.h:55; ROOT::Math::MinimTransformVariable::fUpperdouble fUpperupper parameter limitDefinition MinimTransformVariable.h:146; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double lower, SqrtLowVariableTransformation *trafo)Definition MinimTransformVariable.h:74; ROOT::Math::MinimTransformVariable::IsLimitedbool IsLimited() constDefinition MinimTransformVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariab",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:7804,Modifiability,variab,variable,7804,"() constDefinition MinimTransformVariable.h:119; ROOT::Math::MinimTransformVariable::FixValuedouble FixValue() constDefinition MinimTransformVariable.h:121; ROOT::Math::MinimTransformVariable::IsFixedbool IsFixed() constDefinition MinimTransformVariable.h:109; ROOT::Math::MinimTransformVariable::operator=MinimTransformVariable & operator=(const MinimTransformVariable &rhs)Definition MinimTransformVariable.h:95; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double upper, SqrtUpVariableTransformation *trafo)Definition MinimTransformVariable.h:80; ROOT::Math::MinimTransformVariable::fFixbool fFixfix variableDefinition MinimTransformVariable.h:140; ROOT::Math::MinimTransformVariable::ExternalToInternaldouble ExternalToInternal(double x) constDefinition MinimTransformVariable.h:134; ROOT::Math::MinimTransformVariable::HasLowerBoundbool HasLowerBound() constDefinition MinimTransformVariable.h:113; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable()Default Constructor for an unlimited variable.Definition MinimTransformVariable.h:55; ROOT::Math::MinimTransformVariable::fUpperdouble fUpperupper parameter limitDefinition MinimTransformVariable.h:146; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double lower, SqrtLowVariableTransformation *trafo)Definition MinimTransformVariable.h:74; ROOT::Math::MinimTransformVariable::IsLimitedbool IsLimited() constDefinition MinimTransformVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:8828,Modifiability,variab,variables,8828,"ctor for an unlimited variable.Definition MinimTransformVariable.h:55; ROOT::Math::MinimTransformVariable::fUpperdouble fUpperupper parameter limitDefinition MinimTransformVariable.h:146; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double lower, SqrtLowVariableTransformation *trafo)Definition MinimTransformVariable.h:74; ROOT::Math::MinimTransformVariable::IsLimitedbool IsLimited() constDefinition MinimTransformVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvari",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:8985,Modifiability,variab,variables,8985,"n MinimTransformVariable.h:146; ROOT::Math::MinimTransformVariable::MinimTransformVariableMinimTransformVariable(double lower, SqrtLowVariableTransformation *trafo)Definition MinimTransformVariable.h:74; ROOT::Math::MinimTransformVariable::IsLimitedbool IsLimited() constDefinition MinimTransformVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvariable has a lower boundDefinition MinimTransformVariable.h:31; ROOT::Math::kDefault@ kDefaultfree variable (unlimited)Definition MinimTransformVariable.h:28",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:9141,Modifiability,variab,variables,9141," *trafo)Definition MinimTransformVariable.h:74; ROOT::Math::MinimTransformVariable::IsLimitedbool IsLimited() constDefinition MinimTransformVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvariable has a lower boundDefinition MinimTransformVariable.h:31; ROOT::Math::kDefault@ kDefaultfree variable (unlimited)Definition MinimTransformVariable.h:28; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcore",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:9379,Modifiability,variab,variable,9379,"rmVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvariable has a lower boundDefinition MinimTransformVariable.h:31; ROOT::Math::kDefault@ kDefaultfree variable (unlimited)Definition MinimTransformVariable.h:28; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimTransformVariable.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:9510,Modifiability,variab,variableDefinition,9510,"rmVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvariable has a lower boundDefinition MinimTransformVariable.h:31; ROOT::Math::kDefault@ kDefaultfree variable (unlimited)Definition MinimTransformVariable.h:28; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimTransformVariable.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/MinimTransformVariable_8h_source.html:9880,Modifiability,variab,variable,9880,"rmVariable.h:111; ROOT::Math::MinimTransformVariable::DerivativeIntToExtdouble DerivativeIntToExt(double x) constDefinition MinimTransformVariable.h:129; ROOT::Math::MinimTransformVariable::fBoundsbool fBoundshas double boundDefinition MinimTransformVariable.h:143; ROOT::Math::MinimTransformVariable::fLowerdouble fLowerlower parameter limitDefinition MinimTransformVariable.h:145; ROOT::Math::MinimTransformVariable::fUpBoundbool fUpBoundhas upper bound paramDefinition MinimTransformVariable.h:142; ROOT::Math::SinVariableTransformationSin Transformation class for dealing with double bounded variables.Definition MinimizerVariableTransformation.h:38; ROOT::Math::SqrtLowVariableTransformationSqrt Transformation class for dealing with lower bounded variables.Definition MinimizerVariableTransformation.h:58; ROOT::Math::SqrtUpVariableTransformationSqrt Transformation class for dealing with upper bounded variables.Definition MinimizerVariableTransformation.h:74; xDouble_t x[n]Definition legend1.C:17; MathNamespace for new Math classes and functions.; ROOT::Math::EMinimVariableTypeEMinimVariableTypeEnumeration describing the status of the variable The enumeration are used in the minimizer classes t...Definition MinimTransformVariable.h:27; ROOT::Math::kFix@ kFixfixed variableDefinition MinimTransformVariable.h:29; ROOT::Math::kUpBound@ kUpBoundvariable has an upper boundsDefinition MinimTransformVariable.h:32; ROOT::Math::kBounds@ kBoundsvariable has two boundsDefinition MinimTransformVariable.h:30; ROOT::Math::kLowBound@ kLowBoundvariable has a lower boundDefinition MinimTransformVariable.h:31; ROOT::Math::kDefault@ kDefaultfree variable (unlimited)Definition MinimTransformVariable.h:28; ROOTtbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tb...Definition EExecutionPolicy.hxx:4. mathmathcoreincMathMinimTransformVariable.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MinimTransformVariable_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MinimTransformVariable_8h_source.html
https://root.cern/doc/master/minuit2FitBench2D_8C.html:237,Testability,benchmark,benchmark,237,". ROOT: tutorials/fit/minuit2FitBench2D.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. minuit2FitBench2D.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Minuit2 fit 2D benchmark. . ; FCN=7196.63 FROM MIGRAD STATUS=CONVERGED 169 CALLS 170 TOTAL; EDM=3.06509e-09 STRATEGY= 1 ERROR MATRIX UNCERTAINTY 2.0 per cent; EXT PARAMETER STEP FIRST ; NO. NAME VALUE ERROR SIZE DERIVATIVE ; 1 p0 5.28670e+01 2.67227e-01 1.71567e-03 1.41590e-04; 2 p1 2.00562e+00 9.99521e-03 -6.72962e-05 -5.71949e-03; 3 p2 -1.02279e+00 1.35440e-02 3.22465e-05 2.77027e-03; 4 p3 2.89928e+00 7.94099e-03 -9.38703e-05 -4.55759e-03; 5 p4 3.86656e+00 1.13330e-02 1.06568e-05 -2.11126e-03; Minuit, npass=0 : RT= 0.143 s, Cpu= 0.140 s; ****************************************; Minimizer is Fumili; Chi2 = 7196.63; NDf = 7366; NCalls = 30; p0 = 52.8672 +/- 0.262932 ; p1 = 2.00562 +/- 0.00992527 ; p2 = -1.02279 +/- 0.0135821 ; p3 = 2.89927 +/- 0.00781808 ; p4 = 3.86655 +/- 0.011235 ; Fumili, npass=0 : RT= 0.084 s, Cpu= 0.090 s; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 7196.63; NDf = 7366; Edm = 4.35806e-08; NCalls = 176; p0 = 52.867 +/- 0.264728 ; p1 = 2.00562 +/- 0.00995152 ; p2 = -1.0228 +/- 0.0137398 ; p3 = 2.89928 +/- 0.00798634 ; p4 = 3.86656 +/- 0.0112306 ; Minuit2, npass=0 : RT= 0.096 s, Cpu= 0.100 s; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 7196.63; NDf = 7366; Edm = 2.49758e-07; NCalls = 99; p0 = 52.8669 +/- 0.265504 ; p1 = 2.00562 +/- 0.00992463 ; p2 = -1.0228 +/- 0.0135834 ; p3 = 2.89928 +/- 0.00792724 ; p4 = 3.86656 +/- 0.0113104 ; Fumili2, npass=0 : RT= 0.062 s, Cpu= 0.060 s; ; #include ""TH1.h""; #include ""TF1.h""; #include ""TH2D.h""; #include ""TF2.h""; #include ""TCanvas.h""; #include ""TStopwatch.h""; #include ""TSystem.h""; #include ""TRandom3.h""; #include ""TVirtualFitter.h""; #include ""TPaveLabel.h""; #include ""TStyle.h""; ; ; TF2 *fitFcn;; TH2D *histo;; ; // Quadratic",MatchSource.WIKI,doc/master/minuit2FitBench2D_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2FitBench2D_8C.html
https://root.cern/doc/master/minuit2FitBench_8C.html:6366,Availability,error,error,6366,"r,cputime),""brNDC"");; p->Draw();; p->SetTextColor(kRed+3);; p->SetFillColor(kYellow-8);; pad->Update();; return ok;; }; ; int minuit2FitBench(int npass=20) {; TH1::AddDirectory(false);; TCanvas *c1 = new TCanvas(""FitBench"",""Fitting Demo"",10,10,900,900);; c1->Divide(2,2);; c1->SetFillColor(kYellow-9);; // create a TF1 with the range from 0 to 3 and 6 parameters; fitFcn = new TF1(""fitFcn"",fitFunction,0,3,6);; fitFcn->SetNpx(200);; gStyle->SetOptFit();; gStyle->SetStatY(0.6);; ; bool ok = true;; //with Minuit; c1->cd(1);; ok &= DoFit(""Minuit"",gPad,npass);; ; //with Fumili; c1->cd(2);; ok &= DoFit(""Fumili"",gPad,npass);; ; //with Minuit2; c1->cd(3);; ok &= DoFit(""Minuit2"",gPad,npass);; ; //with Fumili2; c1->cd(4);; ok &= DoFit(""Fumili2"",gPad,npass);; ; c1->SaveAs(""FitBench.root"");; return (ok) ? 0 : 1;; }; MinimizerOptions.h; kRed@ kRedDefinition Rtypes.h:66; kYellow@ kYellowDefinition Rtypes.h:66; TCanvas.h; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; TF1.h; TFrame.h; pwinID h TVirtualViewer3D TVirtualGLPainter pDefinition TGWin32VirtualGLProxy.cxx:51; SetLineColorOption_t Option_t SetLineColorDefinition TGWin32VirtualXProxy.cxx:54; TH1.h; TMath.h; TPaveLabel.h; TROOT.h; TRandom3.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TStopwatch.h; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; TStyle.h; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; TSystem.h; gPad#define gPadDefinition TVirtualPad.h:308; ROOT::Math::MinimizerOptions::SetDefaultMinimizerstatic void SetDefaultMinimizer(const char *type, const char *algo=nullptr)Set the default Minimizer type and corresponding algorithms.Definition MinimizerOptions.cxx:43; TCanvasThe Canvas class.Definition TCanvas.h:23; TF11-Dim function classDefinition TF1.h:233; TF1::Updatevirtual void Update()Called by functions such as SetRange, SetNpx, SetParameters",MatchSource.WIKI,doc/master/minuit2FitBench_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2FitBench_8C.html
https://root.cern/doc/master/minuit2FitBench_8C.html:230,Performance,perform,performance,230,". ROOT: tutorials/fit/minuit2FitBench.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. minuit2FitBench.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Demonstrate performance and usage of Minuit2 and Fumili2 for monodimensional fits. . ; ; *********************************************************************************; Minuit ; *********************************************************************************; pass : 0; ................... FCN=205.276 FROM MINOS STATUS=SUCCESSFUL 44 CALLS 429 TOTAL; EDM=3.83288e-10 STRATEGY= 1 ERROR MATRIX ACCURATE ; EXT PARAMETER STEP FIRST ; NO. NAME VALUE ERROR SIZE DERIVATIVE ; 1 p0 5.13639e+01 2.01329e+00 -2.79418e-04 -2.05471e-06; 2 p1 5.57813e+01 4.80582e+00 3.09127e-03 -9.98919e-07; 3 p2 7.42112e+01 1.87041e+00 -1.20311e-03 -1.93173e-07; 4 p3 4.27344e+02 2.93232e+00 -1.66243e-02 -7.80957e-07; 5 p4 3.58604e-02 3.47005e-04 1.74159e-07 9.80777e-02; 6 p5 1.00001e+00 1.64203e-04 1.64203e-04 3.19213e-02; Minuit, npass=20 : RT= 0.170 s, Cpu= 0.170 s; ; *********************************************************************************; Fumili ; *********************************************************************************; pass : 0; ...................****************************************; Minimizer is Fumili; Chi2 = 206.284; NDf = 194; NCalls = 4; p0 = 51.4325 +/- 2.01397 ; p1 = 55.5412 +/- 4.81253 ; p2 = 74.2976 +/- 1.87298 ; p3 = 427.425 +/- 2.93868 ; p4 = 0.0358559 +/- 0.000357243 ; p5 = 1.00001 +/- 0.00016009 ; Fumili, npass=20 : RT= 0.055 s, Cpu= 0.060 s; ; *********************************************************************************; Minuit2 ; *********************************************************************************; pass : 0; ...................****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 205.34; NDf = 194; Edm = 1.91398e-10; NCalls = 85; p0 = 51.3576 +/- 2.0133 -2.01329 +2.0133 (Minos) ; p1 = 55.8172 +/- 4.",MatchSource.WIKI,doc/master/minuit2FitBench_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2FitBench_8C.html
https://root.cern/doc/master/minuit2GausFit_8C.html:244,Deployability,configurat,configurations,244,. ROOT: tutorials/fit/minuit2GausFit.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. minuit2GausFit.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Perform fits with different configurations using Minuit2 . ; ; Do Fit 1; ; Do Fit 2; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 65.1586; NDf = 56; Edm = 1.93774e-09; NCalls = 69; Constant = 36.3132 +/- 1.52625 -1.51651 +1.53547 (Minos) ; Mean = 0.013082 +/- 0.0347499 -0.0347674 +0.0347613 (Minos) ; Sigma = 1.03413 +/- 0.0288039 -0.0286274 +0.0290102 (Minos) (limited); ; Do Fit 3; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 65.1586; NDf = 56; Edm = 6.86315e-08; NCalls = 57; Constant = 36.327 +/- 2 -1.51685 +1.53726 (Minos) ; Mean = 0.0130817 +/- 2 ; Sigma = 1.03373 +/- 6.72116 (limited); ; Do Fit 4; ****************************************; Minimizer is Minuit2 / Migrad; MinFCN = 43.3935; Chi2 = 86.7869; NDf = 97; Edm = 9.97216e-08; NCalls = 62; Constant = 38.427 +/- 1.48837 -1.46667 +1.51031 (Minos) ; Mean = 0.027601 +/- 0.032831 -0.0328395 +0.0328395 (Minos) ; Sigma = 1.03819 +/- 0.0232194 -0.0227841 +0.0236699 (Minos) (limited); ; Do Fit 1; ; Do Fit 2; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 65.1586; NDf = 56; Edm = 8.05693e-09; NCalls = 45; Constant = 36.3131 +/- 1.52625 -1.51642 +1.53556 (Minos) ; Mean = 0.0130818 +/- 0.0347499 -0.0347671 +0.0347615 (Minos) ; Sigma = 1.03413 +/- 0.0288039 -0.0286291 +0.0290085 (Minos) (limited); ; Do Fit 3; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 65.1586; NDf = 56; Edm = 1.52369e-08; NCalls = 45; Constant = 36.3272 +/- 1.52734 -1.51745 +1.53671 (Minos) ; Mean = 0.0130818 +/- 0.0347499 -0.0347671 +0.0347615 (Minos) ; Sigma = 1.03373 +/- 0.0288151 -0.0286415 +0.0290186 (Minos) (limited); ; Do Fit 4; ****************************************; Minimizer is Minuit2 / Fumili,MatchSource.WIKI,doc/master/minuit2GausFit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2GausFit_8C.html
https://root.cern/doc/master/minuit2GausFit_8C.html:244,Modifiability,config,configurations,244,. ROOT: tutorials/fit/minuit2GausFit.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. minuit2GausFit.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Perform fits with different configurations using Minuit2 . ; ; Do Fit 1; ; Do Fit 2; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 65.1586; NDf = 56; Edm = 1.93774e-09; NCalls = 69; Constant = 36.3132 +/- 1.52625 -1.51651 +1.53547 (Minos) ; Mean = 0.013082 +/- 0.0347499 -0.0347674 +0.0347613 (Minos) ; Sigma = 1.03413 +/- 0.0288039 -0.0286274 +0.0290102 (Minos) (limited); ; Do Fit 3; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 65.1586; NDf = 56; Edm = 6.86315e-08; NCalls = 57; Constant = 36.327 +/- 2 -1.51685 +1.53726 (Minos) ; Mean = 0.0130817 +/- 2 ; Sigma = 1.03373 +/- 6.72116 (limited); ; Do Fit 4; ****************************************; Minimizer is Minuit2 / Migrad; MinFCN = 43.3935; Chi2 = 86.7869; NDf = 97; Edm = 9.97216e-08; NCalls = 62; Constant = 38.427 +/- 1.48837 -1.46667 +1.51031 (Minos) ; Mean = 0.027601 +/- 0.032831 -0.0328395 +0.0328395 (Minos) ; Sigma = 1.03819 +/- 0.0232194 -0.0227841 +0.0236699 (Minos) (limited); ; Do Fit 1; ; Do Fit 2; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 65.1586; NDf = 56; Edm = 8.05693e-09; NCalls = 45; Constant = 36.3131 +/- 1.52625 -1.51642 +1.53556 (Minos) ; Mean = 0.0130818 +/- 0.0347499 -0.0347671 +0.0347615 (Minos) ; Sigma = 1.03413 +/- 0.0288039 -0.0286291 +0.0290085 (Minos) (limited); ; Do Fit 3; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 65.1586; NDf = 56; Edm = 1.52369e-08; NCalls = 45; Constant = 36.3272 +/- 1.52734 -1.51745 +1.53671 (Minos) ; Mean = 0.0130818 +/- 0.0347499 -0.0347671 +0.0347615 (Minos) ; Sigma = 1.03373 +/- 0.0288151 -0.0286415 +0.0290186 (Minos) (limited); ; Do Fit 4; ****************************************; Minimizer is Minuit2 / Fumili,MatchSource.WIKI,doc/master/minuit2GausFit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2GausFit_8C.html
https://root.cern/doc/master/minuit2GausFit_8C.html:2480,Testability,test,testGausFit,2480,"= 1.03413 +/- 0.0288039 -0.0286291 +0.0290085 (Minos) (limited); ; Do Fit 3; ****************************************; Minimizer is Minuit2 / Fumili; Chi2 = 65.1586; NDf = 56; Edm = 1.52369e-08; NCalls = 45; Constant = 36.3272 +/- 1.52734 -1.51745 +1.53671 (Minos) ; Mean = 0.0130818 +/- 0.0347499 -0.0347671 +0.0347615 (Minos) ; Sigma = 1.03373 +/- 0.0288151 -0.0286415 +0.0290186 (Minos) (limited); ; Do Fit 4; ****************************************; Minimizer is Minuit2 / Fumili; MinFCN = 43.3935; Chi2 = 86.7869; NDf = 97; Edm = 3.18744e-08; NCalls = 45; Constant = 38.4264 +/- 1.48835 -1.46601 +1.51097 (Minos) ; Mean = 0.0275931 +/- 0.0328313 -0.0328316 +0.0328474 (Minos) ; Sigma = 1.0382 +/- 0.0232197 -0.0227928 +0.0236612 (Minos) (limited); ; #include ""TH1.h""; #include ""TF1.h""; #include ""TCanvas.h""; #include ""TRandom3.h""; #include ""TVirtualFitter.h""; #include ""TPaveLabel.h""; #include ""TStyle.h""; ; #include <iostream>; #include <string>; ; ; void testGausFit( std::string type = ""Minuit2"", int n = 1000) {; ; gRandom = new TRandom3();; ; TVirtualFitter::SetDefaultFitter(type.c_str() );; ; std::string name;; name = ""h1_"" + type;; TH1D * h1 = new TH1D(name.c_str(),""Chi2 Fit"",100, -5, 5. );; name = ""h2_"" + type;; TH1D * h2 = new TH1D(name.c_str(),""Chi2 Fit with Minos Error"",100, -5, 5. );; name = ""h3_"" + type;; TH1D * h3 = new TH1D(name.c_str(),""Chi2 Fit with Integral and Minos"",100, -5, 5. );; name = ""h4_"" + type;; TH1D * h4 = new TH1D(name.c_str(),""Likelihood Fit with Minos Error"",100, -5, 5. );; ; gStyle->SetOptStat(1111111);; gStyle->SetOptFit(1111111);; ; for (int i = 0; i < n; ++i) {; double x = gRandom->Gaus(0,1);; h1->Fill( x );; h2->Fill( x );; h3->Fill( x );; h4->Fill( x );; }; ; std::string cname = type + ""Canvas"" ;; std::string ctitle = type + "" Gaussian Fit"" ;; TCanvas *c1 = new TCanvas(cname.c_str(),cname.c_str(),10,10,900,900);; c1->Divide(2,2);; ; c1->cd(1);; std::cout << ""\nDo Fit 1\n"";; h1->Fit(""gaus"",""Q"");; h1->Draw();; c1->cd(2);; std::cout << ""\nDo",MatchSource.WIKI,doc/master/minuit2GausFit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2GausFit_8C.html
https://root.cern/doc/master/minuit2GausFit_8C.html:3770,Testability,test,testGausFit,3770,"er(type.c_str() );; ; std::string name;; name = ""h1_"" + type;; TH1D * h1 = new TH1D(name.c_str(),""Chi2 Fit"",100, -5, 5. );; name = ""h2_"" + type;; TH1D * h2 = new TH1D(name.c_str(),""Chi2 Fit with Minos Error"",100, -5, 5. );; name = ""h3_"" + type;; TH1D * h3 = new TH1D(name.c_str(),""Chi2 Fit with Integral and Minos"",100, -5, 5. );; name = ""h4_"" + type;; TH1D * h4 = new TH1D(name.c_str(),""Likelihood Fit with Minos Error"",100, -5, 5. );; ; gStyle->SetOptStat(1111111);; gStyle->SetOptFit(1111111);; ; for (int i = 0; i < n; ++i) {; double x = gRandom->Gaus(0,1);; h1->Fill( x );; h2->Fill( x );; h3->Fill( x );; h4->Fill( x );; }; ; std::string cname = type + ""Canvas"" ;; std::string ctitle = type + "" Gaussian Fit"" ;; TCanvas *c1 = new TCanvas(cname.c_str(),cname.c_str(),10,10,900,900);; c1->Divide(2,2);; ; c1->cd(1);; std::cout << ""\nDo Fit 1\n"";; h1->Fit(""gaus"",""Q"");; h1->Draw();; c1->cd(2);; std::cout << ""\nDo Fit 2\n"";; h2->Fit(""gaus"",""E"");; h2->Draw();; c1->cd(3);; std::cout << ""\nDo Fit 3\n"";; h3->Fit(""gaus"",""IGE"");; h3->Draw();; c1->cd(4);; std::cout << ""\nDo Fit 4\n"";; h4->Fit(""gaus"",""LE"");; h4->Draw();; ; }; ; void minuit2GausFit() {; ; int n = 1000;; testGausFit(""Minuit2"",n);; testGausFit(""Fumili2"",n);; ; }; TCanvas.h; TF1.h; cnameOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char cnameDefinition TGWin32VirtualXProxy.cxx:230; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAl",MatchSource.WIKI,doc/master/minuit2GausFit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2GausFit_8C.html
https://root.cern/doc/master/minuit2GausFit_8C.html:3797,Testability,test,testGausFit,3797,"er(type.c_str() );; ; std::string name;; name = ""h1_"" + type;; TH1D * h1 = new TH1D(name.c_str(),""Chi2 Fit"",100, -5, 5. );; name = ""h2_"" + type;; TH1D * h2 = new TH1D(name.c_str(),""Chi2 Fit with Minos Error"",100, -5, 5. );; name = ""h3_"" + type;; TH1D * h3 = new TH1D(name.c_str(),""Chi2 Fit with Integral and Minos"",100, -5, 5. );; name = ""h4_"" + type;; TH1D * h4 = new TH1D(name.c_str(),""Likelihood Fit with Minos Error"",100, -5, 5. );; ; gStyle->SetOptStat(1111111);; gStyle->SetOptFit(1111111);; ; for (int i = 0; i < n; ++i) {; double x = gRandom->Gaus(0,1);; h1->Fill( x );; h2->Fill( x );; h3->Fill( x );; h4->Fill( x );; }; ; std::string cname = type + ""Canvas"" ;; std::string ctitle = type + "" Gaussian Fit"" ;; TCanvas *c1 = new TCanvas(cname.c_str(),cname.c_str(),10,10,900,900);; c1->Divide(2,2);; ; c1->cd(1);; std::cout << ""\nDo Fit 1\n"";; h1->Fit(""gaus"",""Q"");; h1->Draw();; c1->cd(2);; std::cout << ""\nDo Fit 2\n"";; h2->Fit(""gaus"",""E"");; h2->Draw();; c1->cd(3);; std::cout << ""\nDo Fit 3\n"";; h3->Fit(""gaus"",""IGE"");; h3->Draw();; c1->cd(4);; std::cout << ""\nDo Fit 4\n"";; h4->Fit(""gaus"",""LE"");; h4->Draw();; ; }; ; void minuit2GausFit() {; ; int n = 1000;; testGausFit(""Minuit2"",n);; testGausFit(""Fumili2"",n);; ; }; TCanvas.h; TF1.h; cnameOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char cnameDefinition TGWin32VirtualXProxy.cxx:230; typeOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAl",MatchSource.WIKI,doc/master/minuit2GausFit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/minuit2GausFit_8C.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:1314,Availability,avail,available,1314,"***************; 5 * *; 6 * Copyright (c) 2015 LCG ROOT Math Team, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// random engines based on ROOT; 12 ; 13#ifndef ROOT_Math_MixMaxEngine; 14#define ROOT_Math_MixMaxEngine; 15 ; 16#include <cstdint>; 17#include <vector>; 18 ; 19#include ""Math/TRandomEngine.h""; 20 ; 21 ; 22// struct rng_state_st; /// forward declare generator state; 23 ; 24// typedef struct rng_state_st rng_state_t;; 25 ; 26// namespace mixmax {; 27// template<int Ndim>; 28// class mixmax_engine;; 29// }; 30 ; 31namespace ROOT {; 32 ; 33 namespace Math {; 34 ; 35 template<int N>; 36 class MixMaxEngineImpl;; 37 ; 38/**; 39MixMaxEngine is a wrapper class for the MIXMAX Random number generator.; 40MIXMAX is a matrix-recursive random number generator introduced by; 41G. Savvidy.; 42 ; 43The real implementation of the generator, written in C, is in the mixmax.h and mixmax.cxx files.; 44This generator code is available also at hepforge: http://mixmax.hepforge.org; 45The MIXMAX code has been created and developed by Konstantin Savvidy and it is; 46released under GNU Lesser General Public License v3.; 47 ; 48This wrapper class provides 3 different variants of MIXMAX according to the template para extra parameter N.; 49The extra parameter, `SkipNumber`, is used to perform additional iterations of the generator before returning the random numbers.; 50For example, when `SkipNumber = 2`, the generator will have two extra iterations that will be discarder.; 51 ; 52 - MIXMAX with N = 240. This is a new version of the generator (version 2.0beta) described in the; 53 <a href=""http://dx.doi.org/10.1016/j.chaos.2016.05.003"">2016 paper</a> (3rd reference), with; 54 special number \f$s=487013230256099140\f$, \f$m=2^{51}+1\f$ and having a period of \f$10^{4389}\f$.; 55 ; 56 - MIXMAX with N = 17, from the 2.0 beta version with \f$s=0\f$ and \f$m=2^{36}+1\f$. The period of the; 57 generator is \f$10^{294}\f$.; 58 ;",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:1044,Integrability,wrap,wrapper,1044,". ROOT: math/mathcore/inc/Math/MixMaxEngine.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MixMaxEngine.h. Go to the documentation of this file. 1// @(#)root/mathcore:$Id$; 2// Author: L. Moneta Tue Aug 4 2015; 3 ; 4/**********************************************************************; 5 * *; 6 * Copyright (c) 2015 LCG ROOT Math Team, CERN/PH-SFT *; 7 * *; 8 * *; 9 **********************************************************************/; 10 ; 11// random engines based on ROOT; 12 ; 13#ifndef ROOT_Math_MixMaxEngine; 14#define ROOT_Math_MixMaxEngine; 15 ; 16#include <cstdint>; 17#include <vector>; 18 ; 19#include ""Math/TRandomEngine.h""; 20 ; 21 ; 22// struct rng_state_st; /// forward declare generator state; 23 ; 24// typedef struct rng_state_st rng_state_t;; 25 ; 26// namespace mixmax {; 27// template<int Ndim>; 28// class mixmax_engine;; 29// }; 30 ; 31namespace ROOT {; 32 ; 33 namespace Math {; 34 ; 35 template<int N>; 36 class MixMaxEngineImpl;; 37 ; 38/**; 39MixMaxEngine is a wrapper class for the MIXMAX Random number generator.; 40MIXMAX is a matrix-recursive random number generator introduced by; 41G. Savvidy.; 42 ; 43The real implementation of the generator, written in C, is in the mixmax.h and mixmax.cxx files.; 44This generator code is available also at hepforge: http://mixmax.hepforge.org; 45The MIXMAX code has been created and developed by Konstantin Savvidy and it is; 46released under GNU Lesser General Public License v3.; 47 ; 48This wrapper class provides 3 different variants of MIXMAX according to the template para extra parameter N.; 49The extra parameter, `SkipNumber`, is used to perform additional iterations of the generator before returning the random numbers.; 50For example, when `SkipNumber = 2`, the generator will have two extra iterations that will be discarder.; 51 ; 52 - MIXMAX with N = 240. This is a new version of the generator (version 2.0beta) described in the; 53 <a href=""http://dx.doi.org",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:1520,Integrability,wrap,wrapper,1520,"ngine; 14#define ROOT_Math_MixMaxEngine; 15 ; 16#include <cstdint>; 17#include <vector>; 18 ; 19#include ""Math/TRandomEngine.h""; 20 ; 21 ; 22// struct rng_state_st; /// forward declare generator state; 23 ; 24// typedef struct rng_state_st rng_state_t;; 25 ; 26// namespace mixmax {; 27// template<int Ndim>; 28// class mixmax_engine;; 29// }; 30 ; 31namespace ROOT {; 32 ; 33 namespace Math {; 34 ; 35 template<int N>; 36 class MixMaxEngineImpl;; 37 ; 38/**; 39MixMaxEngine is a wrapper class for the MIXMAX Random number generator.; 40MIXMAX is a matrix-recursive random number generator introduced by; 41G. Savvidy.; 42 ; 43The real implementation of the generator, written in C, is in the mixmax.h and mixmax.cxx files.; 44This generator code is available also at hepforge: http://mixmax.hepforge.org; 45The MIXMAX code has been created and developed by Konstantin Savvidy and it is; 46released under GNU Lesser General Public License v3.; 47 ; 48This wrapper class provides 3 different variants of MIXMAX according to the template para extra parameter N.; 49The extra parameter, `SkipNumber`, is used to perform additional iterations of the generator before returning the random numbers.; 50For example, when `SkipNumber = 2`, the generator will have two extra iterations that will be discarder.; 51 ; 52 - MIXMAX with N = 240. This is a new version of the generator (version 2.0beta) described in the; 53 <a href=""http://dx.doi.org/10.1016/j.chaos.2016.05.003"">2016 paper</a> (3rd reference), with; 54 special number \f$s=487013230256099140\f$, \f$m=2^{51}+1\f$ and having a period of \f$10^{4389}\f$.; 55 ; 56 - MIXMAX with N = 17, from the 2.0 beta version with \f$s=0\f$ and \f$m=2^{36}+1\f$. The period of the; 57 generator is \f$10^{294}\f$.; 58 ; 59 - MIXMAX with N = 256 from the 1.0 version. The period is (for `SkipNumber=0`) \f$10^{4682}\f$.; 60 For this generator we recommend in ROOT using a default value of `SkipNumber=2, while for the; 61 previous two generators skipping is not n",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:5298,Integrability,interface,interface,5298,"inimum integer that can be generated. For MIXMAX is 0; 129 static uint64_t MinInt();; 130 ; 131 /// set the generator seed; 132 void SetSeed(Result_t seed);; 133 ; 134 // generate a random number (virtual interface); 135 double Rndm() override { return Rndm_impl(); }; 136 ; 137 /// generate a double random number (faster interface); 138 inline double operator() () { return Rndm_impl(); }; 139 ; 140 /// generate an array of random numbers; 141 void RndmArray (int n, double * array);; 142 ; 143 /// generate a 64 bit integer number; 144 Result_t IntRndm();; 145 ; 146 /// get name of the generator; 147 static const char *Name();; 148 ; 149 protected:; 150 // protected functions used for testing the generator; 151 ; 152 /// get the state of the generator; 153 void GetState(std::vector<StateInt_t> & state) const;; 154 ; 155 ; 156 ///set the full initial generator state; 157 void SetState(const std::vector<StateInt_t> & state);; 158 ; 159 /// Get the counter (between 0 and Size-1); 160 int Counter() const;; 161 ; 162 ; 163 private:; 164 ; 165 /// implementation function to generate the random number; 166 double Rndm_impl();; 167 ; 168 //rng_state_t * fRngState; ///< mix-max generator state; 169 //mixmax::mixmax_engine<N> * fRng; ///< mixmax internal engine class; 170 MixMaxEngineImpl<N> * fRng; ///< mixmax internal engine class; 171 ; 172 };; 173 ; 174 typedef MixMaxEngine<240,0> MixMaxEngine240;; 175 typedef MixMaxEngine<256,2> MixMaxEngine256;; 176 typedef MixMaxEngine<17,0> MixMaxEngine17;; 177 ; 178 extern template class MixMaxEngine<240,0>;; 179 extern template class MixMaxEngine<256,0>;; 180 extern template class MixMaxEngine<256,2>;; 181 extern template class MixMaxEngine<256,4>;; 182 extern template class MixMaxEngine<17,0>;; 183 extern template class MixMaxEngine<17,1>;; 184 extern template class MixMaxEngine<17,2>;; 185 ; 186 } // end namespace Math; 187 ; 188} // end namespace ROOT; 189 ; 190 ; 191#include ""Math/MixMaxEngine.icc""; 192 ; 193#endif /* ROOT_Math_Mi",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:5416,Integrability,interface,interface,5416,"inimum integer that can be generated. For MIXMAX is 0; 129 static uint64_t MinInt();; 130 ; 131 /// set the generator seed; 132 void SetSeed(Result_t seed);; 133 ; 134 // generate a random number (virtual interface); 135 double Rndm() override { return Rndm_impl(); }; 136 ; 137 /// generate a double random number (faster interface); 138 inline double operator() () { return Rndm_impl(); }; 139 ; 140 /// generate an array of random numbers; 141 void RndmArray (int n, double * array);; 142 ; 143 /// generate a 64 bit integer number; 144 Result_t IntRndm();; 145 ; 146 /// get name of the generator; 147 static const char *Name();; 148 ; 149 protected:; 150 // protected functions used for testing the generator; 151 ; 152 /// get the state of the generator; 153 void GetState(std::vector<StateInt_t> & state) const;; 154 ; 155 ; 156 ///set the full initial generator state; 157 void SetState(const std::vector<StateInt_t> & state);; 158 ; 159 /// Get the counter (between 0 and Size-1); 160 int Counter() const;; 161 ; 162 ; 163 private:; 164 ; 165 /// implementation function to generate the random number; 166 double Rndm_impl();; 167 ; 168 //rng_state_t * fRngState; ///< mix-max generator state; 169 //mixmax::mixmax_engine<N> * fRng; ///< mixmax internal engine class; 170 MixMaxEngineImpl<N> * fRng; ///< mixmax internal engine class; 171 ; 172 };; 173 ; 174 typedef MixMaxEngine<240,0> MixMaxEngine240;; 175 typedef MixMaxEngine<256,2> MixMaxEngine256;; 176 typedef MixMaxEngine<17,0> MixMaxEngine17;; 177 ; 178 extern template class MixMaxEngine<240,0>;; 179 extern template class MixMaxEngine<256,0>;; 180 extern template class MixMaxEngine<256,2>;; 181 extern template class MixMaxEngine<256,4>;; 182 extern template class MixMaxEngine<17,0>;; 183 extern template class MixMaxEngine<17,1>;; 184 extern template class MixMaxEngine<17,2>;; 185 ; 186 } // end namespace Math; 187 ; 188} // end namespace ROOT; 189 ; 190 ; 191#include ""Math/MixMaxEngine.icc""; 192 ; 193#endif /* ROOT_Math_Mi",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:7247,Integrability,wrap,wrapper,7247,"e_t * fRngState; ///< mix-max generator state; 169 //mixmax::mixmax_engine<N> * fRng; ///< mixmax internal engine class; 170 MixMaxEngineImpl<N> * fRng; ///< mixmax internal engine class; 171 ; 172 };; 173 ; 174 typedef MixMaxEngine<240,0> MixMaxEngine240;; 175 typedef MixMaxEngine<256,2> MixMaxEngine256;; 176 typedef MixMaxEngine<17,0> MixMaxEngine17;; 177 ; 178 extern template class MixMaxEngine<240,0>;; 179 extern template class MixMaxEngine<256,0>;; 180 extern template class MixMaxEngine<256,2>;; 181 extern template class MixMaxEngine<256,4>;; 182 extern template class MixMaxEngine<17,0>;; 183 extern template class MixMaxEngine<17,1>;; 184 extern template class MixMaxEngine<17,2>;; 185 ; 186 } // end namespace Math; 187 ; 188} // end namespace ROOT; 189 ; 190 ; 191#include ""Math/MixMaxEngine.icc""; 192 ; 193#endif /* ROOT_Math_MixMaxEngine */; MixMaxEngine.icc; TRandomEngine.h; ROOT::Math::MixMaxEngineImplDefinition MixMaxEngineImpl.h:52; ROOT::Math::MixMaxEngineMixMaxEngine is a wrapper class for the MIXMAX Random number generator.Definition MixMaxEngine.h:102; ROOT::Math::MixMaxEngine::StateInt_tuint64_t StateInt_tDefinition MixMaxEngine.h:110; ROOT::Math::MixMaxEngine::IntRndmResult_t IntRndm()generate a 64 bit integer numberDefinition MixMaxEngine.icc:103; ROOT::Math::MixMaxEngine::GetStatevoid GetState(std::vector< StateInt_t > &state) constget the state of the generatorDefinition MixMaxEngine.icc:141; ROOT::Math::MixMaxEngine::~MixMaxEngine~MixMaxEngine() overrideDefinition MixMaxEngine.icc:42; ROOT::Math::MixMaxEngine::SetSeedvoid SetSeed(Result_t seed)set the generator seedDefinition MixMaxEngine.icc:53; ROOT::Math::MixMaxEngine::BaseTypeTRandomEngine BaseTypeDefinition MixMaxEngine.h:106; ROOT::Math::MixMaxEngine::Rndmdouble Rndm() overrideDefinition MixMaxEngine.h:135; ROOT::Math::MixMaxEngine::Counterint Counter() constGet the counter (between 0 and Size-1)Definition MixMaxEngine.icc:152; ROOT::Math::MixMaxEngine::operator()double operator()()generate a",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:8280,Integrability,interface,interface,8280,"a wrapper class for the MIXMAX Random number generator.Definition MixMaxEngine.h:102; ROOT::Math::MixMaxEngine::StateInt_tuint64_t StateInt_tDefinition MixMaxEngine.h:110; ROOT::Math::MixMaxEngine::IntRndmResult_t IntRndm()generate a 64 bit integer numberDefinition MixMaxEngine.icc:103; ROOT::Math::MixMaxEngine::GetStatevoid GetState(std::vector< StateInt_t > &state) constget the state of the generatorDefinition MixMaxEngine.icc:141; ROOT::Math::MixMaxEngine::~MixMaxEngine~MixMaxEngine() overrideDefinition MixMaxEngine.icc:42; ROOT::Math::MixMaxEngine::SetSeedvoid SetSeed(Result_t seed)set the generator seedDefinition MixMaxEngine.icc:53; ROOT::Math::MixMaxEngine::BaseTypeTRandomEngine BaseTypeDefinition MixMaxEngine.h:106; ROOT::Math::MixMaxEngine::Rndmdouble Rndm() overrideDefinition MixMaxEngine.h:135; ROOT::Math::MixMaxEngine::Counterint Counter() constGet the counter (between 0 and Size-1)Definition MixMaxEngine.icc:152; ROOT::Math::MixMaxEngine::operator()double operator()()generate a double random number (faster interface)Definition MixMaxEngine.h:138; ROOT::Math::MixMaxEngine::Rndm_impldouble Rndm_impl()implementation function to generate the random numberDefinition MixMaxEngine.icc:92; ROOT::Math::MixMaxEngine::Namestatic const char * Name()get name of the generatorDefinition MixMaxEngine.icc:157; ROOT::Math::MixMaxEngine::fRngMixMaxEngineImpl< N > * fRngmixmax internal engine classDefinition MixMaxEngine.h:170; ROOT::Math::MixMaxEngine::RndmArrayvoid RndmArray(int n, double *array)generate an array of random numbersDefinition MixMaxEngine.icc:123; ROOT::Math::MixMaxEngine::SetStatevoid SetState(const std::vector< StateInt_t > &state)set the full initial generator stateDefinition MixMaxEngine.icc:130; ROOT::Math::MixMaxEngine::Sizestatic int Size()Get the size of the generator.Definition MixMaxEngine.icc:147; ROOT::Math::MixMaxEngine::Result_tuint64_t Result_tDefinition MixMaxEngine.h:114; ROOT::Math::MixMaxEngine::MaxIntstatic uint64_t MaxInt()maximum inte",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:1673,Performance,perform,perform,1673,"h""; 20 ; 21 ; 22// struct rng_state_st; /// forward declare generator state; 23 ; 24// typedef struct rng_state_st rng_state_t;; 25 ; 26// namespace mixmax {; 27// template<int Ndim>; 28// class mixmax_engine;; 29// }; 30 ; 31namespace ROOT {; 32 ; 33 namespace Math {; 34 ; 35 template<int N>; 36 class MixMaxEngineImpl;; 37 ; 38/**; 39MixMaxEngine is a wrapper class for the MIXMAX Random number generator.; 40MIXMAX is a matrix-recursive random number generator introduced by; 41G. Savvidy.; 42 ; 43The real implementation of the generator, written in C, is in the mixmax.h and mixmax.cxx files.; 44This generator code is available also at hepforge: http://mixmax.hepforge.org; 45The MIXMAX code has been created and developed by Konstantin Savvidy and it is; 46released under GNU Lesser General Public License v3.; 47 ; 48This wrapper class provides 3 different variants of MIXMAX according to the template para extra parameter N.; 49The extra parameter, `SkipNumber`, is used to perform additional iterations of the generator before returning the random numbers.; 50For example, when `SkipNumber = 2`, the generator will have two extra iterations that will be discarder.; 51 ; 52 - MIXMAX with N = 240. This is a new version of the generator (version 2.0beta) described in the; 53 <a href=""http://dx.doi.org/10.1016/j.chaos.2016.05.003"">2016 paper</a> (3rd reference), with; 54 special number \f$s=487013230256099140\f$, \f$m=2^{51}+1\f$ and having a period of \f$10^{4389}\f$.; 55 ; 56 - MIXMAX with N = 17, from the 2.0 beta version with \f$s=0\f$ and \f$m=2^{36}+1\f$. The period of the; 57 generator is \f$10^{294}\f$.; 58 ; 59 - MIXMAX with N = 256 from the 1.0 version. The period is (for `SkipNumber=0`) \f$10^{4682}\f$.; 60 For this generator we recommend in ROOT using a default value of `SkipNumber=2, while for the; 61 previous two generators skipping is not needed.; 62 ; 63This table describes the properties of the MIXMAX generators. MIXMAX is a genuine 61 bit; 64generator on the G",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/MixMaxEngine_8h_source.html:5785,Testability,test,testing,5785,"inimum integer that can be generated. For MIXMAX is 0; 129 static uint64_t MinInt();; 130 ; 131 /// set the generator seed; 132 void SetSeed(Result_t seed);; 133 ; 134 // generate a random number (virtual interface); 135 double Rndm() override { return Rndm_impl(); }; 136 ; 137 /// generate a double random number (faster interface); 138 inline double operator() () { return Rndm_impl(); }; 139 ; 140 /// generate an array of random numbers; 141 void RndmArray (int n, double * array);; 142 ; 143 /// generate a 64 bit integer number; 144 Result_t IntRndm();; 145 ; 146 /// get name of the generator; 147 static const char *Name();; 148 ; 149 protected:; 150 // protected functions used for testing the generator; 151 ; 152 /// get the state of the generator; 153 void GetState(std::vector<StateInt_t> & state) const;; 154 ; 155 ; 156 ///set the full initial generator state; 157 void SetState(const std::vector<StateInt_t> & state);; 158 ; 159 /// Get the counter (between 0 and Size-1); 160 int Counter() const;; 161 ; 162 ; 163 private:; 164 ; 165 /// implementation function to generate the random number; 166 double Rndm_impl();; 167 ; 168 //rng_state_t * fRngState; ///< mix-max generator state; 169 //mixmax::mixmax_engine<N> * fRng; ///< mixmax internal engine class; 170 MixMaxEngineImpl<N> * fRng; ///< mixmax internal engine class; 171 ; 172 };; 173 ; 174 typedef MixMaxEngine<240,0> MixMaxEngine240;; 175 typedef MixMaxEngine<256,2> MixMaxEngine256;; 176 typedef MixMaxEngine<17,0> MixMaxEngine17;; 177 ; 178 extern template class MixMaxEngine<240,0>;; 179 extern template class MixMaxEngine<256,0>;; 180 extern template class MixMaxEngine<256,2>;; 181 extern template class MixMaxEngine<256,4>;; 182 extern template class MixMaxEngine<17,0>;; 183 extern template class MixMaxEngine<17,1>;; 184 extern template class MixMaxEngine<17,2>;; 185 ; 186 } // end namespace Math; 187 ; 188} // end namespace ROOT; 189 ; 190 ; 191#include ""Math/MixMaxEngine.icc""; 192 ; 193#endif /* ROOT_Math_Mi",MatchSource.WIKI,doc/master/MixMaxEngine_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MixMaxEngine_8h_source.html
https://root.cern/doc/master/motorcycle_8C.html:223,Testability,test,test,223,". ROOT: tutorials/graphs/motorcycle.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. motorcycle.C File ReferenceTutorials » Graphs tutorials. Detailed Description; Macro to test scatterplot smoothers: ksmooth, lowess, supsmu as described in: ; Modern Applied Statistics with S-Plus, 3rd Edition; W.N. Venables and B.D. Ripley; Chapter 9: Smooth Regression, Figure 9.1; Example is a set of data on 133 observations of acceleration against time for a simulated motorcycle accident, taken from Silverman (1985). ; #include ""TString.h""; #include ""TInterpreter.h""; #include <fstream>; #include ""TH1.h""; #include ""TGraphSmooth.h""; #include ""TCanvas.h""; #include ""TSystem.h""; ; ; TCanvas *vC1;; TGraph *grin, *grout;; ; void DrawSmooth(Int_t pad, const char *title, const char *xt, const char *yt); {; vC1->cd(pad);; TH1F *vFrame = gPad->DrawFrame(0,-130,60,70);; vFrame->SetTitle(title);; vFrame->SetTitleSize(0.2);; vFrame->SetXTitle(xt);; vFrame->SetYTitle(yt);; grin->Draw(""P"");; grout->DrawClone(""LPX"");; }; ; void motorcycle(); {; // data taken from R library MASS: mcycle.txt; TString dir = gROOT->GetTutorialDir();; dir.Append(""/graphs/"");; dir.ReplaceAll(""/./"",""/"");; ; // read file and add to fit object; Double_t *x = new Double_t[133];; Double_t *y = new Double_t[133];; Double_t vX, vY;; Int_t vNData = 0;; ifstream vInput;; vInput.open(Form(""%smotorcycle.dat"",dir.Data()));; while (1) {; vInput >> vX >> vY;; if (!vInput.good()) break;; x[vNData] = vX;; y[vNData] = vY;; vNData++;; }//while; vInput.close();; grin = new TGraph(vNData,x,y);; ; // draw graph; vC1 = new TCanvas(""vC1"",""Smooth Regression"",200,10,900,700);; vC1->Divide(2,3);; ; // Kernel Smoother; // create new kernel smoother and smooth data with bandwidth = 2.0; TGraphSmooth *gs = new TGraphSmooth(""normal"");; grout = gs->SmoothKern(grin,""normal"",2.0);; DrawSmooth(1,""Kernel Smoother: bandwidth = 2.0"",""times"",""accel"");; ; // redraw ksmooth with bandwidth = 5.0; grout = gs->Sm",MatchSource.WIKI,doc/master/motorcycle_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/motorcycle_8C.html
https://root.cern/doc/master/mp3player_8C.html:20877,Integrability,interface,interface,20877,"tTextSize(0.06);; Tex.SetTextColor(10);; Tex.DrawLatex(0.06,0.55,""+ GUI Theme Skin"");; ; Tex.SetTextSize(0.06);; Tex.SetTextColor(10);; Tex.DrawLatex(0.06,0.45,""+ Noble White&Black"");; ; Tex.SetTextSize(0.06);; Tex.SetTextColor(10);; Tex.DrawLatex(0.06,0.35,""+ Text Viewer+Image Viewer"");; ; Tex.SetTextSize(0.06);; Tex.SetTextColor(10);; Tex.DrawLatex(0.06,0.25,""+ 20 Hours Playing"");; ; Tex.SetTextSize(0.06);; Tex.SetTextColor(10);; Tex.DrawLatex(0.06,0.15,""+ The Best Quality of Sound"");; ; ; pad1->cd();; }; kFALSEconstexpr Bool_t kFALSEDefinition RtypesCore.h:94; TButton.h; TCanvas.h; TGeoManager.h; TImage.h; TLatex.h; TLine.h; TPaveText.h; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TAttText::SetTextColorvirtual void SetTextColor(Color_t tcolor=1)Set the text color.Definition TAttText.h:44; TAttText::SetTextSizevirtual void SetTextSize(Float_t tsize=1)Set the text size.Definition TAttText.h:47; TButtonA TButton object is a user interface object.Definition TButton.h:18; TButton::Drawvoid Draw(Option_t *option="""") overrideDraw this button with its current attributes.Definition TButton.cxx:139; TCanvasThe Canvas class.Definition TCanvas.h:23; TCanvas::cdTVirtualPad * cd(Int_t subpadnumber=0) overrideSet current canvas & pad.Definition TCanvas.cxx:719; TGeoCombiTransClass describing rotation + translation.Definition TGeoMatrix.h:317; TGeoManagerThe manager class for any TGeo geometry.Definition TGeoManager.h:44; TGeoManager::MakeTubeTGeoVolume * MakeTube(const char *name, TGeoMedium *medium, Double_t rmin, Double_t rmax, Double_t dz)Make in one step a volume pointing to a tube shape with given medium.Definition TGeoManager.cxx:3204; TGeoManager::CloseGeometryvoid CloseGeometry(Option_t *option=""d"")Closing geometry implies checking the geometry validity, fixing shapes with negative parameters (run-...Definition TGeoManager.cxx:1480; TGeoManager::MakeBoxTGeoVolume * MakeBox(const char *name, TGeoMedium *",MatchSource.WIKI,doc/master/mp3player_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mp3player_8C.html
https://root.cern/doc/master/mp3player_8C.html:24060,Usability,simpl,simple,24060," using geometry ...Definition TGeoMedium.h:23; TGeoRotationClass describing rotations.Definition TGeoMatrix.h:168; TGeoTranslationClass describing translations.Definition TGeoMatrix.h:116; TGeoVolumeTGeoVolume, TGeoVolumeMulti, TGeoVolumeAssembly are the volume classes.Definition TGeoVolume.h:43; TGeoVolume::Drawvoid Draw(Option_t *option="""") overridedraw top volume according to optionDefinition TGeoVolume.cxx:1206; TGeoVolume::AddNodeOverlapvirtual void AddNodeOverlap(TGeoVolume *vol, Int_t copy_no, TGeoMatrix *mat=nullptr, Option_t *option="""")Add a TGeoNode to the list of nodes.Definition TGeoVolume.cxx:1044; TGeoVolume::SetLineColorvoid SetLineColor(Color_t lcolor) overrideSet the line color.Definition TGeoVolume.cxx:2169; TLatexTo draw Mathematical Formula.Definition TLatex.h:18; TLatex::DrawLatexTLatex * DrawLatex(Double_t x, Double_t y, const char *text)Make a copy of this object with the new parameters And copy object attributes.Definition TLatex.cxx:1943; TLineUse the TLine constructor to create a simple line.Definition TLine.h:22; TPadThe most important graphics class in the ROOT system.Definition TPad.h:28; TPad::cdTVirtualPad * cd(Int_t subpadnumber=0) overrideSet Current pad.Definition TPad.cxx:693; TPad::Drawvoid Draw(Option_t *option="""") overrideDraw Pad in Current pad (re-parent pad if necessary).Definition TPad.cxx:1364; TPaveTextA Pave (see TPave) with text, lines or/and boxes inside.Definition TPaveText.h:21; TPaveText::AddTextvirtual TText * AddText(Double_t x1, Double_t y1, const char *label)Add a new Text line to this pavetext at given coordinates.Definition TPaveText.cxx:191; TPaveText::Drawvoid Draw(Option_t *option="""") overrideDraw this pavetext with its current attributes.Definition TPaveText.cxx:242; ptTPaveText * ptDefinition entrylist_figure1.C:7; AuthorEun Young Kim, Dept. of Physics, Univ. of Seoul ; Definition in file mp3player.C. tutorialsgeommp3player.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:28 (GVA Time) usi",MatchSource.WIKI,doc/master/mp3player_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mp3player_8C.html
https://root.cern/doc/master/mrt_8py.html:1424,Deployability,install,installation,1424,". ROOT: tutorials/pyroot/mrt.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; mrt.py File ReferenceTutorials » PyRoot tutorials. Detailed Description; Build ROOT Ntuple from other source. ; This program reads the ‘aptuple.txt’ file row by row, then creates the Ntuple by adding row by row.; opening file /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/pyroot/aptuple.txt ...; writing file aptuple.root ...; done; ; import sys, os; from ROOT import TFile, TNtuple, TROOT; ; ; ifn = os.path.join(str(TROOT.GetTutorialDir()), 'pyroot', 'aptuple.txt'); ofn = 'aptuple.root'; ; print('opening file %s ...' % ifn); infile = open( ifn, 'r' ); lines = infile.readlines(); title = lines[0]; labels = lines[1].split(); ; print('writing file %s ...' % ofn); outfile = TFile( ofn, 'RECREATE', 'ROOT file with an NTuple' ); ntuple = TNtuple( 'ntuple', title, ':'.join( labels ) ); ; for line in lines[2:]:; words = line.split(); row = map( float, words ); ntuple.Fill(*row); ; outfile.Write(); ; print('done'); TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TNtupleA simple TTree restricted to a list of float variables only.Definition TNtuple.h:28; TROOT::GetTutorialDirstatic const TString & GetTutorialDir()Get the tutorials directory in the installation. Static utility function.Definition TROOT.cxx:3119; AuthorWim Lavrijsen ; Definition in file mrt.py. tutorialspyrootmrt.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/mrt_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mrt_8py.html
https://root.cern/doc/master/mrt_8py.html:1289,Modifiability,variab,variables,1289,". ROOT: tutorials/pyroot/mrt.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; mrt.py File ReferenceTutorials » PyRoot tutorials. Detailed Description; Build ROOT Ntuple from other source. ; This program reads the ‘aptuple.txt’ file row by row, then creates the Ntuple by adding row by row.; opening file /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/pyroot/aptuple.txt ...; writing file aptuple.root ...; done; ; import sys, os; from ROOT import TFile, TNtuple, TROOT; ; ; ifn = os.path.join(str(TROOT.GetTutorialDir()), 'pyroot', 'aptuple.txt'); ofn = 'aptuple.root'; ; print('opening file %s ...' % ifn); infile = open( ifn, 'r' ); lines = infile.readlines(); title = lines[0]; labels = lines[1].split(); ; print('writing file %s ...' % ofn); outfile = TFile( ofn, 'RECREATE', 'ROOT file with an NTuple' ); ntuple = TNtuple( 'ntuple', title, ':'.join( labels ) ); ; for line in lines[2:]:; words = line.split(); row = map( float, words ); ntuple.Fill(*row); ; outfile.Write(); ; print('done'); TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TNtupleA simple TTree restricted to a list of float variables only.Definition TNtuple.h:28; TROOT::GetTutorialDirstatic const TString & GetTutorialDir()Get the tutorials directory in the installation. Static utility function.Definition TROOT.cxx:3119; AuthorWim Lavrijsen ; Definition in file mrt.py. tutorialspyrootmrt.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/mrt_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mrt_8py.html
https://root.cern/doc/master/mrt_8py.html:1246,Usability,simpl,simple,1246,". ROOT: tutorials/pyroot/mrt.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; mrt.py File ReferenceTutorials » PyRoot tutorials. Detailed Description; Build ROOT Ntuple from other source. ; This program reads the ‘aptuple.txt’ file row by row, then creates the Ntuple by adding row by row.; opening file /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/pyroot/aptuple.txt ...; writing file aptuple.root ...; done; ; import sys, os; from ROOT import TFile, TNtuple, TROOT; ; ; ifn = os.path.join(str(TROOT.GetTutorialDir()), 'pyroot', 'aptuple.txt'); ofn = 'aptuple.root'; ; print('opening file %s ...' % ifn); infile = open( ifn, 'r' ); lines = infile.readlines(); title = lines[0]; labels = lines[1].split(); ; print('writing file %s ...' % ofn); outfile = TFile( ofn, 'RECREATE', 'ROOT file with an NTuple' ); ntuple = TNtuple( 'ntuple', title, ':'.join( labels ) ); ; for line in lines[2:]:; words = line.split(); row = map( float, words ); ntuple.Fill(*row); ; outfile.Write(); ; print('done'); TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TNtupleA simple TTree restricted to a list of float variables only.Definition TNtuple.h:28; TROOT::GetTutorialDirstatic const TString & GetTutorialDir()Get the tutorials directory in the installation. Static utility function.Definition TROOT.cxx:3119; AuthorWim Lavrijsen ; Definition in file mrt.py. tutorialspyrootmrt.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/mrt_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/mrt_8py.html
https://root.cern/doc/master/MsgLogger_8h_source.html:411,Deployability,integrat,integrated,411,". ROOT: tmva/tmva/inc/TMVA/MsgLogger.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MsgLogger.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Attila Krasznahorkay, Andreas Hoecker, Joerg Stelzer, Eckhard von Toerne; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MsgLogger *; 8 * *; 9 * *; 10 * Description: *; 11 * TMVA output logger class producing nicely formatted log messages *; 12 * *; 13 * Author: *; 14 * Attila Krasznahorkay <Attila.Krasznahorkay@cern.ch> - CERN, Switzerland *; 15 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 17 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 18 * *; 19 * Copyright (c) 2005-2011: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * U. of Bonn, Germany *; 24 * *; 25 * Redistribution and use in source and binary forms, with or without *; 26 * modification, are permitted according to the terms listed in LICENSE *; 27 * (see tmva/doc/LICENSE) *; 28 **********************************************************************************/; 29 ; 30#ifndef ROOT_TMVA_MsgLogger; 31#define ROOT_TMVA_MsgLogger; 32 ; 33//////////////////////////////////////////////////////////////////////////; 34// //; 35// MsgLogger //; 36// //; 37// ostringstream derivative to redirect and format output //; 38// //; 39//////////////////////////////////////////////////////////////////////////; 40 ; 41// STL include(s):; 42#include <string>; 43#include <sstream>; 44#include <iostream>; 45#include <map>; 46#include <atomic>; 47 ; 48// ROOT include(s); 49#include ""TObject.h""; 50 ; 51#include ""TMVA/Types.h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:411,Integrability,integrat,integrated,411,". ROOT: tmva/tmva/inc/TMVA/MsgLogger.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MsgLogger.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Attila Krasznahorkay, Andreas Hoecker, Joerg Stelzer, Eckhard von Toerne; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MsgLogger *; 8 * *; 9 * *; 10 * Description: *; 11 * TMVA output logger class producing nicely formatted log messages *; 12 * *; 13 * Author: *; 14 * Attila Krasznahorkay <Attila.Krasznahorkay@cern.ch> - CERN, Switzerland *; 15 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 17 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 18 * *; 19 * Copyright (c) 2005-2011: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * U. of Bonn, Germany *; 24 * *; 25 * Redistribution and use in source and binary forms, with or without *; 26 * modification, are permitted according to the terms listed in LICENSE *; 27 * (see tmva/doc/LICENSE) *; 28 **********************************************************************************/; 29 ; 30#ifndef ROOT_TMVA_MsgLogger; 31#define ROOT_TMVA_MsgLogger; 32 ; 33//////////////////////////////////////////////////////////////////////////; 34// //; 35// MsgLogger //; 36// //; 37// ostringstream derivative to redirect and format output //; 38// //; 39//////////////////////////////////////////////////////////////////////////; 40 ; 41// STL include(s):; 42#include <string>; 43#include <sstream>; 44#include <iostream>; 45#include <map>; 46#include <atomic>; 47 ; 48// ROOT include(s); 49#include ""TObject.h""; 50 ; 51#include ""TMVA/Types.h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:606,Integrability,message,messages,606,". ROOT: tmva/tmva/inc/TMVA/MsgLogger.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MsgLogger.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Attila Krasznahorkay, Andreas Hoecker, Joerg Stelzer, Eckhard von Toerne; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MsgLogger *; 8 * *; 9 * *; 10 * Description: *; 11 * TMVA output logger class producing nicely formatted log messages *; 12 * *; 13 * Author: *; 14 * Attila Krasznahorkay <Attila.Krasznahorkay@cern.ch> - CERN, Switzerland *; 15 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 17 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 18 * *; 19 * Copyright (c) 2005-2011: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * U. of Bonn, Germany *; 24 * *; 25 * Redistribution and use in source and binary forms, with or without *; 26 * modification, are permitted according to the terms listed in LICENSE *; 27 * (see tmva/doc/LICENSE) *; 28 **********************************************************************************/; 29 ; 30#ifndef ROOT_TMVA_MsgLogger; 31#define ROOT_TMVA_MsgLogger; 32 ; 33//////////////////////////////////////////////////////////////////////////; 34// //; 35// MsgLogger //; 36// //; 37// ostringstream derivative to redirect and format output //; 38// //; 39//////////////////////////////////////////////////////////////////////////; 40 ; 41// STL include(s):; 42#include <string>; 43#include <sstream>; 44#include <iostream>; 45#include <map>; 46#include <atomic>; 47 ; 48// ROOT include(s); 49#include ""TObject.h""; 50 ; 51#include ""TMVA/Types.h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:3106,Integrability,message,message,3106,"FO );; 62 MsgLogger( const std::string& source, EMsgType minType = kINFO );; 63 MsgLogger( EMsgType minType = kINFO );; 64 MsgLogger( const MsgLogger& parent );; 65 ~MsgLogger();; 66 ; 67 // Accessors; 68 void SetSource ( const std::string& source ) { fStrSource = source; }; 69 EMsgType GetMinType() const { return fMinType; }; 70 void SetMinType( EMsgType minType ) { fMinType = minType; }; 71 std::string GetSource() const { return fStrSource; }; 72 std::string GetPrintedSource() const;; 73 std::string GetFormattedSource() const;; 74 ; 75 static UInt_t GetMaxSourceSize();; 76 ; 77 // Needed for copying; 78 MsgLogger& operator= ( const MsgLogger& parent );; 79 ; 80 // Stream modifier(s); 81 static MsgLogger& Endmsg( MsgLogger& logger );; 82 ; 83 // Accept stream modifiers; 84 MsgLogger& operator<< ( MsgLogger& ( *_f )( MsgLogger& ) );; 85 MsgLogger& operator<< ( std::ostream& ( *_f )( std::ostream& ) );; 86 MsgLogger& operator<< ( std::ios& ( *_f )( std::ios& ) );; 87 ; 88 // Accept message type specification; 89 MsgLogger& operator<< ( EMsgType type );; 90 ; 91 // For all the ""conventional"" inputs; 92 template <class T> MsgLogger& operator<< ( T arg ) {; 93 *(std::ostringstream*)this << arg;; 94 return *this;; 95 }; 96 ; 97 // Temporally disables all the loggers (Caution! Use with care !); 98 static void InhibitOutput();; 99 static void EnableOutput();; 100 ; 101 private:; 102 ; 103 // private utility routines; 104 void Send();; 105 void InitMaps();; 106 void WriteMsg( EMsgType type, const std::string& line ) const;; 107 ; 108 const TObject* fObjSource; ///< the source TObject (used for name); 109 std::string fStrSource; ///< alternative string source; 110 static const std::string fgPrefix; ///< the prefix of the source name; 111 static const std::string fgSuffix; ///< suffix following source name; 112 EMsgType fActiveType; ///< active type; 113 static const UInt_t fgMaxSourceSize; ///< maximum length of source name; 114 static std::atomic<Bool_t> fgOutputSupressed; ",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:3534,Integrability,rout,routines,3534,"return fStrSource; }; 72 std::string GetPrintedSource() const;; 73 std::string GetFormattedSource() const;; 74 ; 75 static UInt_t GetMaxSourceSize();; 76 ; 77 // Needed for copying; 78 MsgLogger& operator= ( const MsgLogger& parent );; 79 ; 80 // Stream modifier(s); 81 static MsgLogger& Endmsg( MsgLogger& logger );; 82 ; 83 // Accept stream modifiers; 84 MsgLogger& operator<< ( MsgLogger& ( *_f )( MsgLogger& ) );; 85 MsgLogger& operator<< ( std::ostream& ( *_f )( std::ostream& ) );; 86 MsgLogger& operator<< ( std::ios& ( *_f )( std::ios& ) );; 87 ; 88 // Accept message type specification; 89 MsgLogger& operator<< ( EMsgType type );; 90 ; 91 // For all the ""conventional"" inputs; 92 template <class T> MsgLogger& operator<< ( T arg ) {; 93 *(std::ostringstream*)this << arg;; 94 return *this;; 95 }; 96 ; 97 // Temporally disables all the loggers (Caution! Use with care !); 98 static void InhibitOutput();; 99 static void EnableOutput();; 100 ; 101 private:; 102 ; 103 // private utility routines; 104 void Send();; 105 void InitMaps();; 106 void WriteMsg( EMsgType type, const std::string& line ) const;; 107 ; 108 const TObject* fObjSource; ///< the source TObject (used for name); 109 std::string fStrSource; ///< alternative string source; 110 static const std::string fgPrefix; ///< the prefix of the source name; 111 static const std::string fgSuffix; ///< suffix following source name; 112 EMsgType fActiveType; ///< active type; 113 static const UInt_t fgMaxSourceSize; ///< maximum length of source name; 114 static std::atomic<Bool_t> fgOutputSupressed; ///< disable the output globally (used by generic booster); 115 static std::atomic<Bool_t> fgInhibitOutput; ///< flag to suppress all output; 116 ; 117 static std::atomic<const std::map<EMsgType, std::string>*> fgTypeMap; ///< matches output types with strings; 118 static std::atomic<const std::map<EMsgType, std::string>*> fgColorMap; ///< matches output types with terminal colors; 119 EMsgType fMinType; ///< minimum type for",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:8758,Integrability,message,message,8758,"eSize()returns the maximum source sizeDefinition MsgLogger.cxx:165; TMVA::MsgLogger::InhibitOutputstatic void InhibitOutput()Definition MsgLogger.cxx:67; TMVA::MsgLogger::fObjSourceconst TObject * fObjSourcethe source TObject (used for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffix following source nameDefinition MsgLogger.h:111; TMVA::MsgLogger::EnableOutputstatic void EnableOutput()Definition MsgLogger.cxx:68; TMVA::MsgLogger::fMinTypeEMsgType fMinTypeminimum type for outputDefinition MsgLogger.h:119; TMVA::MsgLogger::fgTypeMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgTypeMapmatches output types with stringsDefinition MsgLogger.h:117; TMVA::MsgLogger::GetSourcestd::string GetSource() constDefinition MsgLogger.h:71; TMVA::MsgLogger::GetFormattedSourcestd::string GetFormattedSource() constmake sure the source name is no longer than fgMaxSourceSize:Definition MsgLogger.cxx:143; TMVA::MsgLogger::InitMapsvoid InitMaps()Create the message type and color maps.Definition MsgLogger.cxx:270; TMVA::MsgLogger::fActiveTypeEMsgType fActiveTypeactive typeDefinition MsgLogger.h:112; TMVA::MsgLogger::WriteMsgvoid WriteMsg(EMsgType type, const std::string &line) constputting the output string, the message type, and the color switcher together into a single stringDefinition MsgLogger.cxx:220; TMVA::MsgLogger::Endmsgstatic MsgLogger & Endmsg(MsgLogger &logger)end lineDefinition MsgLogger.cxx:261; TMVA::MsgLogger::fgInhibitOutputstatic std::atomic< Bool_t > fgInhibitOutputflag to suppress all outputDefinition MsgLogger.h:115; TMVA::MsgLogger::Sendvoid Send()activates the logger writerDefinition MsgLogger.cxx:185; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TObjectMother of all ROOT objects.Definition TObject.h:41; unsigned int; lineTLine * lineDefinition entrylistblock_figure1.C:235; TMVAcreate variable transformationsDefinition G",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:9018,Integrability,message,message,9018,"for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffix following source nameDefinition MsgLogger.h:111; TMVA::MsgLogger::EnableOutputstatic void EnableOutput()Definition MsgLogger.cxx:68; TMVA::MsgLogger::fMinTypeEMsgType fMinTypeminimum type for outputDefinition MsgLogger.h:119; TMVA::MsgLogger::fgTypeMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgTypeMapmatches output types with stringsDefinition MsgLogger.h:117; TMVA::MsgLogger::GetSourcestd::string GetSource() constDefinition MsgLogger.h:71; TMVA::MsgLogger::GetFormattedSourcestd::string GetFormattedSource() constmake sure the source name is no longer than fgMaxSourceSize:Definition MsgLogger.cxx:143; TMVA::MsgLogger::InitMapsvoid InitMaps()Create the message type and color maps.Definition MsgLogger.cxx:270; TMVA::MsgLogger::fActiveTypeEMsgType fActiveTypeactive typeDefinition MsgLogger.h:112; TMVA::MsgLogger::WriteMsgvoid WriteMsg(EMsgType type, const std::string &line) constputting the output string, the message type, and the color switcher together into a single stringDefinition MsgLogger.cxx:220; TMVA::MsgLogger::Endmsgstatic MsgLogger & Endmsg(MsgLogger &logger)end lineDefinition MsgLogger.cxx:261; TMVA::MsgLogger::fgInhibitOutputstatic std::atomic< Bool_t > fgInhibitOutputflag to suppress all outputDefinition MsgLogger.h:115; TMVA::MsgLogger::Sendvoid Send()activates the logger writerDefinition MsgLogger.cxx:185; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TObjectMother of all ROOT objects.Definition TObject.h:41; unsigned int; lineTLine * lineDefinition entrylistblock_figure1.C:235; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; Types.h. tmvatmvaincTMVAMsgLogger.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:9706,Modifiability,variab,variable,9706,"for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffix following source nameDefinition MsgLogger.h:111; TMVA::MsgLogger::EnableOutputstatic void EnableOutput()Definition MsgLogger.cxx:68; TMVA::MsgLogger::fMinTypeEMsgType fMinTypeminimum type for outputDefinition MsgLogger.h:119; TMVA::MsgLogger::fgTypeMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgTypeMapmatches output types with stringsDefinition MsgLogger.h:117; TMVA::MsgLogger::GetSourcestd::string GetSource() constDefinition MsgLogger.h:71; TMVA::MsgLogger::GetFormattedSourcestd::string GetFormattedSource() constmake sure the source name is no longer than fgMaxSourceSize:Definition MsgLogger.cxx:143; TMVA::MsgLogger::InitMapsvoid InitMaps()Create the message type and color maps.Definition MsgLogger.cxx:270; TMVA::MsgLogger::fActiveTypeEMsgType fActiveTypeactive typeDefinition MsgLogger.h:112; TMVA::MsgLogger::WriteMsgvoid WriteMsg(EMsgType type, const std::string &line) constputting the output string, the message type, and the color switcher together into a single stringDefinition MsgLogger.cxx:220; TMVA::MsgLogger::Endmsgstatic MsgLogger & Endmsg(MsgLogger &logger)end lineDefinition MsgLogger.cxx:261; TMVA::MsgLogger::fgInhibitOutputstatic std::atomic< Bool_t > fgInhibitOutputflag to suppress all outputDefinition MsgLogger.h:115; TMVA::MsgLogger::Sendvoid Send()activates the logger writerDefinition MsgLogger.cxx:185; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TObjectMother of all ROOT objects.Definition TObject.h:41; unsigned int; lineTLine * lineDefinition entrylistblock_figure1.C:235; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; Types.h. tmvatmvaincTMVAMsgLogger.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:562,Testability,log,logger,562,". ROOT: tmva/tmva/inc/TMVA/MsgLogger.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MsgLogger.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Attila Krasznahorkay, Andreas Hoecker, Joerg Stelzer, Eckhard von Toerne; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MsgLogger *; 8 * *; 9 * *; 10 * Description: *; 11 * TMVA output logger class producing nicely formatted log messages *; 12 * *; 13 * Author: *; 14 * Attila Krasznahorkay <Attila.Krasznahorkay@cern.ch> - CERN, Switzerland *; 15 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 17 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 18 * *; 19 * Copyright (c) 2005-2011: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * U. of Bonn, Germany *; 24 * *; 25 * Redistribution and use in source and binary forms, with or without *; 26 * modification, are permitted according to the terms listed in LICENSE *; 27 * (see tmva/doc/LICENSE) *; 28 **********************************************************************************/; 29 ; 30#ifndef ROOT_TMVA_MsgLogger; 31#define ROOT_TMVA_MsgLogger; 32 ; 33//////////////////////////////////////////////////////////////////////////; 34// //; 35// MsgLogger //; 36// //; 37// ostringstream derivative to redirect and format output //; 38// //; 39//////////////////////////////////////////////////////////////////////////; 40 ; 41// STL include(s):; 42#include <string>; 43#include <sstream>; 44#include <iostream>; 45#include <map>; 46#include <atomic>; 47 ; 48// ROOT include(s); 49#include ""TObject.h""; 50 ; 51#include ""TMVA/Types.h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:602,Testability,log,log,602,". ROOT: tmva/tmva/inc/TMVA/MsgLogger.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. MsgLogger.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Attila Krasznahorkay, Andreas Hoecker, Joerg Stelzer, Eckhard von Toerne; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : MsgLogger *; 8 * *; 9 * *; 10 * Description: *; 11 * TMVA output logger class producing nicely formatted log messages *; 12 * *; 13 * Author: *; 14 * Attila Krasznahorkay <Attila.Krasznahorkay@cern.ch> - CERN, Switzerland *; 15 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 17 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 18 * *; 19 * Copyright (c) 2005-2011: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * U. of Bonn, Germany *; 24 * *; 25 * Redistribution and use in source and binary forms, with or without *; 26 * modification, are permitted according to the terms listed in LICENSE *; 27 * (see tmva/doc/LICENSE) *; 28 **********************************************************************************/; 29 ; 30#ifndef ROOT_TMVA_MsgLogger; 31#define ROOT_TMVA_MsgLogger; 32 ; 33//////////////////////////////////////////////////////////////////////////; 34// //; 35// MsgLogger //; 36// //; 37// ostringstream derivative to redirect and format output //; 38// //; 39//////////////////////////////////////////////////////////////////////////; 40 ; 41// STL include(s):; 42#include <string>; 43#include <sstream>; 44#include <iostream>; 45#include <map>; 46#include <atomic>; 47 ; 48// ROOT include(s); 49#include ""TObject.h""; 50 ; 51#include ""TMVA/Types.h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:2845,Testability,log,logger,2845,"h""; 52 ; 53// Local include(s):; 54 ; 55namespace TMVA {; 56 ; 57 class MsgLogger : public std::ostri",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:3384,Testability,log,loggers,3384," EMsgType GetMinType() const { return fMinType; }; 70 void SetMinType( EMsgType minType ) { fMinType = minType; }; 71 std::string GetSource() const { return fStrSource; }; 72 std::string GetPrintedSource() const;; 73 std::string GetFormattedSource() const;; 74 ; 75 static UInt_t GetMaxSourceSize();; 76 ; 77 // Needed for copying; 78 MsgLogger& operator= ( const MsgLogger& parent );; 79 ; 80 // Stream modifier(s); 81 static MsgLogger& Endmsg( MsgLogger& logger );; 82 ; 83 // Accept stream modifiers; 84 MsgLogger& operator<< ( MsgLogger& ( *_f )( MsgLogger& ) );; 85 MsgLogger& operator<< ( std::ostream& ( *_f )( std::ostream& ) );; 86 MsgLogger& operator<< ( std::ios& ( *_f )( std::ios& ) );; 87 ; 88 // Accept message type specification; 89 MsgLogger& operator<< ( EMsgType type );; 90 ; 91 // For all the ""conventional"" inputs; 92 template <class T> MsgLogger& operator<< ( T arg ) {; 93 *(std::ostringstream*)this << arg;; 94 return *this;; 95 }; 96 ; 97 // Temporally disables all the loggers (Caution! Use with care !); 98 static void InhibitOutput();; 99 static void EnableOutput();; 100 ; 101 private:; 102 ; 103 // private utility routines; 104 void Send();; 105 void InitMaps();; 106 void WriteMsg( EMsgType type, const std::string& line ) const;; 107 ; 108 const TObject* fObjSource; ///< the source TObject (used for name); 109 std::string fStrSource; ///< alternative string source; 110 static const std::string fgPrefix; ///< the prefix of the source name; 111 static const std::string fgSuffix; ///< suffix following source name; 112 EMsgType fActiveType; ///< active type; 113 static const UInt_t fgMaxSourceSize; ///< maximum length of source name; 114 static std::atomic<Bool_t> fgOutputSupressed; ///< disable the output globally (used by generic booster); 115 static std::atomic<Bool_t> fgInhibitOutput; ///< flag to suppress all output; 116 ; 117 static std::atomic<const std::map<EMsgType, std::string>*> fgTypeMap; ///< matches output types with strings; 118 static std::",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:4631,Testability,log,logging,4631,"n! Use with care !); 98 static void InhibitOutput();; 99 static void EnableOutput();; 100 ; 101 private:; 102 ; 103 // private utility routines; 104 void Send();; 105 void InitMaps();; 106 void WriteMsg( EMsgType type, const std::string& line ) const;; 107 ; 108 const TObject* fObjSource; ///< the source TObject (used for name); 109 std::string fStrSource; ///< alternative string source; 110 static const std::string fgPrefix; ///< the prefix of the source name; 111 static const std::string fgSuffix; ///< suffix following source name; 112 EMsgType fActiveType; ///< active type; 113 static const UInt_t fgMaxSourceSize; ///< maximum length of source name; 114 static std::atomic<Bool_t> fgOutputSupressed; ///< disable the output globally (used by generic booster); 115 static std::atomic<Bool_t> fgInhibitOutput; ///< flag to suppress all output; 116 ; 117 static std::atomic<const std::map<EMsgType, std::string>*> fgTypeMap; ///< matches output types with strings; 118 static std::atomic<const std::map<EMsgType, std::string>*> fgColorMap; ///< matches output types with terminal colors; 119 EMsgType fMinType; ///< minimum type for output; 120 ; 121 ClassDef(MsgLogger,0) // Ostringstream derivative to redirect and format logging output; 122 }; // class MsgLogger; 123 ; 124 inline MsgLogger& MsgLogger::operator<< ( MsgLogger& (*_f)( MsgLogger& ) ); 125 {; 126 return (_f)(*this);; 127 }; 128 ; 129 inline MsgLogger& MsgLogger::operator<< ( std::ostream& (*_f)( std::ostream& ) ); 130 {; 131 (_f)(*this);; 132 return *this;; 133 }; 134 ; 135 inline MsgLogger& MsgLogger::operator<< ( std::ios& ( *_f )( std::ios& ) ); 136 {; 137 (_f)(*this);; 138 return *this;; 139 }; 140 ; 141 inline MsgLogger& MsgLogger::operator<< ( EMsgType type ); 142 {; 143 fActiveType = type;; 144 return *this;; 145 }; 146 ; 147 // Shortcut; 148 inline MsgLogger& Endl(MsgLogger& ml) { return MsgLogger::Endmsg(ml); }; 149 ; 150}; 151 ; 152#endif // TMVA_MsgLogger; UInt_tunsigned int UInt_tDefinition RtypesCore.",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:7100,Testability,log,logger,7100,"ar Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TObject.h; TMVA::MsgLoggerostringstream derivative to redirect and format outputDefinition MsgLogger.h:57; TMVA::MsgLogger::SetMinTypevoid SetMinType(EMsgType minType)Definition MsgLogger.h:70; TMVA::MsgLogger::fgMaxSourceSizestatic const UInt_t fgMaxSourceSizemaximum length of source nameDefinition MsgLogger.h:113; TMVA::MsgLogger::fgPrefixstatic const std::string fgPrefixthe prefix of the source nameDefinition MsgLogger.h:110; TMVA::MsgLogger::~MsgLogger~MsgLogger()destructorDefinition MsgLogger.cxx:121; TMVA::MsgLogger::SetSourcevoid SetSource(const std::string &source)Definition MsgLogger.h:68; TMVA::MsgLogger::operator=MsgLogger & operator=(const MsgLogger &parent)assignment operatorDefinition MsgLogger.cxx:128; TMVA::MsgLogger::GetPrintedSourcestd::string GetPrintedSource() constthe full logger prefixDefinition MsgLogger.cxx:173; TMVA::MsgLogger::fStrSourcestd::string fStrSourcealternative string sourceDefinition MsgLogger.h:109; TMVA::MsgLogger::fgOutputSupressedstatic std::atomic< Bool_t > fgOutputSupresseddisable the output globally (used by generic booster)Definition MsgLogger.h:114; TMVA::MsgLogger::fgColorMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgColorMapmatches output types with terminal colorsDefinition MsgLogger.h:118; TMVA::MsgLogger::operator<<MsgLogger & operator<<(MsgLogger &(*_f)(MsgLogger &))Definition MsgLogger.h:124; TMVA::MsgLogger::GetMaxSourceSizestatic UInt_t GetMaxSourceSize()returns the maximum source sizeDefinition MsgLogger.cxx:165; TMVA::MsgLogger::InhibitOutputstatic void InhibitOutput()Definition MsgLogger.cxx:67; TMVA::MsgLogger::fObjSourceconst TObject * fObjSourcethe source TObject (used for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffi",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:9174,Testability,log,logger,9174,"for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffix following source nameDefinition MsgLogger.h:111; TMVA::MsgLogger::EnableOutputstatic void EnableOutput()Definition MsgLogger.cxx:68; TMVA::MsgLogger::fMinTypeEMsgType fMinTypeminimum type for outputDefinition MsgLogger.h:119; TMVA::MsgLogger::fgTypeMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgTypeMapmatches output types with stringsDefinition MsgLogger.h:117; TMVA::MsgLogger::GetSourcestd::string GetSource() constDefinition MsgLogger.h:71; TMVA::MsgLogger::GetFormattedSourcestd::string GetFormattedSource() constmake sure the source name is no longer than fgMaxSourceSize:Definition MsgLogger.cxx:143; TMVA::MsgLogger::InitMapsvoid InitMaps()Create the message type and color maps.Definition MsgLogger.cxx:270; TMVA::MsgLogger::fActiveTypeEMsgType fActiveTypeactive typeDefinition MsgLogger.h:112; TMVA::MsgLogger::WriteMsgvoid WriteMsg(EMsgType type, const std::string &line) constputting the output string, the message type, and the color switcher together into a single stringDefinition MsgLogger.cxx:220; TMVA::MsgLogger::Endmsgstatic MsgLogger & Endmsg(MsgLogger &logger)end lineDefinition MsgLogger.cxx:261; TMVA::MsgLogger::fgInhibitOutputstatic std::atomic< Bool_t > fgInhibitOutputflag to suppress all outputDefinition MsgLogger.h:115; TMVA::MsgLogger::Sendvoid Send()activates the logger writerDefinition MsgLogger.cxx:185; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TObjectMother of all ROOT objects.Definition TObject.h:41; unsigned int; lineTLine * lineDefinition entrylistblock_figure1.C:235; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; Types.h. tmvatmvaincTMVAMsgLogger.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/MsgLogger_8h_source.html:9396,Testability,log,logger,9396,"for name)Definition MsgLogger.h:108; TMVA::MsgLogger::fgSuffixstatic const std::string fgSuffixsuffix following source nameDefinition MsgLogger.h:111; TMVA::MsgLogger::EnableOutputstatic void EnableOutput()Definition MsgLogger.cxx:68; TMVA::MsgLogger::fMinTypeEMsgType fMinTypeminimum type for outputDefinition MsgLogger.h:119; TMVA::MsgLogger::fgTypeMapstatic std::atomic< const std::map< EMsgType, std::string > * > fgTypeMapmatches output types with stringsDefinition MsgLogger.h:117; TMVA::MsgLogger::GetSourcestd::string GetSource() constDefinition MsgLogger.h:71; TMVA::MsgLogger::GetFormattedSourcestd::string GetFormattedSource() constmake sure the source name is no longer than fgMaxSourceSize:Definition MsgLogger.cxx:143; TMVA::MsgLogger::InitMapsvoid InitMaps()Create the message type and color maps.Definition MsgLogger.cxx:270; TMVA::MsgLogger::fActiveTypeEMsgType fActiveTypeactive typeDefinition MsgLogger.h:112; TMVA::MsgLogger::WriteMsgvoid WriteMsg(EMsgType type, const std::string &line) constputting the output string, the message type, and the color switcher together into a single stringDefinition MsgLogger.cxx:220; TMVA::MsgLogger::Endmsgstatic MsgLogger & Endmsg(MsgLogger &logger)end lineDefinition MsgLogger.cxx:261; TMVA::MsgLogger::fgInhibitOutputstatic std::atomic< Bool_t > fgInhibitOutputflag to suppress all outputDefinition MsgLogger.h:115; TMVA::MsgLogger::Sendvoid Send()activates the logger writerDefinition MsgLogger.cxx:185; TMVA::MsgLogger::GetMinTypeEMsgType GetMinType() constDefinition MsgLogger.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TObjectMother of all ROOT objects.Definition TObject.h:41; unsigned int; lineTLine * lineDefinition entrylistblock_figure1.C:235; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; Types.h. tmvatmvaincTMVAMsgLogger.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:58 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/MsgLogger_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/MsgLogger_8h_source.html
https://root.cern/doc/master/multidimfit_8C.html:2448,Energy Efficiency,power,powers,2448," 6235 1147 87.5 0.333 5 5.095 44.16 0 0 0 2; 10 5218 1018 87.5 0.333 12 4.983 40.99 0 2 0 0; 11 4193 1025 87.5 0.667 53 5.229 37.5 0 0 4 0; 12 3299 893.8 88.8 0.333 6 -4.058 54.27 0 0 1 1; 13 2458 841.2 88.8 0.333 7 -4.155 48.73 0 1 0 1; 14 1933 524.7 88.8 0.333 13 -3.291 48.45 0 1 1 0; 15 1675 258.1 88.8 0.5 19 4.211 14.56 1 0 0 2; 16 1334 340.6 88.8 0.5 26 -4.731 15.22 1 1 0 1; 17 1079 255.5 88.8 0.5 33 3.953 16.35 1 0 2 0; 18 788.2 290.4 88.8 0.5 34 4.687 13.22 1 2 0 0; 19 709.2 78.94 89.4 0.5 21 2.23 15.88 0 1 1 1; 20 473.4 235.8 89.4 0.5 23 -3.543 18.78 1 0 1 1; 21 235.4 238 89.4 0.5 28 -3.976 15.06 1 1 1 0; Results of Parameterisation:; ----------------------------; Total reduction of square residuals 5.063e+05; Relative precision obtained: 0.01185; Error obtained: 235.4; Multiple correlation coefficient: 0.9995; Reduced Chi square over sample: 0.4975; Maximum residual value: 3.243; Minimum residual value: -2.59; Estimated root mean square: 0.6862; Maximum powers used: 1 2 4 2 ; Function codes of candidate functions.; 1: considered, 2: too little contribution, 3: accepted.; 3333333333 1133311113 1313113131 1113311111 1111111111 1113111111; 1111111111 1111111111 1111111111 1111111111 1111111111 1111111111; 111111; Loop over candidates stopped because max allowed studies reached; ; Coefficients:; -------------; # Value Error Powers; ---------------------------------------; 0 -4.371 0.08798 0 0 0 0; 1 43.15 0.1601 1 0 0 0; 2 13.43 0.08032 0 0 0 1; 3 13.46 0.07805 0 0 1 0; 4 13.4 0.08054 0 1 0 0; 5 13.33 0.1423 1 1 0 0; 6 13.3 0.1367 1 0 0 1; 7 13.35 0.1331 1 0 1 0; 8 4.497 0.1511 0 0 0 2; 9 4.639 0.1585 0 2 0 0; 10 4.89 0.164 0 0 4 0; 11 -3.7 0.1364 0 0 1 1; 12 -3.986 0.1438 0 1 0 1; 13 -3.862 0.1458 0 1 1 0; 14 4.361 0.2614 1 0 0 2; 15 -4.026 0.2555 1 1 0 1; 16 4.57 0.2477 1 0 2 0; 17 4.698 0.2729 1 2 0 0; 18 2.838 0.2525 0 1 1 1; 19 -3.489 0.2292 1 0 1 1; 20 -3.976 0.2566 1 1 1 0; ; Results of Fit:; ---------------; Test sample size: 2100; Multiple correlation c",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:7051,Energy Efficiency,power,powers,7051,"eference run; ; ; // the right coefficients (before fit); double GoodCoeffsNoFit[] = {; -4.37056,; 43.1468,; 13.432,; 13.4632,; 13.3964,; 13.328,; 13.3016,; 13.3519,; 4.49724,; 4.63876,; 4.89036,; -3.69982,; -3.98618,; -3.86195,; 4.36054,; -4.02597,; 4.57037,; 4.69845,; 2.83819,; -3.48855,; -3.97612; };; ; // the right coefficients (after fit); double GoodCoeffs[] = {; -4.399,; 43.15,; 13.41,; 13.49,; 13.4,; 13.23,; 13.34,; 13.29,; 4.523,; 4.659,; 4.948,; -4.026,; -4.045,; -3.939,; 4.421,; -4.006,; 4.626,; 4.378,; 3.516,; -4.111,; -3.823,; };; ; // Good Powers; int GoodPower[] = {; 1, 1, 1, 1,; 2, 1, 1, 1,; 1, 1, 1, 2,; 1, 1, 2, 1,; 1, 2, 1, 1,; 2, 2, 1, 1,; 2, 1, 1, 2,; 2, 1, 2, 1,; 1, 1, 1, 3,; 1, 3, 1, 1,; 1, 1, 5, 1,; 1, 1, 2, 2,; 1, 2, 1, 2,; 1, 2, 2, 1,; 2, 1, 1, 3,; 2, 2, 1, 2,; 2, 1, 3, 1,; 2, 3, 1, 1,; 1, 2, 2, 2,; 2, 1, 2, 2,; 2, 2, 2, 1; };; ; int nc = fit->GetNCoefficients();; int nv = fit->GetNVariables();; const int *powers = fit->GetPowers();; const int *pindex = fit->GetPowerIndex();; if (nc != 21) return 1;; const TVectorD *coeffs = fit->GetCoefficients();; int k = 0;; for (int i=0;i<nc;i++) {; if (doFit) {; if (!TMath::AreEqualRel((*coeffs)[i],GoodCoeffs[i],1e-3)) return 2;; }; else {; if (TMath::Abs((*coeffs)[i] - GoodCoeffsNoFit[i]) > 5e-5) return 2;; }; for (int j=0;j<nv;j++) {; if (powers[pindex[i]*nv+j] != GoodPower[k]) return 3;; k++;; }; }; ; // now test the result of the generated function; gROOT->ProcessLine("".L MDF.C"");; ; double refMDF = (doFit) ? 43.95 : 43.98;; // this does not work in CLing since the function is not defined; //double x[] = {5,5,5,5};; //double rMDF = MDF(x);; //LM: need to return the address of the result since it is casted to a long (this should not be in a tutorial !); std::intptr_t iret = gROOT->ProcessLine("" double xvalues[] = {5,5,5,5}; double result=MDF(xvalues); &result;"");; double rMDF = * ( (double*)iret);; //printf(""%f\n"",rMDF);; if (TMath::Abs(rMDF -refMDF) > 1e-2) return 4;; return 0;; }; ; //______________",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:7431,Energy Efficiency,power,powers,7431,"eference run; ; ; // the right coefficients (before fit); double GoodCoeffsNoFit[] = {; -4.37056,; 43.1468,; 13.432,; 13.4632,; 13.3964,; 13.328,; 13.3016,; 13.3519,; 4.49724,; 4.63876,; 4.89036,; -3.69982,; -3.98618,; -3.86195,; 4.36054,; -4.02597,; 4.57037,; 4.69845,; 2.83819,; -3.48855,; -3.97612; };; ; // the right coefficients (after fit); double GoodCoeffs[] = {; -4.399,; 43.15,; 13.41,; 13.49,; 13.4,; 13.23,; 13.34,; 13.29,; 4.523,; 4.659,; 4.948,; -4.026,; -4.045,; -3.939,; 4.421,; -4.006,; 4.626,; 4.378,; 3.516,; -4.111,; -3.823,; };; ; // Good Powers; int GoodPower[] = {; 1, 1, 1, 1,; 2, 1, 1, 1,; 1, 1, 1, 2,; 1, 1, 2, 1,; 1, 2, 1, 1,; 2, 2, 1, 1,; 2, 1, 1, 2,; 2, 1, 2, 1,; 1, 1, 1, 3,; 1, 3, 1, 1,; 1, 1, 5, 1,; 1, 1, 2, 2,; 1, 2, 1, 2,; 1, 2, 2, 1,; 2, 1, 1, 3,; 2, 2, 1, 2,; 2, 1, 3, 1,; 2, 3, 1, 1,; 1, 2, 2, 2,; 2, 1, 2, 2,; 2, 2, 2, 1; };; ; int nc = fit->GetNCoefficients();; int nv = fit->GetNVariables();; const int *powers = fit->GetPowers();; const int *pindex = fit->GetPowerIndex();; if (nc != 21) return 1;; const TVectorD *coeffs = fit->GetCoefficients();; int k = 0;; for (int i=0;i<nc;i++) {; if (doFit) {; if (!TMath::AreEqualRel((*coeffs)[i],GoodCoeffs[i],1e-3)) return 2;; }; else {; if (TMath::Abs((*coeffs)[i] - GoodCoeffsNoFit[i]) > 5e-5) return 2;; }; for (int j=0;j<nv;j++) {; if (powers[pindex[i]*nv+j] != GoodPower[k]) return 3;; k++;; }; }; ; // now test the result of the generated function; gROOT->ProcessLine("".L MDF.C"");; ; double refMDF = (doFit) ? 43.95 : 43.98;; // this does not work in CLing since the function is not defined; //double x[] = {5,5,5,5};; //double rMDF = MDF(x);; //LM: need to return the address of the result since it is casted to a long (this should not be in a tutorial !); std::intptr_t iret = gROOT->ProcessLine("" double xvalues[] = {5,5,5,5}; double result=MDF(xvalues); &result;"");; double rMDF = * ( (double*)iret);; //printf(""%f\n"",rMDF);; if (TMath::Abs(rMDF -refMDF) > 1e-2) return 4;; return 0;; }; ; //______________",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:12203,Energy Efficiency,schedul,scheduler,12203,""");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostream.h; TApplication.h; TBrowser.h; TCanvas.h; TFile.h; TH1.h; TMath.h; TMultiDimFit.h; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; TRandom.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TSystem.h; TVectorD.h; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TMultiDimFitMultidimensional Fits in ROOT.Definition TMultiDimFit.h:15; TMultiDimFit::kMonomials@ kMonomialsDefinition TMultiDimFit.h:19; TRandomThis is the base class for the ROOT Random number generators.Definition TRandom.h:27; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::RndmDouble_t Rndm() overrideMachine independent random number generator.Definition TRandom.cxx:559; TVectorT< Double_t >; xDouble_t x[n]Definition legend1.C:17; PyTorch_Generate_CNN_Model.fitfit(model, train_loader, val_loader, num_epochs, batch_size, optimizer, criterion, save_best, scheduler)Definition PyTorch_Generate_CNN_Model.py:34; TMath::SqrtDouble_t Sqrt(Double_t x)Returns the square root of x.Definition TMath.h:662; TMath::AreEqualRelBool_t AreEqualRel(Double_t af, Double_t bf, Double_t relPrec)Comparing floating points.Definition TMath.h:426; TMath::AbsShort_t Abs(Short_t d)Returns the absolute value of parameter Short_t d.Definition TMathBase.h:123; outputstatic void output(); AuthorsRene Brun, Christian Holm Christensen ; Definition in file multidimfit.C. tutorialsfitmultidimfit.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:27 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:9145,Modifiability,variab,variables,9145,"< ""*************************************************"" << std::endl;; std::cout << std::endl;; ; // Initialize global TRannom object.; gRandom = new TRandom();; ; // Open output file; TFile* output = new TFile(""mdf.root"", ""RECREATE"");; ; // Global data parameters; int nVars = 4;; int nData = 500;; double x[4];; ; // make fit object and set parameters on it.; TMultiDimFit* fit = new TMultiDimFit(nVars, TMultiDimFit::kMonomials,""v"");; ; int mPowers[] = { 6 , 6, 6, 6 };; fit->SetMaxPowers(mPowers);; fit->SetMaxFunctions(1000);; fit->SetMaxStudy(1000);; fit->SetMaxTerms(30);; fit->SetPowerLimit(1);; fit->SetMinAngle(10);; fit->SetMaxAngle(10);; fit->SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:9598,Modifiability,parameteriz,parameterization,9598,"< ""*************************************************"" << std::endl;; std::cout << std::endl;; ; // Initialize global TRannom object.; gRandom = new TRandom();; ; // Open output file; TFile* output = new TFile(""mdf.root"", ""RECREATE"");; ; // Global data parameters; int nVars = 4;; int nData = 500;; double x[4];; ; // make fit object and set parameters on it.; TMultiDimFit* fit = new TMultiDimFit(nVars, TMultiDimFit::kMonomials,""v"");; ; int mPowers[] = { 6 , 6, 6, 6 };; fit->SetMaxPowers(mPowers);; fit->SetMaxFunctions(1000);; fit->SetMaxStudy(1000);; fit->SetMaxTerms(30);; fit->SetPowerLimit(1);; fit->SetMinAngle(10);; fit->SetMaxAngle(10);; fit->SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:9717,Modifiability,variab,variables,9717,"< ""*************************************************"" << std::endl;; std::cout << std::endl;; ; // Initialize global TRannom object.; gRandom = new TRandom();; ; // Open output file; TFile* output = new TFile(""mdf.root"", ""RECREATE"");; ; // Global data parameters; int nVars = 4;; int nData = 500;; double x[4];; ; // make fit object and set parameters on it.; TMultiDimFit* fit = new TMultiDimFit(nVars, TMultiDimFit::kMonomials,""v"");; ; int mPowers[] = { 6 , 6, 6, 6 };; fit->SetMaxPowers(mPowers);; fit->SetMaxFunctions(1000);; fit->SetMaxStudy(1000);; fit->SetMaxTerms(30);; fit->SetPowerLimit(1);; fit->SetMinAngle(10);; fit->SetMaxAngle(10);; fit->SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:10239,Modifiability,variab,variables,10239,"SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->Fit(""M"");; ; // Print result; fit->Print(""fc v"");; ; // Write code to file; fit->MakeCode();; ; // Write histograms to disk, and close file; output->Write();; output->Close();; delete output;; ; // Compare results with reference run; int compare = CompareResults(fit, doFit);; if (!compare) {; printf(""\nmultidimfit .............................................. OK\n"");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostr",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:10387,Modifiability,parameteriz,parameterizatio,10387,"SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->Fit(""M"");; ; // Print result; fit->Print(""fc v"");; ; // Write code to file; fit->MakeCode();; ; // Write histograms to disk, and close file; output->Write();; output->Close();; delete output;; ; // Compare results with reference run; int compare = CompareResults(fit, doFit);; if (!compare) {; printf(""\nmultidimfit .............................................. OK\n"");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostr",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:12170,Performance,optimiz,optimizer,12170,""");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostream.h; TApplication.h; TBrowser.h; TCanvas.h; TFile.h; TH1.h; TMath.h; TMultiDimFit.h; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; TRandom.h; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; TSystem.h; TVectorD.h; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TMultiDimFitMultidimensional Fits in ROOT.Definition TMultiDimFit.h:15; TMultiDimFit::kMonomials@ kMonomialsDefinition TMultiDimFit.h:19; TRandomThis is the base class for the ROOT Random number generators.Definition TRandom.h:27; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::RndmDouble_t Rndm() overrideMachine independent random number generator.Definition TRandom.cxx:559; TVectorT< Double_t >; xDouble_t x[n]Definition legend1.C:17; PyTorch_Generate_CNN_Model.fitfit(model, train_loader, val_loader, num_epochs, batch_size, optimizer, criterion, save_best, scheduler)Definition PyTorch_Generate_CNN_Model.py:34; TMath::SqrtDouble_t Sqrt(Double_t x)Returns the square root of x.Definition TMath.h:662; TMath::AreEqualRelBool_t AreEqualRel(Double_t af, Double_t bf, Double_t relPrec)Comparing floating points.Definition TMath.h:426; TMath::AbsShort_t Abs(Short_t d)Returns the absolute value of parameter Short_t d.Definition TMathBase.h:123; outputstatic void output(); AuthorsRene Brun, Christian Holm Christensen ; Definition in file multidimfit.C. tutorialsfitmultidimfit.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:27 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:7503,Testability,test,test,7503,"eference run; ; ; // the right coefficients (before fit); double GoodCoeffsNoFit[] = {; -4.37056,; 43.1468,; 13.432,; 13.4632,; 13.3964,; 13.328,; 13.3016,; 13.3519,; 4.49724,; 4.63876,; 4.89036,; -3.69982,; -3.98618,; -3.86195,; 4.36054,; -4.02597,; 4.57037,; 4.69845,; 2.83819,; -3.48855,; -3.97612; };; ; // the right coefficients (after fit); double GoodCoeffs[] = {; -4.399,; 43.15,; 13.41,; 13.49,; 13.4,; 13.23,; 13.34,; 13.29,; 4.523,; 4.659,; 4.948,; -4.026,; -4.045,; -3.939,; 4.421,; -4.006,; 4.626,; 4.378,; 3.516,; -4.111,; -3.823,; };; ; // Good Powers; int GoodPower[] = {; 1, 1, 1, 1,; 2, 1, 1, 1,; 1, 1, 1, 2,; 1, 1, 2, 1,; 1, 2, 1, 1,; 2, 2, 1, 1,; 2, 1, 1, 2,; 2, 1, 2, 1,; 1, 1, 1, 3,; 1, 3, 1, 1,; 1, 1, 5, 1,; 1, 1, 2, 2,; 1, 2, 1, 2,; 1, 2, 2, 1,; 2, 1, 1, 3,; 2, 2, 1, 2,; 2, 1, 3, 1,; 2, 3, 1, 1,; 1, 2, 2, 2,; 2, 1, 2, 2,; 2, 2, 2, 1; };; ; int nc = fit->GetNCoefficients();; int nv = fit->GetNVariables();; const int *powers = fit->GetPowers();; const int *pindex = fit->GetPowerIndex();; if (nc != 21) return 1;; const TVectorD *coeffs = fit->GetCoefficients();; int k = 0;; for (int i=0;i<nc;i++) {; if (doFit) {; if (!TMath::AreEqualRel((*coeffs)[i],GoodCoeffs[i],1e-3)) return 2;; }; else {; if (TMath::Abs((*coeffs)[i] - GoodCoeffsNoFit[i]) > 5e-5) return 2;; }; for (int j=0;j<nv;j++) {; if (powers[pindex[i]*nv+j] != GoodPower[k]) return 3;; k++;; }; }; ; // now test the result of the generated function; gROOT->ProcessLine("".L MDF.C"");; ; double refMDF = (doFit) ? 43.95 : 43.98;; // this does not work in CLing since the function is not defined; //double x[] = {5,5,5,5};; //double rMDF = MDF(x);; //LM: need to return the address of the result since it is casted to a long (this should not be in a tutorial !); std::intptr_t iret = gROOT->ProcessLine("" double xvalues[] = {5,5,5,5}; double result=MDF(xvalues); &result;"");; double rMDF = * ( (double*)iret);; //printf(""%f\n"",rMDF);; if (TMath::Abs(rMDF -refMDF) > 1e-2) return 4;; return 0;; }; ; //______________",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:9774,Testability,test,test,9774,"< ""*************************************************"" << std::endl;; std::cout << std::endl;; ; // Initialize global TRannom object.; gRandom = new TRandom();; ; // Open output file; TFile* output = new TFile(""mdf.root"", ""RECREATE"");; ; // Global data parameters; int nVars = 4;; int nData = 500;; double x[4];; ; // make fit object and set parameters on it.; TMultiDimFit* fit = new TMultiDimFit(nVars, TMultiDimFit::kMonomials,""v"");; ; int mPowers[] = { 6 , 6, 6, 6 };; fit->SetMaxPowers(mPowers);; fit->SetMaxFunctions(1000);; fit->SetMaxStudy(1000);; fit->SetMaxTerms(30);; fit->SetPowerLimit(1);; fit->SetMinAngle(10);; fit->SetMaxAngle(10);; fit->SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:10036,Testability,test,test,10036,"SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->Fit(""M"");; ; // Print result; fit->Print(""fc v"");; ; // Write code to file; fit->MakeCode();; ; // Write histograms to disk, and close file; output->Write();; output->Close();; delete output;; ; // Compare results with reference run; int compare = CompareResults(fit, doFit);; if (!compare) {; printf(""\nmultidimfit .............................................. OK\n"");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostr",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multidimfit_8C.html:10429,Testability,test,test,10429,"SetMinRelativeError(.01);; ; // variables to hold the temporary input data; double d;; double e;; ; // Print out the start parameters; fit->Print(""p"");; ; printf(""======================================\n"");; ; // Create training sample; int i;; for (i = 0; i < nData ; i++) {; ; // Make some data; makeData(x,d,e);; ; // Add the row to the fit object; fit->AddRow(x,d,e);; }; ; // Print out the statistics; fit->Print(""s"");; ; // Book histograms; fit->MakeHistograms();; ; // Find the parameterization; fit->FindParameterization();; ; // Print coefficents; fit->Print(""rc"");; ; // Get the min and max of variables from the training sample, used; // for cuts in test sample.; double *xMax = new double[nVars];; double *xMin = new double[nVars];; for (i = 0; i < nVars; i++) {; xMax[i] = (*fit->GetMaxVariables())(i);; xMin[i] = (*fit->GetMinVariables())(i);; }; ; nData = fit->GetNCoefficients() * 100;; int j;; ; // Create test sample; for (i = 0; i < nData ; i++) {; // Make some data; makeData(x,d,e);; ; for (j = 0; j < nVars; j++); if (x[j] < xMin[j] || x[j] > xMax[j]); break;; ; // If we get through the loop above, all variables are in range; if (j == nVars); // Add the row to the fit object; fit->AddTestRow(x,d,e);; else; i--;; }; //delete gRandom;; ; // Test the parameterizatio and coefficents using the test sample.; if (doFit); fit->Fit(""M"");; ; // Print result; fit->Print(""fc v"");; ; // Write code to file; fit->MakeCode();; ; // Write histograms to disk, and close file; output->Write();; output->Close();; delete output;; ; // Compare results with reference run; int compare = CompareResults(fit, doFit);; if (!compare) {; printf(""\nmultidimfit .............................................. OK\n"");; } else {; printf(""\nmultidimfit .............................................. fails case %d\n"",compare);; }; ; // We're done; delete fit;; delete [] xMin;; delete [] xMax;; return compare;; }; d#define d(i)Definition RSha256.hxx:102; e#define e(i)Definition RSha256.hxx:103; Riostr",MatchSource.WIKI,doc/master/multidimfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multidimfit_8C.html
https://root.cern/doc/master/multifit_8C.html:388,Availability,error,errors,388,". ROOT: tutorials/fit/multifit.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Fitting multiple functions to different ranges of a 1-D histogram Example showing how to fit in a sub-range of an histogram A histogram is created and filled with the bin contents and errors defined in the table below. ; Three Gaussians are fitted in sub-ranges of this histogram. A new function (a sum of 3 Gaussians) is fitted on another subrange Note that when fitting simple functions, such as Gaussians, the initial values of parameters are automatically computed by ROOT. In the more complicated case of the sum of 3 Gaussians, the initial values of parameters must be given. In this particular case, the initial values are taken from the result of the individual fits. ; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0848003; NDf = 7; Edm = 8.86911e-08; NCalls = 106; Constant = 4.96664 +/- 2.83221 ; Mean = 95.4663 +/- 12.3905 ; Sigma = 6.82779 +/- 7.49131 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0771026; NDf = 7; Edm = 1.00182e-07; NCalls = 73; Constant = 5.96312 +/- 1.14355 ; Mean = 100.467 +/- 1.53372 ; Sigma = 3.54806 +/- 1.16899 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.00877492; NDf = 8; Edm = 4.98832e-06; NCalls = 87; Constant = 0.912053 +/- 0.435309 ; Mean = 116.304 +/- 8.32344 ; Sigma = 8.38103 +/- 18.5139 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.31282; NDf = 31; Edm = 3.25006e-06; NCalls = 495; p0 = 4.91052 +/- 1.41324 ; p1 = 94.4492 +/- 3.71244 ; p2 = 5.9461 +/- 2.41662 ; p3 = 3.22456 +/- 3.11384 ; p4 = 101.662 +/- 1.67862 ; p5 = 2.48631 +/- 1.91151 ; p6 = 0.911626 +/- 0.368736 ; p7 = 117.581 +/- 5.06092 ; p8 = 7.59194 +/- 8.78217 ; ; #include ""TH1.h""; #include ""TF1.h""; ; ",MatchSource.WIKI,doc/master/multifit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8C.html
https://root.cern/doc/master/multifit_8C.html:576,Usability,simpl,simple,576,". ROOT: tutorials/fit/multifit.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Fitting multiple functions to different ranges of a 1-D histogram Example showing how to fit in a sub-range of an histogram A histogram is created and filled with the bin contents and errors defined in the table below. ; Three Gaussians are fitted in sub-ranges of this histogram. A new function (a sum of 3 Gaussians) is fitted on another subrange Note that when fitting simple functions, such as Gaussians, the initial values of parameters are automatically computed by ROOT. In the more complicated case of the sum of 3 Gaussians, the initial values of parameters must be given. In this particular case, the initial values are taken from the result of the individual fits. ; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0848003; NDf = 7; Edm = 8.86911e-08; NCalls = 106; Constant = 4.96664 +/- 2.83221 ; Mean = 95.4663 +/- 12.3905 ; Sigma = 6.82779 +/- 7.49131 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0771026; NDf = 7; Edm = 1.00182e-07; NCalls = 73; Constant = 5.96312 +/- 1.14355 ; Mean = 100.467 +/- 1.53372 ; Sigma = 3.54806 +/- 1.16899 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.00877492; NDf = 8; Edm = 4.98832e-06; NCalls = 87; Constant = 0.912053 +/- 0.435309 ; Mean = 116.304 +/- 8.32344 ; Sigma = 8.38103 +/- 18.5139 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.31282; NDf = 31; Edm = 3.25006e-06; NCalls = 495; p0 = 4.91052 +/- 1.41324 ; p1 = 94.4492 +/- 3.71244 ; p2 = 5.9461 +/- 2.41662 ; p3 = 3.22456 +/- 3.11384 ; p4 = 101.662 +/- 1.67862 ; p5 = 2.48631 +/- 1.91151 ; p6 = 0.911626 +/- 0.368736 ; p7 = 117.581 +/- 5.06092 ; p8 = 7.59194 +/- 8.78217 ; ; #include ""TH1.h""; #include ""TF1.h""; ; ",MatchSource.WIKI,doc/master/multifit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8C.html
https://root.cern/doc/master/multifit_8C_source.html:438,Availability,error,errors,438,". ROOT: tutorials/fit/multifit.C Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.C. Go to the documentation of this file. 1/// \file; 2/// \ingroup tutorial_fit; 3/// \notebook -js; 4/// Fitting multiple functions to different ranges of a 1-D histogram; 5/// Example showing how to fit in a sub-range of an histogram; 6/// A histogram is created and filled with the bin contents and errors; 7/// defined in the table below.; 8/// Three Gaussians are fitted in sub-ranges of this histogram.; 9/// A new function (a sum of 3 Gaussians) is fitted on another subrange; 10/// Note that when fitting simple functions, such as Gaussians, the initial; 11/// values of parameters are automatically computed by ROOT.; 12/// In the more complicated case of the sum of 3 Gaussians, the initial values; 13/// of parameters must be given. In this particular case, the initial values; 14/// are taken from the result of the individual fits.; 15///; 16/// \macro_image; 17/// \macro_output; 18/// \macro_code; 19///; 20/// \author Rene Brun; 21 ; 22#include ""TH1.h""; 23#include ""TF1.h""; 24 ; 25void multifit(); 26{; 27 const int np = 49;; 28 float x[np] = {1.913521, 1.953769, 2.347435, 2.883654, 3.493567, 4.047560, 4.337210, 4.364347, 4.563004,; 29 5.054247, 5.194183, 5.380521, 5.303213, 5.384578, 5.563983, 5.728500, 5.685752, 5.080029,; 30 4.251809, 3.372246, 2.207432, 1.227541, 0.8597788, 0.8220503, 0.8046592, 0.7684097, 0.7469761,; 31 0.8019787, 0.8362375, 0.8744895, 0.9143721, 0.9462768, 0.9285364, 0.8954604, 0.8410891, 0.7853871,; 32 0.7100883, 0.6938808, 0.7363682, 0.7032954, 0.6029015, 0.5600163, 0.7477068, 1.188785, 1.938228,; 33 2.602717, 3.472962, 4.465014, 5.177035};; 34 ; 35 // The histogram are filled with bins defined in the array x.; 36 TH1F *h = new TH1F(""h"", ""Example of several fits in subranges"", np, 85, 134);; 37 h->SetMaximum(7);; 38 ; 39 for (int i = 0; i < np; i++) {; 40 h->SetBinContent(i + 1, x[i]);; 41 }; 42 ; 43 // Defin",MatchSource.WIKI,doc/master/multifit_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8C_source.html
https://root.cern/doc/master/multifit_8C_source.html:649,Usability,simpl,simple,649,". ROOT: tutorials/fit/multifit.C Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.C. Go to the documentation of this file. 1/// \file; 2/// \ingroup tutorial_fit; 3/// \notebook -js; 4/// Fitting multiple functions to different ranges of a 1-D histogram; 5/// Example showing how to fit in a sub-range of an histogram; 6/// A histogram is created and filled with the bin contents and errors; 7/// defined in the table below.; 8/// Three Gaussians are fitted in sub-ranges of this histogram.; 9/// A new function (a sum of 3 Gaussians) is fitted on another subrange; 10/// Note that when fitting simple functions, such as Gaussians, the initial; 11/// values of parameters are automatically computed by ROOT.; 12/// In the more complicated case of the sum of 3 Gaussians, the initial values; 13/// of parameters must be given. In this particular case, the initial values; 14/// are taken from the result of the individual fits.; 15///; 16/// \macro_image; 17/// \macro_output; 18/// \macro_code; 19///; 20/// \author Rene Brun; 21 ; 22#include ""TH1.h""; 23#include ""TF1.h""; 24 ; 25void multifit(); 26{; 27 const int np = 49;; 28 float x[np] = {1.913521, 1.953769, 2.347435, 2.883654, 3.493567, 4.047560, 4.337210, 4.364347, 4.563004,; 29 5.054247, 5.194183, 5.380521, 5.303213, 5.384578, 5.563983, 5.728500, 5.685752, 5.080029,; 30 4.251809, 3.372246, 2.207432, 1.227541, 0.8597788, 0.8220503, 0.8046592, 0.7684097, 0.7469761,; 31 0.8019787, 0.8362375, 0.8744895, 0.9143721, 0.9462768, 0.9285364, 0.8954604, 0.8410891, 0.7853871,; 32 0.7100883, 0.6938808, 0.7363682, 0.7032954, 0.6029015, 0.5600163, 0.7477068, 1.188785, 1.938228,; 33 2.602717, 3.472962, 4.465014, 5.177035};; 34 ; 35 // The histogram are filled with bins defined in the array x.; 36 TH1F *h = new TH1F(""h"", ""Example of several fits in subranges"", np, 85, 134);; 37 h->SetMaximum(7);; 38 ; 39 for (int i = 0; i < np; i++) {; 40 h->SetBinContent(i + 1, x[i]);; 41 }; 42 ; 43 // Defin",MatchSource.WIKI,doc/master/multifit_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8C_source.html
https://root.cern/doc/master/multifit_8py.html:403,Availability,error,errors,403,". ROOT: tutorials/fit/multifit.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; multifit.py File ReferenceTutorials » Fit Tutorials. Detailed Description; Fitting multiple functions to different ranges of a 1-D histogram Example showing how to fit in a sub-range of an histogram A histogram is created and filled with the bin contents and errors defined in the table below. ; Three Gaussians are fitted in sub-ranges of this histogram. A new function (a sum of 3 Gaussians) is fitted on another subrange Note that when fitting simple functions, such as Gaussians, the initial values of parameters are automatically computed by ROOT. In the more complicated case of the sum of 3 Gaussians, the initial values of parameters must be given. In this particular case, the initial values are taken from the result of the individual fits. ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0848003; NDf = 7; Edm = 8.86911e-08; NCalls = 106; Constant = 4.96664 +/- 2.83221 ; Mean = 95.4663 +/- 12.3905 ; Sigma = 6.82779 +/- 7.49131 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0771026; NDf = 7; Edm = 1.00182e-07; NCalls = 73; Constant = 5.96312 +/- 1.14355 ; Mean = 100.467 +/- 1.53372 ; Sigma = 3.54806 +/- 1.16899 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.00877492; NDf = 8; Edm = 4.98832e-06; NCalls = 87; Constant = 0.912053 +/- 0.435309 ; Mean = 116.304 +/- 8.32344 ; Sigma = 8.38103 +/- 18.5139 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.31282; NDf = 31; Edm = 3.25006e-06; NCalls = 495; p0 = 4.91052 +/- 1.41324 ; p1 = 94.4492 +/- 3.71244 ; p2 = 5.9461 +/- 2.41662 ; p3 = 3.22456 +/- 3.11384 ; p4 = 101.662 +/- 1.67862 ; p5 = 2.48631 +/- 1.91151 ; p6 = 0.911626 +/- 0.368736 ; p7 = 117.581 +/- 5.06092 ; p8 = 7.59194 +/- 8.78217 ; [ 4.96663958 95.46632975 6.",MatchSource.WIKI,doc/master/multifit_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8py.html
https://root.cern/doc/master/multifit_8py.html:591,Usability,simpl,simple,591,". ROOT: tutorials/fit/multifit.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; multifit.py File ReferenceTutorials » Fit Tutorials. Detailed Description; Fitting multiple functions to different ranges of a 1-D histogram Example showing how to fit in a sub-range of an histogram A histogram is created and filled with the bin contents and errors defined in the table below. ; Three Gaussians are fitted in sub-ranges of this histogram. A new function (a sum of 3 Gaussians) is fitted on another subrange Note that when fitting simple functions, such as Gaussians, the initial values of parameters are automatically computed by ROOT. In the more complicated case of the sum of 3 Gaussians, the initial values of parameters must be given. In this particular case, the initial values are taken from the result of the individual fits. ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0848003; NDf = 7; Edm = 8.86911e-08; NCalls = 106; Constant = 4.96664 +/- 2.83221 ; Mean = 95.4663 +/- 12.3905 ; Sigma = 6.82779 +/- 7.49131 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.0771026; NDf = 7; Edm = 1.00182e-07; NCalls = 73; Constant = 5.96312 +/- 1.14355 ; Mean = 100.467 +/- 1.53372 ; Sigma = 3.54806 +/- 1.16899 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.00877492; NDf = 8; Edm = 4.98832e-06; NCalls = 87; Constant = 0.912053 +/- 0.435309 ; Mean = 116.304 +/- 8.32344 ; Sigma = 8.38103 +/- 18.5139 (limited); ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 0.31282; NDf = 31; Edm = 3.25006e-06; NCalls = 495; p0 = 4.91052 +/- 1.41324 ; p1 = 94.4492 +/- 3.71244 ; p2 = 5.9461 +/- 2.41662 ; p3 = 3.22456 +/- 3.11384 ; p4 = 101.662 +/- 1.67862 ; p5 = 2.48631 +/- 1.91151 ; p6 = 0.911626 +/- 0.368736 ; p7 = 117.581 +/- 5.06092 ; p8 = 7.59194 +/- 8.78217 ; [ 4.96663958 95.46632975 6.",MatchSource.WIKI,doc/master/multifit_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8py.html
https://root.cern/doc/master/multifit_8py_source.html:430,Availability,error,errors,430,". ROOT: tutorials/fit/multifit.py Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.py. Go to the documentation of this file. 1## \file; 2## \ingroup tutorial_fit; 3## \notebook; 4## Fitting multiple functions to different ranges of a 1-D histogram; 5## Example showing how to fit in a sub-range of an histogram; 6## A histogram is created and filled with the bin contents and errors; 7## defined in the table below.; 8## Three Gaussians are fitted in sub-ranges of this histogram.; 9## A new function (a sum of 3 Gaussians) is fitted on another subrange; 10## Note that when fitting simple functions, such as Gaussians, the initial; 11## values of parameters are automatically computed by ROOT.; 12## In the more complicated case of the sum of 3 Gaussians, the initial values; 13## of parameters must be given. In this particular case, the initial values; 14## are taken from the result of the individual fits.; 15##; 16## \macro_image; 17## \macro_output; 18## \macro_code; 19##; 20## \authors Jonas Rembser, Rene Brun (C++ version); 21 ; 22import ROOT; 23 ; 24import numpy as np; 25 ; 26n_x = 49; 27 ; 28# fmt: off; 29x = np.array( [ 1.913521, 1.953769, 2.347435, 2.883654, 3.493567, 4.047560,; 30 4.337210, 4.364347, 4.563004, 5.054247, 5.194183, 5.380521, 5.303213,; 31 5.384578, 5.563983, 5.728500, 5.685752, 5.080029, 4.251809, 3.372246,; 32 2.207432, 1.227541, 0.8597788, 0.8220503, 0.8046592, 0.7684097, 0.7469761,; 33 0.8019787, 0.8362375, 0.8744895, 0.9143721, 0.9462768, 0.9285364,; 34 0.8954604, 0.8410891, 0.7853871, 0.7100883, 0.6938808, 0.7363682,; 35 0.7032954, 0.6029015, 0.5600163, 0.7477068, 1.188785, 1.938228, 2.602717,; 36 3.472962, 4.465014, 5.177035, ], dtype=np.float32,); 37# fmt: on; 38 ; 39# The histogram are filled with bins defined in the array x.; 40h = ROOT.TH1F(""h"", ""Example of several fits in subranges"", n_x, 85, 134); 41h.SetMaximum(7); 42 ; 43for i, x_i in enumerate(x):; 44 h.SetBinContent(i + 1, x[i]); 45 ",MatchSource.WIKI,doc/master/multifit_8py_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8py_source.html
https://root.cern/doc/master/multifit_8py_source.html:637,Usability,simpl,simple,637,". ROOT: tutorials/fit/multifit.py Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. multifit.py. Go to the documentation of this file. 1## \file; 2## \ingroup tutorial_fit; 3## \notebook; 4## Fitting multiple functions to different ranges of a 1-D histogram; 5## Example showing how to fit in a sub-range of an histogram; 6## A histogram is created and filled with the bin contents and errors; 7## defined in the table below.; 8## Three Gaussians are fitted in sub-ranges of this histogram.; 9## A new function (a sum of 3 Gaussians) is fitted on another subrange; 10## Note that when fitting simple functions, such as Gaussians, the initial; 11## values of parameters are automatically computed by ROOT.; 12## In the more complicated case of the sum of 3 Gaussians, the initial values; 13## of parameters must be given. In this particular case, the initial values; 14## are taken from the result of the individual fits.; 15##; 16## \macro_image; 17## \macro_output; 18## \macro_code; 19##; 20## \authors Jonas Rembser, Rene Brun (C++ version); 21 ; 22import ROOT; 23 ; 24import numpy as np; 25 ; 26n_x = 49; 27 ; 28# fmt: off; 29x = np.array( [ 1.913521, 1.953769, 2.347435, 2.883654, 3.493567, 4.047560,; 30 4.337210, 4.364347, 4.563004, 5.054247, 5.194183, 5.380521, 5.303213,; 31 5.384578, 5.563983, 5.728500, 5.685752, 5.080029, 4.251809, 3.372246,; 32 2.207432, 1.227541, 0.8597788, 0.8220503, 0.8046592, 0.7684097, 0.7469761,; 33 0.8019787, 0.8362375, 0.8744895, 0.9143721, 0.9462768, 0.9285364,; 34 0.8954604, 0.8410891, 0.7853871, 0.7100883, 0.6938808, 0.7363682,; 35 0.7032954, 0.6029015, 0.5600163, 0.7477068, 1.188785, 1.938228, 2.602717,; 36 3.472962, 4.465014, 5.177035, ], dtype=np.float32,); 37# fmt: on; 38 ; 39# The histogram are filled with bins defined in the array x.; 40h = ROOT.TH1F(""h"", ""Example of several fits in subranges"", n_x, 85, 134); 41h.SetMaximum(7); 42 ; 43for i, x_i in enumerate(x):; 44 h.SetBinContent(i + 1, x[i]); 45 ",MatchSource.WIKI,doc/master/multifit_8py_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multifit_8py_source.html
https://root.cern/doc/master/multigraph_8C.html:2606,Availability,error,error,2606," {.6,.8,.7,.4,.3,.3,.4,.5,.6,.7};; auto gr2 = new TGraphErrors(n2,x2,y2,ex2,ey2);; gr2->SetMarkerColor(kRed);; gr2->SetMarkerStyle(20);; ; gr2->Fit(""pol5"",""q"");; auto func2 = (TF1 *) gr2->GetListOfFunctions()->FindObject(""pol5"");; func2->SetLineColor(kRed);; func2->SetLineStyle(2);; ; mg->Add(gr2);; ; mg->Draw(""ap"");; ; //force drawing of canvas to generate the fit TPaveStats; c1->Update();; ; auto stats1 = (TPaveStats*) gr1->GetListOfFunctions()->FindObject(""stats"");; auto stats2 = (TPaveStats*) gr2->GetListOfFunctions()->FindObject(""stats"");; ; if (stats1 && stats2) {; stats1->SetTextColor(kBlue);; stats2->SetTextColor(kRed);; stats1->SetX1NDC(0.12); stats1->SetX2NDC(0.32); stats1->SetY1NDC(0.82);; stats2->SetX1NDC(0.72); stats2->SetX2NDC(0.92); stats2->SetY1NDC(0.75);; c1->Modified();; }; }; Int_tint Int_tDefinition RtypesCore.h:45; Float_tfloat Float_tDefinition RtypesCore.h:57; Double_tdouble Double_tDefinition RtypesCore.h:59; kRed@ kRedDefinition Rtypes.h:66; kBlue@ kBlueDefinition Rtypes.h:66; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; y2Option_t Option_t TPoint TPoint const char y2Definition TGWin32VirtualXProxy.cxx:70; gStyleR__EXTERN TStyle * gStyleDefinition TStyle.h:436; TCanvasThe Canvas class.Definition TCanvas.h:23; TF11-Dim function classDefinition TF1.h:233; TGraphErrorsA TGraphErrors is a TGraph with error bars.Definition TGraphErrors.h:26; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TPaveStatsThe histogram statistics painter class.Definition TPaveStats.h:18; TStyle::SetOptFitvoid SetOptFit(Int_t fit=1)The type of information about fit parameters printed in the histogram statistics box can be selected ...Definition TStyle.cxx:1593; c1return c1Definition legend1.C:41; AuthorRene Brun ; Definition in file multigraph.C. tutorialsgraphsmultigraph.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:29 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/multigraph_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/multigraph_8C.html
https://root.cern/doc/master/myfit_8C.html:381,Modifiability,variab,variables,381,". ROOT: tutorials/fit/myfit.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. myfit.C File ReferenceTutorials » Fit Tutorials. Detailed Description; Get in memory an histogram from a root file and fit a user defined function. ; Note that a user defined function must always be defined as in this example:; first parameter: array of variables (in this example only 1-dimension); second parameter: array of parameters Note also that in case of user defined functions, one must set an initial value for each parameter. ; ****************************************; Minimizer is Minuit2 / Migrad; Chi2 = 36.7428; NDf = 47; Edm = 2.03167e-06; NCalls = 101; Constant = 797.969 +/- 6.79742 ; Mean_value = -7.42918e-05 +/- 0.00734861 ; Sigma = 0.998754 +/- 0.0071337 ; Integral of function = 1907.36; ; #include <TCanvas.h>; #include <TF1.h>; #include <TFile.h>; #include <TH1F.h>; #include <TInterpreter.h>; #include <TROOT.h>; ; #include <cmath>; ; double fitf(double *x, double *par); {; double arg = 0;; if (par[2] != 0) arg = (x[0] - par[1])/par[2];; ; double fitval = par[0]*std::exp(-0.5*arg*arg);; return fitval;; }; void myfit(); {; TString dir = gROOT->GetTutorialDir();; dir.Append(""/hsimple.C"");; dir.ReplaceAll(""/./"",""/"");; if (!gInterpreter->IsLoaded(dir.Data())) gInterpreter->LoadMacro(dir.Data());; TFile *hsimpleFile = (TFile*)gROOT->ProcessLineFast(""hsimple(1)"");; if (!hsimpleFile) return;; ; TCanvas *c1 = new TCanvas(""c1"",""the fit canvas"",500,400);; ; TH1F *hpx = (TH1F*)hsimpleFile->Get(""hpx"");; ; // Creates a Root function based on function fitf above; TF1 *func = new TF1(""fitf"",fitf,-2,2,3);; ; // Sets initial values and parameter names; func->SetParameters(100,0,1);; func->SetParNames(""Constant"",""Mean_value"",""Sigma"");; ; // Fit histogram in range defined by function; hpx->Fit(func,""r"");; ; // Gets integral of function between fit limits; printf(""Integral of function = %g\n"",func->Integral(-2,2));; }; TCanvas.h; TF1.h; TFile.h; ",MatchSource.WIKI,doc/master/myfit_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/myfit_8C.html
https://root.cern/doc/master/na49view_8C.html:256,Safety,detect,detector,256,". ROOT: tutorials/geom/na49view.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. na49view.C File ReferenceTutorials » Geometry tutorials. Detailed Description; This macro generates with 2 views of the NA49 detector using the old obsolete geometry package. . ; void na49view() {; TCanvas *c1 = new TCanvas(""c1"",""The NA49 canvas"",200,10,700,780);; ; gBenchmark->Start(""na49view"");; ; TPad *all = new TPad(""all"",""A Global view of NA49"",0.02,0.02,0.48,0.82,28);; TPad *tof = new TPad(""tof"",""One Time Of Flight element"",0.52,0.02,0.98,0.82,28);; all->Draw();; tof->Draw();; TPaveLabel *na49title = new TPaveLabel(0.04,0.86,0.96,0.98,""Two views of the NA49 detector"");; na49title->SetFillColor(32);; na49title->Draw();; //; TFile *nageom = new TFile(""na49.root"");; if (!nageom || nageom->IsZombie()) return;; TGeometry *n49 =(TGeometry*)gROOT->FindObject(""na49"");; n49->SetBomb(1.2);; n49->cd(); //Set current geometry; all->cd(); //Set current pad; n49->Draw();; c1->Update();; tof->cd();; TNode *TOFR1 = n49->GetNode(""TOFR1"");; TOFR1->Draw();; c1->Update();; ; gBenchmark->Show(""na49view"");; ; // To have a better and dynamic view of any of these pads,; // you can click with the middle button of your mouse to select it.; // Then select ""View with x3d"" in the VIEW menu of the Canvas.; // Once in x3d, you are in wireframe mode by default.; // You can switch to:; // - Hidden Line mode by typing E; // - Solid mode by typing R; // - Wireframe mode by typing W; // - Stereo mode by clicking S (and you need special glasses); // - To leave x3d type Q; }; gBenchmarkR__EXTERN TBenchmark * gBenchmarkDefinition TBenchmark.h:59; gROOT#define gROOTDefinition TROOT.h:406; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TBenchmark::Startvirtual void Start(const char *name)Starts Benchmark with the specified name.Definition TBenchmark.cxx:172; TBenchmark::Showvirtual void Show(const char *name",MatchSource.WIKI,doc/master/na49view_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49view_8C.html
https://root.cern/doc/master/na49view_8C.html:701,Safety,detect,detector,701,". ROOT: tutorials/geom/na49view.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. na49view.C File ReferenceTutorials » Geometry tutorials. Detailed Description; This macro generates with 2 views of the NA49 detector using the old obsolete geometry package. . ; void na49view() {; TCanvas *c1 = new TCanvas(""c1"",""The NA49 canvas"",200,10,700,780);; ; gBenchmark->Start(""na49view"");; ; TPad *all = new TPad(""all"",""A Global view of NA49"",0.02,0.02,0.48,0.82,28);; TPad *tof = new TPad(""tof"",""One Time Of Flight element"",0.52,0.02,0.98,0.82,28);; all->Draw();; tof->Draw();; TPaveLabel *na49title = new TPaveLabel(0.04,0.86,0.96,0.98,""Two views of the NA49 detector"");; na49title->SetFillColor(32);; na49title->Draw();; //; TFile *nageom = new TFile(""na49.root"");; if (!nageom || nageom->IsZombie()) return;; TGeometry *n49 =(TGeometry*)gROOT->FindObject(""na49"");; n49->SetBomb(1.2);; n49->cd(); //Set current geometry; all->cd(); //Set current pad; n49->Draw();; c1->Update();; tof->cd();; TNode *TOFR1 = n49->GetNode(""TOFR1"");; TOFR1->Draw();; c1->Update();; ; gBenchmark->Show(""na49view"");; ; // To have a better and dynamic view of any of these pads,; // you can click with the middle button of your mouse to select it.; // Then select ""View with x3d"" in the VIEW menu of the Canvas.; // Once in x3d, you are in wireframe mode by default.; // You can switch to:; // - Hidden Line mode by typing E; // - Solid mode by typing R; // - Wireframe mode by typing W; // - Stereo mode by clicking S (and you need special glasses); // - To leave x3d type Q; }; gBenchmarkR__EXTERN TBenchmark * gBenchmarkDefinition TBenchmark.h:59; gROOT#define gROOTDefinition TROOT.h:406; TAttFill::SetFillColorvirtual void SetFillColor(Color_t fcolor)Set the fill area color.Definition TAttFill.h:37; TBenchmark::Startvirtual void Start(const char *name)Starts Benchmark with the specified name.Definition TBenchmark.cxx:172; TBenchmark::Showvirtual void Show(const char *name",MatchSource.WIKI,doc/master/na49view_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49view_8C.html
https://root.cern/doc/master/na49view_8py.html:268,Safety,detect,detector,268,". ROOT: tutorials/pyroot/na49view.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; na49view.py File ReferenceTutorials » PyRoot tutorials. Detailed Description; This macro generates two views of the NA49 detector. ; To have a better and dynamic view of any of these pads, you can click with the middle button of your mouse to select it. Then select ""View with x3d"" in the VIEW menu of the Canvas. Once in x3d, you are in wireframe mode by default. You can switch to:; Hidden Line mode by typing E; Solid mode by typing R; Wireframe mode by typing W; Stereo mode by clicking S (and you need special glasses); To leave x3d type Q. ; import ROOT; ; c1 = ROOT.TCanvas( 'c1', 'The NA49 canvas', 200, 10, 700, 780 ); ; ROOT.gBenchmark.Start( 'na49view' ); ; all = ROOT.TPad( 'all', 'A Global view of NA49', 0.02, 0.02, 0.48, 0.82, 28 ); tof = ROOT.TPad( 'tof', 'One Time Of Flight element', 0.52, 0.02, 0.98, 0.82, 28 ); all.Draw();; tof.Draw();; na49title = ROOT.TPaveLabel( 0.04, 0.86, 0.96, 0.98, 'Two views of the NA49 detector' ); na49title.SetFillColor( 32 ); na49title.Draw(); #; nageom = ROOT.TFile( 'py-na49.root' ); n49 = ROOT.gROOT.FindObject( 'na49' ); n49.SetBomb( 1.2 ); n49.cd() # Set current geometry; all.cd() # Set current pad; n49.Draw(); c1.Update(); tof.cd(); TOFR1 = n49.GetNode( 'TOFR1' ); TOFR1.Draw(); c1.Update(); ; ROOT.gBenchmark.Show( 'na49view' ); AuthorWim Lavrijsen ; Definition in file na49view.py. tutorialspyrootna49view.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/na49view_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49view_8py.html
https://root.cern/doc/master/na49view_8py.html:1081,Safety,detect,detector,1081,". ROOT: tutorials/pyroot/na49view.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; na49view.py File ReferenceTutorials » PyRoot tutorials. Detailed Description; This macro generates two views of the NA49 detector. ; To have a better and dynamic view of any of these pads, you can click with the middle button of your mouse to select it. Then select ""View with x3d"" in the VIEW menu of the Canvas. Once in x3d, you are in wireframe mode by default. You can switch to:; Hidden Line mode by typing E; Solid mode by typing R; Wireframe mode by typing W; Stereo mode by clicking S (and you need special glasses); To leave x3d type Q. ; import ROOT; ; c1 = ROOT.TCanvas( 'c1', 'The NA49 canvas', 200, 10, 700, 780 ); ; ROOT.gBenchmark.Start( 'na49view' ); ; all = ROOT.TPad( 'all', 'A Global view of NA49', 0.02, 0.02, 0.48, 0.82, 28 ); tof = ROOT.TPad( 'tof', 'One Time Of Flight element', 0.52, 0.02, 0.98, 0.82, 28 ); all.Draw();; tof.Draw();; na49title = ROOT.TPaveLabel( 0.04, 0.86, 0.96, 0.98, 'Two views of the NA49 detector' ); na49title.SetFillColor( 32 ); na49title.Draw(); #; nageom = ROOT.TFile( 'py-na49.root' ); n49 = ROOT.gROOT.FindObject( 'na49' ); n49.SetBomb( 1.2 ); n49.cd() # Set current geometry; all.cd() # Set current pad; n49.Draw(); c1.Update(); tof.cd(); TOFR1 = n49.GetNode( 'TOFR1' ); TOFR1.Draw(); c1.Update(); ; ROOT.gBenchmark.Show( 'na49view' ); AuthorWim Lavrijsen ; Definition in file na49view.py. tutorialspyrootna49view.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/na49view_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49view_8py.html
https://root.cern/doc/master/na49_8C.html:467,Performance,optimiz,optimize,467,". ROOT: tutorials/geom/na49.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. na49.C File ReferenceTutorials » Geometry tutorials. Detailed Description; This file has been generated automatically via the root utility toroot from an interactive version of GEANT (see ROOT class TGeometry header for an example of use) This shows an example of the old geometry package (now obsolete) ; ; #ifdef __CLING__; #pragma cling optimize(0); #endif; ; #include ""TMaterial.h""; #include ""TMixture.h""; #include ""TNode.h""; #include ""TGeometry.h""; #include ""TRotMatrix.h""; #include ""TBenchmark.h""; #include ""TBRIK.h""; #include ""TTRAP.h""; #include ""TTUBE.h""; ; void na49() {; gBenchmark->Start(""na49"");; ; TMaterial *mat;; TMixture *mix;; TRotMatrix *rot;; TNode *Node;; ; TGeometry *na49 = new TGeometry(""na49"",""na49.C"");; ; ; //-----------List of Materials and Mixtures--------------; ; mat = new TMaterial(""mat1"",""HYDROGEN"",1.01,1,.0708);; mat = new TMaterial(""mat2"",""DEUTERIUM"",2.01,1,.162);; mat = new TMaterial(""mat3"",""HELIUM"",4,2,.125);; mat = new TMaterial(""mat4"",""LITHIUM"",6.94,3,.534);; mat = new TMaterial(""mat5"",""BERILLIUM"",9.01,4,1.848);; mat = new TMaterial(""mat6"",""CARBON"",12.01,6,2.265);; mat = new TMaterial(""mat7"",""NITROGEN"",14.01,7,.808);; mat = new TMaterial(""mat8"",""NEON"",20.18,10,1.207);; mat = new TMaterial(""mat9"",""ALUMINIUM"",26.97999,13,2.7);; mat = new TMaterial(""mat10"",""IRON"",55.84999,26,7.869999);; mat = new TMaterial(""mat11"",""COPPER"",63.54,29,8.96);; mat = new TMaterial(""mat12"",""TUNGSTEN"",183.85,74,19.29999);; mat = new TMaterial(""mat13"",""LEAD"",207.19,82,11.35);; mat = new TMaterial(""mat14"",""URANIUM"",238.0299,92,18.95);; mat = new TMaterial(""mat15"",""AIR"",14.60999,7.3,.001205);; mat = new TMaterial(""mat16"",""VACUUM"",0,0,0);; mat = new TMaterial(""mat17"",""JUNK"",28.09,14,2.329999);; mat = new TMaterial(""mat18"",""JUNK"",28.09,14,2.329999);; mat = new TMaterial(""mat19"",""JUNK"",28.09,14,2.329999);; mat = new TMaterial(""mat20"",""SILICON"",28.",MatchSource.WIKI,doc/master/na49_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49_8C.html
https://root.cern/doc/master/na49_8C.html:948452,Safety,detect,detector,948452,"eAttributes();; ; // Set Node attributes; CAVE1->SetVisibility(2); //node is not drawn but its sons are drawn; VT1_1->SetVisibility(-4); //Node is not drawn. Its immediate sons are drawn; VT2_1->SetVisibility(-4);; MTL_1->SetVisibility(-4);; MTR_1->SetVisibility(-4);; TOFR1->SetVisibility(-4);; ; gBenchmark->Show(""na49"");; }; TBRIK.h; TBenchmark.h; gBenchmarkR__EXTERN TBenchmark * gBenchmarkDefinition TBenchmark.h:59; TGeometry.h; TMaterial.h; TMixture.h; TNode.h; TRotMatrix.h; TTRAP.h; TTUBE.h; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TBRIKA box with faces perpendicular to the axes.Definition TBRIK.h:26; TBenchmark::Startvirtual void Start(const char *name)Starts Benchmark with the specified name.Definition TBenchmark.cxx:172; TBenchmark::Showvirtual void Show(const char *name)Stops Benchmark name and Prints results.Definition TBenchmark.cxx:155; TGeometryTGeometry description.Definition TGeometry.h:39; TMaterialManages a detector material.Definition TMaterial.h:28; TMixtureManages a detector mixture.Definition TMixture.h:27; TMixture::DefineElementvirtual void DefineElement(Int_t n, Float_t a, Float_t z, Float_t w)Define one mixture element.Definition TMixture.cxx:86; TNodeTNode description.Definition TNode.h:33; TNode::cdvirtual void cd(const char *path=nullptr)Change Current Reference node to this.Definition TNode.cxx:249; TNode::ImportShapeAttributesvirtual void ImportShapeAttributes()Copy shape attributes as node attributes.Definition TNode.cxx:409; TNode::SetVisibilityvirtual void SetVisibility(Int_t vis=1)Set visibility for this node and its sons.Definition TNode.cxx:758; TRotMatrixManages a detector rotation matrix.Definition TRotMatrix.h:28; TShape::SetVisibilityvirtual void SetVisibility(Int_t vis)Definition TShape.h:62; TTRAPA general trapezoid.Definition TTRAP.h:33; TTUBEA tube.Definition TTUBE.h:32; AuthorRene Brun ; Definition in file na49.C. tutorialsgeomna49.C. ROOT master - Reference",MatchSource.WIKI,doc/master/na49_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49_8C.html
https://root.cern/doc/master/na49_8C.html:948515,Safety,detect,detector,948515,"2); //node is not drawn but its sons are drawn; VT1_1->SetVisibility(-4); //Node is not drawn. Its immediate sons are drawn; VT2_1->SetVisibility(-4);; MTL_1->SetVisibility(-4);; MTR_1->SetVisibility(-4);; TOFR1->SetVisibility(-4);; ; gBenchmark->Show(""na49"");; }; TBRIK.h; TBenchmark.h; gBenchmarkR__EXTERN TBenchmark * gBenchmarkDefinition TBenchmark.h:59; TGeometry.h; TMaterial.h; TMixture.h; TNode.h; TRotMatrix.h; TTRAP.h; TTUBE.h; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TBRIKA box with faces perpendicular to the axes.Definition TBRIK.h:26; TBenchmark::Startvirtual void Start(const char *name)Starts Benchmark with the specified name.Definition TBenchmark.cxx:172; TBenchmark::Showvirtual void Show(const char *name)Stops Benchmark name and Prints results.Definition TBenchmark.cxx:155; TGeometryTGeometry description.Definition TGeometry.h:39; TMaterialManages a detector material.Definition TMaterial.h:28; TMixtureManages a detector mixture.Definition TMixture.h:27; TMixture::DefineElementvirtual void DefineElement(Int_t n, Float_t a, Float_t z, Float_t w)Define one mixture element.Definition TMixture.cxx:86; TNodeTNode description.Definition TNode.h:33; TNode::cdvirtual void cd(const char *path=nullptr)Change Current Reference node to this.Definition TNode.cxx:249; TNode::ImportShapeAttributesvirtual void ImportShapeAttributes()Copy shape attributes as node attributes.Definition TNode.cxx:409; TNode::SetVisibilityvirtual void SetVisibility(Int_t vis=1)Set visibility for this node and its sons.Definition TNode.cxx:758; TRotMatrixManages a detector rotation matrix.Definition TRotMatrix.h:28; TShape::SetVisibilityvirtual void SetVisibility(Int_t vis)Definition TShape.h:62; TTRAPA general trapezoid.Definition TTRAP.h:33; TTUBEA tube.Definition TTUBE.h:32; AuthorRene Brun ; Definition in file na49.C. tutorialsgeomna49.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:28 (GVA Time) using Do",MatchSource.WIKI,doc/master/na49_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49_8C.html
https://root.cern/doc/master/na49_8C.html:949142,Safety,detect,detector,949142,"rawn but its sons are drawn; VT1_1->SetVisibility(-4); //Node is not drawn. Its immediate sons are drawn; VT2_1->SetVisibility(-4);; MTL_1->SetVisibility(-4);; MTR_1->SetVisibility(-4);; TOFR1->SetVisibility(-4);; ; gBenchmark->Show(""na49"");; }; TBRIK.h; TBenchmark.h; gBenchmarkR__EXTERN TBenchmark * gBenchmarkDefinition TBenchmark.h:59; TGeometry.h; TMaterial.h; TMixture.h; TNode.h; TRotMatrix.h; TTRAP.h; TTUBE.h; TAttLine::SetLineColorvirtual void SetLineColor(Color_t lcolor)Set the line color.Definition TAttLine.h:40; TBRIKA box with faces perpendicular to the axes.Definition TBRIK.h:26; TBenchmark::Startvirtual void Start(const char *name)Starts Benchmark with the specified name.Definition TBenchmark.cxx:172; TBenchmark::Showvirtual void Show(const char *name)Stops Benchmark name and Prints results.Definition TBenchmark.cxx:155; TGeometryTGeometry description.Definition TGeometry.h:39; TMaterialManages a detector material.Definition TMaterial.h:28; TMixtureManages a detector mixture.Definition TMixture.h:27; TMixture::DefineElementvirtual void DefineElement(Int_t n, Float_t a, Float_t z, Float_t w)Define one mixture element.Definition TMixture.cxx:86; TNodeTNode description.Definition TNode.h:33; TNode::cdvirtual void cd(const char *path=nullptr)Change Current Reference node to this.Definition TNode.cxx:249; TNode::ImportShapeAttributesvirtual void ImportShapeAttributes()Copy shape attributes as node attributes.Definition TNode.cxx:409; TNode::SetVisibilityvirtual void SetVisibility(Int_t vis=1)Set visibility for this node and its sons.Definition TNode.cxx:758; TRotMatrixManages a detector rotation matrix.Definition TRotMatrix.h:28; TShape::SetVisibilityvirtual void SetVisibility(Int_t vis)Definition TShape.h:62; TTRAPA general trapezoid.Definition TTRAP.h:33; TTUBEA tube.Definition TTUBE.h:32; AuthorRene Brun ; Definition in file na49.C. tutorialsgeomna49.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:28 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/na49_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/na49_8C.html
https://root.cern/doc/master/namespaceCPyCppyy.html:16172,Availability,error,errors,16172,"const std::string &name, cf_t fac);  ; CPYCPPYY_EXTERN bool RegisterConverter (const std::string &name, ConverterFactory_t);  ; CPYCPPYY_EXTERN bool RegisterConverterAlias (const std::string &name, const std::string &target);  ; CPYCPPYY_EXTERN bool RegisterExecutor (const std::string &name, ExecutorFactory_t);  ; CPYCPPYY_EXTERN bool RegisterExecutorAlias (const std::string &name, const std::string &target);  ; bool ReleasesGIL (CallContext *ctxt);  ; static std::regex s_fnptr (""\\‍(:*\\*&*\\‍)"");  ; CPYCPPYY_EXTERN bool Scope_Check (PyObject *pyobject);  ; CPYCPPYY_EXTERN bool Scope_CheckExact (PyObject *pyobject);  ; static bool ScopeFlagCheck (CPPInstance *self, CPPScope::EFlags flag);  ; static void ScopeFlagSet (CPPInstance *self, CPPScope::EFlags flag);  ; static PyObject * SelectAndForward (TemplateProxy *pytmpl, CPPOverload *pymeth, CPyCppyy_PyArgs_t args, size_t nargsf, PyObject *kwds, bool implicitOkay, bool use_targs, uint64_t sighash, std::vector< Utility::PyError_t > &errors);  ; CPYCPPYY_EXTERN bool Sequence_Check (PyObject *pyobject);  ; static void sync_templates (PyObject *pyclass, const std::string &mtCppName, const std::string &mtName);  ; static std::string targs2str (TemplateProxy *pytmpl);  ; static PyObject * TC2CppName (PyObject *pytc, const char *cpd, bool allow_voidp);  ; template<typename T > ; bool TemplateProxy_Check (T *object);  ; template<typename T > ; bool TemplateProxy_CheckExact (T *object);  ; TemplateProxy * TemplateProxy_New (const std::string &cppname, const std::string &pyname, PyObject *pyclass);  ; static PyObject * tpp_call (TemplateProxy *pytmpl, PyObject *args, PyObject *kwds);  ; static int tpp_clear (TemplateProxy *pytmpl);  ; static void tpp_dealloc (TemplateProxy *pytmpl);  ; static TemplateProxy * tpp_descr_get (TemplateProxy *pytmpl, PyObject *pyobj, PyObject *);  ; static PyObject * tpp_doc (TemplateProxy *pytmpl, void *);  ; static int tpp_doc_set (TemplateProxy *pytmpl, PyObject *val, void *);  ; static PyObject",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespaceCPyCppyy.html:55021,Availability,error,errors,55021,"efinition at line 144 of file CallContext.h. ◆ s_fnptr(). static std::regex CPyCppyy::s_fnptr ; (; ""\\‍(:*\\*&*\\‍)"" ; ). static . ◆ Scope_Check(). bool CPyCppyy::Scope_Check ; (; PyObject * ; pyobject). Definition at line 146 of file API.cxx. ◆ Scope_CheckExact(). bool CPyCppyy::Scope_CheckExact ; (; PyObject * ; pyobject). Definition at line 156 of file API.cxx. ◆ ScopeFlagCheck(). static bool CPyCppyy::ScopeFlagCheck ; (; CPPInstance * ; self, . CPPScope::EFlags ; flag . ). inlinestatic . Definition at line 740 of file CPPInstance.cxx. ◆ ScopeFlagSet(). static void CPyCppyy::ScopeFlagSet ; (; CPPInstance * ; self, . CPPScope::EFlags ; flag . ). inlinestatic . Definition at line 744 of file CPPInstance.cxx. ◆ SelectAndForward(). static PyObject * CPyCppyy::SelectAndForward ; (; TemplateProxy * ; pytmpl, . CPPOverload * ; pymeth, . CPyCppyy_PyArgs_t ; args, . size_t ; nargsf, . PyObject * ; kwds, . bool ; implicitOkay, . bool ; use_targs, . uint64_t ; sighash, . std::vector< Utility::PyError_t > & ; errors . ). inlinestatic . Definition at line 468 of file TemplateProxy.cxx. ◆ Sequence_Check(). bool CPyCppyy::Sequence_Check ; (; PyObject * ; pyobject). Definition at line 188 of file API.cxx. ◆ sync_templates(). static void CPyCppyy::sync_templates ; (; PyObject * ; pyclass, . const std::string & ; mtCppName, . const std::string & ; mtName . ). inlinestatic . Definition at line 137 of file ProxyWrappers.cxx. ◆ targs2str(). static std::string CPyCppyy::targs2str ; (; TemplateProxy * ; pytmpl). inlinestatic . Definition at line 444 of file TemplateProxy.cxx. ◆ TC2CppName(). static PyObject * CPyCppyy::TC2CppName ; (; PyObject * ; pytc, . const char * ; cpd, . bool ; allow_voidp . ). static . Definition at line 20 of file TemplateProxy.cxx. ◆ TemplateProxy_Check(). template<typename T > . bool CPyCppyy::TemplateProxy_Check ; (; T * ; object). inline . Definition at line 79 of file TemplateProxy.h. ◆ TemplateProxy_CheckExact(). template<typename T > . bool CPyCppyy::Temp",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespaceCPyCppyy.html:20294,Performance,perform,performance,20294,"mplateProxy *pytmpl, bool use_targs, uint64_t sighash, CPPOverload *pymeth);  ; bool UseStrictOwnership (CallContext *ctxt);  ; static void vectoriter_dealloc (vectoriterobject *vi);  ; static PyObject * vectoriter_iternext (vectoriterobject *vi);  . Variables; PyTypeObject CPPDataMember_Type;  ; PyTypeObject CPPExcInstance_Type;  ; PyTypeObject CPPInstance_Type;  ; PyTypeObject CPPOverload_Type;  ; PyTypeObject CPPScope_Type;  ; PyTypeObject CustomInstanceMethod_Type;  ; static PyMemberDef dm_members [];  ; static PyMethodDef dm_methods [];  ; static PyNumberMethods ep_as_number;  ; static PyMethodObject * free_list;  ; PyObject * gAbrtException = nullptr;  ; PyObject * gBusException = nullptr;  ; std::ostringstream gCapturedError;  ; static ConvFactories_t gConvFactories;  ; PyObject * gDefaultObject = nullptr;  ; bool gDictLookupActive = false;  ; dict_lookup_func gDictLookupOrg = 0;  ; static ExecFactories_t gExecFactories;  ; PyObject * gIllException = nullptr;  ; std::set< std::string > gIteratorTypes;  ; PyObject * gNullPtrObject = nullptr;  ; std::streambuf * gOldErrorBuffer = nullptr;  ; std::set< Cppyy::TCppType_t > gPinnedTypes;  ; PyObject * gPyTypeMap = nullptr;  ; PyObject * gSegvException = nullptr;  ; PyObject * gThisModule = nullptr;  ; PyTypeObject IndexIter_Type;  ; PyTypeObject InstanceArrayIter_Type;  ; PyTypeObject LowLevelView_Type;  ; static PyGetSetDef meta_getset [];  ; static PyMethodDef meta_methods [];  ; static int numfree = 0;  ; static PyNumberMethods op_as_number;  ; static PySequenceMethods op_as_sequence;  ; static PyGetSetDef op_getset [];  ; static PyMethodDef op_methods [];  ; PyTypeObject RefFloat_Type;  Custom ""builtins,"" detectable by type, for pass by ref and improved performance. ;  ; PyTypeObject RefInt_Type;  ; const int SMALL_ARGS_N = 8;  ; PyTypeObject TemplateProxy_Type;  ; static PyMappingMethods tpp_as_mapping;  ; static PyGetSetDef tpp_getset [];  ; static PyMethodDef tpp_methods [];  ; static PyGetSetDef tptc_getset",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespaceCPyCppyy.html:68385,Performance,perform,performance,68385,"},; {(char*)""__reshape__"", (PyCFunction)op_reshape, METH_O,; (char*)""cast pointer to 1D array type""},; {(char*)nullptr, nullptr, 0, nullptr}; }; CPyCppyy::op_reshapestatic PyObject * op_reshape(CPPInstance *self, PyObject *shape)Definition CPPInstance.cxx:330; CPyCppyy::op_dispatchstatic PyObject * op_dispatch(PyObject *self, PyObject *args, PyObject *)Definition CPPInstance.cxx:277; CPyCppyy::op_destructstatic PyObject * op_destruct(CPPInstance *self)Definition CPPInstance.cxx:266. Definition at line 405 of file CPPInstance.cxx. ◆ RefFloat_Type. PyTypeObject CPyCppyy::RefFloat_Type. Initial value:= { ; PyObject_HEAD_INIT( &PyType_Type ) 0 ,; (char*)""cppyy.Double"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,; Py_TPFLAGS_DEFAULT | Py_TPFLAGS_CHECKTYPES |; Py_TPFLAGS_BASETYPE, ; (char*)""CPyCppyy float object for pass by reference"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0,; &PyFloat_Type, ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0; ; ; ; ; ; ; ; ; ; }. Custom ""builtins,"" detectable by type, for pass by ref and improved performance. ; Definition at line 27 of file CustomPyTypes.cxx. ◆ RefInt_Type. PyTypeObject CPyCppyy::RefInt_Type. Initial value:= { ; PyObject_HEAD_INIT( &PyType_Type ) 0 ,; (char*)""cppyy.Long"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,; Py_TPFLAGS_DEFAULT | Py_TPFLAGS_CHECKTYPES |; Py_TPFLAGS_BASETYPE; ; ; ; , ; (char*)""CPyCppyy long object for pass by reference"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0,; &PyInt_Type, ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0; ; ; ; ; ; ; ; ; ; }. Definition at line 49 of file CustomPyTypes.cxx. ◆ SMALL_ARGS_N. const int CPyCppyy::SMALL_ARGS_N = 8. Definition at line 13 of file CallContext.h. ◆ TemplateProxy_Type. PyTypeObject CPyCppyy::TemplateProxy_Type. Definition at line 890 of file TemplateProxy.cxx. ◆ tpp_as_mapping. PyMappingMethods CPyCppyy::tpp_as_mapping. static . Initial value:= {; nullptr, (binaryfunc)tpp_subscript, nullptr; }. Definition at line 754 of file TemplateProxy.cxx. ◆ tpp_getset. PyGetSetDef CPyCppyy",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespaceCPyCppyy.html:20245,Safety,detect,detectable,20245,"mplateProxy *pytmpl, bool use_targs, uint64_t sighash, CPPOverload *pymeth);  ; bool UseStrictOwnership (CallContext *ctxt);  ; static void vectoriter_dealloc (vectoriterobject *vi);  ; static PyObject * vectoriter_iternext (vectoriterobject *vi);  . Variables; PyTypeObject CPPDataMember_Type;  ; PyTypeObject CPPExcInstance_Type;  ; PyTypeObject CPPInstance_Type;  ; PyTypeObject CPPOverload_Type;  ; PyTypeObject CPPScope_Type;  ; PyTypeObject CustomInstanceMethod_Type;  ; static PyMemberDef dm_members [];  ; static PyMethodDef dm_methods [];  ; static PyNumberMethods ep_as_number;  ; static PyMethodObject * free_list;  ; PyObject * gAbrtException = nullptr;  ; PyObject * gBusException = nullptr;  ; std::ostringstream gCapturedError;  ; static ConvFactories_t gConvFactories;  ; PyObject * gDefaultObject = nullptr;  ; bool gDictLookupActive = false;  ; dict_lookup_func gDictLookupOrg = 0;  ; static ExecFactories_t gExecFactories;  ; PyObject * gIllException = nullptr;  ; std::set< std::string > gIteratorTypes;  ; PyObject * gNullPtrObject = nullptr;  ; std::streambuf * gOldErrorBuffer = nullptr;  ; std::set< Cppyy::TCppType_t > gPinnedTypes;  ; PyObject * gPyTypeMap = nullptr;  ; PyObject * gSegvException = nullptr;  ; PyObject * gThisModule = nullptr;  ; PyTypeObject IndexIter_Type;  ; PyTypeObject InstanceArrayIter_Type;  ; PyTypeObject LowLevelView_Type;  ; static PyGetSetDef meta_getset [];  ; static PyMethodDef meta_methods [];  ; static int numfree = 0;  ; static PyNumberMethods op_as_number;  ; static PySequenceMethods op_as_sequence;  ; static PyGetSetDef op_getset [];  ; static PyMethodDef op_methods [];  ; PyTypeObject RefFloat_Type;  Custom ""builtins,"" detectable by type, for pass by ref and improved performance. ;  ; PyTypeObject RefInt_Type;  ; const int SMALL_ARGS_N = 8;  ; PyTypeObject TemplateProxy_Type;  ; static PyMappingMethods tpp_as_mapping;  ; static PyGetSetDef tpp_getset [];  ; static PyMethodDef tpp_methods [];  ; static PyGetSetDef tptc_getset",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespaceCPyCppyy.html:68336,Safety,detect,detectable,68336,"},; {(char*)""__reshape__"", (PyCFunction)op_reshape, METH_O,; (char*)""cast pointer to 1D array type""},; {(char*)nullptr, nullptr, 0, nullptr}; }; CPyCppyy::op_reshapestatic PyObject * op_reshape(CPPInstance *self, PyObject *shape)Definition CPPInstance.cxx:330; CPyCppyy::op_dispatchstatic PyObject * op_dispatch(PyObject *self, PyObject *args, PyObject *)Definition CPPInstance.cxx:277; CPyCppyy::op_destructstatic PyObject * op_destruct(CPPInstance *self)Definition CPPInstance.cxx:266. Definition at line 405 of file CPPInstance.cxx. ◆ RefFloat_Type. PyTypeObject CPyCppyy::RefFloat_Type. Initial value:= { ; PyObject_HEAD_INIT( &PyType_Type ) 0 ,; (char*)""cppyy.Double"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,; Py_TPFLAGS_DEFAULT | Py_TPFLAGS_CHECKTYPES |; Py_TPFLAGS_BASETYPE, ; (char*)""CPyCppyy float object for pass by reference"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0,; &PyFloat_Type, ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0; ; ; ; ; ; ; ; ; ; }. Custom ""builtins,"" detectable by type, for pass by ref and improved performance. ; Definition at line 27 of file CustomPyTypes.cxx. ◆ RefInt_Type. PyTypeObject CPyCppyy::RefInt_Type. Initial value:= { ; PyObject_HEAD_INIT( &PyType_Type ) 0 ,; (char*)""cppyy.Long"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,; Py_TPFLAGS_DEFAULT | Py_TPFLAGS_CHECKTYPES |; Py_TPFLAGS_BASETYPE; ; ; ; , ; (char*)""CPyCppyy long object for pass by reference"", ; 0, 0, 0, 0, 0, 0, 0, 0, 0,; &PyInt_Type, ; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0; ; ; ; ; ; ; ; ; ; }. Definition at line 49 of file CustomPyTypes.cxx. ◆ SMALL_ARGS_N. const int CPyCppyy::SMALL_ARGS_N = 8. Definition at line 13 of file CallContext.h. ◆ TemplateProxy_Type. PyTypeObject CPyCppyy::TemplateProxy_Type. Definition at line 890 of file TemplateProxy.cxx. ◆ tpp_as_mapping. PyMappingMethods CPyCppyy::tpp_as_mapping. static . Initial value:= {; nullptr, (binaryfunc)tpp_subscript, nullptr; }. Definition at line 754 of file TemplateProxy.cxx. ◆ tpp_getset. PyGetSetDef CPyCppyy",MatchSource.WIKI,doc/master/namespaceCPyCppyy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceCPyCppyy.html
https://root.cern/doc/master/namespacePyROOT.html:583,Energy Efficiency,reduce,reduce,583,". ROOT: PyROOT Namespace Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Functions |; Variables ; PyROOT Namespace Reference. Classes; class  RegulatorCleanup;  A TObject-derived class to inject the memory regulation logic in the ROOT list of cleanups. More...;  ; class  RPyROOTApplication;  Interactive application for Python. More...;  ; class  TMemoryRegulator;  Manages TObject-derived objects created in a PyROOT application. More...;  . Functions; PyObject * AddCPPInstancePickling (PyObject *self, PyObject *args);  Set reduce attribute for CPPInstance objects. ;  ; PyObject * AddPrettyPrintingPyz (PyObject *self, PyObject *args);  Add pretty printing pythonization. ;  ; PyObject * AddTClassDynamicCastPyz (PyObject *self, PyObject *args);  Add pythonization for TClass::DynamicCast. ;  ; PyObject * AddTObjectEqNePyz (PyObject *self, PyObject *args);  Add pythonization for equality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 ",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:1663,Energy Efficiency,reduce,reduce,1663,"Object *args);  Add pretty printing pythonization. ;  ; PyObject * AddTClassDynamicCastPyz (PyObject *self, PyObject *args);  Add pythonization for TClass::DynamicCast. ;  ; PyObject * AddTObjectEqNePyz (PyObject *self, PyObject *args);  Add pythonization for equality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 of file CPPInstancePyz.cxx. ◆ AddPrettyPrintingPyz(). PyObject * PyROOT::AddPrettyPrintingPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pretty printing pythonization. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. This function adds the following pythonizations to print the object more user-friendly than cppyy by using the output of cling::printValue as the return value of the special method str. ; Definition at line 119 of file GenericPyz.cxx. ◆ AddTClassDynamicCastPyz(). PyObject * PyROOT::AddTClassDynamicCastPyz ; (; PyObject * ; self, . PyObject * ",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:243,Integrability,inject,inject,243,". ROOT: PyROOT Namespace Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Functions |; Variables ; PyROOT Namespace Reference. Classes; class  RegulatorCleanup;  A TObject-derived class to inject the memory regulation logic in the ROOT list of cleanups. More...;  ; class  RPyROOTApplication;  Interactive application for Python. More...;  ; class  TMemoryRegulator;  Manages TObject-derived objects created in a PyROOT application. More...;  . Functions; PyObject * AddCPPInstancePickling (PyObject *self, PyObject *args);  Set reduce attribute for CPPInstance objects. ;  ; PyObject * AddPrettyPrintingPyz (PyObject *self, PyObject *args);  Add pretty printing pythonization. ;  ; PyObject * AddTClassDynamicCastPyz (PyObject *self, PyObject *args);  Add pythonization for TClass::DynamicCast. ;  ; PyObject * AddTObjectEqNePyz (PyObject *self, PyObject *args);  Add pythonization for equality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 ",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:1907,Integrability,wrap,wrapped,1907,"uality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 of file CPPInstancePyz.cxx. ◆ AddPrettyPrintingPyz(). PyObject * PyROOT::AddPrettyPrintingPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pretty printing pythonization. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. This function adds the following pythonizations to print the object more user-friendly than cppyy by using the output of cling::printValue as the return value of the special method str. ; Definition at line 119 of file GenericPyz.cxx. ◆ AddTClassDynamicCastPyz(). PyObject * PyROOT::AddTClassDynamicCastPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pythonization for TClass::DynamicCast. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. TClass::DynamicCast returns a void* that the user",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:1952,Integrability,inject,injected,1952,"uality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 of file CPPInstancePyz.cxx. ◆ AddPrettyPrintingPyz(). PyObject * PyROOT::AddPrettyPrintingPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pretty printing pythonization. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. This function adds the following pythonizations to print the object more user-friendly than cppyy by using the output of cling::printValue as the return value of the special method str. ; Definition at line 119 of file GenericPyz.cxx. ◆ AddTClassDynamicCastPyz(). PyObject * PyROOT::AddTClassDynamicCastPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pythonization for TClass::DynamicCast. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. TClass::DynamicCast returns a void* that the user",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:243,Security,inject,inject,243,". ROOT: PyROOT Namespace Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Functions |; Variables ; PyROOT Namespace Reference. Classes; class  RegulatorCleanup;  A TObject-derived class to inject the memory regulation logic in the ROOT list of cleanups. More...;  ; class  RPyROOTApplication;  Interactive application for Python. More...;  ; class  TMemoryRegulator;  Manages TObject-derived objects created in a PyROOT application. More...;  . Functions; PyObject * AddCPPInstancePickling (PyObject *self, PyObject *args);  Set reduce attribute for CPPInstance objects. ;  ; PyObject * AddPrettyPrintingPyz (PyObject *self, PyObject *args);  Add pretty printing pythonization. ;  ; PyObject * AddTClassDynamicCastPyz (PyObject *self, PyObject *args);  Add pythonization for TClass::DynamicCast. ;  ; PyObject * AddTObjectEqNePyz (PyObject *self, PyObject *args);  Add pythonization for equality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 ",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:1952,Security,inject,injected,1952,"uality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 of file CPPInstancePyz.cxx. ◆ AddPrettyPrintingPyz(). PyObject * PyROOT::AddPrettyPrintingPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pretty printing pythonization. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. This function adds the following pythonizations to print the object more user-friendly than cppyy by using the output of cling::printValue as the return value of the special method str. ; Definition at line 119 of file GenericPyz.cxx. ◆ AddTClassDynamicCastPyz(). PyObject * PyROOT::AddTClassDynamicCastPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pythonization for TClass::DynamicCast. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. TClass::DynamicCast returns a void* that the user",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:272,Testability,log,logic,272,". ROOT: PyROOT Namespace Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Functions |; Variables ; PyROOT Namespace Reference. Classes; class  RegulatorCleanup;  A TObject-derived class to inject the memory regulation logic in the ROOT list of cleanups. More...;  ; class  RPyROOTApplication;  Interactive application for Python. More...;  ; class  TMemoryRegulator;  Manages TObject-derived objects created in a PyROOT application. More...;  . Functions; PyObject * AddCPPInstancePickling (PyObject *self, PyObject *args);  Set reduce attribute for CPPInstance objects. ;  ; PyObject * AddPrettyPrintingPyz (PyObject *self, PyObject *args);  Add pretty printing pythonization. ;  ; PyObject * AddTClassDynamicCastPyz (PyObject *self, PyObject *args);  Add pythonization for TClass::DynamicCast. ;  ; PyObject * AddTObjectEqNePyz (PyObject *self, PyObject *args);  Add pythonization for equality and inequality operators in TObject. ;  ; PyObject * BranchPyz (PyObject *self, PyObject *args);  Add pythonization for TTree::Branch. ;  ; PyObject * ClearProxiedObjects (PyObject *self, PyObject *args);  ; PyObject * CPPInstanceExpand (PyObject *self, PyObject *args);  Deserialize pickled objects. ;  ; PyObject * GetBranchAttr (PyObject *self, PyObject *args);  ; void Init ();  ; PyObject * RegisterConverterAlias (PyObject *, PyObject *args);  ; PyObject * RegisterExecutorAlias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 ",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespacePyROOT.html:2410,Usability,user-friendly,user-friendly,2410,"Alias (PyObject *, PyObject *args);  . Variables; PyObject * gRootModule = nullptr;  . Function Documentation. ◆ AddCPPInstancePickling(). PyObject * PyROOT::AddCPPInstancePickling ; (; PyObject * ; self, . PyObject * ; args . ). Set reduce attribute for CPPInstance objects. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. The C++ function op_reduce defined above is wrapped in a Python method so that it can be injected in CPPInstance ; Definition at line 123 of file CPPInstancePyz.cxx. ◆ AddPrettyPrintingPyz(). PyObject * PyROOT::AddPrettyPrintingPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pretty printing pythonization. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. This function adds the following pythonizations to print the object more user-friendly than cppyy by using the output of cling::printValue as the return value of the special method str. ; Definition at line 119 of file GenericPyz.cxx. ◆ AddTClassDynamicCastPyz(). PyObject * PyROOT::AddTClassDynamicCastPyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pythonization for TClass::DynamicCast. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the arguments received from Python. TClass::DynamicCast returns a void* that the user still has to cast (it will have the proper offset, though). Fix this by providing the requested binding if the cast succeeded. ; Definition at line 68 of file TClassPyz.cxx. ◆ AddTObjectEqNePyz(). PyObject * PyROOT::AddTObjectEqNePyz ; (; PyObject * ; self, . PyObject * ; args . ). Add pythonization for equality and inequality operators in TObject. ; Parameters. [in]selfAlways null, since this is a module function. ; [in]argsPointer to a Python tuple object containing the argument",MatchSource.WIKI,doc/master/namespacePyROOT.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespacePyROOT.html
https://root.cern/doc/master/namespaceRooFit.html:22587,Integrability,wrap,wraps,22587,"unction3Binding.h. ◆ CFUNCD4DDDB. typedef double(* RooFit::CFUNCD4DDDB) (double, double, double, bool). Definition at line 32 of file RooCFunction4Binding.h. ◆ CFUNCD4DDDD. typedef double(* RooFit::CFUNCD4DDDD) (double, double, double, double). Definition at line 30 of file RooCFunction4Binding.h. ◆ CFUNCD4DDDI. typedef double(* RooFit::CFUNCD4DDDI) (double, double, double, Int_t). Definition at line 31 of file RooCFunction4Binding.h. ◆ ModelConfig. using RooFit::ModelConfig = typedef RooStats::ModelConfig. Definition at line 374 of file ModelConfig.h. ◆ OwningPtr. template<typename T > . using RooFit::OwningPtr = typedef T *. An alias for raw pointers for indicating that the return type of a RooFit function is an owning pointer that must be deleted by the caller. ; For RooFit developers, it can be very useful to make this an alias to std::unique_ptr<T>, in order to check that your code has no memory problems. Changing this alias is equivalent to forcing all code immediately wraps the result of functions returning a RooFit::OwningPtr<T> in a std::unique_ptr<T>. ; Definition at line 35 of file Config.h. ◆ SuperFloat. typedef double RooFit::SuperFloat. Definition at line 29 of file Floats.h. ◆ SuperFloatPrecision. typedef std::numeric_limits<double> RooFit::SuperFloatPrecision. Definition at line 30 of file Floats.h. Enumeration Type Documentation. ◆ MPSplit. enum RooFit::MPSplit. EnumeratorBulkPartition ; Interleave ; SimComponents ; Hybrid . Definition at line 65 of file RooGlobalFunc.h. ◆ MsgLevel. enum RooFit::MsgLevel. Verbosity level for RooMsgService::StreamConfig in RooMsgService. . EnumeratorDEBUG ; INFO ; PROGRESS ; WARNING ; ERROR ; FATAL . Definition at line 60 of file RooGlobalFunc.h. ◆ MsgTopic. enum RooFit::MsgTopic. Topics for a RooMsgService::StreamConfig in RooMsgService. . EnumeratorGeneration ; Minimization ; Plotting ; Fitting ; Integration ; LinkStateMgmt ; Eval ; Caching ; Optimization ; ObjectHandling ; InputArguments ; Tracing ; Contents ; Dat",MatchSource.WIKI,doc/master/namespaceRooFit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceRooFit.html
https://root.cern/doc/master/namespaceRooFit.html:895,Modifiability,enhance,enhance,895,". ROOT: RooFit Namespace Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces |; Classes |; Typedefs |; Enumerations |; Functions ; RooFit Namespace Reference. The namespace RooFit contains mostly switches that change the behaviour of functions of PDFs (or other types of arguments). ; More... Namespaces; namespace  Detail;  ; namespace  Experimental;  ; namespace  JSONIO;  ; namespace  MultiProcess;  ; namespace  TestStatistics;  Namespace for new RooFit test statistic calculation. ;  . Classes; class  EvalBackend;  ; class  EvalContext;  ; class  Evaluator;  Evaluates a RooAbsReal object in other ways than recursive graph traversal. More...;  ; struct  NodeInfo;  A struct used by the Evaluator to store information on the RooAbsArgs in the computation graph. More...;  ; struct  UniqueId;  A UniqueId can be added as a class member to enhance any class with a unique identifier for each instantiated object. More...;  . Typedefs; typedef double(* CFUNCD1D) (double);  ; typedef double(* CFUNCD1I) (Int_t);  ; typedef double(* CFUNCD2DD) (double, double);  ; typedef double(* CFUNCD2DI) (double, Int_t);  ; typedef double(* CFUNCD2ID) (Int_t, double);  ; typedef double(* CFUNCD2II) (Int_t, Int_t);  ; typedef double(* CFUNCD2UD) (UInt_t, double);  ; typedef double(* CFUNCD3DDB) (double, double, bool);  ; typedef double(* CFUNCD3DDD) (double, double, double);  ; typedef double(* CFUNCD3DII) (double, Int_t, Int_t);  ; typedef double(* CFUNCD3UDD) (UInt_t, double, double);  ; typedef double(* CFUNCD3UDU) (UInt_t, double, UInt_t);  ; typedef double(* CFUNCD3UUD) (UInt_t, UInt_t, double);  ; typedef double(* CFUNCD4DDDB) (double, double, double, bool);  ; typedef double(* CFUNCD4DDDD) (double, double, double, double);  ; typedef double(* CFUNCD4DDDI) (double, double, double, Int_t);  ; using ModelConfig = RooStats::ModelConfig;  ; template<typename T > ; using OwningPtr = T *;  An alias for raw pointers for indicating that the retur",MatchSource.WIKI,doc/master/namespaceRooFit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceRooFit.html
https://root.cern/doc/master/namespaceRooFit.html:5839,Modifiability,variab,variables,5839,"Real * bindFunction (const char *name, CFUNCD1D func, RooAbsReal &x);  ; RooAbsReal * bindFunction (const char *name, CFUNCD1I func, RooAbsReal &x);  ; RooAbsReal * bindFunction (const char *name, CFUNCD2DD func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsReal * bindFunction (const char *name, CFUNCD2DI func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsReal * bindFunction (const char *name, CFUNCD2ID func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsReal * bindFunction (const char *name, CFUNCD2II func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsReal * bindFunction (const char *name, CFUNCD2UD func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3DDB func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3DDD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3DII func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3UDD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3UDU func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3UUD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDB func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDI func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, const ROOT::Math::IBaseFunctionMultiDim &ftor, const RooArgList &vars);  ; RooAbsReal * bindFunction (const char *name, const ROOT::Math::IBaseFunctionOneDim &ftor, RooAbsReal &vars);  ; RooAbsReal * bindFunction (TF1 *func, RooAbsReal &x);  Bind a TFx function to RooFit variables.",MatchSource.WIKI,doc/master/namespaceRooFit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceRooFit.html
https://root.cern/doc/master/namespaceRooFit.html:5990,Modifiability,variab,variables,5990,"Function (const char *name, CFUNCD3UDU func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD3UUD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDB func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDD func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, CFUNCD4DDDI func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, RooAbsReal &w);  ; RooAbsReal * bindFunction (const char *name, const ROOT::Math::IBaseFunctionMultiDim &ftor, const RooArgList &vars);  ; RooAbsReal * bindFunction (const char *name, const ROOT::Math::IBaseFunctionOneDim &ftor, RooAbsReal &vars);  ; RooAbsReal * bindFunction (TF1 *func, RooAbsReal &x);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsReal * bindFunction (TF1 *func, RooAbsReal &x, const RooArgList &params);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsReal * bindFunction (TF2 *func, RooAbsReal &x, RooAbsReal &y);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsReal * bindFunction (TF2 *func, RooAbsReal &x, RooAbsReal &y, const RooArgList &params);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsReal * bindFunction (TF3 *func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsReal * bindFunction (TF3 *func, RooAbsReal &x, RooAbsReal &y, RooAbsReal &z, const RooArgList &params);  Bind a TFx function to RooFit variables. Also see RooTFnBinding. ;  ; RooAbsPdf * bindPdf (const char *name, CFUNCD1D func, RooAbsReal &x);  ; RooAbsPdf * bindPdf (const char *name, CFUNCD1I func, RooAbsReal &x);  ; RooAbsPdf * bindPdf (const char *name, CFUNCD2DD func, RooAbsReal &x, RooAbsReal &y);  ; RooAbsPdf * bindPdf (const cha",MatchSource.WIKI,doc/master/namespaceRooFit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/namespaceRooFit.html
