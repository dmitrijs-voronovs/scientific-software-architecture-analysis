id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3889,Usability,simpl,simplifies,3889,"gether, this means we need to adjust the; /// type of boolean operations to be regbank legal. All SALU booleans need to be; /// widened to 32-bits, and all VALU booleans need to be s1 values.; ///; /// A noteworthy exception to the s1-means-vcc rule is for legalization artifact; /// casts. G_TRUNC s1 results, and G_SEXT/G_ZEXT/G_ANYEXT sources are never vcc; /// bank. A non-boolean source (such as a truncate from a 1-bit load from; /// memory) will require a copy to the VCC bank which will require clearing the; /// high bits and inserting a compare.; ///; /// \par Constant bus restriction; ///; /// VALU instructions have a limitation known as the constant bus; /// restriction. Most VALU instructions can use SGPR operands, but may read at; /// most 1 SGPR or constant literal value (this to 2 in gfx10 for most; /// instructions). This is one unique SGPR, so the same SGPR may be used for; /// multiple operands. From a register bank perspective, any combination of; /// operands should be legal as an SGPR, but this is contextually dependent on; /// the SGPR operands all being the same register. There is therefore optimal to; /// choose the SGPR with the most uses to minimize the number of copies.; ///; /// We avoid trying to solve this problem in RegBankSelect. Any VALU G_*; /// operation should have its source operands all mapped to VGPRs (except for; /// VCC), inserting copies from any SGPR operands. This the most trivial legal; /// mapping. Anything beyond the simplest 1:1 instruction selection would be too; /// complicated to solve here. Every optimization pattern or instruction; /// selected to multiple outputs would have to enforce this rule, and there; /// would be additional complexity in tracking this rule for every G_*; /// operation. By forcing all inputs to VGPRs, it also simplifies the task of; /// picking the optimal operand combination from a post-isel optimization pass.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:44,Security,access,access,44,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:103,Testability,assert,assert,103,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:127,Usability,clear,clear,127,"// 32-bit extract of a 64-bit value is just access of a subregister, so free.; // TODO: Cost of 0 hits assert, though it's not clear it's what we really; // want.; // TODO: 32-bit insert to a 64-bit SGPR may incur a non-free copy due to SGPR; // alignment restrictions, but this probably isn't important.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:28,Performance,load,load,28,// Can't do a scalar atomic load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:20,Performance,load,loads,20,// Don't use scalar loads for volatile accesses to non-constant address; // spaces.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:39,Security,access,accesses,39,// Don't use scalar loads for volatile accesses to non-constant address; // spaces.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:61,Performance,load,load,61,"// Memory must be known constant, or not written before this load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:39,Performance,load,load,39,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:119,Performance,load,load,119,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:317,Performance,load,loads,317,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:339,Performance,load,load,339,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:274,Safety,detect,detecting,274,"// It may be possible to have a vgpr = load sgpr mapping here, because; // the mubuf instructions support this kind of load, but probably for only; // gfx7 and older. However, the addressing mode matching in the instruction; // selector should be able to do a better job of detecting and selecting; // these kinds of loads from the vgpr = load vgpr mapping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:412,Availability,Mask,Mask,412,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:610,Availability,Mask,Mask,610,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:130,Performance,perform,perform,130,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:368,Performance,perform,performs,368,"/// Legalize instruction \p MI where operands in \p OpIndices must be SGPRs. If; /// any of the required SGPR operands are VGPRs, perform a waterfall loop to; /// execute the instruction for each unique combination of values in all lanes; /// in the wave. The block will be split such that rest of the instructions are; /// moved to a new block.; ///; /// Essentially performs this loop:; //; /// Save Execution Mask; /// For (Lane : Wavefront) {; /// Enable Lane, Disable all other lanes; /// SGPR = read SGPR value for current lane from VGPR; /// VGPRResult[Lane] = use_op SGPR; /// }; /// Restore Execution Mask; ///; /// There is additional complexity to try for compare values to identify the; /// unique values used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:66,Availability,mask,mask,66,// Don't bother using generic instructions/registers for the exec mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, save the original EXEC value to VCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:17,Availability,mask,mask,17,// Save the EXEC mask before the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:20,Availability,mask,mask,20,// Restore the EXEC mask after the loop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Safety,avoid,avoid,16,// Use a set to avoid extra readfirstlanes in the case where multiple operands; // are the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:87,Performance,load,loads,87,// There are some special cases that we need to look at for 32 bit and 96; // bit SGPR loads otherwise we have nothing to do.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Performance,load,loads,10,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:163,Performance,load,loads,163,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:183,Performance,load,load,183,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:132,Security,access,access,132,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:210,Security,access,access,210,"// Scalar loads of size 8 or 16 bit with proper alignment may be widened to; // 32 bit. Check to see if we need to widen the memory access, 8 or 16 bit; // scalar loads should have a load size of 32 but memory access size of less; // than 32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,extend,extending,14,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:24,Performance,load,load,24,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:73,Security,access,access,73,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:100,Usability,clear,clear,100,// This is an extending load from a sub-dword size. Widen the memory; // access size to 4 bytes and clear the extra high bits appropriately,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:8,Modifiability,extend,extend,8,// Must extend the sign bit into higher bits for a G_SEXTLOAD,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:8,Modifiability,extend,extend,8,// Must extend zero into higher bits with an AND for a G_ZEXTLOAD,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:55,Performance,load,loads,55,// We do not need to touch the higher bits for regular loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:25,Availability,avail,available,25,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Performance,load,loads,10,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:46,Performance,load,loads,46,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:140,Performance,load,load,140,"// 96-bit loads are only available for vector loads. We need to split this; // into a 64-bit part, and 32 (unless we can widen to a 128-bit load).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:11,Performance,load,loads,11,// 128-bit loads are supported for all instruction types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,variab,variable,14,// Handle the variable sgpr + vgpr case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Performance,load,loads,16,"// TODO: 96-bit loads were widened to 128-bit results. Shrink the result if we; // can, but we need to track an MMO for that.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Performance,load,load,56,"// If only the offset is divergent, emit a MUBUF buffer load instead. We can; // assume that the buffer is unswizzled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer(imm)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:84,Performance,load,load,84,"// TODO: If only the resource is a VGPR, it may be better to execute the; // scalar load in the waterfall loop if the resource is expected to frequently; // be dynamically uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:38,Safety,avoid,avoid,38,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:89,Testability,log,logic,89,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:79,Integrability,Depend,Depending,79,"// Use the 32-bit bitfield extract instruction if the width is a constant.; // Depending on the width size, use either the low or high 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:66,Modifiability,extend,extend,66,"// Use bitfield extract on the lower 32-bit source, and then sign-extend; // or clear the upper 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:80,Usability,clear,clear,80,"// Use bitfield extract on the lower 32-bit source, and then sign-extend; // or clear the upper 32-bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:28,Usability,clear,clear,28,// Ensure the high bits are clear to insert the offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:50,Safety,avoid,avoid,50,// TODO: It might be worth using a pseudo here to avoid scc clobber and; // register class constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:32,Modifiability,extend,extending,32,// Return a suitable opcode for extending the operands of Opc when widening.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:101,Modifiability,extend,extend,101,"// Emit a legalized extension from <2 x s16> to 2 32-bit components, avoiding; // any illegal vector extend or unmerge operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:69,Safety,avoid,avoiding,69,"// Emit a legalized extension from <2 x s16> to 2 32-bit components, avoiding; // any illegal vector extend or unmerge operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:353,Availability,down,down,353,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:206,Energy Efficiency,power,power,206,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:304,Performance,load,load,304,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:41,Integrability,depend,dependency,41,// Use a v_mov_b32 here to make the exec dependency explicit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Modifiability,extend,extending,14,"/// Implement extending a 32-bit value to a 64-bit value. \p Lo32Reg is the; /// original 32-bit source value (to be inserted in the low part of the combined; /// 64-bit result), and \p Hi32Reg is the high half of the combined 64-bit; /// value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:34,Modifiability,extend,extended,34,// Replicate sign bit from 32-bit extended part.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on where the source registers came from, the generic code may; // have decided to split the inputs already or not. If not, we still need to; // extract the values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:22,Availability,avail,available,22,"// 64-bit and is only available on the SALU, so split into 2 32-bit ops if; // there is a VGPR input.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on where the source registers came from, the generic code may; // have decided to split the inputs already or not. If not, we still need to; // extract the values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:106,Availability,down,down,106,"// Special case for s_mul_u64. There is not a vector equivalent of; // s_mul_u64. Hence, we have to break down s_mul_u64 into 32-bit vector; // multiplications.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:156,Modifiability,extend,extended,156,"// This is a special case for s_mul_u64. We use; // G_AMDGPU_S_MUL_I64_I32 opcode to represent an s_mul_u64 operation; // where the 33 higher bits are sign-extended and; // G_AMDGPU_S_MUL_U64_U32 opcode to represent an s_mul_u64 operation; // where the 32 higher bits are zero-extended. In case scalar registers are; // selected, both opcodes are lowered as s_mul_u64. If the vector registers; // are selected, then G_AMDGPU_S_MUL_I64_I32 and; // G_AMDGPU_S_MUL_U64_U32 are lowered with a vector mad instruction.; // Insert basic copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:277,Modifiability,extend,extended,277,"// This is a special case for s_mul_u64. We use; // G_AMDGPU_S_MUL_I64_I32 opcode to represent an s_mul_u64 operation; // where the 33 higher bits are sign-extended and; // G_AMDGPU_S_MUL_U64_U32 opcode to represent an s_mul_u64 operation; // where the 32 higher bits are zero-extended. In case scalar registers are; // selected, both opcodes are lowered as s_mul_u64. If the vector registers; // are selected, then G_AMDGPU_S_MUL_I64_I32 and; // G_AMDGPU_S_MUL_U64_U32 are lowered with a vector mad instruction.; // Insert basic copies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:14,Availability,repair,repair,14,// Nothing to repair,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Availability,Down,Downstream,3,"// Downstream users have expectations for the high bit behavior, so freeze; // incoming undefined bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Modifiability,Extend,Extend,3,// Extend in the low bits and propagate the sign bit to the high half.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:35,Modifiability,extend,extend,35,"// The low bits are unchanged, and extend in the high bits.; // No freeze required",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:27,Energy Efficiency,efficient,efficiently,27,"// We can narrow this more efficiently than Helper can by using ffbh/ffbl; // which return -1 when the input is zero:; // (ctlz_zero_undef hi:lo) -> (umin (ffbh hi), (add (ffbh lo), 32)); // (cttz_zero_undef hi:lo) -> (umin (add (ffbl hi), 32), (ffbl lo)); // (ffbh hi:lo) -> (umin (ffbh hi), (uaddsat (ffbh lo), 32)); // (ffbl hi:lo) -> (umin (uaddsat (ffbh hi), 32), (ffbh lo))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Modifiability,Extend,Extend,3,"// Extend to 32-bit, and then extend the low half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:30,Modifiability,extend,extend,30,"// Extend to 32-bit, and then extend the low half.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:41,Integrability,depend,dependency,41,// Use a v_mov_b32 here to make the exec dependency explicit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:38,Safety,avoid,avoid,38,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:89,Testability,log,logic,89,// Remove the original instruction to avoid potentially confusing the; // waterfall loop logic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:3,Availability,Mask,Mask,3,// Mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:68,Safety,safe,safe,68,"// This is only allowed to execute with 1 lane, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Safety,safe,safe,56,"// Only the first lane is executes, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,Safety,safe,safe,56,"// Only the first lane is executes, so readfirstlane is safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:16,Safety,avoid,avoid,16,// Use a set to avoid extra readfirstlanes in the case where multiple; // operands are the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:91,Performance,optimiz,optimized,91,// Also move the copy from the scratch rsrc descriptor into the loop; // to allow it to be optimized away.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:59,Performance,load,load,59,// We have a uniform instruction so we want to use an SMRD load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:47,Performance,load,load,47,"// FIXME: Do we want to add a mapping for FLAT load, or should we just; // handle that during instruction selection?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:15,Testability,log,logic,15,"// The default logic bothers to analyze impossible alternative mappings. We; // want the most straightforward mapping, so just directly handle this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:133,Availability,mask,masking,133,// The default handling is broken and doesn't handle illegal SGPR->VGPR copies; // properly.; //; // TODO: There are additional exec masking dependencies to analyze.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:141,Integrability,depend,dependencies,141,// The default handling is broken and doesn't handle illegal SGPR->VGPR copies; // properly.; //; // TODO: There are additional exec masking dependencies to analyze.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:10,Modifiability,extend,extend,10,"// Scalar extend can use 64-bit BFE, but VGPRs require extending to; // 32-bits, and then to 64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:55,Modifiability,extend,extending,55,"// Scalar extend can use 64-bit BFE, but VGPRs require extending to; // 32-bits, and then to 64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:50,Availability,down,down,50,"// This is a weird case, because we need to break down the mapping based on; // the register bank of a different operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp:52,Testability,test,testing,52,// Check the GPU isn't generic. Generic is used for testing only; // and we don't want this pass to interfere with it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURemoveIncompatibleFunctions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:101,Security,access,access,101,"// Even if FLAT_SCRATCH is implicitly used, it has no effect if flat; // instructions aren't used to access the scratch buffer. Inline assembly may; // need it though.; //; // If we only have implicit uses of flat_scr on flat instructions, it is not; // really needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:239,Availability,error,errored,239,"// Avoid crashing on undefined behavior with an illegal call to a; // kernel. If a callsite's calling convention doesn't match the; // function's, it's undefined behavior. If the callsite calling; // convention does match, that would have errored earlier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp:3,Safety,Avoid,Avoid,3,"// Avoid crashing on undefined behavior with an illegal call to a; // kernel. If a callsite's calling convention doesn't match the; // function's, it's undefined behavior. If the callsite calling; // convention does match, that would have errored earlier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h:72,Integrability,depend,depending,72,// Total number of VGPRs is actually a combination of AGPR and VGPR; // depending on architecture - and some alignment constraints,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUResourceUsageAnalysis.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1413,Modifiability,variab,variable,1413,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:736,Safety,avoid,avoid,736,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:687,Security,access,access,687,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1522,Testability,stub,stub,1522,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1673,Testability,stub,stub,1673,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:1383,Usability,simpl,simple,1383,"//===- AMDGPURewriteOutArgumentsPass.cpp - Create struct returns ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass attempts to replace out argument usage with a return of a; /// struct.; ///; /// We can support returning a lot of values directly in registers, but; /// idiomatic C code frequently uses a pointer argument to return a second value; /// rather than returning a struct by value. GPU stack access is also quite; /// painful, so we want to avoid that if possible. Passing a stack object; /// pointer to a function also requires an additional address expansion code; /// sequence to convert the pointer to be relative to the kernel's scratch wave; /// offset register since the callee doesn't know what stack frame the incoming; /// pointer is relative to.; ///; /// The goal is to try rewriting code that looks like this:; ///; /// int foo(int a, int b, int* out) {; /// *out = bar();; /// return a + b;; /// }; ///; /// into something like this:; ///; /// std::pair<int, int> foo(int a, int b) {; /// return std::pair(a + b, bar());; /// }; ///; /// Typically the incoming pointer is a simple alloca for a temporary variable; /// to use the API, which if replaced with a struct return will be easily SROA'd; /// out when the stub function we create is inlined; ///; /// This pass introduces the struct return, but leaves the unused pointer; /// arguments and introduces a new stub function calling the struct returning; /// body. DeadArgumentElimination should be run after this to clean these up.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:109,Performance,optimiz,optimized,109,"// It is possible to see stores to the same argument multiple times,; // but we expect these would have been optimized out already.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:103,Testability,stub,stub,103,"// Move the body of the function into the new rewritten function, and replace; // this function with a stub.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp:25,Testability,stub,stub,25,// The function is now a stub we want to inline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteOutArguments.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:947,Deployability,pipeline,pipeline,947,"//===- AMDGPURewriteUndefForPHI.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This file implements the idea to rewrite undef incoming operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1615,Deployability,pipeline,pipeline,1615,"g operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef corre",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1691,Energy Efficiency,allocate,allocates,1691," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:1949,Energy Efficiency,allocate,allocated,1949," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:2309,Integrability,depend,depend,2309," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:412,Modifiability,rewrite,rewrite,412,"//===- AMDGPURewriteUndefForPHI.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This file implements the idea to rewrite undef incoming operand for certain; // PHIs in structurized CFG. This pass only works on IR that has gone through; // StructurizedCFG pass, and this pass has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:2410,Modifiability,rewrite,rewrite,2410," has some additional limitation that make; // it can only run after SIAnnotateControlFlow.; //; // To achieve optimal code generation for AMDGPU, we assume that uniformity; // analysis reports the PHI in join block of divergent branch as uniform if; // it has one unique uniform value plus additional undefined/poisoned incoming; // value. That is to say the later compiler pipeline will ensure such PHI always; // return uniform value and ensure it work correctly. Let's take a look at two; // typical patterns in structured CFG that need to be taken care: (In both; // patterns, block %if terminate with divergent branch.); //; // Pattern A: Block with undefined incoming value dominates defined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%undef, %if], [%uniform, %then]; //; // Pattern B: Block with defined incoming value dominates undefined predecessor; // %if; // | \; // | %then; // | /; // %endif: %phi = phi [%uniform, %if], [%undef, %then]; //; // For pattern A, by reporting %phi as uniform, the later pipeline need to make; // sure it be handled correctly. The backend usually allocates a scalar register; // and if any thread in a wave takes %then path, the scalar register will get; // the %uniform value.; //; // For pattern B, we will replace the undef operand with the other defined value; // in this pass. So the scalar register allocated for such PHI will get correct; // liveness. Without this transformation, the scalar register may be overwritten; // in the %then block.; //; // Limitation note:; // If the join block of divergent threads is a loop header, the pass cannot; // handle it correctly right now. For below case, the undef in %phi should also; // be rewritten. Currently we depend on SIAnnotateControlFlow to split %header; // block to get a separate join block, then we can rewrite the undef correctly.; // %if; // | \; // | %then; // | /; // -> %header: %phi = phi [%uniform, %if], [%undef, %then], [%uniform2, %header]; // | |; // \---",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:3,Deployability,Update,Update,3,// Update DominateBB if necessary.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp:194,Availability,avail,available,194,// We only replace the undef when DominateBB truly dominates all the; // other predecessors with undefined incoming value. Make sure DominateBB; // dominates BB so that UniqueDefinedIncoming is available in BB and; // afterwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURewriteUndefForPHI.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:64,Performance,load,load,64,"// Checks that for every predecessor Pred that can reach a VMEM load,; // none of Pred's successors can reach a VMEM load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:117,Performance,load,load,117,"// Checks that for every predecessor Pred that can reach a VMEM load,; // none of Pred's successors can reach a VMEM load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:13,Performance,load,loads,13,"// Find VMEM loads that may be executed before long-enough sequences of; // VALU instructions. We currently assume that backedges/loops, branch; // probabilities and other details can be ignored, so we essentially; // determine the largest number of VALU instructions along every; // possible path from the start of the function that may potentially be; // executed provided no backedge is ever taken.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:83,Performance,load,loads,83,// Lower the priority on edges where control leaves blocks from which; // the VMEM loads are reachable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:161,Usability,simpl,simplification,161,"// Where lowering the priority in predecessors is not possible, the; // block receiving control either was not part of a loop in the first; // place or the loop simplification/canonicalization pass should have; // already tried to split the edge and insert a preheader, and if for; // whatever reason it failed to do so, then this leaves us with the; // only option of lowering the priority within the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:128,Security,access,access,128,"// Targets must either support 64-bit offsets for MUBUF instructions, and/or; // support flat operations, otherwise they cannot access a 64-bit global; // address space",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:211,Security,access,access,211,"// Unless +-flat-for-global is specified, turn on FlatForGlobal for targets; // that do not support ADDR64 variants of MUBUF instructions. Such targets; // cannot use a 64 bit offset with a MUBUF instruction to access the global; // address space",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:128,Availability,avail,available,128,"// Unless +-flat-for-global is specified, use MUBUF instructions for global; // address space access if flat operations are not available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:94,Security,access,access,94,"// Unless +-flat-for-global is specified, use MUBUF instructions for global; // address space access if flat operations are not available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:55,Availability,down,down,55,// If reqd_work_group_size is present it narrows value down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:12,Energy Efficiency,allocate,allocate,12,"// We don't allocate the segment if we know the implicit arguments weren't; // used, even if the ABI implies we need them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:72,Performance,load,loads,72,// Being able to dereference past the end is useful for emitting scalar loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:34,Energy Efficiency,schedul,scheduler,34,// Track register pressure so the scheduler can try to decrease; // pressure once register usage is above the threshold defined by; // SIRegisterInfo::getRegPressureSetLimit(),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:21,Availability,down,down,21,// Enabling both top down and bottom up scheduling seems to give us less; // register spills than just using one of these approaches on its own.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:40,Energy Efficiency,schedul,scheduling,40,// Enabling both top down and bottom up scheduling seems to give us less; // register spills than just using one of these approaches on its own.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:56,Energy Efficiency,Schedul,Scheduler,56,// Enabling ShouldTrackLaneMasks crashes the SI Machine Scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:114,Security,access,access,114,// In principle we do not need to reserve SGPR pair used for flat_scratch if; // we know flat instructions do not access the stack anywhere in the; // program. For now assume it's needed if we have flat instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:145,Energy Efficiency,Schedul,ScheduleDAGInstrs,145,"// Work around the fact that SIInstrInfo::fixImplicitOperands modifies; // implicit operands which come from the MCInstrDesc, which can fool; // ScheduleDAGInstrs::addPhysRegDataDeps into treating them as implicit; // pseudo operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:78,Availability,avail,available,78,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:246,Energy Efficiency,power,power,246,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:252,Energy Efficiency,consumption,consumption,252,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:60,Integrability,depend,dependency,60,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:22,Performance,latency,latency,22,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:275,Performance,throttle,throttle,275,// Scan for MFMA long latency instructions and try to add a dependency; // of available SALU instructions to give them a chance to fill MFMA; // shadow. That is desirable to fill MFMA shadow with SALU instructions; // rather than VALU to prevent power consumption bursts and throttle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:97,Energy Efficiency,schedul,scheduled,97,// Find up to Lat independent scalar instructions as early as; // possible such that they can be scheduled after this MFMA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:69,Safety,detect,detect,69,// FIXME: Should have analysis or something rather than attribute to detect; // calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp:73,Safety,detect,detecting,73,// TODO: This could be refined a lot. The attribute is a poor way of; // detecting calls or stack objects that may require it before argument; // lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h:70,Performance,load,load,70,/// Creates value range metadata on an workitemid.* intrinsic call or load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:36,Testability,test,tests,36,// Option to disable vectorizer for tests.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:28,Performance,load,loads,28,// Option to control global loads scalarization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:20,Usability,simpl,simplifications,20,// Enable lib calls simplifications,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:24,Performance,optimiz,optimization,24,// Enable Mode register optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:25,Testability,test,tests,25,// Option is used in lit tests to prevent deadcoding of patterns inspected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:346,Security,access,accessed,346,"// 32-bit private, local, and region pointers. 64-bit global, constant and; // flat. 160-bit non-integral fat buffer pointers that include a 128-bit; // buffer descriptor and a 32-bit offset, which are indexed by 32-bit values; // (address space 7), and 128-bit non-integral buffer resourcees (address; // space 8) which cannot be non-trivilally accessed by LLVM memory operations; // like getelementptr.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:48,Deployability,pipeline,pipeline,48,// Add promote kernel arguments pass to the opt pipeline right before; // infer address spaces which is needed to do actual address space; // rewriting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:44,Deployability,pipeline,pipeline,44,// Add infer address spaces pass to the opt pipeline after inlining; // but before SROA to increase SROA opportunities.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:101,Performance,optimiz,optimizations,101,"// This should run after inlining to have any chance of doing; // anything, and before other cleanup optimizations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:32,Performance,load,loaded,32,// It must be a generic pointer loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:25,Performance,load,loaded,25,"// For a generic pointer loaded from the constant memory, it could be assumed; // as a global pointer since the constant memory is only populated on the; // host side. As implied by the offload programming model, only global; // pointers could be referenced on the host side.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:97,Testability,log,logic,97,// Check the global pointer predication based on; // (!is_share(p) && !is_private(p)). Note that logic 'and' is commutative and; // the order of 'is_shared' and 'is_private' is not significant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:86,Integrability,depend,depend,86,// This needs to be done before we create a new subtarget since any; // creation will depend on the TM and the code generation flags on the; // function that reside in TargetOptions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:19,Security,expose,exposes,19,// ReassociateGEPs exposes more opportunities for SLSR. See; // the example in reassociate-geps-and-slsr.ll.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:35,Availability,redundant,redundant,35,"// NaryReassociate on GEPs creates redundant common expressions, so run; // EarlyCSE after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:35,Safety,redund,redundant,35,"// NaryReassociate on GEPs creates redundant common expressions, so run; // EarlyCSE after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:63,Modifiability,variab,variables,63,// Replace OpenCL enqueued block function pointers with global variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:14,Performance,optimiz,optimizer,14,// Run atomic optimizer before Atomic Expand,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:91,Integrability,depend,depends,91,// TODO: Move this right after structurizeCFG to avoid extra divergence; // analysis. This depends on stopping SIAnnotateControlFlow from making; // control flow modifications.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:49,Safety,avoid,avoid,49,// TODO: Move this right after structurizeCFG to avoid extra divergence; // analysis. This depends on stopping SIAnnotateControlFlow from making; // control flow modifications.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:13,Energy Efficiency,schedul,scheduler,13,// Allow the scheduler to run before SIWholeQuadMode inserts exec manipulation; // instructions that cause scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:107,Energy Efficiency,schedul,scheduling,107,// Allow the scheduler to run before SIWholeQuadMode inserts exec manipulation; // instructions that cause scheduling barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:28,Performance,optimiz,optimization,28,"// This is not an essential optimization and it has a noticeable impact on; // compilation time, so we only enable it from O2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:210,Availability,failure,failure,210,"// FIXME: when an instruction has a Killed operand, and the instruction is; // inside a bundle, seems only the BUNDLE instruction appears as the Kills of; // the register in LiveVariables, this would trigger a failure in verifier,; // we should fix it and enable the verifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:10,Energy Efficiency,allocate,allocated,10,"// Commit allocated register changes. This is mostly necessary because too; // many things rely on the use lists of the physical registers, such as the; // verifier. This is only necessary with allocators which use LiveIntervals,; // since FastRegAlloc does the replacements itself.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:58,Energy Efficiency,schedul,scheduler,58,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:174,Energy Efficiency,schedul,scheduling,174,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:227,Energy Efficiency,schedul,scheduled,227,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:272,Energy Efficiency,schedul,schedule,272,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:7,Safety,hazard,hazard,7,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:113,Safety,hazard,hazards,113,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:393,Safety,hazard,hazard,393,"// The hazard recognizer that runs as part of the post-ra scheduler does not; // guarantee to be able handle all hazards correctly. This is because if there; // are multiple scheduling regions in a basic block, the regions are scheduled; // bottom up, so when we begin to schedule a region we don't know what; // instructions were emitted directly before it.; //; // Here we add a stand-alone hazard recognizer pass which can handle all; // cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:23,Integrability,depend,dependent,23,// Fixup the subtarget dependent default value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:32,Availability,mask,mask,32,// Check and apply the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:53,Integrability,Interface,Interface,53,"//===-- AMDGPUTargetMachine.h - AMDGPU TargetMachine Interface --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:420,Integrability,interface,interface,420,"//===-- AMDGPUTargetMachine.h - AMDGPU TargetMachine Interface --*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h:189,Performance,optimiz,optimization,189,/// Check if a pass is enabled given \p Opt option. The option always; /// overrides defaults if explicitly used. Otherwise its default will; /// be used given that a pass shall work at an optimization \p Level; /// minimum.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp:16,Security,access,access,16,// Set metadata access for the explicit section,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetObjectFile.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:71,Energy Efficiency,allocate,allocate,71,// If the amount of scratch memory to eliminate exceeds our ability to allocate; // it into registers we gain nothing by aggressively inlining functions for that; // heuristic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:75,Modifiability,variab,variable,75,"// Inhibit unroll for local memory if we have seen addressing not to; // a variable, most likely we will be unable to combine it.; // Do not unroll too deep inner loops for local memory to give a chance; // to unroll an outer loop for a more important reason.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:16,Integrability,depend,depends,16,// Check if GEP depends on a value defined by this loop itself.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:93,Integrability,depend,depend,93,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:120,Safety,safe,safely,120,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:74,Security,expose,exposed,74,"// The default assumption needs to be ecc is enabled, but no directly; // exposed operations depend on it, so it can be safely inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:358,Safety,avoid,avoid,358,"// NB: RCID is not an RCID. In fact it is 0 or 1 for scalar or vector; // registers. See getRegisterClassForType for the implementation.; // In this case vector registers are not vector in terms of; // VGPRs, but those which can hold multiple values.; // This is really the number of registers to fill when vectorizing /; // interleaving loops, so we lie to avoid trying to use all registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:106,Security,access,access,106,"// We allow vectorization of flat stores, even though we may need to decompose; // them later if they may access private memory. We don't have enough context; // here, and legalization can handle it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:57,Performance,load,loads,57,"// FIXME: Really we would like to issue multiple 128-bit loads and stores per; // iteration. Should we report a larger size and let it legalize?; //; // FIXME: Should we use narrower types for local/region, or account for when; // unaligned access is legal?; //; // FIXME: This could use fine tuning and microbenchmarks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:241,Security,access,access,241,"// FIXME: Really we would like to issue multiple 128-bit loads and stores per; // iteration. Should we report a larger size and let it legalize?; //; // FIXME: Should we use narrower types for local/region, or account for when; // unaligned access is legal?; //; // FIXME: This could use fine tuning and microbenchmarks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:177,Energy Efficiency,efficient,efficient,177,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:19,Security,access,access,19,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:102,Security,access,accesses,102,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:211,Security,access,accesses,211,"// A (multi-)dword access at an address == 2 (mod 4) will be decomposed by the; // hardware into byte accesses. If you assume all alignments are equally; // probable, it's more efficient on average to use short accesses for this; // case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:41,Security,access,accesses,41,"// Global memory works best with 16-byte accesses. Private memory will also; // hit this, although they'll be decomposed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:40,Integrability,contract,contract,40,// Estimate all types may be fused with contract/unsafe flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:49,Safety,unsafe,unsafe,49,// Estimate all types may be fused with contract/unsafe flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:35,Safety,unsafe,unsafe,35,"// TODO: This is more complicated, unsafe flags etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:8,Safety,unsafe,unsafe,8,// Fast unsafe fdiv lowering:; // f32 rcp; // f32 fmul,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:264,Safety,avoid,avoided,264,"// Extracts are just reads of a subregister, so are free. Inserts are; // considered free because we don't want to have any cost for scalarizing; // operations, and we don't have to copy into a different register class.; // Dynamic indexing isn't free and is best avoided.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:188,Performance,load,loads,188,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:238,Performance,load,loads,238,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:505,Energy Efficiency,power,power,505,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:443,Performance,perform,perform,443,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:456,Performance,optimiz,optimization,456,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:80,Availability,mask,masking,80,// All valid 64-bit to 32-bit casts work by chopping off the high; // bits. Any masking only clearing the low bits will also apply in the new; // address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:93,Usability,clear,clearing,93,// All valid 64-bit to 32-bit casts work by chopping off the high; // bits. Any masking only clearing the low bits will also apply in the new; // address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:45,Security,access,access,45,"// With op_sel VOP3P instructions freely can access the low half or high; // half of a register, so any swizzle is free.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:256,Integrability,depend,dependencies,256,// The cost of passing function arguments through the stack:; // 1 instruction to put a function argument on the stack in the caller.; // 1 instruction to take a function argument from the stack in callee.; // 1 instruction is explicitly take care of data dependencies in callee; // function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:85,Performance,optimiz,optimized,85,"// If we have a pointer to a private array passed into a function; // it will not be optimized out, leaving scratch usage.; // This function calculates the total size in bytes of the memory that would; // end in scratch if the call was not inlined.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:73,Performance,optimiz,optimized,73,"// Below the cutoff, assume that the private memory objects would be; // optimized",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:70,Integrability,depend,depending,70,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:110,Performance,optimiz,optimized,110,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:583,Testability,log,logic,583,"// Above the cutoff, we give a cost to each private memory object; // depending its size. If the array can be optimized by SROA this cost is not; // added to the total-cost in the inliner cost analysis.; //; // We choose the total cost of the alloca such that their sum cancels the; // bonus given in the threshold (ArgAllocaCost).; //; // Cost_Alloca_0 + ... + Cost_Alloca_N == ArgAllocaCost; //; // Awkwardly, the ArgAllocaCost bonus is multiplied by threshold-multiplier,; // the single-bb bonus and the vector-bonus.; //; // We compensate the first two multipliers, by repeating logic from the; // inliner-cost in here. The vector-bonus is 0 on AMDGPU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:11,Performance,load,load,11,// Maximum load or store can handle 8 dwords for scalar and 4 for; // vector ALU. Let's assume anything above 8 dwords is expensive; // even if legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h:9,Performance,cache,cache,9,/// Data cache line size for LoopDataPrefetch pass. Has no use before GFX12.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h:22,Performance,load,load,22,/// How much before a load we should place the prefetch instruction.; /// This is currently measured in number of IR instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp:98,Energy Efficiency,efficient,efficient,98,/// \returns true if \p BB is reachable through only uniform branches.; /// XXX - Is there a more efficient way to find this?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp:28,Usability,simpl,simplifycfg,28,// FIXME: add PDT here once simplifycfg is ready.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUUnifyDivergentExitNodes.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:109,Integrability,depend,dependencies,109,"//---------------------------------------------------------------------------//; // AMD Kernel Code, and its dependencies //; //---------------------------------------------------------------------------//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:31,Availability,mask,mask,31,// Sets val bits for specified mask in specified dst packed instance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:27,Availability,mask,mask,27,// Gets bits for specified mask from specified src packed instance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:126,Availability,mask,mask,126,"/// Every amd_*_code_t has the following properties, which are composed of; /// a number of bit fields. Every bit field has a mask (AMD_CODE_PROPERTY_*),; /// bit width (AMD_CODE_PROPERTY_*_WIDTH, and bit shift amount; /// (AMD_CODE_PROPERTY_*_SHIFT) for convenient access. Unused bits must be 0.; ///; /// (Note that bit fields cannot be used as their layout is; /// implementation defined in the C standard and so cannot be used to; /// specify an ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:266,Security,access,access,266,"/// Every amd_*_code_t has the following properties, which are composed of; /// a number of bit fields. Every bit field has a mask (AMD_CODE_PROPERTY_*),; /// bit width (AMD_CODE_PROPERTY_*_WIDTH, and bit shift amount; /// (AMD_CODE_PROPERTY_*_SHIFT) for convenient access. Unused bits must be 0.; ///; /// (Note that bit fields cannot be used as their layout is; /// implementation defined in the C standard and so cannot be used to; /// specify an ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1050,Modifiability,config,configures,1050,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:429,Performance,load,load,429,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:602,Performance,load,loaded,602,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:649,Performance,load,loads,649,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:805,Performance,load,load,805,"/// The interleave (swizzle) element size in bytes required by the; /// code for private memory. This must be 2, 4, 8 or 16. This value; /// is provided to the finalizer when it is invoked and is recorded; /// here. The hardware will interleave the memory requests of each; /// lane of a wavefront by this element size to ensure each; /// work-item gets a distinct memory location. Therefore, the; /// finalizer ensures that all load and store operations done to; /// private memory do not exceed this size. For example, if the; /// element size is 4 (32-bits or dword) and a 64-bit value must be; /// loaded, the finalizer will generate two 32-bit loads. This; /// ensures that the interleaving will get the work-item; /// specific dword for both halves of the 64-bit value. If it just; /// did a 64-bit load then it would get one dword which belonged to; /// its own work-item, but the second dword would belong to the; /// adjacent lane work-item since the interleaving is in dwords.; ///; /// The value used must match the value that the runtime configures; /// the GPU flat scratch (SH_STATIC_MEM_CONFIG.ELEMENT_SIZE). This; /// is generally DWORD.; ///; /// uSE VALUES FROM THE AMD_ELEMENT_BYTE_SIZE_T ENUM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:210,Security,access,accessed,210,/// This is a bit set indicating which control directives have been; /// specified. If the value is 0 then there are no control directives specified; /// and the rest of the fields can be ignored. The bits are accessed using the; /// hsa_ext_control_directives_present_mask_t. Any control directive that is not; /// enabled in this bit set must have the value of all 0s.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:246,Performance,perform,performance,246,"/// If enableBreakExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the BREAK; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. If the kernel being finalized; /// has any enablebreakexceptions control directives, then the values specified; /// by this argument are unioned with the values in these control; /// directives. If any of the functions the kernel calls have an; /// enablebreakexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:248,Performance,perform,performance,248,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:347,Performance,perform,performance,347,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:151,Safety,DETECT,DETECT,151,"/// If enableDetectExceptions is not enabled then must be 0, otherwise must be; /// non-0 and specifies the set of HSAIL exceptions that must have the DETECT; /// policy enabled. If this set is not empty then the generated code may have; /// lower performance than if the set is empty. However, an implementation; /// should endeavour to make the performance impact small. If the kernel being; /// finalized has any enabledetectexceptions control directives, then the; /// values specified by this argument are unioned with the values in these; /// control directives. If any of the functions the kernel calls have an; /// enabledetectexceptions control directive, then they must be equal or a; /// subset of, this union.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:110,Energy Efficiency,allocate,allocated,110,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:241,Energy Efficiency,allocate,allocated,241,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:983,Energy Efficiency,allocate,allocated,983,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:648,Modifiability,variab,variables,648,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:797,Safety,detect,detect,797,"/// If maxDynamicGroupSize is not enabled then must be 0, and any amount of; /// dynamic group segment can be allocated for a dispatch, otherwise the value; /// specifies the maximum number of bytes of dynamic group segment that can be; /// allocated for a dispatch. If the kernel being finalized has any; /// maxdynamicsize control directives, then the values must be the same, and; /// must be the same as this argument if it is enabled. This value can be used; /// by the finalizer to determine the maximum number of bytes of group memory; /// used by each work-group by adding this value to the group memory required; /// for all group segment variables used by the kernel and all functions it; /// calls, and group memory used to implement other HSAIL features such as; /// fbarriers and the detect exception operations. This can allow the finalizer; /// to determine the expected number of work-groups that can be executed by a; /// compute unit and allow more resources to be allocated to the work-items if; /// it is known that fewer work-groups can be executed due to group memory; /// limitations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:631,Energy Efficiency,allocate,allocated,631,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:740,Energy Efficiency,allocate,allocated,740,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:963,Energy Efficiency,allocate,allocate,963,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:790,Performance,perform,performance,790,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1027,Performance,perform,performance,1027,"/// If requestedWorkgroupsPerCu is not enabled then must be 0, and the; /// finalizer is free to generate ISA that may result in any number of; /// work-groups executing on a single compute unit. Otherwise, the finalizer; /// should attempt to generate ISA that will allow the specified number of; /// work-groups to execute on a single compute unit. This is only a hint and; /// can be ignored by the finalizer. If the kernel being finalized, or any of; /// the functions it calls, has a requested control directive, then the values; /// must be the same. This can be used to determine the number of resources; /// that should be allocated to a single work-group and work-item. For example,; /// a low value may allow more resources to be allocated, resulting in higher; /// per work-item performance, as it is known there will never be more than the; /// specified number of work-groups actually executing on the compute; /// unit. Conversely, a high value may allocate fewer resources, resulting in; /// lower per work-item performance, which is offset by the fact it allows more; /// work-groups to actually execute on the compute unit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:640,Performance,optimiz,optimize,640,"/// If requiredWorkgroupSize is not enabled then all elements for Dim3 must be; /// 0, and the produced code can be dispatched with any legal work-group range; /// consistent with the dispatch dimensions. Otherwise, the code produced must; /// always be dispatched with the specified work-group range. No element of the; /// specified range must be 0. It must be consistent with required_dimensions; /// and max_flat_workgroup_size. If the kernel being finalized, or any of the; /// functions it calls, has a requiredworkgroupsize control directive, then the; /// values must be the same. Specifying a value can allow the finalizer to; /// optimize work-group id operations, and if the number of work-items in the; /// work-group is less than the WAVESIZE then barrier operations can be; /// optimized to just a memory fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:792,Performance,optimiz,optimized,792,"/// If requiredWorkgroupSize is not enabled then all elements for Dim3 must be; /// 0, and the produced code can be dispatched with any legal work-group range; /// consistent with the dispatch dimensions. Otherwise, the code produced must; /// always be dispatched with the specified work-group range. No element of the; /// specified range must be 0. It must be consistent with required_dimensions; /// and max_flat_workgroup_size. If the kernel being finalized, or any of the; /// functions it calls, has a requiredworkgroupsize control directive, then the; /// values must be the same. Specifying a value can allow the finalizer to; /// optimize work-group id operations, and if the number of work-items in the; /// work-group is less than the WAVESIZE then barrier operations can be; /// optimized to just a memory fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:445,Performance,optimiz,optimize,445,"/// If requiredDim is not enabled then must be 0 and the produced kernel code; /// can be dispatched with 1, 2 or 3 dimensions. If enabled then the value is; /// 1..3 and the code produced must only be dispatched with a dimension that; /// matches. Other values are illegal. If the kernel being finalized, or any of; /// the functions it calls, has a requireddimsize control directive, then the; /// values must be the same. This can be used to optimize the code generated to; /// compute the absolute and flat work-group and work-item id, and the dim; /// HSAIL operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4521,Energy Efficiency,allocate,allocated,4521,"):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((grid",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1836,Performance,Queue,Queue,1836,"n SGPR; /// number.; ///; /// The initial SGPRs comprise up to 16 User SRGPs that are set up by CP and; /// apply to all waves of the grid. It is possible to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1921,Performance,Cache,Cache,1921,"e to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2818,Performance,Queue,Queue,2818,".; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the ke",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2939,Performance,queue,queue,2939,"?; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:2978,Performance,queue,queued,2978,"?; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instr",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3213,Performance,load,load,3213,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3233,Performance,load,loading,3233,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4061,Performance,load,loaded,4061,ss of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the ,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4354,Performance,load,load,4354," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4374,Performance,load,loading,4374," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4942,Performance,load,load,4942,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4962,Performance,load,loading,4962,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:10066,Performance,load,loads,10066,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:3226,Safety,avoid,avoids,3226,"kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add tid enable: 1; /// - ATC: from SH_MEM_CONFIG.PRIVATE_ATC,; /// - Hash_enable: ???; /// - Heap: ???; /// - Mtype: from SH_STATIC_MEM_CONFIG.PRIVATE_MTYPE; /// - Type: 0 (a buffer) (???); ///; /// Dispatch Ptr (enable_sgpr_dispatch_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AQL dispatch packet; /// for kernel actually executing.; ///; /// Queue Ptr (enable_sgpr_queue_ptr):; /// Number of User SGPR registers: 2. 64 bit address of AmdQueue object for; /// AQL queue on which the dispatch packet was queued.; ///; /// Kernarg Segment Ptr (enable_sgpr_kernarg_segment_ptr):; /// Number of User SGPR registers: 2. 64 bit address of Kernarg segment. This; /// is directly copied from the kernargPtr in the dispatch packet. Having CP; /// load it once avoids loading it at the beginning of every wavefront.; ///; /// Dispatch Id (enable_sgpr_dispatch_id):; /// Number of User SGPR registers: 2. 64 bit Dispatch ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4367,Safety,avoid,avoids,4367," ID of the dispatch; /// packet being executed.; ///; /// Flat Scratch Init (enable_sgpr_flat_scratch_init):; /// Number of User SGPR registers: 2. This is 2 SGPRs.; ///; /// For CI/VI:; /// The first SGPR is a 32 bit byte offset from SH_MEM_HIDDEN_PRIVATE_BASE; /// to base of memory for scratch for this dispatch. This is the same offset; /// used in computing the Scratch Segment Buffer base address. The value of; /// Scratch Wave Offset must be added by the kernel code and moved to; /// SGPRn-4 for use as the FLAT SCRATCH BASE in flat memory instructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X di",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:4955,Safety,avoid,avoids,4955,"nstructions.; ///; /// The second SGPR is 32 bit byte size of a single work-item's scratch; /// memory usage. This is directly loaded from the dispatch packet Private; /// Segment Byte Size and rounded up to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// The kernel code must move to SGPRn-3 for use as the FLAT SCRATCH SIZE in; /// flat memory instructions. Having CP load it once avoids loading it at; /// the beginning of every wavefront.; ///; /// For PI:; /// This is the 64 bit base address of the scratch backing memory for; /// allocated by CP for this dispatch.; ///; /// Private Segment Size (enable_sgpr_private_segment_size):; /// Number of User SGPR registers: 1. The 32 bit byte size of a single; /// work-item's scratch memory allocation. This is the value from the dispatch; /// packet. Private Segment Byte Size rounded up by CP to a multiple of DWORD.; ///; /// \todo [Does CP need to round this to >4 byte alignment?]; ///; /// Having CP load it once avoids loading it at the beginning of every; /// wavefront.; ///; /// \todo [This will not be used for CI/VI since it is the same value as; /// the second SGPR of Flat Scratch Init. However, it is need for PI which; /// changes meaning of Flat Scratchg Init..]; ///; /// Grid Work-Group Count X (enable_sgpr_grid_workgroup_count_x):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the X dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.x+workgroupSize.x-1)/workgroupSize.x).; ///; /// Grid Work-Group Count Y (enable_sgpr_grid_workgroup_count_y):; /// Number of User SGPR registers: 1. 32 bit count of the number of; /// work-groups in the Y dimension for the grid being executed. Computed from; /// the fields in the HsaDispatchPacket as; /// ((gridSize.y+workgroupSize.y-1)/workgroupSize.y).; ///; /// Only initialized if <16 previous SGPRs initialized.; ///; /// Grid Wor",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:8803,Safety,avoid,avoid,8803,"led register is VGPR1 etc.; disabled registers do not have an VGPR; /// number.; ///; /// VGPR register initial state is defined as follows:; ///; /// Work-Item Id X (always initialized):; /// Number of registers: 1. 32 bit work item id in X dimension of work-group; /// for wavefront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Y dimension of work-group; /// for wavefront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - AT",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:1465,Security,access,access,1465," /// which ones are actually setup in the amd_kernel_code_t object using the; /// enable_sgpr_* bit fields. The register numbers used for enabled registers; /// are dense starting at SGPR0: the first enabled register is SGPR0, the next; /// enabled register is SGPR1 etc.; disabled registers do not have an SGPR; /// number.; ///; /// The initial SGPRs comprise up to 16 User SRGPs that are set up by CP and; /// apply to all waves of the grid. It is possible to specify more than 16 User; /// SGPRs using the enable_sgpr_* bit fields, in which case only the first 16; /// are actually initialized. These are then immediately followed by the System; /// SGPRs that are set up by ADC/SPI and can have different values for each wave; /// of the grid dispatch.; ///; /// SGPR register initial state is defined as follows:; ///; /// Private Segment Buffer (enable_sgpr_private_segment_buffer):; /// Number of User SGPR registers: 4. V# that can be used, together with; /// Scratch Wave Offset as an offset, to access the Private/Spill/Arg; /// segments using a segment address. It must be set as follows:; /// - Base address: of the scratch memory area used by the dispatch. It; /// does not include the scratch wave offset. It will be the per process; /// SH_HIDDEN_PRIVATE_BASE_VMID plus any offset from this dispatch (for; /// example there may be a per pipe offset, or per AQL Queue offset).; /// - Stride + data_format: Element Size * Index Stride (???); /// - Cache swizzle: ???; /// - Swizzle enable: SH_STATIC_MEM_CONFIG.SWIZZLE_ENABLE (must be 1 for; /// scratch); /// - Num records: Flat Scratch Work Item Size / Element Size (???); /// - Dst_sel_*: ???; /// - Num_format: ???; /// - Element_size: SH_STATIC_MEM_CONFIG.ELEMENT_SIZE (will be DWORD, must; /// agree with amd_kernel_code_t.privateElementSize); /// - Index_stride: SH_STATIC_MEM_CONFIG.INDEX_STRIDE (will be 64 as must; /// be number of wavefront lanes for scratch, must agree with; /// amd_kernel_code_t.wavefrontSize); /// - Add t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9237,Security,access,accessed,9237,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9368,Security,access,access,9368,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:9916,Security,access,access,9916,"ront lane.; ///; /// Work-Item Id X (enable_vgpr_workitem_id > 0):; /// Number of registers: 1. 32 bit work item id in Z dimension of work-group; /// for wavefront lane.; ///; ///; /// The setting of registers is being done by existing GPU hardware as follows:; /// 1) SGPRs before the Work-Group Ids are set by CP using the 16 User Data; /// registers.; /// 2) Work-group Id registers X, Y, Z are set by SPI which supports any; /// combination including none.; /// 3) Scratch Wave Offset is also set by SPI which is why its value cannot; /// be added into the value Flat Scratch Offset which would avoid the; /// Finalizer generated prolog having to do the add.; /// 4) The VGPRs are set by SPI which only supports specifying either (X),; /// (X, Y) or (X, Y, Z).; ///; /// Flat Scratch Dispatch Offset and Flat Scratch Size are adjacent SGRRs so; /// they can be moved as a 64 bit value to the hardware required SGPRn-3 and; /// SGPRn-4 respectively using the Finalizer ?FLAT_SCRATCH? Register.; ///; /// The global segment can be accessed either using flat operations or buffer; /// operations. If buffer operations are used then the Global Buffer used to; /// access HSAIL Global/Readonly/Kernarg (which are combine) segments using a; /// segment address is not passed into the kernel code by CP since its base; /// address is always 0. Instead the Finalizer generates prolog code to; /// initialize 4 SGPRs with a V# that has the following properties, and then; /// uses that in the buffer instructions:; /// - base address of 0; /// - no swizzle; /// - ATC=1; /// - MTYPE set to support memory coherence specified in; /// amd_kernel_code_t.globalMemoryCoherence; ///; /// When the Global Buffer is used to access the Kernarg segment, must add the; /// dispatch packet kernArgPtr to a kernarg segment address before using this V#.; /// Alternatively scalar loads can be used if the kernarg offset is uniform, as; /// the kernarg segment is constant for the duration of the kernel execution.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:444,Energy Efficiency,allocate,allocate,444,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:228,Performance,cache,cache,228,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:403,Performance,cache,cache,403,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:607,Performance,cache,cache,607,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:613,Performance,perform,performance,613,"/// Byte offset (possibly negative) from start of amd_kernel_code_t; /// object to kernel's entry point instruction. The actual code for; /// the kernel is required to be 256 byte aligned to match hardware; /// requirements (SQ cache line is 16). The code must be position; /// independent code (PIC) for AMD devices to give runtime the; /// option of copying code to discrete GPU memory or APU L2; /// cache. The Finalizer should endeavour to allocate all kernel; /// machine code in contiguous memory pages so that a device; /// pre-fetcher will tend to only pre-fetch Kernel Code objects,; /// improving cache performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:213,Availability,avail,available,213,/// Range of bytes to consider prefetching expressed as an offset; /// and size. The offset is from the start (possibly negative) of; /// amd_kernel_code_t object. Set both to 0 if no prefetch; /// information is available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:117,Energy Efficiency,allocate,allocated,117,/// The amount of group segment memory required by a work-group in; /// bytes. This does not include any dynamically allocated group; /// segment memory that may be added when the kernel is; /// dispatched.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:120,Energy Efficiency,allocate,allocate,120,/// Number of fbarrier's used in the kernel and all functions it; /// calls. If the implementation uses group memory to allocate the; /// fbarriers then that amount must already be included in the; /// workgroup_group_segment_byte_size total.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:116,Energy Efficiency,power,power,116,/// The maximum byte alignment of variables used by the kernel in; /// the specified memory segment. Expressed as a power of two. Must; /// be at least HSA_POWERTWO_16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:34,Modifiability,variab,variables,34,/// The maximum byte alignment of variables used by the kernel in; /// the specified memory segment. Expressed as a power of two. Must; /// be at least HSA_POWERTWO_16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:34,Energy Efficiency,power,power,34,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:58,Energy Efficiency,power,power,58,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h:205,Energy Efficiency,allocate,allocated,205,"/// Wavefront size expressed as a power of two. Must be a power of 2; /// in range 1..64 inclusive. Used to support runtime query that; /// obtains wavefront size, which may be used by application to; /// allocated dynamic group memory and set the dispatch work-group; /// size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDKernelCodeT.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp:605,Energy Efficiency,schedul,scheduler,605,"//===- GCNCreateVOPD.cpp - Create VOPD Instructions ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Combine VALU pairs into VOPD instructions; /// Only works on wave32; /// Has register requirements, we reject creating VOPD if the requirements are; /// not met.; /// shouldCombineVOPD mutator in postRA machine scheduler puts candidate; /// instructions for VOPD back-to-back; ///; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNCreateVOPD.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:31,Performance,optimiz,optimization,31,"//=======- GCNDPPCombine.cpp - optimization for DPP instructions ---==========//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // The pass combines V_MOV_B32_dpp instruction with its VALU uses as a DPP src0; // operand. If any of the use instruction cannot be combined with the mov the; // whole sequence is reverted.; //; // $old = ...; // $dpp_value = V_MOV_B32_dpp $old, $vgpr_to_be_read_from_other_lane,; // dpp_controls..., $row_mask, $bank_mask, $bound_ctrl; // $res = VALU $dpp_value [, src1]; //; // to; //; // $res = VALU_DPP $combined_old, $vgpr_to_be_read_from_other_lane, [src1,]; // dpp_controls..., $row_mask, $bank_mask, $combined_bound_ctrl; //; // Combining rules :; //; // if $row_mask and $bank_mask are fully enabled (0xF) and; // $bound_ctrl==DPP_BOUND_ZERO or $old==0; // -> $combined_old = undef,; // $combined_bound_ctrl = DPP_BOUND_ZERO; //; // if the VALU op is binary and; // $bound_ctrl==DPP_BOUND_OFF and; // $old==identity value (immediate) for the VALU op; // -> $combined_old = src1,; // $combined_bound_ctrl = DPP_BOUND_OFF; //; // Otherwise cancel.; //; // The mov_dpp instruction should reside in the same BB as all its uses; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:47,Safety,avoid,avoid,47,// Do not shrink True16 instructions pre-RA to avoid the restriction in; // register allocation from only being able to use 128 VGPRs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:22,Availability,Mask,Mask,22,"// Prior checks cover Mask with VOPC condition, but not on purpose",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:3,Security,Validat,Validate,3,// Validate OP_SEL has to be set to all 0 and OP_SEL_HI has to be set to; // all 1.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp:96,Testability,assert,assert,96,// OldOpndValue is either undef (IMPLICIT_DEF) or immediate or something else; // We could use: assert(!OldOpndValue || OldOpndValue->isImm()); // but the third option is used to distinguish undef from non-immediate; // to reuse IMPLICIT_DEF instruction later,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNDPPCombine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:427,Energy Efficiency,schedul,scheduling,427,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:39,Safety,Hazard,Hazard,39,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:404,Safety,hazard,hazard,404,"//===-- GCNHazardRecognizers.cpp - GCN Hazard Recognizer Impls ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file implements hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:85,Safety,Hazard,Hazard,85,//===----------------------------------------------------------------------===//; // Hazard Recognizer Implementation; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:84,Energy Efficiency,schedul,scheduler,84,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:21,Safety,Hazard,HazardRecognizerMode,21,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:122,Safety,hazard,hazards,122,"// If we are not in ""HazardRecognizerMode"" and therefore not being run from; // the scheduler, track possible stalls from hazards but don't insert noops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:36,Safety,hazard,hazards,36,// Check bundled MachineInstr's for hazards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:12,Energy Efficiency,schedul,scheduler,12,"// When the scheduler detects a stall, it will call AdvanceCycle() without; // emitting any instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:22,Safety,detect,detects,22,"// When the scheduler detects a stall, it will call AdvanceCycle() without; // emitting any instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:16,Safety,hazard,hazard,16,// Search for a hazard in a block and its predecessors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:151,Safety,hazard,hazard,151,// Returns a minimum wait states since \p I walking all predecessors.; // Only scans until \p IsExpired does not return true.; // Can only be run in a hazard recognizer mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:91,Safety,Hazard,Hazard,91,//===----------------------------------------------------------------------===//; // No-op Hazard Detection; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:98,Safety,Detect,Detection,98,//===----------------------------------------------------------------------===//; // No-op Hazard Detection; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:35,Performance,load,loads,35,"// We need to make sure not to put loads and stores in the same clause if they; // use the same address. For now, just start a new clause whenever we see a; // store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:109,Safety,hazard,hazard,109,"// If the set of defs and uses intersect then we cannot add this instruction; // to the clause, so we have a hazard.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Safety,hazard,hazard,13,// This SMRD hazard only affects SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:33,Usability,undo,undocumented,33,"// This fixes what appears to be undocumented hardware behavior in SI where; // s_mov writing a descriptor and s_buffer_load_dword reading the descriptor; // needs some number of nops in between. We don't know how many we need, but; // let's use 4. This wasn't discovered before probably because the only; // case when this happens is when we expand a 64-bit pointer into a full; // descriptor and use s_buffer_load_dword instead of s_load_dword, which was; // probably never encountered in the closed-source land.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:15,Safety,hazard,hazard,15,// There is no hazard if the instruction does not use vector regs; // (like wbinvl1),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:37,Safety,hazard,hazard,37,// For MUBUF/MTBUF instructions this hazard only exists if the; // instruction is not using a register in the soffset field.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:30,Safety,hazard,hazard,30,"// MIMG instructions create a hazard if they don't use a 256-bit T# and; // the store size is greater than 8 bytes and they have more than two bits; // of their dmask set.; // All our MIMG definitions use a 256-bit T#, so we can skip checking for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:27,Safety,hazard,hazard,27,// Helper to check for the hazard where VMEM instructions that store more than; // 8 bytes can have there store data over written by the next instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:23,Safety,hazard,hazard,23,// This checks for the hazard where VMEM instructions that store more than; // 8 bytes can have there store data over written by the next instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:173,Integrability,rout,routines,173,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:19,Safety,hazard,hazards,19,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:166,Safety,Hazard,Hazard,166,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:266,Safety,hazard,hazards,266,"// This checks for hazards associated with inline asm statements.; // Since inline asms can contain just about anything, we use this; // to call/leverage other check*Hazard routines. Note that; // this function doesn't attempt to address all possible inline asm; // hazards (good luck), but is a collection of what has been; // problematic thus far.; // see checkVALUHazards()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:77,Availability,alive,alive,77,"// V_NOP will be discarded by SQ.; // Use V_MOV_B32 v?, v?. Register must be alive so use src0 of V_PERMLANE*; // which is always a VGPR and available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:141,Availability,avail,available,141,"// V_NOP will be discarded by SQ.; // Use V_MOV_B32 v?, v?. Register must be alive so use src0 of V_PERMLANE*; // which is always a VGPR and available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// These instructions cannot not mitigate the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:52,Safety,hazard,hazard,52,// Reducing lgkmcnt count to 0 always mitigates the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:41,Safety,hazard,hazard,41,// SOPP instructions cannot mitigate the hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:169,Integrability,depend,dependent,169,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:57,Safety,hazard,hazard,57,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:120,Safety,risk,risk,120,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:278,Safety,risk,risk,278,"// At this point the SALU can be assumed to mitigate the hazard; // because either:; // (a) it is independent of the at risk SMEM (breaking chain),; // or; // (b) it is dependent on the SMEM, in which case an appropriate; // s_waitcnt lgkmcnt _must_ exist between it and the at risk; // SMEM instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:44,Safety,hazard,hazard,44,// Check if the necessary condition for the hazard is met: both LDS and VMEM; // instructions need to appear in the same function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:22,Safety,hazard,hazard,22,// TODO: On GFX12 the hazard should expire on S_WAIT_LOADCNT/SAMPLECNT/BVHCNT; // according to the type of VMEM instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:53,Safety,detect,detection,53,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:25,Testability,test,testing,25,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:53,Safety,detect,detection,53,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:25,Testability,test,testing,25,// This overloads expiry testing with all the hazard detection,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:46,Safety,hazard,hazard,46,// Instructions which cause va_vdst==0 expire hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:3,Safety,Hazard,Hazard,3,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:66,Safety,hazard,hazard,66,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:80,Safety,avoid,avoided,80,// Hazard is observed - insert a wait on va_dst counter to ensure hazard is; // avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:26,Safety,hazard,hazard,26,// Exception: there is no hazard if the wmma instructions are of the same; // type and there is no input modifier on src2 of the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:44,Safety,hazard,hazard,44,// Insert V_SWAP_B32 instruction(s) and run hazard recognizer on them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:102,Safety,hazard,hazard,102,// Instructions emitted after the current instruction will be processed by the; // parent loop of the hazard recognizer in a natural way.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:14,Safety,hazard,hazard,14,"// Re-running hazard recognizer on the modified instruction is not necessary,; // inserted V_SWAP_B32 has already both read and write new registers so; // hazards related to these register has already been handled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:155,Safety,hazard,hazards,155,"// Re-running hazard recognizer on the modified instruction is not necessary,; // inserted V_SWAP_B32 has already both read and write new registers so; // hazards related to these register has already been handled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Deployability,update,update,13,"// We do not update liveness, so verifier may see it as undef.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:57,Performance,perform,performance,57,// Pad neighboring MFMA with noops for better inter-wave performance.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:23,Safety,hazard,hazards,23,// On gfx90a+ relevant hazards are checked in checkMAIVALUHazards(),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:8,Safety,hazard,hazard,8,// Only hazard if register is defined by a VALU and a DGEMM is found after; // after the def.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:26,Safety,hazard,hazard,26,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:247,Safety,avoid,avoid,247,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:258,Safety,hazard,hazard,258,// Workaround for HW data hazard bug observed only in GFX90A. When there; // is a DGEMM instruction in-between a VALU and a VMEM instruction it; // causes the SQ to incorrectly not insert two wait states between the two; // instructions needed to avoid data hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:72,Availability,mask,mask,72,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:7,Safety,hazard,hazard,7,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:132,Safety,hazard,hazard,132,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:278,Safety,hazard,hazard,278,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:318,Safety,avoid,avoid,318,"// The hazard sequence is three instructions:; // 1. VALU reads SGPR as mask; // 2. SALU writes SGPR; // 3. SALU reads SGPR; // The hazard can expire if the distance between 2 and 3 is sufficient.; // In practice this happens <10% of the time, hence this always assumes; // the hazard exists if 1 and 2 are present to avoid searching.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:32,Availability,mask,mask,32,// These implicitly read VCC as mask source.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:14,Availability,mask,mask,14,// Only check mask register overlaps.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:41,Safety,hazard,hazard,41,// s_waitcnt_depctr sa_sdst(0) mitigates hazard.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:58,Safety,Hazard,HazardReg,58,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:82,Safety,hazard,hazard,82,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:107,Safety,Hazard,HazardReg,107,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:8,Security,access,access,8,// VALU access to any SGPR or literal constant other than HazardReg; // mitigates hazard. No need to check HazardReg here as this will; // only be called when !IsHazardFn.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:13,Safety,hazard,hazard,13,// Check for hazard,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp:3,Deployability,Update,Update,3,// Update offsets of any references in the bundle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:424,Energy Efficiency,schedul,scheduling,424,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:37,Safety,Hazard,Hazard,37,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:401,Safety,hazard,hazard,401,"//===-- GCNHazardRecognizers.h - GCN Hazard Recognizers ---------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling on GCN processors.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:37,Energy Efficiency,schedul,scheduler,37,// Distinguish if we are called from scheduler or hazard recognizer,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:50,Safety,hazard,hazard,50,// Distinguish if we are called from scheduler or hazard recognizer,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:8,Modifiability,variab,variable,8,"// This variable stores the instruction that has been emitted this cycle. It; // will be added to EmittedInstrs, when AdvanceCycle() or RecedeCycle() is; // called.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:48,Safety,hazard,hazards,48,// Advance over a MachineInstr bundle. Look for hazards in the bundled; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:39,Safety,hazard,hazard,39,// Run on an individual instruction in hazard recognizer mode. This can be; // used on a newly inserted instruction before returning from PreEmitNoops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:12,Performance,latency,latency,12,"/// Pad the latency between neighboring MFMA instructions with s_nops. The; /// percentage of wait states to fill with s_nops is specified by the command; /// line option '-amdgpu-mfma-padding-ratio'.; ///; /// For example, with '-amdgpu-mfma-padding-ratio=100':; ///; /// 2 pass MFMA instructions have a latency of 2 wait states. Therefore, a; /// 'S_NOP 1' will be added between sequential MFMA instructions.; ///; /// V_MFMA_F32_4X4X1F32; /// V_MFMA_F32_4X4X1F32; ///-->; /// V_MFMA_F32_4X4X1F32; /// S_NOP 1; /// V_MFMA_F32_4X4X1F32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h:305,Performance,latency,latency,305,"/// Pad the latency between neighboring MFMA instructions with s_nops. The; /// percentage of wait states to fill with s_nops is specified by the command; /// line option '-amdgpu-mfma-padding-ratio'.; ///; /// For example, with '-amdgpu-mfma-padding-ratio=100':; ///; /// 2 pass MFMA instructions have a latency of 2 wait states. Therefore, a; /// 'S_NOP 1' will be added between sequential MFMA instructions.; ///; /// V_MFMA_F32_4X4X1F32; /// V_MFMA_F32_4X4X1F32; ///-->; /// V_MFMA_F32_4X4X1F32; /// S_NOP 1; /// V_MFMA_F32_4X4X1F32",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:27,Energy Efficiency,schedul,scheduler,27,/// CurCycle - The current scheduler state corresponds to this cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:41,Availability,down,down,41,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:24,Energy Efficiency,schedul,schedule,24,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:61,Energy Efficiency,schedul,scheduling,61,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:100,Energy Efficiency,schedul,scheduled,100,"// Lower priority means schedule further down. For bottom-up scheduling, lower; // priority SUs are scheduled before higher priority SUs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:212,Energy Efficiency,schedul,scheduled,212,"// If SU does not have a register use, i.e. it doesn't produce a value; // that would be consumed (e.g. store), then it terminates a chain of; // computation. Give it a large SethiUllman number so it will be; // scheduled right before its predecessors that it doesn't lengthen; // their live ranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:39,Energy Efficiency,schedul,schedule,39,"// If SU does not have a register def, schedule it close to its uses; // because it does not lengthen any live ranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:30,Energy Efficiency,schedul,scheduled,30,/// closestSucc - Returns the scheduled cycle of the successor which is; /// closest to the current cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:126,Integrability,depend,dependencies,126,"/// calcMaxScratches - Returns an cost estimate of the worse case requirement; /// for scratch registers, i.e. number of data dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:90,Performance,latency,latency-based,90,"// Return -1 if left has higher priority, 1 if right has higher priority.; // Return 0 if latency-based priority is equivalent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:3,Energy Efficiency,Schedul,Scheduling,3,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:87,Energy Efficiency,schedul,scheduled,87,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:149,Performance,latency,latency,149,// Scheduling an instruction that uses a VReg whose postincrement has not yet; // been scheduled will induce a copy. Model this as an extra cycle of latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:21,Energy Efficiency,schedul,scheduling,21,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:36,Performance,latency,latency,36,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:79,Performance,latency,latency,79,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:147,Safety,Hazard,HazardRecognizer,147,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:62,Availability,down,down,62,// Prioritize by Sethi-Ulmann number and push CopyToReg nodes down.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:7,Energy Efficiency,schedul,schedule,7,"// Try schedule def + use closer when Sethi-Ullman numbers are the same.; // e.g.; // t1 = op t2, c1; // t3 = op t4, c2; //; // and the following instructions are both ready.; // t2 = op c3; // t4 = op c4; //; // Then schedule t2 = op first.; // i.e.; // t4 = op c4; // t2 = op c3; // t1 = op t2, c1; // t3 = op t4, c2; //; // This creates more short live intervals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:218,Energy Efficiency,schedul,schedule,218,"// Try schedule def + use closer when Sethi-Ullman numbers are the same.; // e.g.; // t1 = op t2, c1; // t3 = op t4, c2; //; // and the following instructions are both ready.; // t2 = op c3; // t4 = op c4; //; // Then schedule t2 = op first.; // i.e.; // t4 = op c4; // t2 = op c3; // t1 = op t2, c1; // t3 = op t4, c2; //; // This creates more short live intervals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:52,Energy Efficiency,schedul,scheduled,52,// How many registers becomes live when the node is scheduled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:98,Availability,avail,available,98,"// Check to see if any of the pending instructions are ready to issue. If; // so, add them to the available queue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:108,Performance,queue,queue,108,"// Check to see if any of the pending instructions are ready to issue. If; // so, add them to the available queue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:13,Energy Efficiency,schedul,scheduler,13,/// Move the scheduler state forward by the specified number of Cycles.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:8,Security,access,accessors,8,// shim accessors for different order containers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:2,Testability,assert,assert,2,//assert(Rgn.End == Sch.RegionEnd);,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:95,Energy Efficiency,schedul,schedule,95,// DAG SUnits are stored using original region's order; // so just use SUnits as the restoring schedule,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:10,Testability,stub,stub,10,// just a stub to make base class happy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:3,Energy Efficiency,schedul,scheduleRegions,3,"// scheduleRegions walks bottom to top, so its likely we just get next; // instruction to track",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:40,Energy Efficiency,schedul,schedule,40,// returns max pressure for a tentative schedule,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:56,Energy Efficiency,schedul,schedule,56,// R.End points to the boundary instruction but the; // schedule doesn't include it,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:10,Energy Efficiency,schedul,schedule,10,// Detach schedule from SUnits and interleave it with debug values.; // Returned schedule becomes independent of DAG state.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:81,Energy Efficiency,schedul,schedule,81,// Detach schedule from SUnits and interleave it with debug values.; // Returned schedule becomes independent of DAG state.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:27,Energy Efficiency,schedul,scheduler,27,"// minimal required region scheduler, works for ranges of SUnits*,; // SUnits or MachineIntrs*",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:32,Deployability,update,update,32,// Reset read - undef flags and update them later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule consisting of MachineInstr* is considered 'detached'; // and already interleaved with debug values,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:78,Testability,assert,assert,78,"// Unfortunately placeDebugValues incorrectly modifies RegionEnd, restore; // assert(R.End == RegionEnd);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:161,Energy Efficiency,schedul,scheduler,161,///////////////////////////////////////////////////////////////////////////////; // Legacy MaxOccupancy Strategy; // Tries to increase occupancy applying minreg scheduler for a sequence of; // most demanding regions. Obtained schedules are saved as BestSchedule for a; // region.; // TargetOcc is the best achievable occupancy for a kernel.; // Returns better occupancy on success or current occupancy on fail.; // BestSchedules aren't deleted on fail.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:226,Energy Efficiency,schedul,schedules,226,///////////////////////////////////////////////////////////////////////////////; // Legacy MaxOccupancy Strategy; // Tries to increase occupancy applying minreg scheduler for a sequence of; // most demanding regions. Obtained schedules are saved as BestSchedule for a; // region.; // TargetOcc is the best achievable occupancy for a kernel.; // Returns better occupancy on success or current occupancy on fail.; // BestSchedules aren't deleted on fail.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:9,Testability,assert,assert,9,// TODO: assert Regions are sorted descending by pressure,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:43,Energy Efficiency,schedul,scheduling,43,// This is really weird but for some magic scheduling regions twice; // gives performance improvement,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:78,Performance,perform,performance,78,// This is really weird but for some magic scheduling regions twice; // gives performance improvement,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:63,Energy Efficiency,schedul,scheduling,63,// running first pass with TargetOccupancy = 0 mimics previous scheduling; // approach and is a performance magic,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:96,Performance,perform,performance,96,// running first pass with TargetOccupancy = 0 mimics previous scheduling; // approach and is a performance magic,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp:88,Energy Efficiency,schedul,scheduler,88,///////////////////////////////////////////////////////////////////////////////; // ILP scheduler port,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:37,Energy Efficiency,Schedul,Scheduler,37,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:500,Energy Efficiency,schedul,schedule,500,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:582,Energy Efficiency,schedul,schedules,582,"//===- GCNIterativeScheduler.h - GCN Scheduler ------------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines the class GCNIterativeScheduler, which uses an iterative; /// approach to find a best schedule for GCN architecture. It basically makes; /// use of various lightweight schedules, scores them, chooses best one based on; /// their scores, and finally implements the chosen one.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:141,Energy Efficiency,schedul,schedule,141,// Fields except for BestSchedule are supposed to reflect current IR state; // `const` fields are to emphasize they shouldn't change for any schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:8,Energy Efficiency,schedul,schedule,8,// best schedule for the region so far (not scheduled yet),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h:44,Energy Efficiency,schedul,scheduled,44,// best schedule for the region so far (not scheduled yet),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNIterativeScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:505,Energy Efficiency,schedul,scheduler,505,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:498,Usability,simpl,simple,498,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:537,Usability,learn,learn,537,"//===- GCNMinRegStrategy.cpp ----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; ///; /// \file; /// This file defines and implements the class GCNMinRegScheduler, which; /// implements an experimental, simple scheduler whose main goal is to learn; /// ways about consuming less possible registers for a region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp:9,Performance,queue,queue,9,// Ready queue,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNMinRegStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:59,Performance,optimiz,optimize,59,// NSA with non-sequential address which we can try; // to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:135,Modifiability,extend,extend,135,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:411,Modifiability,variab,variable,411,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:150,Performance,optimiz,optimization,150,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp:224,Testability,log,logic,224,"// TODO: address the below limitation to handle GFX11 BVH instructions; // Bail if address is not a VGPR32. That should be possible to extend the; // optimization to work with subregs of a wider register tuples, but the; // logic to find free registers will be much more complicated with much; // less chances for success. That seems reasonable to assume that in most; // cases a tuple is used because a vector variable contains different; // parts of an address and it is either already consecutive or cannot; // be reassigned if not. If needed it is better to rely on register; // coalescer to process such address tuples.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNNSAReassign.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp:28,Availability,avail,available,28,"// For now, reserve highest available SGPR pair. After RA,; // shift down to a lower unused pair of SGPRs; // If all registers are used, then findUnusedRegister will return; // AMDGPU::NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp:69,Availability,down,down,69,"// For now, reserve highest available SGPR pair. After RA,; // shift down to a lower unused pair of SGPRs; // If all registers are used, then findUnusedRegister will return; // AMDGPU::NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRALongBranchReg.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:159,Safety,avoid,avoid,159,"// Some subtargets cannot do an AGPR to AGPR copy directly, and need an; // intermdiate temporary VGPR register. Try to find the defining; // accvgpr_write to avoid temporary registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:61,Deployability,update,update,61,"// Reg uses were changed, collect unique set of registers to update; // live intervals at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp:52,Deployability,update,updated,52,"// For AGPR reg, check if live intervals need to be updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNPreRAOptimizations.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:158,Availability,mask,mask,158,// We don't rely on read-undef flag because in case of tentative schedule; // tracking it isn't set correctly yet. This works correctly however since; // use mask has been tracked before using LIS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:65,Energy Efficiency,schedul,schedule,65,// We don't rely on read-undef flag because in case of tentative schedule; // tracking it isn't set correctly yet. This works correctly however since; // use mask has been tracked before using LIS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:38,Deployability,update,updated,38,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:19,Energy Efficiency,schedul,schedule,19,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:97,Energy Efficiency,schedul,schedule,97,// For a tentative schedule LIS isn't updated yet but livemask should; // remain the same on any schedule. Subreg defs can be reordered but they; // all must dominate uses anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:3,Deployability,Update,Update,3,// Update MaxPressure with defs pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:13,Availability,alive,alive,13,// Make uses alive.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:3,Deployability,Update,Update,3,// Update MaxPressure with uses plus early-clobber defs pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:28,Availability,mask,mask,28,// Remove dead registers or mask bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:24,Availability,mask,mask,24,// Add new registers or mask bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp:103,Availability,Mask,Mask,103,"// Return lanemask of Reg's subregs that are live-through at [Begin, End] and; // are fully covered by Mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h:26,Usability,clear,clear,26,// Return MaxPressure and clear it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRegPressure.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:746,Modifiability,rewrite,rewrites,746,"//===-------------- GCNRewritePartialRegUses.cpp --------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// RenameIndependentSubregs pass leaves large partially used super registers,; /// for example:; /// undef %0.sub4:VReg_1024 = ...; /// %0.sub5:VReg_1024 = ...; /// %0.sub6:VReg_1024 = ...; /// %0.sub7:VReg_1024 = ...; /// use %0.sub4_sub5_sub6_sub7; /// use %0.sub6_sub7; ///; /// GCNRewritePartialRegUses goes right after RenameIndependentSubregs and; /// rewrites such partially used super registers with registers of minimal size:; /// undef %0.sub0:VReg_128 = ...; /// %0.sub1:VReg_128 = ...; /// %0.sub2:VReg_128 = ...; /// %0.sub3:VReg_128 = ...; /// use %0.sub0_sub1_sub2_sub3; /// use %0.sub2_sub3; ///; /// This allows to avoid subreg lanemasks tracking during register pressure; /// calculation and creates more possibilities for the code unaware of lanemasks; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:1020,Safety,avoid,avoid,1020,"//===-------------- GCNRewritePartialRegUses.cpp --------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; /// RenameIndependentSubregs pass leaves large partially used super registers,; /// for example:; /// undef %0.sub4:VReg_1024 = ...; /// %0.sub5:VReg_1024 = ...; /// %0.sub6:VReg_1024 = ...; /// %0.sub7:VReg_1024 = ...; /// use %0.sub4_sub5_sub6_sub7; /// use %0.sub6_sub7; ///; /// GCNRewritePartialRegUses goes right after RenameIndependentSubregs and; /// rewrites such partially used super registers with registers of minimal size:; /// undef %0.sub0:VReg_128 = ...; /// %0.sub1:VReg_128 = ...; /// %0.sub2:VReg_128 = ...; /// %0.sub3:VReg_128 = ...; /// use %0.sub0_sub1_sub2_sub3; /// use %0.sub2_sub3; ///; /// This allows to avoid subreg lanemasks tracking during register pressure; /// calculation and creates more possibilities for the code unaware of lanemasks; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Modifiability,Rewrite,Rewrite,4,/// Rewrite partially used register Reg by shifting all its subregisters to; /// the right and replacing the original register with a register of minimal; /// size. Return true if the change has been made.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Deployability,Update,Update,4,/// Update live intervals after rewriting OldReg to NewReg with SubRegs map; /// describing OldSubReg -> NewSubReg mapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:119,Performance,cache,cached,119,"/// Find subreg index with a given Offset and Size, return 0 if there is no; /// such subregister index. The result is cached in SubRegs data-member.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,"/// Cache for getSubReg method: {Offset, Size} -> SubReg index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:15,Availability,mask,mask,15,/// Return bit mask that contains all register classes that are projected into; /// RC by SubRegIdx. The result is cached in SuperRegMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:115,Performance,cache,cached,115,/// Return bit mask that contains all register classes that are projected into; /// RC by SubRegIdx. The result is cached in SuperRegMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,"/// Cache for getSuperRegClassMask method: { RC, SubRegIdx } -> Class bitmask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:122,Performance,cache,cached,122,/// Return bitmask containing all allocatable register classes with registers; /// aligned at AlignNumBits. The result is cached in; /// AllocatableAndAlignedRegClassMasks data-member.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp:4,Performance,Cache,Cache,4,/// Cache for getAllocatableAndAlignedRegClassMask method:; /// AlignNumBits -> Class bitmask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNRewritePartialRegUses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:35,Energy Efficiency,Schedul,Scheduler,35,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:539,Energy Efficiency,schedul,scheduling,539,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:636,Energy Efficiency,schedul,schedule,636,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:682,Energy Efficiency,schedul,scheduling,682,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:803,Energy Efficiency,schedul,scheduling,803,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:924,Energy Efficiency,schedul,scheduling,924,"//===-- GCNSchedStrategy.cpp - GCN Scheduler Strategy ---------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This contains a MachineSchedStrategy implementation for maximizing wave; /// occupancy on GCN hardware.; ///; /// This pass will apply multiple scheduling stages to the same function.; /// Regions are first recorded in GCNScheduleDAGMILive::schedule. The actual; /// entry point for the scheduling of those regions is; /// GCNScheduleDAGMILive::runSchedStages.; /// Generally, the reason for having multiple scheduling stages is to account; /// for the kernel-wide effect of register usage on occupancy. Usually, only a; /// few scheduling regions will have register pressure high enough to limit; /// occupancy for the kernel, so constraints can be relaxed to improve ILP in; /// other regions.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:184,Energy Efficiency,schedul,scheduler,184,"// Set the initial TargetOccupnacy to the maximum occupancy that we can; // achieve for this function. This effectively sets a lower bound on the; // 'Critical' register limits in the scheduler.; // Allow for lower occupancy targets if kernel is wave limited or memory; // bound, and using the relaxed occupancy feature.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:12,Availability,error,error,12,// Subtract error margin and bias from register limits and avoid overflow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:59,Safety,avoid,avoid,59,// Subtract error margin and bias from register limits and avoid overflow.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:32,Energy Efficiency,schedul,scheduling,32,"// FIXME: I think for bottom up scheduling, the register pressure is cached; // and can be retrieved by DAG->getPressureDif(SU).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:69,Performance,cache,cached,69,"// FIXME: I think for bottom up scheduling, the register pressure is cached; // and can be retrieved by DAG->getPressureDif(SU).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:108,Energy Efficiency,schedul,scheduler,108,"// If two instructions increase the pressure of different register sets; // by the same amount, the generic scheduler will prefer to schedule the; // instruction that increases the set with the least amount of registers,; // which in our case would be SGPRs. This is rarely what we want, so; // when we report excess/critical register pressure, we do it either; // only for VGPRs or only for SGPRs.; // FIXME: Better heuristics to determine whether to prefer SGPRs or VGPRs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:133,Energy Efficiency,schedul,schedule,133,"// If two instructions increase the pressure of different register sets; // by the same amount, the generic scheduler will prefer to schedule the; // instruction that increases the set with the least amount of registers,; // which in our case would be SGPRs. This is rarely what we want, so; // when we report excess/critical register pressure, we do it either; // only for VGPRs or only for SGPRs.; // FIXME: Better heuristics to determine whether to prefer SGPRs or VGPRs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:274,Deployability,update,update,274,// FIXME: We have to enter REG-EXCESS before we reach the actual threshold; // to increase the likelihood we don't go over the limits. We should improve; // the analysis to look through dependencies to find the path with the least; // register pressure.; // We only need to update the RPDelta for instructions that increase register; // pressure. Instructions that decrease or keep reg pressure the same will be; // marked as RegExcess in tryCandidate() when they are compared with; // instructions that increase the register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:186,Integrability,depend,dependencies,186,// FIXME: We have to enter REG-EXCESS before we reach the actual threshold; // to increase the likelihood we don't go over the limits. We should improve; // the analysis to look through dependencies to find the path with the least; // register pressure.; // We only need to update the RPDelta for instructions that increase register; // pressure. Instructions that decrease or keep reg pressure the same will be; // marked as RegExcess in tryCandidate() when they are compared with; // instructions that increase the register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:90,Energy Efficiency,reduce,reduce,90,"// Register pressure is considered 'CRITICAL' if it is approaching a value; // that would reduce the wave occupancy for the execution unit. When; // register pressure is 'CRITICAL', increasing SGPR and VGPR pressure both; // has the same cost, so we don't need to prefer one over the other.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Energy Efficiency,Schedul,Schedule,3,"// Schedule as far as possible in the direction of no choice. This is most; // efficient, but also provides the best heuristics for CriticalPSets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:79,Energy Efficiency,efficient,efficient,79,"// Schedule as far as possible in the direction of no choice. This is most; // efficient, but also provides the best heuristics for CriticalPSets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Availability,down,down,15,"// Set the top-down policy based on the state of the current top zone and; // the instructions outside the zone, including the bottom zone.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:56,Energy Efficiency,schedul,scheduled,56,// See if BotCand is still valid (because we previously scheduled from Top).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid spilling by exceeding the register limit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,consumption,consumption,27,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:55,Energy Efficiency,schedul,schedule,55,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid critical resource consumption and balance the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:26,Energy Efficiency,reduce,reduce,26,// Unconditionally try to reduce latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:33,Performance,latency,latency,33,// Unconditionally try to reduce latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:46,Availability,down,downstream,46,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:94,Energy Efficiency,reduce,reduce,94,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:291,Energy Efficiency,schedul,scheduler,291,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:70,Performance,optimiz,optimizations,70,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:191,Performance,Optimiz,Optimizations,191,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:225,Performance,load,loads,225,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:323,Performance,load,loads,323,// Keep clustered nodes together to encourage downstream peephole; // optimizations which may reduce resource requirements.; //; // This is a best effort to set things up for a post-RA pass. Optimizations; // like generating loads of multiple registers should ideally be done within; // the scheduler pass by combining the loads during DAG postprocessing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:53,Energy Efficiency,schedul,scheduled,53,// Avoid increasing the max critical pressure in the scheduled region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid increasing the max critical pressure in the scheduled region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Safety,Avoid,Avoid,3,// Avoid increasing the max pressure of the entire region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Energy Efficiency,schedul,scheduling,15,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:46,Energy Efficiency,schedul,scheduling,46,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:60,Performance,perform,performed,60,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:181,Energy Efficiency,schedul,scheduling,181,"// If the block has the only successor then live-ins of that successor are; // live-outs of the current block. We can reuse calculated live set if the; // successor will be sent to scheduling past current block.; // However, due to the bug in LiveInterval analysis it may happen that two; // predecessors of the same successor block have different lane bitmasks for; // a live-out register. Workaround that by sticking to one-to-one relationship; // i.e. one predecessor with one successor block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Energy Efficiency,Schedul,Scheduler,3,// Scheduler sends regions from the end of the block upwards.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:16,Energy Efficiency,schedul,scheduling,16,// Start actual scheduling here. This function is called by the base; // MachineScheduler after all regions have been recorded by; // GCNScheduleDAGMILive::schedule().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:156,Energy Efficiency,schedul,schedule,156,// Start actual scheduling here. This function is called by the base; // MachineScheduler after all regions have been recorded by; // GCNScheduleDAGMILive::schedule().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:13,Energy Efficiency,schedul,scheduling,13,// Setup for scheduling the region and check whether it should be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:22,Energy Efficiency,reduce,reduce,22,// Aggressivly try to reduce register pressure in the unclustered high RP; // stage. Temporarily increase occupancy target in the region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:132,Energy Efficiency,schedul,scheduled,132,// Don't bother trying to improve ILP in lower RP regions if occupancy has not; // been dropped. All regions will have already been scheduled with the ideal; // occupancy targets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:36,Performance,cache,cached,36,// FIXME: This pass will invalidate cached MBBLiveIns for regions; // inbetween the defs and region we sinked the def to. Cached pressure; // for regions where a def is sinked from will also be invalidated. Will; // need to be fixed if there is another pass after this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:122,Performance,Cache,Cached,122,// FIXME: This pass will invalidate cached MBBLiveIns for regions; // inbetween the defs and region we sinked the def to. Cached pressure; // for regions where a def is sinked from will also be invalidated. Will; // need to be fixed if there is another pass after this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:14,Energy Efficiency,schedul,scheduling,14,// Skip empty scheduling regions (0 or 1 schedulable instructions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:41,Energy Efficiency,schedul,schedulable,41,// Skip empty scheduling regions (0 or 1 schedulable instructions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:42,Energy Efficiency,schedul,scheduling,42,// Save original instruction order before scheduling for possible revert.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:111,Testability,test,testing,111,"// We may need to reschedule this region if it wasn't rescheduled in the last; // stage, or if we found it was testing critical register pressure limits in; // the unclustered reschedule stage. The later is because we may not have been; // able to raise the min occupancy in the previous stage so the region may be; // overly constrained even if it was already rescheduled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:87,Energy Efficiency,schedul,schedule,87,// Get real RP for the region if it hasn't be calculated before. After the; // initial schedule stage real RP will be collected after scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:134,Energy Efficiency,schedul,scheduling,134,// Get real RP for the region if it hasn't be calculated before. After the; // initial schedule stage real RP will be collected after scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:10,Energy Efficiency,schedul,scheduling,10,// Revert scheduling if we have dropped occupancy or there is some other; // reason that the original schedule is better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:102,Energy Efficiency,schedul,schedule,102,// Revert scheduling if we have dropped occupancy or there is some other; // reason that the original schedule is better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:24,Energy Efficiency,schedul,scheduling,24,// Check the results of scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:83,Energy Efficiency,schedul,scheduled,83,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:134,Energy Efficiency,schedul,scheduling,134,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:203,Energy Efficiency,schedul,schedule,203,"// We may not be able to keep the current target occupancy because of the just; // scheduled region. We might still be able to revert scheduling if the; // occupancy before was higher, or if the current schedule has register; // pressure higher than the excess limits which could lead to more spilling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,schedul,schedule,27,// Revert if this region's schedule would cause a drop in occupancy or; // spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:16,Energy Efficiency,reduce,reduced,16,"// If RP is not reduced in the unclustered reschedule stage, revert to the; // old schedule.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:83,Energy Efficiency,schedul,schedule,83,"// If RP is not reduced in the unclustered reschedule stage, revert to the; // old schedule.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:27,Energy Efficiency,schedul,schedule,27,// Do not attempt to relax schedule even more if we are already spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:30,Deployability,update,update,30,// Reset read-undef flags and update them later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:19,Energy Efficiency,schedul,schedule,19,"// After reverting schedule, debug instrs will now be at the end of the block; // and RegionEnd will point to the first debug instr. Increment RegionEnd; // pass debug instrs to the actual end of the scheduling region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:200,Energy Efficiency,schedul,scheduling,200,"// After reverting schedule, debug instrs will now be at the end of the block; // and RegionEnd will point to the first debug instr. Increment RegionEnd; // pass debug instrs to the actual end of the scheduling region.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:59,Safety,avoid,avoid,59,// Collect regions with rematerializable reg as live-in to avoid; // searching later when updating RP.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:30,Modifiability,variab,variables,30,// Temporary copies of cached variables we will be modifying and replacing if; // sinking succeeds.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:23,Performance,cache,cached,23,// Temporary copies of cached variables we will be modifying and replacing if; // sinking succeeds.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:68,Deployability,update,updated,68,// Make copies of register pressure and live-ins cache that will be updated; // as we rematerialize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:49,Performance,cache,cache,49,// Make copies of register pressure and live-ins cache that will be updated; // as we rematerialize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,// Update region boundaries in scheduling region we sinked from since we; // may sink an instruction that was at the beginning or end of its region,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:31,Energy Efficiency,schedul,scheduling,31,// Update region boundaries in scheduling region we sinked from since we; // may sink an instruction that was at the beginning or end of its region,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,// Update region boundaries in region we sinked to.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:15,Deployability,update,update,15,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:77,Deployability,Update,Update,77,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:22,Performance,cache,cached,22,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:77,Usability,Undo,Undo,77,// Occupancy was not improved for all regions that were at MinOccupancy.; // Undo sinking and remove newly rematerialized instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:20,Deployability,update,update,20,// Remove OldMI and update LIS,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:3,Deployability,Update,Update,3,"// Update live-ins, register pressure, and regions caches.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:51,Performance,cache,caches,51,"// Update live-ins, register pressure, and regions caches.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:172,Energy Efficiency,schedul,scheduling,172,"// When removing, we will have to check both beginning and ending of the region.; // When inserting, we will only have to check if we are inserting NewMI in front; // of a scheduling region and do not need to check the ending since we will only; // ever be inserting before an already existing MI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:33,Energy Efficiency,Schedul,Scheduler,33,"//===-- GCNSchedStrategy.h - GCN Scheduler Strategy -*- C++ -*-------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:22,Energy Efficiency,schedul,scheduler,22,/// This is a minimal scheduler strategy. The main difference between this; /// and the GenericScheduler is that GCNSchedStrategy uses different; /// heuristics to determine excess/critical pressure sets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,Schedul,Scheduling,3,// Scheduling stages for this strategy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,schedul,schedule,3,// schedule() have seen register pressure over the critical limits and had to; // track register pressure for actual scheduling heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:117,Energy Efficiency,schedul,scheduling,117,// schedule() have seen register pressure over the critical limits and had to; // track register pressure for actual scheduling heuristics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Energy Efficiency,Schedul,Schedule,3,// Schedule known to have excess register pressure. Be more conservative in; // increasing ILP and preserving VGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:6,Availability,error,error,6,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:168,Energy Efficiency,reduce,reduce,168,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:48,Performance,perform,performance,48,// An error margin is necessary because of poor performance of the generic RP; // tracker and can be adjusted up for tuning heuristics to try and more; // aggressively reduce register pressure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,/// The goal of this scheduling strategy is to maximize kernel occupancy (i.e.; /// maximum number of waves per simd).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,/// The goal of this scheduling strategy is to maximize ILP for a single wave; /// (i.e. latency hiding).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:89,Performance,latency,latency,89,/// The goal of this scheduling strategy is to maximize ILP for a single wave; /// (i.e. latency hiding).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:49,Energy Efficiency,schedul,scheduling,49,// Occupancy target at the beginning of function scheduling cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:34,Energy Efficiency,schedul,scheduled,34,"// Records if a region is not yet scheduled, or schedule has been reverted,; // or we generally desire to reschedule it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:48,Energy Efficiency,schedul,schedule,48,"// Records if a region is not yet scheduled, or schedule has been reverted,; // or we generally desire to reschedule it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:18,Performance,cache,cache,18,// Region live-in cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Performance,cache,cache,19,// Region pressure cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:33,Performance,cache,cache,33,// Temporary basic block live-in cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:15,Performance,cache,cache,15,// Compute and cache live-ins and pressure for all regions in block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:3,Deployability,Update,Update,3,// Update region boundaries when removing MI or inserting NewMI before MI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:37,Energy Efficiency,schedul,scheduling,37,// GCNSchedStrategy applies multiple scheduling stages to a function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:27,Energy Efficiency,schedul,scheduled,27,// The current block being scheduled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:52,Energy Efficiency,schedul,scheduling,52,// Record the original order of instructions before scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:13,Energy Efficiency,schedul,scheduling,13,// RP before scheduling the current region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:12,Energy Efficiency,schedul,scheduling,12,// RP after scheduling the current region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:26,Energy Efficiency,schedul,scheduling,26,// Initialize state for a scheduling stage. Returns false if the current stage; // should be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:36,Energy Efficiency,schedul,scheduling,36,// Finalize state after finishing a scheduling pass on the function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:13,Energy Efficiency,schedul,scheduling,13,// Setup for scheduling a region. Returns false if the current region should; // be skipped.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Energy Efficiency,schedul,scheduling,19,// Check result of scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:22,Energy Efficiency,schedul,schedule,22,// computes the given schedule virtual execution time in clocks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:19,Energy Efficiency,schedul,scheduling,19,// Returns true if scheduling should be reverted.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:27,Energy Efficiency,schedul,schedule,27,// Returns true if the new schedule may result in more spilling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:21,Energy Efficiency,schedul,scheduling,21,// Attempt to revert scheduling for this region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:18,Energy Efficiency,schedul,scheduling,18,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:101,Energy Efficiency,schedul,scheduling,101,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:155,Energy Efficiency,schedul,schedule,155,// Retry function scheduling if we found resulting occupancy and it is; // lower than used for other scheduling passes. This will give more freedom; // to schedule low register pressure blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:119,Energy Efficiency,reduce,reduce,119,"// Each region at MinOccupancy will have their own list of trivially; // rematerializable instructions we can remat to reduce RP. The list maps an; // instruction to the position we should remat before, usually the MI using; // the rematerializable instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:32,Energy Efficiency,reduce,reduce,32,// TODO: Should also attempt to reduce RP of SGPRs and AGPRs; // Attempt to reduce RP of VGPR by sinking trivially rematerializable; // instructions. Returns true if we were able to sink instruction(s).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:76,Energy Efficiency,reduce,reduce,76,// TODO: Should also attempt to reduce RP of SGPRs and AGPRs; // Attempt to reduce RP of VGPR by sinking trivially rematerializable; // instructions. Returns true if we were able to sink instruction(s).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:130,Integrability,wrap,wrap,130,"/// True if the offset field of DS instructions works as expected. On SI, the; /// offset uses a 16-bit adder and does not always wrap properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:39,Usability,usab,usable,39,/// Condition output from div_scale is usable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:15,Safety,hazard,hazard,15,/// Extra wait hazard is needed in some cases before; /// s_cbranch_vccnz/s_cbranch_vccz.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:28,Deployability,update,update,28,/// Writes to VCC_LO/VCC_HI update the VCCZ flag.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:14,Safety,hazard,hazard,14,/// Number of hazard wait states for s_setreg_b32/s_setreg_imm32_b32.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:42,Performance,perform,perform,42,"/// \returns If MUBUF instructions always perform range checking, even for; /// buffer resources used for private memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:121,Security,access,access,121,"/// \returns If MUBUF instructions always perform range checking, even for; /// buffer resources used for private memory access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:138,Testability,test,testing,138,"// True if the hardware rewinds and replays GWS operations if a wave is; // preempted.; //; // If this is false, a GWS operation requires testing if a nack set the; // MEM_VIOL bit, and repeating if so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:14,Energy Efficiency,allocate,allocated,14,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:391,Safety,avoid,avoid,391,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:220,Security,access,access,220,"// Scratch is allocated in 256 dword per wave blocks for the entire; // wavefront. When viewed from the perspective of an arbitrary workitem, this; // is 4-byte aligned.; //; // Only 4-byte alignment is really needed to access anything. Transformations; // on the pointer value itself may rely on the alignment / known low bits of; // the pointer. Set this to something above the minimum to avoid needing; // dynamic realignment in common cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:10,Integrability,wrap,wrappers,10,// static wrappers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:51,Performance,load,load,51,// \returns true if the subtarget supports DWORDX3 load/store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:54,Energy Efficiency,allocate,allocated,54,// Shift amount of a 64 bit shift cannot be a highest allocated register; // if also at the end of the allocation block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:17,Safety,hazard,hazard,17,// Has one cycle hazard on transcendental instruction feeding a; // non transcendental VALU.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:17,Safety,hazard,hazard,17,// Has one cycle hazard on a VALU instruction partially writing dst with; // a shift of result bits feeding another VALU instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:37,Performance,LOAD,LOADcnt,37,"/// \returns true if the target uses LOADcnt/SAMPLEcnt/BVHcnt, DScnt/KMcnt; /// and STOREcnt rather than VMcnt, LGKMcnt and VScnt respectively.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:37,Modifiability,extend,extends,37,// \returns true if S_GETPC_B64 zero-extends the result from 48 bits instead; // of sign-extending.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:89,Modifiability,extend,extending,89,// \returns true if S_GETPC_B64 zero-extends the result from 48 bits instead; // of sign-extending.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:62,Energy Efficiency,schedul,scheduler,62,// \returns true if it's beneficial on this subtarget for the scheduler to; // cluster stores as well as loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:105,Performance,load,loads,105,// \returns true if it's beneficial on this subtarget for the scheduler to; // cluster stores as well as loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:40,Safety,hazard,hazard,40,"// \returns true if the subtarget has a hazard requiring an ""s_nop 0""; // instruction before ""s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h:56,Integrability,message,message,56,// Currently all targets that support the dealloc VGPRs message also require; // the nop.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSubtarget.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:414,Energy Efficiency,schedul,scheduling,414,"//===- GCNVOPDUtils.cpp - GCN VOPD Utils ------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AMDGPU DAG scheduling; /// mutation to pair VOPD instructions back to back. It also contains; // subroutines useful in the creation of VOPD instructions; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:15,Integrability,depend,dependent,15,// Cannot pair dependent instructions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:72,Performance,cache,cache,72,// On GFX12 if both OpX and OpY are V_MOV_B32 then OPY uses SRC2 source-cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:61,Energy Efficiency,schedul,scheduled,61,"/// Check if the instr pair, FirstMI and SecondMI, should be scheduled; /// together. Given SecondMI, when FirstMI is unspecified, then check if; /// SecondMI may be part of a fused pair at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:4,Energy Efficiency,Adapt,Adapts,4,/// Adapts design from MacroFusion; /// Puts valid candidate instructions back-to-back so they can easily; /// be turned into VOPD instructions; /// Greedily pairs instruction candidates. O(n^2) algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp:4,Modifiability,Adapt,Adapts,4,/// Adapts design from MacroFusion; /// Puts valid candidate instructions back-to-back so they can easily; /// be turned into VOPD instructions; /// Greedily pairs instruction candidates. O(n^2) algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h:412,Energy Efficiency,schedul,scheduling,412,"//===- GCNVOPDUtils.h - GCN VOPD Utils ------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This file contains the AMDGPU DAG scheduling; /// mutation to pair VOPD instructions back to back. It also contains; // subroutines useful in the creation of VOPD instructions; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNVOPDUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp:25,Performance,cache,cacheline,25,// Functions needs to be cacheline (256B) aligned.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:377,Energy Efficiency,allocate,allocate,377,"// We are being conservative here. We only require this work-around if; // CurrentSubEntries > 3 &&; // (CurrentSubEntries % 4 == 3 || CurrentSubEntries % 4 == 0); //; // We have to be conservative, because we don't know for certain that; // our stack allocation algorithm for Evergreen/NI is correct. Applying this; // work-around when CurrentSubEntries > 3 allows us to over-allocate stack; // resources without any problems.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:117,Energy Efficiency,allocate,allocate,117,"// Some documentation says that this is not necessary on Evergreen,; // but experimentation has show that we need to allocate 1 extra; // sub-entry for the first non-WQM push.; // +1 For the push operation.; // +1 Extra space required.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate new literal reg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ControlFlowFinalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h:85,Modifiability,Config,Config,85,//===----------------------------------------------------------------------===//; // Config register definitions; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Defines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:517,Security,access,access,517,"//===-- R600EmitClauseMarkers.cpp - Emit CF_ALU ---------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Add CF_ALU. R600 Alu instructions are grouped in clause which can hold; /// 128 Alu instructions ; these instructions can access up to 4 prefetched; /// 4 lines of 16 registers from constant buffers. Such ALU clauses are; /// initiated by CF_ALU instructions.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:280,Energy Efficiency,schedul,scheduler,280,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:293,Energy Efficiency,schedul,schedule,293,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:147,Safety,safe,safe,147,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:92,Usability,clear,cleared,92,"// TODO: Is this true? kill flag appears to work OK below; // Register kill flags have been cleared by the time we get to this; // pass, but it is safe to assume that all uses of this register; // occur in the same basic block as its definition, because; // it is illegal for the scheduler to schedule them in; // different blocks.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp:84,Safety,safe,safe,84,"// We don't use the ADDR field until R600ControlFlowFinalizer pass, where; // it is safe to assume it is 0. However if we always put 0 here, the ifcvt; // pass may assume that identical ALU clause starter at the beginning of a; // true and false branch can be factorized which is not the case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600EmitClauseMarkers.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:145,Availability,mask,masked,145,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:186,Availability,mask,masked,186,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:227,Availability,mask,masked,227,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:374,Availability,mask,masked,374,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:421,Availability,mask,masked,421,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:468,Availability,mask,masked,468,"// Expand the instruction; //; // Reduction instructions:; // T0_X = DP4 T1_XYZW, T2_XYZW; // becomes:; // TO_X = DP4 T1_X, T2_X; // TO_Y (write masked) = DP4 T1_Y, T2_Y; // TO_Z (write masked) = DP4 T1_Z, T2_Z; // TO_W (write masked) = DP4 T1_W, T2_W; //; // Vector instructions:; // T0_X = MULLO_INT T1_X, T2_X; // becomes:; // T0_X = MULLO_INT T1_X, T2_X; // T0_Y (write masked) = MULLO_INT T1_X, T2_X; // T0_Z (write masked) = MULLO_INT T1_X, T2_X; // T0_W (write masked) = MULLO_INT T1_X, T2_X; //; // Cube instructions:; // T0_XYZW = CUBE T1_XYZW; // becomes:; // TO_X = CUBE T1_Z, T1_Y; // T0_Y = CUBE T1_Z, T1_X; // T0_Z = CUBE T1_X, T1_Z; // T0_W = CUBE T1_Y, T1_Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp:3,Availability,Mask,Mask,3,// Mask the write if the original instruction does not write to; // the current Channel.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ExpandSpecialInstrs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp:37,Energy Efficiency,allocate,allocated,37,/// \returns The number of registers allocated for \p FI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600FrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:25,Performance,queue,queue,25,// The value from output queue A (denoted by register OQAP) can; // only be fetched during the first cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:39,Availability,avail,available,39,// R600::BRANCH* instructions are only available after isel and are not; // handled,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:3,Modifiability,Variab,Variable,3,// Variable sized objects are not supported,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp:73,Energy Efficiency,schedul,scheduling,73,"//XXX: The r600g finalizer expects this to be 1, once we've moved the; //scheduling to the backend, we can change the default to 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:48,Integrability,Interface,Interface,48,"//===-- R600InstrInfo.h - R600 Instruction Info Interface -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600InstrInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:395,Integrability,Interface,Interface,395,"//===-- R600InstrInfo.h - R600 Instruction Info Interface -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600InstrInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:272,Availability,avail,available,272,"/// Given the order VEC_012 < VEC_021 < VEC_120 < VEC_102 < VEC_201 < VEC_210; /// returns true and the first (in lexical order) BankSwizzle affectation; /// starting from the one already provided in the Instruction Group MIs that; /// fits Read Port limitations in BS if available. Otherwise returns false; /// and undefined content in BS.; /// isLastAluTrans should be set if the last Alu of MIs will be executed on; /// Trans ALU. In this case, ValidTSwizzle returns the BankSwizzle value to; /// apply to the last instruction.; /// PV holds GPR to PV registers in the Instruction Group MIs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:34,Security,access,access,34,/// An instruction group can only access 2 channel pair (either [XY] or [ZW]); /// from KCache bank on R700+. This function check if MI set in input meet; /// this limitations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:38,Security,access,accessed,38,/// Reserve the registers that may be accessed using indirect addressing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:183,Performance,load,loads,183,"/// Calculate the ""Indirect Address"" for the given \p RegIndex and; /// \p Channel; ///; /// We model indirect addressing using a virtual address space that can be; /// accessed with loads and stores. The ""Indirect Address"" is the memory; /// address in this virtual address space that maps to the given \p RegIndex; /// and \p Channel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:169,Security,access,accessed,169,"/// Calculate the ""Indirect Address"" for the given \p RegIndex and; /// \p Channel; ///; /// We model indirect addressing using a virtual address space that can be; /// accessed with loads and stores. The ""Indirect Address"" is the memory; /// address in this virtual address space that maps to the given \p RegIndex; /// and \p Channel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:47,Performance,load,loading,47,"/// \returns The register class to be used for loading and storing values; /// from an ""Indirect Address"" .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:54,Security,access,accessed,54,/// \returns the smallest register index that will be accessed by an indirect; /// read or write or -1 if indirect addressing is not used by this program.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:53,Security,access,accessed,53,/// \returns the largest register index that will be accessed by an indirect; /// read or write or -1 if indirect addressing is not used by this program.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:97,Performance,perform,performs,97,/// Build instruction(s) for an indirect register write.; ///; /// \returns The instruction that performs the indirect register write,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:96,Performance,perform,performs,96,/// Build instruction(s) for an indirect register read.; ///; /// \returns The instruction that performs the indirect register read,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:178,Safety,avoid,avoid,178,/// buildDefaultInstruction - This function returns a MachineInstr with all; /// the instruction modifiers initialized to their default values. You can; /// use this function to avoid manually specifying each instruction modifier; /// operand when building a new instruction.; ///; /// \returns a MachineInstr with all the instruction modifiers initialized; /// to their default values.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h:4,Usability,Clear,Clear,4,/// Clear the specified flag on the instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600InstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:247,Energy Efficiency,schedul,scheduler,247,// BUILD_VECTOR was lowered into an IMPLICIT_DEF + 4 INSERT_SUBREG; // that adds a 128 bits reg copy when going through TwoAddressInstructions; // pass. We want to avoid 128 bits copies as much as possible because they; // can't be bundled by our scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:164,Safety,avoid,avoid,164,// BUILD_VECTOR was lowered into an IMPLICIT_DEF + 4 INSERT_SUBREG; // that adds a 128 bits reg copy when going through TwoAddressInstructions; // pass. We want to avoid 128 bits copies as much as possible because they; // can't be bundled by our scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp:91,Energy Efficiency,schedul,scheduling,91,"/// This pass converts a legalized DAG into a R600-specific; // DAG, ready for instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:12,Performance,load,loads,12,// Legalize loads and stores to the private address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:66,Performance,load,loads,66,// Workaround for LegalizeDAG asserting on expansion of i1 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:30,Testability,assert,asserting,30,// Workaround for LegalizeDAG asserting on expansion of i1 vector loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:30,Testability,assert,asserting,30,// Workaround for LegalizeDAG asserting on expansion of i1 vector stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:112,Safety,avoid,avoid,112,// We don't have 64-bit shifts. Thus we need either SHX i64 or SHX_PARTS i32; // to be Legal/Custom in order to avoid library calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Modifiability,Extend,Extend,3,// Extend sign.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Performance,Load,Load,3,// Load dword; // TODO: can we be smarter about machine pointer info?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Availability,Mask,Mask,3,// Mask the value to the right type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:13,Availability,mask,mask,13,// Shift the mask in place,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:14,Availability,mask,mask,14,// Invert the mask. NOTE: if we had native ROL instructions we could; // use inverted mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:86,Availability,mask,mask,86,// Invert the mask. NOTE: if we had native ROL instructions we could; // use inverted mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:57,Integrability,depend,depend,57,"// If we are part of expanded vector, make our neighbors depend on this store",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:34,Integrability,depend,depend,34,// Make all other vector elements depend on this store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:85,Integrability,depend,dependencies,85,// It is beneficial to create MSKOR here instead of combiner to avoid; // artificial dependencies introduced by RMW,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:64,Safety,avoid,avoid,64,// It is beneficial to create MSKOR here instead of combiner to avoid; // artificial dependencies introduced by RMW,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:11,Availability,mask,mask,11,// Put the mask in correct place,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:3,Performance,Load,Load,3,// Load dword; // TODO: can we be smarter about machine pointer info?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:35,Performance,load,load,35,// This is still used for explicit load from addrspace(8),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:87,Performance,load,load,87,"//TODO: Does this even work?; // non-constant ptr can't be folded, keeps it as a v4f32 load",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:348,Modifiability,extend,extended,348,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:138,Performance,LOAD,LOAD,138,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:178,Performance,load,loads,178,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:257,Performance,load,loads,257,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:409,Performance,load,loads,409,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:7,Performance,load,load,7,// Get load source type if scalarized.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:6,Availability,mask,mask,6,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:116,Energy Efficiency,reduce,reduce,116,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:159,Integrability,depend,dependencies,159,"// We mask write here to teach later passes that the ith element of this; // vector is undef. Thus we can use it to reduce 128 bits reg usage,; // break false dependencies and additionally make assembly easier to read.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:24,Performance,load,loads,24,//TODO: Support smaller loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:96,Performance,Optimiz,Optimizations,96,//===----------------------------------------------------------------------===//; // Custom DAG Optimizations; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:14,Performance,optimiz,optimizations,14,// Try common optimizations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:47,Integrability,Interface,Interface,47,"//===-- R600ISelLowering.h - R600 DAG Lowering Interface -*- C++ -*--------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:413,Integrability,interface,interface,413,"//===-- R600ISelLowering.h - R600 DAG Lowering Interface -*- C++ -*--------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:280,Performance,perform,performing,280,"// R600 has ""custom"" lowering for truncating stores despite not supporting; // those instructions. If we allow that custom lowering in the DAG combiner; // then all truncates are merged into truncating stores, giving worse code; // generation. This hook prevents the DAG combiner performing that combine.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h:164,Performance,load,load,164,/// Each OpenCL kernel has nine implicit parameters that are stored in the; /// first nine dwords of a Vertex Buffer. These implicit parameters are; /// lowered to load instructions which retrieve the values from the Vertex; /// Buffer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:4,Performance,Perform,Perform,4,/// Perform the CFG structurization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:4,Performance,Perform,Perform,4,/// Perform the CFG preparation; /// This step will remove every unconditionnal/dead jump instructions and make; /// sure all loops have an exit block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:40,Availability,failure,failures,40,// FIXME: This pass causes verification failures.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:170,Integrability,rout,routine,170,/// MachineBasicBlock::ReplaceUsesOfBlockWith doesn't serve the purpose; /// because the AMDGPU instruction is not recognized as terminator fix this; /// and retire this routine,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:10,Usability,Simpl,Simplify,10,// FIXME: Simplify,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:166,Integrability,interface,interface,166,"// TODO to fix up jump table so later phase won't be confused. if; // (jumpTableInfo->isEmpty() == false) { need to clean the jump table, but; // there isn't such an interface yet. alternatively, replace all the other; // blocks in the jump table with the entryBlk //}",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:34,Testability,test,test,34,//Use the worse block ordering to test the algorithm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:8,Integrability,wrap,wrap,8,// Misc wrap up to maintain the consistency of the Function representation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:25,Deployability,release,release,25,"// Detach retired Block, release memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:9,Usability,Simpl,Simplify,9,// TODO: Simplify,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:8,Availability,down,down,8,// walk down the postDomTree,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:39,Performance,optimiz,optimize,39,"// XXX: We have an opportunity here to optimize the ""branch into if"" case; // here. Branch into if looks like this:; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true; // \ /; // done; //; // The diamond_head block begins the ""if"" and the diamond_true block; // is the block being ""branched into"".; //; // If MigrateTrue is true, then TrueBB is the block being ""branched into""; // and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:551,Performance,optimiz,optimization,551,"// XXX: We have an opportunity here to optimize the ""branch into if"" case; // here. Branch into if looks like this:; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true; // \ /; // done; //; // The diamond_head block begins the ""if"" and the diamond_true block; // is the block being ""branched into"".; //; // If MigrateTrue is true, then TrueBB is the block being ""branched into""; // and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:1387,Performance,optimiz,optimization,1387,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:1599,Performance,optimiz,optimization,1599,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:2052,Performance,optimiz,optimization,2052,"and if MigrateFalse is true, then FalseBB is the block being; // ""branched into""; //; // Here is the pseudo code for how I think the optimization should work:; // 1. Insert MOV GPR0, 0 before the branch instruction in diamond_head.; // 2. Insert MOV GPR0, 1 before the branch instruction in branch_from.; // 3. Move the branch instruction from diamond_head into its own basic; // block (new_block).; // 4. Add an unconditional branch from diamond_head to new_block; // 5. Replace the branch instruction in branch_from with an unconditional; // branch to new_block. If branch_from has multiple predecessors, then; // we need to replace the True/False block in the branch; // instruction instead of replacing it.; // 6. Change the condition of the branch instruction in new_block from; // COND to (COND || GPR0); //; // In order insert these MOV instruction, we will need to use the; // RegisterScavenger. Usually liveness stops being tracked during; // the late machine optimization passes, however if we implement; // bool TargetRegisterInfo::requiresRegisterScavenging(; // const MachineFunction &MF); // and have it return true, liveness will be tracked correctly; // by generic optimization passes. We will also need to make sure that; // all of our target-specific passes that run after regalloc and before; // the CFGStructurizer track liveness and we will need to modify this pass; // to correctly track liveness.; //; // After the above changes, the new CFG should look like this:; // entry; // / |; // diamond_head branch_from; // \ /; // new_block; // / |; // diamond_false diamond_true; // \ /; // done; //; // Without this optimization, we are forced to duplicate the diamond_true; // block and we will end up with a CFG like this:; //; // entry; // / |; // diamond_head branch_from; // / \ |; // diamond_false diamond_true diamond_true (duplicate); // \ / |; // done --------------------|; //; // Duplicating diamond_true can be very costly especially if it has a; // lot of instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:24,Safety,avoid,avoid,24,"//insert R600::ENDIF to avoid special case ""input landBlk == NULL""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:96,Availability,failure,failure,96,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:118,Energy Efficiency,schedul,scheduling,118,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:86,Testability,assert,assertion,86,"// XXX: We are running this after RA, so creating virtual registers will; // cause an assertion failure in the PostRA scheduling pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:3,Deployability,update,update,3,// update landBlk,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp:30,Safety,safe,safely,30,//now branchInst can be erase safely,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineCFGStructurizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:40,Energy Efficiency,Schedul,Scheduler,40,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:408,Energy Efficiency,Schedul,Scheduler,408,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:50,Integrability,Interface,Interface,50,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:418,Integrability,interface,interface,418,"//===-- R600MachineScheduler.cpp - R600 Scheduler Interface -*- C++ -*-----===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:95,Usability,Guid,Guide,95,// We use the heuristic provided by AMD Accelerated Parallel Processing; // OpenCL Programming Guide :; // The approx. number of WF that allows TEX inst to hide ALU inst is :; // 500 (cycles for TEX) / (AluFetchRatio * 8 (cycles for ALU)),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:247,Availability,Avail,Available,247,"// We assume the local GPR requirements to be ""dominated"" by the requirement; // of the TEX clause (which consumes 128 bits regs) ; ALU inst before and; // after TEX are indeed likely to consume or generate values from/for the; // TEX clause.; // Available[IDFetch].size() * 2 : GPRs required in the Fetch clause; // We assume that fetch instructions are either TnXYZW = TEX TnXYZW (need; // one GPR) or TmXYZW = TnXYZW (need 2 GPR).; // (TODO : use RegisterPressure); // If we are going too use too many GPR, we flush Fetch instruction to lower; // register pressure on 128 bits regs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:37,Energy Efficiency,schedul,schedule,37,"// There is no export clause, we can schedule one as soon as its ready",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:48,Energy Efficiency,schedul,scheduling,48,"// MI will become a KILL, don't considers it in scheduling",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:13,Energy Efficiency,schedul,scheduling,13,// Bottom up scheduling : predX must comes first,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp:28,Availability,avail,available,28,"// If there is a T_XYZW alu available, use it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:38,Energy Efficiency,Schedul,Scheduler,38,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:408,Energy Efficiency,Schedul,Scheduler,408,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:48,Integrability,Interface,Interface,48,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h:418,Integrability,interface,interface,418,"//===-- R600MachineScheduler.h - R600 Scheduler Interface -*- C++ -*-------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// R600 Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600MachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp:1200,Security,access,access,1200,"//===- R600OpenCLImageTypeLoweringPass.cpp ------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass resolves calls to OpenCL image attribute, image resource ID and; /// sampler resource ID getter functions.; ///; /// Image attributes (size and format) are expected to be passed to the kernel; /// as kernel arguments immediately following the image argument itself,; /// therefore this pass adds image size and format arguments to the kernel; /// functions in the module. The kernel functions with image arguments are; /// re-created using the new signature. The new arguments are added to the; /// kernel metadata with kernel_arg_type set to ""image_size"" or ""image_format"".; /// Note: this pass may invalidate pointers to functions.; ///; /// Resource IDs of read-only images, write-only images and samplers are; /// defined to be their index among the kernel arguments of the same; /// type and access qualifier.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp:3,Security,Validat,Validation,3,// Validation checks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OpenCLImageTypeLoweringPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:1023,Energy Efficiency,reduce,reduce,1023,"//===- R600MergeVectorRegisters.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass merges inputs of swizzeable instructions into vector sharing; /// common data and/or have enough undef subreg using swizzle abilities.; ///; /// For instance let's consider the following pseudo code :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = REG_SEQ %1, sub0, %3, sub1, undef, sub2, %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub1, sub2, sub3; ///; /// is turned into :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = INSERT_SUBREG %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub2, sub1, sub3; ///; /// This allow regalloc to reduce register pressure for vector registers and; /// to reduce MOV count.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:1081,Energy Efficiency,reduce,reduce,1081,"//===- R600MergeVectorRegisters.cpp ---------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass merges inputs of swizzeable instructions into vector sharing; /// common data and/or have enough undef subreg using swizzle abilities.; ///; /// For instance let's consider the following pseudo code :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = REG_SEQ %1, sub0, %3, sub1, undef, sub2, %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub1, sub2, sub3; ///; /// is turned into :; /// %5 = REG_SEQ %1, sub0, %2, sub1, %3, sub2, undef, sub3; /// ...; /// %7 = INSERT_SUBREG %4, sub3; /// (swizzable Inst) %7, SwizzleMask : sub0, sub2, sub1, sub3; ///; /// This allow regalloc to reduce register pressure for vector registers and; /// to reduce MOV count.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp:3,Deployability,Update,Update,3,// Update RSI,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600OptimizeVectorRegisters.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:53,Integrability,depend,dependency,53,// isLegalToPruneDependencies - Is it legal to prune dependency between SUI; // and SUJ.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:105,Integrability,depend,dependence,105,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:246,Integrability,depend,dependence,246,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:290,Integrability,depend,dependence,290,"//; // Loop over all basic blocks and remove KILL pseudo-instructions; // These instructions confuse the dependence analysis. Consider:; // D0 = ... (Insn 0); // R0 = KILL R0, D0 (Insn 1); // R0 = ... (Insn 2); // Here, Insn 1 will result in the dependence graph not emitting an output; // dependence between Insn 0 and Insn 2. This can lead to incorrect; // packetization; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:8,Energy Efficiency,schedul,scheduling,8,// Find scheduling regions and schedule / packetize each region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:31,Energy Efficiency,schedul,schedule,31,// Find scheduling regions and schedule / packetize each region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp:14,Energy Efficiency,schedul,scheduling,14,// Skip empty scheduling regions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Packetizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h:48,Integrability,Interface,Interface,48,"//===-- R600RegisterInfo.h - R600 Register Info Interface ------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600RegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h:395,Integrability,Interface,Interface,395,"//===-- R600RegisterInfo.h - R600 Register Info Interface ------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for R600RegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600RegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600Subtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp:86,Integrability,depend,depend,86,// This needs to be done before we create a new subtarget since any; // creation will depend on the TM and the code generation flags on the; // function that reside in TargetOptions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h:51,Integrability,Interface,Interface,51,"//===-- R600TargetMachine.h - AMDGPU TargetMachine Interface ----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h:420,Integrability,interface,interface,420,"//===-- R600TargetMachine.h - AMDGPU TargetMachine Interface ----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// The AMDGPU TargetMachine interface definition for hw codegen targets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp:106,Security,access,access,106,"// We allow vectorization of flat stores, even though we may need to decompose; // them later if they may access private memory. We don't have enough context; // here, and legalization can handle it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp:264,Safety,avoid,avoided,264,"// Extracts are just reads of a subregister, so are free. Inserts are; // considered free because we don't want to have any cost for scalarizing; // operations, and we don't have to copy into a different register class.; // Dynamic indexing isn't free and is best avoided.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600TargetTransformInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Security,access,accesses,20,// FLAT instruction accesses FLAT_GLBL segment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Security,access,accesses,20,// FLAT instruction accesses FLAT_SCRATCH segment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:35,Availability,mask,mask,35,// v_cmp_class_* etc. use a 10-bit mask for what operation is checked.; // The result is true if any of these tests are true.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:110,Testability,test,tests,110,// v_cmp_class_* etc. use a 10-bit mask for what operation is checked.; // The result is true if any of these tests are true.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:31,Availability,mask,masks,31,// Input operand modifiers bit-masks; // NEG and SEXT share same bit-mask because they can't be set simultaneously.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:69,Availability,mask,mask,69,// Input operand modifiers bit-masks; // NEG and SEXT share same bit-mask because they can't be set simultaneously.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:16,Modifiability,extend,extend,16,// Integer sign-extend modifier,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:26,Availability,mask,mask,26,// VOP3 dst op_sel (share mask with OP_SEL_1),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Performance,cache,cache,20,// Below are GFX12+ cache policy bits; // Temporal hint,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:20,Performance,load,load,20,// unused value for load insts; // Bits of TH for atomics,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h:3,Integrability,Message,Message,3,"// Message ID, width(4) [3:0].",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIDefines.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:2157,Performance,perform,perform,2157,"---------------------===//; //; /// \file; /// Copies from VGPR to SGPR registers are illegal and the register coalescer; /// will sometimes generate these illegal copies in situations like this:; ///; /// Register Class <vsrc> is the union of <vgpr> and <sgpr>; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// %1 <vsrc> = COPY %0 <sgpr>; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <vsrc> = PHI %1 <vsrc>, <%bb.0>, %3 <vrsc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <vsrc>; ///; ///; /// The coalescer will begin at BB0 and eliminate its copy, then the resulting; /// code will look like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <vsrc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now that the result of the PHI instruction is an SGPR, the register; /// allocator is now forced to constrain the register class of %3 to; /// <sgpr> so we end up with final code like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <sgpr> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <sgpr>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now this code contains an illegal copy from a VGPR to an SGPR.; ///; /// In order to avoid this problem, this pass searches for PHI instructions; /// which define a <vsrc> register and constrains its definition class to; /// <vgpr> if the user of the PHI's definition register is a vector instruction.; /// If the PHI's definition class is constrained to <vgpr> then the coalescer; /// will be unable to perform the COPY removal from the above example which; /// ultimately led to the creation of an illegal COPY.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:1838,Safety,avoid,avoid,1838,"---------------------===//; //; /// \file; /// Copies from VGPR to SGPR registers are illegal and the register coalescer; /// will sometimes generate these illegal copies in situations like this:; ///; /// Register Class <vsrc> is the union of <vgpr> and <sgpr>; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// %1 <vsrc> = COPY %0 <sgpr>; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <vsrc> = PHI %1 <vsrc>, <%bb.0>, %3 <vrsc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <vsrc>; ///; ///; /// The coalescer will begin at BB0 and eliminate its copy, then the resulting; /// code will look like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <vsrc> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <vsrc>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now that the result of the PHI instruction is an SGPR, the register; /// allocator is now forced to constrain the register class of %3 to; /// <sgpr> so we end up with final code like this:; ///; /// BB0:; /// %0 <sgpr> = SCALAR_INST; /// ...; /// BRANCH %cond BB1, BB2; /// BB1:; /// %2 <vgpr> = VECTOR_INST; /// %3 <sgpr> = COPY %2 <vgpr>; /// BB2:; /// %4 <sgpr> = PHI %0 <sgpr>, <%bb.0>, %3 <sgpr>, <%bb.1>; /// %5 <vgpr> = VECTOR_INST %4 <sgpr>; ///; /// Now this code contains an illegal copy from a VGPR to an SGPR.; ///; /// In order to avoid this problem, this pass searches for PHI instructions; /// which define a <vsrc> register and constrains its definition class to; /// <vgpr> if the user of the PHI's definition register is a vector instruction.; /// If the PHI's definition class is constrained to <vgpr> then the coalescer; /// will be unable to perform the COPY removal from the above example which; /// ultimately led to the creation of an illegal COPY.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:250,Security,expose,exposes,250,"// Distribute an SGPR->VGPR copy of a REG_SEQUENCE into a VGPR REG_SEQUENCE.; //; // SGPRx = ...; // SGPRy = REG_SEQUENCE SGPRx, sub0 ...; // VGPRz = COPY SGPRy; //; // ==>; //; // VGPRx = COPY SGPRx; // VGPRz = REG_SEQUENCE VGPRx, sub0; //; // This exposes immediate folding opportunities when materializing 64-bit; // immediates.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:10,Energy Efficiency,schedul,schedule,10,// Try to schedule SGPR initializations as early as possible in the MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:55,Security,access,access,55,// Some architectures allow more than one constant bus access without; // SGPR restriction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:96,Performance,perform,perform,96,// Check for trivially easy constant prop into one of the operands; // If this is the case then perform the operation now to resolve SGPR; // issue. If we don't do that here we will always insert a mov to m0; // that can't be resolved in later operand folding pass,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp:20,Testability,test,tests,20,// HACK to make MIR tests with no uses happy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFixSGPRCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:105,Safety,avoid,avoid,105,// TODO: Add heuristic that the frame index might not fit in the addressing mode; // immediate offset to avoid materializing in loops.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:123,Safety,avoid,avoid,123,"// If the literal can be inlined as-is, apply it and short-circuit the; // tests below. The main motivation for this is to avoid unintuitive; // uses of opsel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:75,Testability,test,tests,75,"// If the literal can be inlined as-is, apply it and short-circuit the; // tests below. The main motivation for this is to avoid unintuitive; // uses of opsel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:216,Deployability,pipeline,pipeline,216,"// Replace integer addition by subtraction and vice versa if it allows; // folding the immediate to an inline constant.; //; // We should only ever get here for SrcIdx == 1 due to canonicalization; // earlier in the pipeline, but we double-check here to be safe / fully; // general.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:257,Safety,safe,safe,257,"// Replace integer addition by subtraction and vice versa if it allows; // folding the immediate to an inline constant.; //; // We should only ever get here for SrcIdx == 1 due to canonicalization; // earlier in the pipeline, but we double-check here to be safe / fully; // general.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:38,Safety,avoid,avoid,38,"// Keep the old instruction around to avoid breaking iterators, but; // replace it with a dummy instruction to remove uses.; //; // FIXME: We should not invert how this pass looks at operands to avoid; // this. Should track set of foldable movs instead of looking for uses; // when looking at a use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:195,Safety,avoid,avoid,195,"// Keep the old instruction around to avoid breaking iterators, but; // replace it with a dummy instruction to remove uses.; //; // FIXME: We should not invert how this pass looks at operands to avoid; // this. Should track set of foldable movs instead of looking for uses; // when looking at a use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:113,Safety,risk,risk,113,"// If we are already folding into another operand of MI, then; // we can't commute the instruction, otherwise we risk making the; // other fold illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:139,Safety,avoid,avoided,139,"// One of operands might be an Imm operand, and OpNo may refer to it after; // the call of commuteInstruction() below. Such situations are avoided; // here explicitly as OpNo must be a register operand to be a candidate; // for memory folding.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:209,Safety,avoid,avoid,209,// Special case for s_fmac_f32 if we are trying to fold into Src0 or Src1.; // By changing into fmamk we can untie Src2.; // If folding for Src0 happens first and it is identical operand to Src1 we; // should avoid transforming into fmamk which requires commuting as it would; // cause folding into Src1 to fail later on due to wrong OpNo used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:31,Security,access,access,31,// Verify that this is a stack access.; // FIXME: Should probably use stack pseudos before frame lowering.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:81,Safety,safe,safe,81,"// A frame index will resolve to a positive constant, so it should always be; // safe to fold the addressing mode, even pre-GFX9.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:157,Availability,redundant,redundant,157,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:147,Safety,avoid,avoid,147,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:157,Safety,redund,redundant,157,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:129,Testability,log,logic,129,// Don't fold into a copy to a physical register with the same class. Doing; // so would interfere with the register coalescer's logic which would avoid; // redundant initializations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:226,Safety,avoid,avoid,226,"// That is very tricky to store a value into an AGPR. v_accvgpr_write_b32; // can only accept VGPR or inline immediate. Recreate a reg_sequence with; // its initializers right here, so we will rematerialize immediates and; // avoid copies via different reg classes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:53,Safety,avoid,avoid,53,"// Direct copy from SGPR to AGPR is not possible. To avoid creation; // of exploded copies SGPR->VGPR->AGPR in the copyPhysReg() later,; // create a copy here and track if we already have such a copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:10,Usability,simpl,simplify,10,// Try to simplify operations with a constant that may appear after instruction; // selection.; // TODO: See if a frame index with a fixed offset can fold.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:37,Usability,simpl,simpler,37,// Try to fold an instruction into a simpler one,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:3,Usability,Clear,Clear,3,// Clear kill flags.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:103,Availability,redundant,redundant,103,// FIXME: Probably shouldn't bother trying to fold if not an; // SGPR. PeepholeOptimizer can eliminate redundant VGPR->VGPR; // copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:103,Safety,redund,redundant,103,// FIXME: Probably shouldn't bother trying to fold if not an; // SGPR. PeepholeOptimizer can eliminate redundant VGPR->VGPR; // copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:19,Usability,simpl,simple,19,"// Specially track simple redefs of m0 to the same value in a block, so we; // can erase the later ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:128,Modifiability,flexible,flexible,128,"// Use of output modifiers forces VOP3 encoding for a VOP2 mac/fmac; // instruction, so we might as well convert it to the more flexible VOP3-only; // mad/fma form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:128,Modifiability,flexible,flexible,128,"// Use of output modifiers forces VOP3 encoding for a VOP2 mac/fmac; // instruction, so we might as well convert it to the more flexible VOP3-only; // mad/fma form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite the PHI's incoming values to ARC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:27,Performance,load,load,27,// Attempt to convert VGPR load to an AGPR load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:43,Performance,load,load,43,// Attempt to convert VGPR load to an AGPR load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:466,Performance,cache,caches,466,"// tryFoldPhiAGPR will aggressively try to create AGPR PHIs.; // For GFX90A and later, this is pretty much always a good thing, but for GFX908; // there's cases where it can create a lot more AGPR-AGPR copies, which are; // expensive on this architecture due to the lack of V_ACCVGPR_MOV.; //; // This function looks at all AGPR PHIs in a basic block and collects their; // operands. Then, it checks for register that are used more than once across; // all PHIs and caches them in a VGPR. This prevents ExpandPostRAPseudo from; // having to create one VGPR temporary per use, which can get very messy if; // these PHIs come from a broken-up large PHI (e.g. 32 AGPR phis, one per vector; // element).; //; // Example; // a:; // %in:agpr_256 = COPY %foo:vgpr_256; // c:; // %x:agpr_32 = ..; // b:; // %0:areg = PHI %in.sub0:agpr_32, %a, %x, %c; // %1:areg = PHI %in.sub0:agpr_32, %a, %y, %c; // %2:areg = PHI %in.sub0:agpr_32, %a, %z, %c; // =>; // a:; // %in:agpr_256 = COPY %foo:vgpr_256; // %tmp:vgpr_32 = V_ACCVGPR_READ_B32_e64 %in.sub0:agpr_32; // %tmp_agpr:agpr_32 = COPY %tmp; // c:; // %x:agpr_32 = ..; // b:; // %0:areg = PHI %tmp_agpr, %a, %x, %c; // %1:areg = PHI %tmp_agpr, %a, %y, %c; // %2:areg = PHI %tmp_agpr, %a, %z, %c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:60,Performance,cache,cache,60,"// For all (Reg, SubReg) pair that are used more than once, cache the value in; // a VGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:400,Modifiability,extend,extends,400,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:633,Modifiability,extend,extend,633,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:539,Performance,load,load,539,"//===-- SIFormMemoryClauses.cpp -------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass extends the live ranges of registers used as pointers in; /// sequences of adjacent SMEM and VMEM instructions if XNACK is enabled. A; /// load that would overwrite a pointer would require breaking the soft clause.; /// Artificially extend the live ranges of the pointer operands by adding; /// implicit-def early-clobber operands throughout the soft clause.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:16,Performance,load,load,16,"// If this is a load instruction where the result has been coalesced with an operand, then we cannot clause it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:105,Availability,alive,alive,105,// NB: skip advanceBeforeNext() call. Since all defs will be marked; // early-clobber they will all stay alive at least to the end of the; // clause. Therefor we should not decrease pressure even if load; // pointer becomes dead and could otherwise be reused for destination.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:199,Performance,load,load,199,// NB: skip advanceBeforeNext() call. Since all defs will be marked; // early-clobber they will all stay alive at least to the end of the; // clause. Therefor we should not decrease pressure even if load; // pointer becomes dead and could otherwise be reused for destination.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:56,Availability,mask,masks,56,// Collect register defs and uses along with their lane masks and states.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:179,Deployability,update,updated,179,"// Check register def/use conflicts, occupancy limits and collect def/use maps.; // Return true if instruction can be bundled with previous. If it cannot; // def/use maps are not updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:5,Performance,load,load,5,// A load from pointer which was loaded inside the same bundle is an; // impossible clause because we will need to write and read the same; // register inside. In this case processRegUses will return false.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:33,Performance,load,loaded,33,// A load from pointer which was loaded inside the same bundle is an; // impossible clause because we will need to write and read the same; // register inside. In this case processRegUses will return false.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:43,Modifiability,extend,extend,43,// Collect the register operands we should extend the live ranges of.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:19,Modifiability,extend,extend,19,"// We only want to extend the live ranges of used registers. If they; // already have existing uses beyond the bundle, we don't need the kill.; //; // It's possible all of the use registers were already live past the; // bundle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:76,Availability,avail,available,76,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:114,Availability,failure,failure,114,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:292,Availability,redundant,redundant,292,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:250,Energy Efficiency,reduce,reduce,250,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:161,Modifiability,Rewrite,Rewrite,161,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:286,Safety,avoid,avoid,286,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:292,Safety,redund,redundant,292,"// Find a register matching \p RC from \p LiveUnits which is unused and; // available throughout the function. On failure, returns AMDGPU::NoRegister.; // TODO: Rewrite the loop here to iterate over MCRegUnits instead of; // MCRegisters. This should reduce the number of iterations and avoid redundant; // checking.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:195,Integrability,wrap,wrapping,195,"// Find a scratch register that we can use in the prologue. We avoid using; // callee-save registers since they may appear to be free when this is called; // from canUseAsPrologue (during shrink wrapping), but then no longer be free; // when this is called from emitPrologue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:63,Safety,avoid,avoid,63,"// Find a scratch register that we can use in the prologue. We avoid using; // callee-save registers since they may appear to be free when this is called; // from canUseAsPrologue (during shrink wrapping), but then no longer be free; // when this is called from emitPrologue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:303,Safety,detect,detect,303,"// We don't need this if we only have spills since there is no user facing; // scratch.; // TODO: If we know we don't have flat instructions earlier, we can omit; // this from the input registers.; //; // TODO: We only need to know if we access scratch space through a flat; // pointer. Because we only detect if flat instructions are used at all,; // this will be used more often than necessary on VI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:238,Security,access,access,238,"// We don't need this if we only have spills since there is no user facing; // scratch.; // TODO: If we know we don't have flat instructions earlier, we can omit; // this from the input registers.; //; // TODO: We only need to know if we access scratch space through a flat; // pointer. Because we only detect if flat instructions are used at all,; // this will be used more often than necessary on VI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:22,Performance,load,load,22,// Find unused reg to load flat scratch init into,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:3,Availability,Mask,Mask,3,// Mask the offset in [47:0] of the descriptor,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:9,Availability,down,down,9,// Shift down registers reserved for the scratch RSRC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:53,Availability,down,down,53,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:135,Safety,safe,safer,135,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:317,Security,access,access,317,// We reserved the last registers for this. Shift it down to the end of those; // which were actually used.; //; // FIXME: It might be safer to use a pseudoregister before replacement.; // FIXME: We should be able to eliminate unused input registers. We only; // cannot do this for the resources required for scratch access. For now we; // skip over user SGPRs and may leave unused holes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:249,Energy Efficiency,allocate,allocateSystemSGPRs,249,"// We found the SRSRC first because it needs four registers and has an; // alignment requirement. If the SRSRC that we found is clobbering with; // the scratch wave offset, which may be in a fixed SGPR or a free SGPR; // chosen by SITargetLowering::allocateSystemSGPRs, COPY the scratch; // wave offset to a free SGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:78,Deployability,update,update,78,"// Add the scratch wave offset into the scratch RSRC.; //; // We only want to update the first 48 bits, which is the base address; // pointer, without touching the adjacent 16 bits of flags. We know this add; // cannot carry-out from bit 47, otherwise the scratch allocation would be; // impossible to fit in the 48-bit global address space.; //; // TODO: Evaluate if it is better to just construct an SRD using the flat; // scratch init and some constants rather than update the one we are passed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:469,Deployability,update,update,469,"// Add the scratch wave offset into the scratch RSRC.; //; // We only want to update the first 48 bits, which is the base address; // pointer, without touching the adjacent 16 bits of flags. We know this add; // cannot carry-out from bit 47, otherwise the scratch allocation would be; // impossible to fit in the 48-bit global address space.; //; // TODO: Evaluate if it is better to just construct an SRD using the flat; // scratch init and some constants rather than update the one we are passed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:66,Safety,avoid,avoids,66,// Copy FP to the scratch register now and emit the CFI entry. It avoids; // the extra FP copy needed in the other two cases when FP is spilled to; // memory or to a VGPR lane.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:151,Energy Efficiency,allocate,allocated,151,"// If we need a base pointer, set it up here. It's whatever the value of; // the stack pointer is at this point. Any variable size objects will be; // allocated after this, so we can still use the base pointer to reference; // the incoming arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:117,Modifiability,variab,variable,117,"// If we need a base pointer, set it up here. It's whatever the value of; // the stack pointer is at this point. Any variable size objects will be; // allocated after this, so we can still use the base pointer to reference; // the incoming arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,"// Allocate spill slots for WWM reserved VGPRs.; // For chain functions, we only need to do this if we have calls to; // llvm.amdgcn.cs.chain.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:125,Deployability,update,update,125,"// FIXME: The dead frame indices are replaced with a null register from; // the debug value instructions. We should instead, update it with the; // correct register value. But not sure the register value alone is",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:31,Energy Efficiency,allocate,allocated,31,"// At this point we've already allocated all spilled SGPRs to VGPRs if we; // can. Any remaining SGPR spills will go to memory, so move them back to the; // default stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:37,Availability,redundant,redundant,37,"// FIXME: The other checks should be redundant with allStackObjectsAreDead,; // but currently hasNonSpillStackObjects is set only from source; // allocas. Stack temps produced from legalization are not counted currently.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:37,Safety,redund,redundant,37,"// FIXME: The other checks should be redundant with allStackObjectsAreDead,; // but currently hasNonSpillStackObjects is set only from source; // allocas. Stack temps produced from legalization are not counted currently.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:48,Availability,avail,available,48,"// On gfx908, we had initially reserved highest available VGPR for AGPR; // copy. Now since we are done with RA, check if there exist an unused VGPR; // which is lower than the eariler reserved VGPR before RA. If one exist,; // use it for AGPR copy instead of one reserved before RA.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:130,Safety,avoid,avoid,130,// Reserve this newly identified VGPR (for AGPR copy); // reserved registers should already be frozen at this point; // so we can avoid calling MRI.freezeReservedRegs and just use; // MRI.reserveReg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:36,Availability,avail,available,36,"// We initally reserved the highest available SGPR pair for long branches; // now, after RA, we shift down to a lower unused one if one exists",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:102,Availability,down,down,102,"// We initally reserved the highest available SGPR pair for long branches; // now, after RA, we shift down to a lower unused one if one exists",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:151,Availability,down,down,151,"// If LongBranchReservedReg is null then we didn't find a long branch; // and never reserved a register to begin with so there is nothing to; // shift down. Then if UnusedLowSGPR is null, there isn't available lower; // register to use so just keep the original one we set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:200,Availability,avail,available,200,"// If LongBranchReservedReg is null then we didn't find a long branch; // and never reserved a register to begin with so there is nothing to; // shift down. Then if UnusedLowSGPR is null, there isn't available lower; // register to use so just keep the original one we set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:138,Safety,predict,predict,138,"// hasFP only knows about stack objects that already exist. We're now; // determining the stack slots that will be created, so we have to predict; // them. Stack objects force FP usage with calls.; //; // Note a new VGPR CSR may be introduced if one is used for the spill, but we; // don't want to report it here.; //; // FIXME: Is this really hasReservedCallFrame?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:360,Deployability,pipeline,pipeline,360,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:323,Energy Efficiency,allocate,allocateWWMSpill,323,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:405,Energy Efficiency,allocate,allocated,405,// WRITELANE instructions used for SGPR spills can overwrite the inactive; // lanes of VGPRs and callee must spill and restore them even if they are; // marked Caller-saved.; // TODO: Handle this elsewhere at an early point. Walking through all MBBs; // here would be a bad heuristic. A better way should be by calling; // allocateWWMSpill during the regalloc pipeline whenever a physical; // register is allocated for the intended virtual registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:109,Performance,load,loads,109,// Do not save AGPRs prior to GFX90A because there was no easy way to do so.; // In gfx908 there was do AGPR loads and stores and thus spilling also; // require a temporary VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:267,Energy Efficiency,allocate,allocate,267,"// We have to anticipate introducing CSR VGPR spills or spill of caller; // save VGPR reserved for SGPR spills as we now always create stack entry; // for it, if we don't have any stack objects already, since we require a FP; // if there is a call and stack. We will allocate a VGPR for SGPR spills if; // there are any SGPR spills. Whether they are CSR spills or otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:43,Energy Efficiency,allocate,allocated,43,"// We need the emergency stack slots to be allocated in range of the; // MUBUF/flat scratch immediate offset from the base register, so assign these; // first at the incoming SP position.; //; // TODO: We could try sorting the objects to find a hole in the first bytes; // rather than allocating as close to possible. This could save a lot of space; // on frames with alignment requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:25,Energy Efficiency,reduce,reduced,25,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:216,Integrability,depend,depending,216,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:261,Testability,log,logically,261,"// This is essentially a reduced version of hasFP for entry functions. Since the; // stack pointer is known 0 on entry to kernels, we never really need an FP; // register. We may need to initialize the stack pointer depending on the frame; // properties, which logically overlaps many of the cases where an ordinary; // function would require an FP.; // Also used for chain functions. While not technically entry functions, chain; // functions may need to set up a stack pointer in some situations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:101,Modifiability,variab,variable,101,"// We still need to initialize the SP if we're doing anything weird that; // references the SP, like variable sized stack objects.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:801,Energy Efficiency,schedul,scheduler,801,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:465,Performance,load,load,465,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:492,Performance,cache,cache,492,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:564,Safety,detect,detected,564,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:670,Safety,detect,detection,670,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:1048,Testability,log,logically,1048,"//===- SIInsertHardClauses.cpp - Insert Hard Clauses ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert s_clause instructions to form hard clauses.; ///; /// Clausing load instructions can give cache coherency benefits. Before gfx10,; /// the hardware automatically detected ""soft clauses"", which were sequences of; /// memory instructions of the same type. In gfx10 this detection was removed,; /// and the s_clause instruction was introduced to explicitly mark ""hard; /// clauses"".; ///; /// It's the scheduler's job to form the clauses by putting similar memory; /// instructions next to each other. Our job is just to insert an s_clause; /// instruction to mark the start of each clause.; ///; /// Note that hard clauses are very similar to, but logically distinct from, the; /// groups of instructions that have to be restartable when XNACK is enabled.; /// The rules are slightly different in each case. For example an s_nop; /// instruction breaks a restartable group, but can appear in the middle of a; /// hard clause. (Before gfx10 there wasn't a distinction, and both were called; /// ""soft clauses"" or just ""clauses"".); ///; /// The SIFormMemoryClauses pass and GCNHazardRecognizer deal with restartable; /// groups, not hard clauses.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:33,Security,access,access,33,// Common:; // Instructions that access LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:81,Integrability,message,message,81,"// Instructions that are not allowed in a hard clause: SALU, export, branch,; // message, GDS, s_waitcnt and anything else not mentioned above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:160,Safety,safe,safe,160,"// Don't form VALU clauses. It's not clear what benefit they give, if any.; // In practice s_nop is the only internal instruction we're likely to see.; // It's safe to treat the rest as illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:37,Usability,clear,clear,37,"// Don't form VALU clauses. It's not clear what benefit they give, if any.; // In practice s_nop is the only internal instruction we're likely to see.; // It's safe to treat the rest as illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:134,Energy Efficiency,schedul,scheduler,134,"// Note that we lie to shouldClusterMemOps about the size of the; // cluster. When shouldClusterMemOps is called from the machine; // scheduler it limits the size of the cluster to avoid increasing; // register pressure too much, but this pass runs after register; // allocation so there is no need for that kind of limit.; // We also lie about the Offset and OffsetIsScalable parameters,; // as they aren't used in the SIInstrInfo implementation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:181,Safety,avoid,avoid,181,"// Note that we lie to shouldClusterMemOps about the size of the; // cluster. When shouldClusterMemOps is called from the machine; // scheduler it limits the size of the cluster to avoid increasing; // register pressure too much, but this pass runs after register; // allocation so there is no need for that kind of limit.; // We also lie about the Offset and OffsetIsScalable parameters,; // as they aren't used in the SIInstrInfo implementation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp:3,Modifiability,Extend,Extend,3,// Extend the current clause.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertHardClauses.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:575,Security,access,access,575,"//===- SIInsertWaitcnts.cpp - Insert Wait Instructions --------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Insert wait instructions for memory reads and writes.; ///; /// Memory reads and writes are issued asynchronously, so we need to insert; /// S_WAITCNT instructions when we want to access any of their results or; /// overwrite any register that's used asynchronously.; ///; /// TODO: This pass currently keeps one timeline per hardware counter. A more; /// finely-grained approach that keeps one timeline per event type could; /// sometimes get away with generating weaker s_waitcnt instructions. For; /// example, when both SMEM and LDS are in flight and we need to wait for; /// the i-th-last LDS instruction, then an lgkmcnt(i) is actually sufficient,; /// but the pass will currently generate a conservative lgkmcnt(0) because; /// multiple event types are in flight.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:8,Integrability,message,message,8,// send message,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:308,Performance,load,load,308,// The mapping is:; // 0 .. SQ_MAX_PGM_VGPRS-1 real VGPRs; // SQ_MAX_PGM_VGPRS .. NUM_ALL_VGPRS-1 extra VGPR-like slots; // NUM_ALL_VGPRS .. NUM_ALL_VGPRS+SQ_MAX_PGM_SGPRS-1 real SGPRs; // We reserve a fixed number of VGPR slots in the scoring tables for; // special tokens like SCMEM_LDS (needed for buffer load to LDS).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:234,Deployability,update,updated,234,// Reserved slots for DS.; // Artificial register slots to track LDS writes into specific LDS locations; // if a location is known. When slots are exhausted or location is; // unknown use the first slot. The first slot is also always updated in; // addition to known location's slot to properly generate waits if dependent; // instruction's location is unknown.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:313,Integrability,depend,dependent,313,// Reserved slots for DS.; // Artificial register slots to track LDS writes into specific LDS locations; // if a location is known. When slots are exhausted or location is; // unknown use the first slot. The first slot is also always updated in; // addition to known location's slot to properly generate waits if dependent; // instruction's location is unknown.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:56,Availability,mask,masks,56,// Mapping from event to counter according to the table masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:97,Energy Efficiency,efficient,efficient,97,// wait_cnt scores for every vgpr.; // Keep track of the VgprUB and SgprUB to make merge at join efficient.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:22,Testability,log,logic,22,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:340,Testability,log,logic,340,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:352,Usability,simpl,simpler,352,"// This abstracts the logic for generating and updating S_WAIT* instructions; // away from the analysis that determines where they are needed. This was; // done because the set of counters and instructions for waiting on them; // underwent a major shift with gfx12, sufficiently so that having this; // abstraction allows the main analysis logic to be simpler than it would; // otherwise have had to become.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:117,Deployability,update,updated,117,"// Edits an existing sequence of wait count instructions according; // to an incoming Waitcnt value, which is itself updated to reflect; // any new wait count instructions which may need to be generated by; // WaitcntGenerator::createNewWaitcnt(). It will return true if any edits; // were made.; //; // This editing will usually be merely updated operands, but it may also; // delete instructions if the incoming Wait value indicates they are not; // needed. It may also remove existing instructions for which a wait; // is needed if it can be determined that it is better to generate new; // instructions later, as can happen on gfx12.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:340,Deployability,update,updated,340,"// Edits an existing sequence of wait count instructions according; // to an incoming Waitcnt value, which is itself updated to reflect; // any new wait count instructions which may need to be generated by; // WaitcntGenerator::createNewWaitcnt(). It will return true if any edits; // were made.; //; // This editing will usually be merely updated operands, but it may also; // delete instructions if the incoming Wait value indicates they are not; // needed. It may also remove existing instructions for which a wait; // is needed if it can be determined that it is better to generate new; // instructions later, as can happen on gfx12.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:27,Availability,mask,masks,27,// Returns an array of bit masks which can be used to map values in; // WaitEventType to corresponding counter values in InstCounterType.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:75,Integrability,message,message,75,// S_ENDPGM instructions before which we should insert a DEALLOC_VGPRS; // message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:13,Security,access,access,13,// Maps VMEM access types to their corresponding WaitEventType.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:11,Performance,load,loads,11,"// LDS DMA loads are also stores, but on the LDS side. On the VMEM side; // these should use VM_CNT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:37,Security,access,access,37,// FLAT and SCRATCH instructions may access scratch. Other VMEM; // instructions do not.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:40,Deployability,update,update,40,// PendingEvents and ScoreUB need to be update regardless if this event; // changes the score of a register or not.; // Examples including vm_cnt when buffer-store or lgkm_cnt when send-message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:186,Integrability,message,message,186,// PendingEvents and ScoreUB need to be update regardless if this event; // changes the score of a register or not.; // Examples including vm_cnt when buffer-store or lgkm_cnt when send-message.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:112,Deployability,patch,patching,112,"// For export the destination registers are really temps that; // can be used as the actual source after export patching, so; // we need to treat them like sources and set the EXP_CNT; // score.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,update,updateVMCntOnly,3,"// updateVMCntOnly should only leave us with VGPRs; // MUBUF, MTBUF, MIMG, FlatGlobal, and FlatScratch only have VGPR/AGPR; // defs. That's required for a sane index into `VgprMemTypes` below",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:100,Performance,load,load,100,// MUBUF and FLAT LDS DMA operations need a wait on vmcnt before LDS; // written can be accessed. A load from LDS to VMEM does not need a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Security,access,accessed,88,// MUBUF and FLAT LDS DMA operations need a wait on vmcnt before LDS; // written can be accessed. A load from LDS to VMEM does not need a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:175,Availability,avail,available,175,// Alias scope information gives a way to definitely identify an; // original memory object and practically produced in the module LDS; // lowering pass. If there is no scope available we will not be able; // to disambiguate LDS aliasing as after the module lowering all LDS; // is squashed into a single big object. Do not attempt to use one of; // the limited LDSDMAStores for something we will not be able to use; // anyway.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:51,Availability,redundant,redundant,51,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:51,Safety,redund,redundant,51,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:4,Usability,Simpl,Simplify,4,"/// Simplify the waitcnt, in the sense of removing redundant counts, and return; /// whether a waitcnt instruction is needed at all.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:211,Availability,redundant,redundant,211,"// The number of outstanding events for this type, T, can be calculated; // as (UB - LB). If the current Count is greater than or equal to the number; // of outstanding events, then the wait for this counter is redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:211,Safety,redund,redundant,211,"// The number of outstanding events for this type, T, can be calculated; // as (UB - LB). If the current Count is greater than or equal to the number; // of outstanding events, then the wait for this counter is redundant.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:35,Safety,avoid,avoid,35,// If a counter has been maxed out avoid overflow by waiting for; // MAX(CounterType) - 1 instead.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,Update,Update,3,"// Update required wait count. If this is a soft waitcnt (= it was added; // by an earlier pass), it may be entirely removed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:3,Deployability,Update,Update,3,"// Update required wait count. If this is a soft waitcnt (= it was added; // by an earlier pass), it may be entirely removed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:353,Availability,redundant,redundant,353,"// Only keep an S_WAIT_LOADCNT_DSCNT if both counters actually need; // to be waited for. Otherwise, let the instruction be deleted so; // the appropriate single counter wait instruction can be inserted; // instead, when new S_WAIT_*CNT instructions are inserted by; // createNewWaitcnt(). As a side effect, resetting the wait counts will; // cause any redundant S_WAIT_LOADCNT or S_WAIT_DSCNT to be removed by; // the loop below that deals with single counter instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:353,Safety,redund,redundant,353,"// Only keep an S_WAIT_LOADCNT_DSCNT if both counters actually need; // to be waited for. Otherwise, let the instruction be deleted so; // the appropriate single counter wait instruction can be inserted; // instead, when new S_WAIT_*CNT instructions are inserted by; // createNewWaitcnt(). As a side effect, resetting the wait counts will; // cause any redundant S_WAIT_LOADCNT or S_WAIT_DSCNT to be removed by; // the loop below that deals with single counter instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:44,Performance,LOAD,LOADcnt,44,"// If it's known that both DScnt and either LOADcnt or STOREcnt (but not; // both) need to be waited for, ensure that there are no existing; // individual wait count instructions for these.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:276,Security,access,access,276,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:370,Security,access,access,370,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:240,Usability,simpl,simply,240,"/// Generate s_waitcnt instruction to be placed before cur_Inst.; /// Instructions of a given type are returned in order,; /// but instructions of different types can complete out of order.; /// We rely on this in-order completion; /// and simply assign a score to the memory access instructions.; /// We keep track of the active ""score bracket"" to determine; /// if an access of a memory read requires an s_waitcnt; /// and if so what the value of each counter is.; /// The ""score bracket"" is bound by the lower bound and upper bound; /// scores (*_score_LB and *_score_ub respectively).; /// If FlushVmCnt is true, that means that we want to generate a s_waitcnt to; /// flush the vmcnt counter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:124,Testability,test,tests,124,"// FIXME: This should have already been handled by the memory legalizer.; // Removing this currently doesn't affect any lit tests, but we need to; // verify that nothing was relying on this. The number of buffer invalidates; // being handled here should not be expanded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:148,Integrability,rout,routines,148,// All waits must be resolved at call return.; // NOTE: this could be improved with knowledge of all call sites or; // with knowledge of the called routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:156,Deployability,release,release,156,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:130,Integrability,message,message,130,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:223,Safety,safe,safe,223,"// Identify S_ENDPGM instructions which may have to wait for outstanding VMEM; // stores. In this case it can be useful to send a message to explicitly; // release all VGPRs before the stores have completed, but it is only safe to; // do this if:; // * there are no outstanding scratch stores; // * we are not in Dynamic VGPR mode",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Testability,log,logic,33,// TODO: the following blocks of logic when we have fence.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:47,Performance,load,load,47,// LDS may have to wait for VMcnt after buffer load to LDS,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:50,Availability,mask,mask,50,// Export & GDS instructions do not read the EXEC mask until after the export; // is granted (which can occur well after the instruction is issued).; // The shader program must flush all EXP operations on the export-count; // before overwriting the EXEC mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:254,Availability,mask,mask,254,// Export & GDS instructions do not read the EXEC mask until after the export; // is granted (which can occur well after the instruction is issued).; // The shader program must flush all EXP operations on the export-count; // before overwriting the EXEC mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:184,Integrability,depend,dependency,184,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:130,Performance,load,load,130,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:151,Performance,load,load,151,// The function is going to insert a wait on everything in its prolog.; // This still needs to be careful if the call target is a load (e.g. a GOT; // load). We also need to check WAW dependency with saved PC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:304,Performance,load,load,304,"// FIXME: Should not be relying on memoperands.; // Look at the source operands of every instruction to see if; // any of them results from a previous memory operation that affects; // its current usage. If so, an s_waitcnt instruction needs to be; // emitted.; // If the source operand was defined by a load, add the s_waitcnt; // instruction.; //; // Two cases are handled for destination operands:; // 1) If the destination operand was defined by a load, add the s_waitcnt; // instruction to guarantee the right WAW order.; // 2) If a destination operand that was used by a recent export/store ins,; // add s_waitcnt on exp_cnt to guarantee the WAR order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:452,Performance,load,load,452,"// FIXME: Should not be relying on memoperands.; // Look at the source operands of every instruction to see if; // any of them results from a previous memory operation that affects; // its current usage. If so, an s_waitcnt instruction needs to be; // emitted.; // If the source operand was defined by a load, add the s_waitcnt; // instruction.; //; // Two cases are handled for destination operands:; // 1) If the destination operand was defined by a load, add the s_waitcnt; // instruction to guarantee the right WAW order.; // 2) If a destination operand that was used by a recent export/store ins,; // add s_waitcnt on exp_cnt to guarantee the WAR order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:26,Performance,load,load,26,// No need to wait before load from VMEM to LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Energy Efficiency,schedul,scheduler,88,"// TODO: Remove this work-around, enable the assert for Bug 457939; // after fixing the scheduler. Also, the Shader Compiler code is; // independent of target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:45,Testability,assert,assert,45,"// TODO: Remove this work-around, enable the assert for Bug 457939; // after fixing the scheduler. Also, the Shader Compiler code is; // independent of target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:30,Performance,LOAD,LOADcnt,30,"// Add a waitcnt to flush the LOADcnt, SAMPLEcnt and BVHcnt counters at the; // end of the given block if needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Availability,redundant,redundant,88,// Try to merge the required wait with preexisting waitcnt instructions.; // Also erase redundant waitcnt.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:88,Safety,redund,redundant,88,// Try to merge the required wait with preexisting waitcnt instructions.; // Also erase redundant waitcnt.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access VMEM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:300,Usability,simpl,simply,300,"// See if any memory operand specifies an address space that involves VMEM.; // Flat operations only supported FLAT, LOCAL (LDS), or address spaces; // involving VMEM such as GLOBAL, CONSTANT, PRIVATE (SCRATCH), etc. The REGION; // (GDS) address space is not supported by flat operations. Therefore, simply; // return true unless only the LDS address space is found.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access LDS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:31,Security,access,access,31,// SCRATCH instructions always access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:29,Security,access,access,29,// GLOBAL instructions never access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:89,Security,access,access,89,// If there are no memory operands then conservatively assume the flat; // operation may access scratch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:81,Deployability,update,update,81,"// Now look at the instruction opcode. If it is a memory access; // instruction, update the upper-bound of the appropriate counter's; // bracket and the destination operand scores.; // TODO: Use the (TSFlags & SIInstrFlags::DS_CNT) property everywhere.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:57,Security,access,access,57,"// Now look at the instruction opcode. If it is a memory access; // instruction, update the upper-bound of the appropriate counter's; // bracket and the destination operand scores.; // TODO: Use the (TSFlags & SIInstrFlags::DS_CNT) property everywhere.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:32,Security,access,access,32,// A Flat memory operation must access at least one address space.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:183,Integrability,depend,dependency,183,"// This is a flat memory operation that access both VMEM and LDS, so note it; // - it will require that both the VM and LGKM be flushed to zero if it is; // pending when a VM or LGKM dependency occurs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:40,Security,access,access,40,"// This is a flat memory operation that access both VMEM and LDS, so note it; // - it will require that both the VM and LGKM be flushed to zero if it is; // pending when a VM or LGKM dependency occurs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:111,Performance,load,load,111,// vccz could be incorrect at a basic block boundary if a predecessor wrote; // to vcc and then issued an smem load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:49,Deployability,update,update,49,"// Up to gfx9, writes to vcc_lo and vcc_hi don't update vccz.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:96,Safety,detect,detect,96,"// There is a hardware bug on CI/SI where SMRD instruction may corrupt; // vccz bit, so when we detect that an instruction may read from a; // corrupt vccz bit, we need to:; // 1. Insert s_waitcnt lgkm(0) to wait for all outstanding SMRD; // operations to complete.; // 2. Restore the correct value of vccz by writing the current value; // of vcc back to vcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:31,Performance,load,loads,31,"// No need to handle invariant loads when avoiding WAR conflicts, as; // there cannot be a vector store to the same memory location.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:42,Safety,avoid,avoiding,42,"// No need to handle invariant loads when avoiding WAR conflicts, as; // there cannot be a vector store to the same memory location.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:50,Energy Efficiency,schedul,scheduler,50,// TODO: Remove this work-around after fixing the scheduler and enable the; // assert above.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:79,Testability,assert,assert,79,// TODO: Remove this work-around after fixing the scheduler and enable the; // assert above.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:80,Deployability,update,updated,80,"// Restore the vccz bit. Any time a value is written to vcc, the vcc; // bit is updated, so we can restore the bit by reading the value of; // vcc and then writing it back to the register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:194,Performance,load,load,194,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:261,Performance,load,loaded,261,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:364,Performance,load,load,364,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:381,Performance,load,loaded,381,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:483,Performance,load,loaded,483,"// Return true if it is better to flush the vmcnt counter in the preheader of; // the given loop. We currently decide to flush in two situations:; // 1. The loop contains vmem store(s), no vmem load and at least one use of a; // vgpr containing a value that is loaded outside of the loop. (Only on; // targets with no vscnt counter).; // 2. The loop contains vmem load(s), but the loaded values are not used in the; // loop, and at least one use of a vgpr containing a value that is loaded; // outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Performance,load,loaded,33,"// If we find a register that is loaded inside the loop, 1. and 2.; // are invalidated and we can exit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:87,Performance,load,loaded,87,"// If at least one of Op's registers is in the score brackets, the; // value is likely loaded outside of the loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:8,Performance,load,load,8,// VMem load vgpr def,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,Performance,load,loaded,33,"// If we find a register that is loaded inside the loop, 1. and 2.; // are invalidated and we can exit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:212,Energy Efficiency,schedul,schedule,212,// Wait for any outstanding memory operations that the input registers may; // depend on. We can't track them and it's better to do the wait after the; // costly call sequence.; // TODO: Could insert earlier and schedule more liberally with operations; // that only use caller preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:79,Integrability,depend,depend,79,// Wait for any outstanding memory operations that the input registers may; // depend on. We can't track them and it's better to do the wait after the; // costly call sequence.; // TODO: Could insert earlier and schedule more liberally with operations; // that only use caller preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:34,Performance,cache,cache,34,"// If scalar writes are used, the cache must be flushed or else the next; // wave to reuse the same scratch memory can be clobbered.; //; // Insert s_dcache_wb at wave termination points if there were any scalar; // stores, and only if the cache hasn't already been flushed. This could; // be improved by looking across blocks for flushes in postdominating; // blocks from the stores but an explicitly requested flush is probably; // very rare.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:240,Performance,cache,cache,240,"// If scalar writes are used, the cache must be flushed or else the next; // wave to reuse the same scratch memory can be clobbered.; //; // Insert s_dcache_wb at wave termination points if there were any scalar; // stores, and only if the cache hasn't already been flushed. This could; // be improved by looking across blocks for flushes in postdominating; // blocks from the stores but an explicitly requested flush is probably; // very rare.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:23,Integrability,message,messages,23,// Insert DEALLOC_VGPR messages before previously identified S_ENDPGM; // instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:149,Testability,test,tests,149,// Must be at least 4 to be able to branch over minimum unconditional branch; // code. This is only for making it possible to write reasonably small tests for; // long branches.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:390,Safety,safe,safe,390,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:180,Testability,log,logic,180,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:274,Testability,log,logic,274,"// Normally VALU use of exec would block the rematerialization, but that; // is OK in this case to have an implicit exec read as all VALU do.; // We really want all of the generic logic for this except for this.; // Another potential implicit use is mode register. The core logic of; // the RA will not attempt rematerialization if mode is set anywhere; // in the function, otherwise it is safe since mode is not changed.; // There is difference to generic method which does not allow; // rematerialization if there are virtual register uses. We allow this,; // therefore this method includes SOP instructions as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:59,Integrability,depend,depends,59,// Returns true if the scalar result of a VALU instruction depends on exec.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:42,Availability,mask,masked,42,// Ignore comparisons which are only used masked with exec.; // This allows some hoisting/sinking of VALU comparisons.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:34,Availability,mask,mask,34,// Allow sinking if MI edits lane mask (divergent i1 in sgpr).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:31,Performance,load,loads,31,// Make sure both are actually loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:48,Performance,load,load,48,// A mayLoad instruction without a def is not a load. Likely a prefetch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:36,Usability,simpl,simplicity,36,// Skip read2 / write2 variants for simplicity.; // TODO: We should report true if the used offsets are adjacent (excluded; // st64 versions).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:32,Performance,load,loads,32,"// XXX - be careful of dataless loads; // getNamedOperandIdx returns the index for MachineInstrs. Since they; // include the output in the operand list, but SDNodes don't, we need to; // subtract the index by one.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Performance,cache,cache,17,// Skip time and cache invalidation instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:23,Security,access,access,23,// MUBUF and MTBUF can access the same addresses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:90,Performance,load,load,90,// The 2 offset instructions use offset0 and offset1 instead. We can treat; // these as a load with a single offset if the 2 offsets are consecutive.; // We will use this for some partially aligned loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:198,Performance,load,loads,198,// The 2 offset instructions use offset0 and offset1 instead. We can treat; // these as a load with a single offset if the 2 offsets are consecutive.; // We will use this for some partially aligned loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Security,access,access,139,"// Only examine the first ""base"" operand of each instruction, on the; // assumption that it represents the real base address of the memory access.; // Other operands are typically offsets or indices from this base address.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:80,Performance,load,loaded,80,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:207,Performance,perform,performance,207,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:335,Performance,load,loads,335,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:377,Performance,load,loads,377,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:457,Performance,Load,LoadSize,457,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:481,Performance,Load,LoadSize,481,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:534,Performance,Load,LoadSize,534,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:587,Performance,Load,LoadSize,587,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:642,Performance,Load,LoadSize,642,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:691,Performance,Load,LoadSize,691,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:15,Safety,avoid,avoid,15,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:292,Safety,avoid,avoids,292,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:351,Safety,avoid,avoids,351,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:313,Energy Efficiency,schedul,schedule,313,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:63,Performance,load,load,63,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:95,Performance,load,loads,95,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:245,Performance,Load,Loads,245,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:416,Performance,load,loads,416,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:89,Energy Efficiency,schedul,schedule,89,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:27,Performance,load,loads,27,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:114,Performance,cache,cacheline,114,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:47,Safety,avoid,avoid,47,"// First try to find defining accvgpr_write to avoid temporary registers.; // In the case of copies of overlapping AGPRs, we conservatively do not; // reuse previous accvgpr_writes. Otherwise, we may incorrectly pick up; // an accvgpr_write used for this same copy due to implicit-defs",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:100,Safety,safe,safe,100,// Check that register source operand is not clobbered before MI.; // Immediate operands are always safe to propagate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:33,Energy Efficiency,allocate,allocated,33,// Registers in the sequence are allocated contiguously so we can just; // use register number to pick one of three round-robin temps.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:44,Safety,avoid,avoid,44,// FIXME: Pass should maintain scavenger to avoid scan through the block on; // every AGPR spill.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:81,Safety,hazard,hazard,81,"// SI_RETURN_TO_EPILOG is a fallthrough to code outside of the function. The; // hazard, even if one exist, won't really be visible. Should we handle it?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:26,Performance,optimiz,optimize,26,// FIXME: We may possibly optimize the COPY once we find ways to make LLVM; // optimizations (mainly Register Coalescer) aware of WWM register liveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:79,Performance,optimiz,optimizations,79,// FIXME: We may possibly optimize the COPY once we find ways to make LLVM; // optimizations (mainly Register Coalescer) aware of WWM register liveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:80,Energy Efficiency,schedul,scheduler,80,// Create a bundle so these instructions won't be re-ordered by the; // post-RA scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Modifiability,variab,variable,139,"// What we want here is an offset from the value returned by s_getpc (which; // is the address of the s_add_u32 instruction) to the global variable, but; // since the encoding of $symbol starts 4 bytes after the start of the; // s_add_u32 instruction, we end up with an offset that is 4 bytes too; // small. This requires us to add 4 to the global variable offset in order; // to compute the correct address. Similarly for the s_addc_u32 instruction,; // the encoding of $symbol starts 12 bytes after the start of the s_add_u32; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:348,Modifiability,variab,variable,348,"// What we want here is an offset from the value returned by s_getpc (which; // is the address of the s_add_u32 instruction) to the global variable, but; // since the encoding of $symbol starts 4 bytes after the start of the; // s_add_u32 instruction, we end up with an offset that is 4 bytes too; // small. This requires us to add 4 to the global variable offset in order; // to compute the correct address. Similarly for the s_addc_u32 instruction,; // the encoding of $symbol starts 12 bytes after the start of the s_add_u32; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:38,Modifiability,extend,extend,38,"// Fix up hardware that does not sign-extend the 48-bit PC value by; // inserting: s_sext_i32_i16 reghi, reghi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:38,Modifiability,extend,extend,38,"// Fix up hardware that does not sign-extend the 48-bit PC value by; // inserting: s_sext_i32_i16 dsthi, dsthi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:59,Deployability,update,updated,59,"// Use a smaller load with the desired size, possibly with updated offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Performance,load,load,17,"// Use a smaller load with the desired size, possibly with updated offset.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:65,Safety,avoid,avoid,65,// If we've previously reserved a register for long branches; // avoid running the scavenger and just use those registers,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:25,Availability,avail,available,25,"// 64-bit select is only available for SALU.; // TODO: Split 96-bit into 64-bit and 32-bit, not 3x 32-bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:56,Modifiability,rewrite,rewrite,56,"// V_FMAMK_F16_t16 takes VGPR_32_Lo128 operands, so the rewrite; // would also require restricting their register classes. For now; // just bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:56,Modifiability,rewrite,rewrite,56,"// V_FMAAK_F16_t16 takes VGPR_32_Lo128 operands, so the rewrite; // would also require restricting their register classes. For now; // just bail out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:241,Security,access,accesses,241,"// TODO: Should we check the address space from the MachineMemOperand? That; // would allow us to distinguish objects we know don't alias based on the; // underlying address space, even if it was lowered to a different one,; // e.g. private accesses lowered to use MUBUF instructions on a scratch; // buffer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:22,Safety,safe,safe,22,// It's not generally safe to move VALU instructions across these since it will; // start using the register as a base index rather than directly.; // XXX - Why isn't hasSideEffects sufficient for these?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:323,Energy Efficiency,schedul,scheduled,323,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:207,Safety,hazard,hazard,207,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:239,Safety,avoid,avoidable,239,// Skipping the check for SP writes in the base implementation. The reason it; // was added was apparently due to compile time concerns.; //; // TODO: Do we really want this barrier? It triggers unnecessary hazard nops; // but is probably avoidable.; // Copied from base implementation.; // Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:140,Energy Efficiency,schedul,scheduling,140,"// Target-independent instructions do not have an implicit-use of EXEC, even; // when they operate on VGPRs. Treating EXEC modifications as scheduling; // boundaries prevents incorrect movements of such instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:108,Availability,mask,mask,108,"// These instructions cause shader I/O that may cause hardware lockups; // when executed with an empty EXEC mask.; //; // Note: exp with VM = DONE = 0 is automatically skipped by hardware when; // EXEC = 0, but checking for that case here seems not worth it; // given the typical code patterns.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:224,Safety,avoid,avoid,224,"// These are like SALU instructions in terms of effects, so it's questionable; // whether we should return true for those.; //; // However, executing them with EXEC = 0 causes them to operate on undefined; // data, which we avoid by returning true here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:40,Availability,mask,mask,40,// This likely will be a condition code mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:21,Safety,safe,safely,21,// Implicit uses may safely overlap true operands,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:83,Energy Efficiency,schedul,scheduler,83,// Allow additional implicit operands. This allows a fixup done by the post; // RA scheduler where the main implicit operand is killed and implicit-defs; // are added for sub-registers that remain live after this instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:19,Security,access,access,19,// FIXME: This can access off the end of the operands() array.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:26,Modifiability,extend,extended,26,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:227,Modifiability,extend,extended,227,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:256,Modifiability,extend,extended,256,"// FIXME: We can use sign extended 64-bit literals, but only for signed; // operands. At the moment we do not know if an operand is signed.; // Such operand will be encoded as its low 32 bits and then either; // correctly sign extended or incorrectly zero extended by HW.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,Performance,load,loads,139,"// If the pointer is store in VGPRs, then we need to move them to; // SGPRs using v_readfirstlane. This is safe because we only select; // loads with uniform pointers to SMRD instruction so we know the; // pointer value is uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:107,Safety,safe,safe,107,"// If the pointer is store in VGPRs, then we need to move them to; // SGPRs using v_readfirstlane. This is safe because we only select; // loads with uniform pointers to SMRD instruction so we know the; // pointer value is uniform.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Usability,Clear,Clear,3,// Clear use list from the old vaddr holding a zero register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update the use list with the pointer we have just moved from vaddr to; // saddr position. Otherwise new vaddr will be missing from the use list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:81,Testability,assert,asserts,81,"// removeOperand doesn't try to fixup tied operand indexes at it goes, so; // it asserts. Untie the operands for now and retie them afterwards.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:49,Integrability,wrap,wrapped,49,"// Emit the actual waterfall loop, executing the wrapped instruction for each; // unique value of \p ScalarOps across all lanes. In the best case we execute 1; // iteration, in the worst case we execute 64 (once per lane).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update ScalarOp operand to use the SGPR ScalarOp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update ScalarOp operand to use the SGPR ScalarOp.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC to matching lanes, saving original to SaveExec.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:17,Availability,mask,mask,17,// Save the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,"// Update dominators. We know that MBB immediately dominates LoopBB, that; // LoopBB immediately dominates BodyBB, and BodyBB immediately dominates; // RemainderBB. RemainderBB immediately dominates all of the successors; // transferred to it from MBB that MBB used to properly dominate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:20,Availability,mask,mask,20,// Restore the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update all the operands so they have the same type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Safety,Avoid,Avoid,3,// Avoid creating no-op copies with the same src and dst reg class. These; // confuse some of the machine passes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update all the operands so they are VGPR register classes. These may; // not be the same register class because REG_SEQUENCE supports mixing; // subregister index types e.g. sub0_sub1 + sub2 + sub3,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:157,Security,access,access,157,"// Legalize MIMG/VIMAGE/VSAMPLE and MUBUF/MTBUF for shaders.; //; // Shaders only generate MUBUF/MTBUF instructions via intrinsics or via; // scratch memory access. In both cases, the legalization never involves; // conversion to the addr64 form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:70,Safety,avoid,avoid,70,"// Legalize a VGPR Rsrc; //; // If the instruction is _ADDR64, we can avoid a waterfall by extracting; // the base pointer from the VGPR Rsrc, adding it to the VAddr, then using; // a zero-value SRsrc.; //; // If the instruction is _OFFSET (both idxen and offen disabled), and we; // support ADDR64 instructions, we can convert to ADDR64 and do the same as; // above.; //; // Otherwise we are on non-ADDR64 hardware, and/or we have; // idxen/offen/bothen and we fall back to a waterfall loop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:18,Performance,load,load,18,// Regular buffer load / store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:82,Modifiability,extend,extended,82,// This is a special case of s_mul_u64 where all the operands are either; // zero extended or sign extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:99,Modifiability,extend,extended,99,// This is a special case of s_mul_u64 where all the operands are either; // zero extended or sign extended.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Usability,Clear,Clear,3,// Clear unused bits of vcc,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Deployability,Update,Update,3,// Update the destination register class.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:216,Availability,mask,mask,216,"// SCC def is not a copy; // Insert a trivial select instead of creating a copy, because a copy from; // SCC would semantically mean just copying a single bit, but we may need; // the result to be a vector condition mask that needs preserving.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:97,Performance,perform,perform,97,"// Using the identity !(x ^ y) == (!x ^ y) == (x ^ !y), we can; // invert either source and then perform the XOR. If either source is a; // scalar register, then we can leave the inversion on the scalar unit to; // achieve a better distribution of scalar and vector instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:115,Performance,perform,performance,115,"// Set MTYPE = 2 (MTYPE_UC = uncached). GFX9 doesn't have this.; // BTW, it disables TC L2 and therefore decreases performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:72,Usability,Clear,Clear,72,"// If TID_ENABLE is set, DATA_FORMAT specifies stride bits [14:17].; // Clear them unless we want a huge stride.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:32,Energy Efficiency,schedul,scheduler,32,/// This is used by the post-RA scheduler (SchedulePostRAList.cpp). The; /// post-RA version of misched uses CreateTargetMIHazardRecognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:43,Energy Efficiency,Schedul,SchedulePostRAList,43,/// This is used by the post-RA scheduler (SchedulePostRAList.cpp). The; /// post-RA version of misched uses CreateTargetMIHazardRecognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:16,Safety,hazard,hazard,16,/// This is the hazard recognizer used at -O0 by the PostRAHazardRecognizer; /// pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:31,Energy Efficiency,schedul,scheduling,31,// Called during:; // - pre-RA scheduling and post-RA scheduling,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:54,Energy Efficiency,schedul,scheduling,54,// Called during:; // - pre-RA scheduling and post-RA scheduling,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Energy Efficiency,schedul,scheduling,101,// Borrowed from Arm Target; // We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:63,Safety,hazard,hazard,63,// Borrowed from Arm Target; // We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:372,Availability,mask,mask,372,"// We need to handle instructions which may be inserted during register; // allocation to handle the prolog. The initial prolog instruction may have; // been separated from the start of the block by spills and copies inserted; // needed by the prolog. However, the insertions for scalar registers can; // always be placed at the BB top as they are independent of the exec mask; // value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:6,Availability,avail,available,6,"// If available, prefer to use vcc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:54,Performance,load,loads,54,"// Try to keep the same value in SOffset for adjacent loads, so that; // the corresponding register contents can be re-used.; //; // Load values with all low-bits (except for alignment bits) set into; // SOffset, so that a larger range of values can be covered using; // s_movk_i32.; //; // Atomic operations fail to work correctly when individual address; // components are unaligned, even if their sum is aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:133,Performance,Load,Load,133,"// Try to keep the same value in SOffset for adjacent loads, so that; // the corresponding register contents can be re-used.; //; // Load values with all low-bits (except for alignment bits) set into; // SOffset, so that a larger range of values can be covered using; // s_movk_i32.; //; // Atomic operations fail to work correctly when individual address; // components are unaligned, even if their sum is aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Integrability,Depend,Depending,3,"// Depending on the used address space and instructions, some immediate offsets; // are allowed and some are not.; // Pre-GFX12, flat instruction offsets can only be non-negative, global and; // scratch instruction offsets can also be negative. On GFX12, offsets can be; // negative for all variants.; //; // There are several bugs related to these offsets:; // On gfx10.1, flat instructions that go into the global address space cannot; // use an offset.; //; // For scratch instructions, the address can be either an SGPR or a VGPR.; // The following offsets can be used, depending on the architecture (x means; // cannot be used):; // +----------------------------+------+------+; // | Address-Mode | SGPR | VGPR |; // +----------------------------+------+------+; // | gfx9 | | |; // | negative, 4-aligned offset | x | ok |; // | negative, unaligned offset | x | ok |; // +----------------------------+------+------+; // | gfx10 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | x |; // +----------------------------+------+------+; // | gfx10.3 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | ok |; // +----------------------------+------+------+; //; // This function ignores the addressing mode, so if an offset cannot be used in; // one addressing mode, it is considered illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:574,Integrability,depend,depending,574,"// Depending on the used address space and instructions, some immediate offsets; // are allowed and some are not.; // Pre-GFX12, flat instruction offsets can only be non-negative, global and; // scratch instruction offsets can also be negative. On GFX12, offsets can be; // negative for all variants.; //; // There are several bugs related to these offsets:; // On gfx10.1, flat instructions that go into the global address space cannot; // use an offset.; //; // For scratch instructions, the address can be either an SGPR or a VGPR.; // The following offsets can be used, depending on the architecture (x means; // cannot be used):; // +----------------------------+------+------+; // | Address-Mode | SGPR | VGPR |; // +----------------------------+------+------+; // | gfx9 | | |; // | negative, 4-aligned offset | x | ok |; // | negative, unaligned offset | x | ok |; // +----------------------------+------+------+; // | gfx10 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | x |; // +----------------------------+------+------+; // | gfx10.3 | | |; // | negative, 4-aligned offset | ok | ok |; // | negative, unaligned offset | ok | ok |; // +----------------------------+------+------+; //; // This function ignores the addressing mode, so if an offset cannot be used in; // one addressing mode, it is considered illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:28,Energy Efficiency,power,power,28,// Use signed division by a power of two to truncate towards 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:140,Safety,risk,risky,140,// These opcodes use indirect register addressing so; // they need special handling by codegen (currently missing).; // Therefore it is too risky to allow these opcodes; // to be selected by dpp combiner or sdwa peepholer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:22,Availability,mask,masks,22,// We don't check reg masks here as they're used only on calls:; // 1. EXEC is only considered const within one BB; // 2. Call should be a terminator instruction if present in a BB,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:188,Performance,load,loads,188,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:238,Performance,load,loads,238,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.; //; // All other loads are not divergent, because if threads issue loads with the; // same arguments, they will always get the same result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:3,Performance,Load,Loads,3,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:101,Performance,load,load,101,"// Loads from the private and flat address spaces are divergent, because; // threads can execute the load instruction with the same inputs and get; // different results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:43,Integrability,Interface,Interface,43,"//===- SIInstrInfo.h - SI Instruction Info Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIInstrInfo.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:395,Integrability,Interface,Interface,395,"//===- SIInstrInfo.h - SI Instruction Info Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIInstrInfo.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:30,Performance,load,load,30,/// Mark the MMO of a uniform load if there are no potentially clobbering stores; /// on any path from the start of an entry function to this load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:142,Performance,load,load,142,/// Mark the MMO of a uniform load if there are no potentially clobbering stores; /// on any path from the start of an entry function to this load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:22,Performance,load,load,22,/// Mark the MMO of a load as the last use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:39,Security,access,accesses,39,"// Is a FLAT encoded instruction which accesses a specific segment,; // i.e. global_* or scratch_*.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:54,Integrability,depend,depend,54,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:103,Integrability,depend,dependencies,103,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:120,Safety,safe,safely,120,"/// Returns true if the instruction could potentially depend on the value of; /// exec. If false, exec dependencies may safely be ignored.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:123,Security,validat,validate,123,/// Check if \p MO would be a valid operand for the given operand; /// definition \p OpInfo. Note this does not attempt to validate constant bus; /// restrictions (e.g. literal constant usage).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:152,Deployability,update,updated,152,"/// Legalize all operands in this instruction. This function may create new; /// instructions and control-flow around \p MI. If present, \p MDT is; /// updated.; /// \returns A new basic block that contains \p MI if new blocks were created.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h:213,Deployability,update,updated,213,"/// Replace the instructions opcode with the equivalent VALU; /// opcode. This function will also move the users of MachineInstruntions; /// in the \p WorkList to the VALU if necessary. If present, \p MDT is; /// updated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:118,Modifiability,extend,extend,118,"// The boolean content concept here is too inflexible. Compares only ever; // really produce a 1-bit result. Any copy/extend from these will turn into a; // select, and zext/1 or sext/-1 are equally cheap. Arbitrarily choose 0/1, as; // it's what most targets use.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,LOAD,LOAD,19,// We only support LOAD/STORE and vector manipulation ops for vectors; // with > 4 elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:239,Performance,load,load,239,"// TODO: For dynamic 64-bit vector inserts/extracts, should emit a pseudo that; // is expanded to avoid having two separate loops in case the index is a VGPR.; // Most operations are naturally 32-bit vector operations. We only support; // load and store of i64 vectors, so promote v2i64 vector operations to v4i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:98,Safety,avoid,avoid,98,"// TODO: For dynamic 64-bit vector inserts/extracts, should emit a pseudo that; // is expanded to avoid having two separate loops in case the index is a VGPR.; // Most operations are naturally 32-bit vector operations. We only support; // load and store of i64 vectors, so promote v2i64 vector operations to v4i32.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid stack access for these.; // TODO: Generalize to more vector types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Security,access,access,15,// Avoid stack access for these.; // TODO: Generalize to more vector types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:27,Availability,failure,failure,27,"// We can't return success/failure, only the old value,; // let LLVM add the comparison",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:324,Energy Efficiency,reduce,reduce,324,"// We only really have 32-bit BFE instructions (and 16-bit on VI).; //; // On SI+ there are 64-bit BFEs, but they are scalar only and there isn't any; // effort to match them now. We want this to be false for i64 cases when the; // extraction isn't restricted to the upper or lower half. Ideally we would; // have some pass reduce 64-bit extracts to 32-bit if possible. Extracts that; // span the midpoint are probably relatively rare, so don't worry about them; // for now.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:69,Safety,avoid,avoiding,69,"// These are really only legal for ieee_mode functions. We should be avoiding; // them for functions that don't have ieee_mode enabled, so just say they are; // legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Performance,Load,Load,9,// F16 - Load/Store Actions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,Load,Load,10,// BF16 - Load/Store Actions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Safety,avoid,avoids,37,"// This isn't really legal, but this avoids the legalizer unrolling it (and; // allows matching fneg (fabs x) patterns)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:171,Usability,clear,clear,171,"/// Map address space 7 to MVT::v5i32 because that's its in-memory; /// representation. This return value is vector-typed because there is no; /// MVT::i160 and it is not clear if one can be added. While this could; /// cause issues during codegen, these address space 7 pointers will be; /// rewritten away by then. Therefore, we can return MVT::v5i32 in order; /// to allow pre-codegen passes that query TargetTransformInfo, often for cost; /// modeling, to work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:115,Security,access,access,115,"// We conservatively set the memory operand of a buffer intrinsic to the; // base resource pointer, so that we can access alias information about; // those pointers. Cases like ""this points at the same value; // but with a different offset"" are handled in; // areMemAccessesTriviallyDisjoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:46,Performance,load,loaded,46,"// If this isn't a gather, we may have excess loaded elements in the; // IR type. Check the dmask for the real number of elements loaded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:130,Performance,load,loaded,130,"// If this isn't a gather, we may have excess loaded elements in the; // IR type. Check the dmask for the real number of elements loaded.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Security,access,access,23,"// This is an abstract access, but we need to specify a type and size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Security,access,access,23,"// This is an abstract access, but we need to specify a type and size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:481,Safety,risk,risky,481,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:53,Security,access,accesses,53,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:518,Security,validat,validated,518,"// Assume the we will use FLAT for all global memory accesses; // on VI.; // FIXME: This assumption is currently wrong. On VI we still use; // MUBUF instructions for the r + i addressing mode. As currently; // implemented, the MUBUF instructions only work on buffer < 4GB.; // It may be possible to support > 4GB buffers with MUBUF instructions,; // by setting the stride value in the resource descriptor which would; // increase the size limit to (stride * 4GB). However, this is risky,; // because it has never been validated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:162,Integrability,Depend,Depending,162,"// MUBUF / MTBUF instructions have a 12-bit unsigned byte offset, and; // additionally can do r + r + i with addr64. 32-bit has more addressing; // mode options. Depending on the resource constant, it can also do; // (i64 r0) + (i32 r1) * (i14 i).; //; // Private arrays end up using a scratch buffer most of the time, so also; // assume those use MUBUF instructions. Scratch loads / stores are currently; // implemented as mubuf instructions with offen bit set, so slightly; // different than the normal addr64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:376,Performance,load,loads,376,"// MUBUF / MTBUF instructions have a 12-bit unsigned byte offset, and; // additionally can do r + r + i with addr64. 32-bit has more addressing; // mode options. Depending on the resource constant, it can also do; // (i64 r0) + (i32 r1) * (i14 i).; //; // Private arrays end up using a scratch buffer most of the time, so also; // assume those use MUBUF instructions. Scratch loads / stores are currently; // implemented as mubuf instructions with offen bit set, so slightly; // different than the normal addr64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:203,Deployability,Update,Update,203,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Performance,load,load,95,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:258,Performance,load,loads,258,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Security,access,access,64,"// There are no SMRD extloads, so if we have to do a small type access we; // will use a MUBUF load.; // FIXME?: We also need to do this if unaligned, but we don't know the; // alignment here.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:133,Security,access,access,133,"// Basic, single offset DS instructions allow a 16-bit unsigned immediate; // field.; // XXX - If doing a 4-byte aligned 8-byte type access, we effectively have; // an 8-bit dword offset but we don't know the alignment here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:20,Integrability,depend,depending,20,"// r + i or just i, depending on HasBaseReg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:86,Security,access,access,86,"// Either, the alignment requirements are ""enabled"", or there is an; // unaligned LDS access related hardware bug though alignment requirements; // are ""disabled"". In either case, we need to check for proper alignment; // requirements.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:219,Performance,load,loads,219,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:288,Performance,load,load,288,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:233,Safety,avoid,avoid,233,"// SI has a hardware bug in the LDS / GDS bounds checking: if the base; // address is negative, then the instruction is incorrectly treated as; // out-of-bounds even if base + offsets is in bounds. Split vectorized; // loads here to avoid emitting ds_read2_b32. We may re-combine the; // load later in the SILoadStoreOptimizer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Security,access,accessing,10,"// 8 byte accessing via ds_read/write_b64 require 8-byte alignment, but we; // can do a 4 byte aligned, 8 byte access in a single operation using; // ds_read2/write2_b32 with adjacent offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:111,Security,access,access,111,"// 8 byte accessing via ds_read/write_b64 require 8-byte alignment, but we; // can do a 4 byte aligned, 8 byte access in a single operation using; // ds_read2/write2_b32 with adjacent offsets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:85,Integrability,depend,depending,85,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:528,Performance,load,load,528,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:744,Performance,load,load,744,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:824,Performance,load,load,824,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:844,Performance,load,load,844,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:668,Security,access,access,668,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:770,Usability,simpl,simply,770,"// We will either select ds_read_b64/ds_write_b64 or ds_read2_b32/; // ds_write2_b32 depending on the alignment. In either case with either; // alignment there is no faster way of doing this.; // The numbers returned here and below are not additive, it is a 'speed; // rank'. They are just meant to be compared to decide if a certain way; // of lowering an operation is faster than another. For that purpose; // naturally aligned operation gets it bitsize to indicate that ""it; // operates with a speed comparable to N-bit wide load"". With the full; // alignment ds128 is slower than ds96 for example. If underaligned it; // is comparable to a speed of a single dword access, which would then; // mean 32 < 128 and it is faster to issue a wide load regardless.; // 1 is simply ""slow, don't do it"". I.e. comparing an aligned load to a; // wider load which will not be aligned anymore the latter is slower.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Security,access,accessing,11,// 12 byte accessing via ds_read/write_b96 require 16-byte alignment on; // gfx8 and older.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Performance,load,load,123,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b96/ds_write_b96, but there will; // be more of them, so overall we will pay less penalty issuing a single; // instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Security,access,access,21,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b96/ds_write_b96, but there will; // be more of them, so overall we will pay less penalty issuing a single; // instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Security,access,accessing,11,"// 16 byte accessing via ds_read/write_b128 require 16-byte alignment on; // gfx8 and older, but we can do a 8 byte aligned, 16 byte access in a; // single operation using ds_read2/write2_b64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:133,Security,access,access,133,"// 16 byte accessing via ds_read/write_b128 require 16-byte alignment on; // gfx8 and older, but we can do a 8 byte aligned, 16 byte access in a; // single operation using ds_read2/write2_b64.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Performance,load,load,123,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b128/ds_write_b128, but there; // will be more of them, so overall we will pay less penalty issuing a; // single instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Security,access,access,21,"// Naturally aligned access is fastest. However, also report it is Fast; // if memory is aligned less than DWORD. A narrow load or store will be; // be equally slow as a single ds_read_b128/ds_write_b128, but there; // will be more of them, so overall we will pay less penalty issuing a; // single instruction.; // See comment on the values above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:140,Security,access,access,140,"// See comment on the values above.; // Note that we have a single-dword or sub-dword here, so if underaligned; // it is a slowest possible access, hence returned value is 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:83,Security,access,access,83,"// FIXME: We have to be conservative here and assume that flat operations; // will access scratch. If we had access to the IR function, then we; // could determine if any private memory was used in the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:109,Security,access,access,109,"// FIXME: We have to be conservative here and assume that flat operations; // will access scratch. If we had access to the IR function, then we; // could determine if any private memory was used in the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Performance,perform,perform,62,"// So long as they are correct, wide global memory operations perform better; // than multiple smaller memory ops -- even when misaligned",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:174,Security,access,accesses,174,// FIXME: Should account for address space here.; // The default fallback uses the private pointer size as a guess for a type to; // use. Make sure we switch these to 64-bit accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Usability,simpl,simple,30,// Flat -> private/local is a simple truncate.; // Flat -> global is no-op,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Usability,Simpl,SimplifySetCC,3,// SimplifySetCC uses this function to determine whether or not it should; // create setcc with i1 operands. We don't have instructions for i1 setcc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:36,Performance,load,loading,36,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:121,Performance,load,load,121,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Safety,avoid,avoid,10,"// Try to avoid using an extload by loading earlier than the argument address,; // and extracting the relevant bits. The load should hopefully be merged with; // the previous argument.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,load,load,10,// Create load nodes to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:44,Testability,assert,assert,44,"// For NON_EXTLOAD, generic code in getLoad assert(ValVT == MemVT)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:122,Availability,mask,mask,122,"// If GridZ is not programmed in an entry function then the hardware will set; // it to all zeros, so there is no need to mask the GridY value in the low; // order bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:80,Energy Efficiency,allocate,allocated,80,"// It's possible for a kernarg intrinsic call to appear in a kernel with; // no allocated segment, in which case we do not add the user sgpr; // argument, so just return null.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Safety,safe,safely,10,// We can safely skip PS inputs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special inputs passed in VGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:131,Availability,Mask,Mask,131,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:238,Availability,Mask,Mask,238,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Energy Efficiency,allocate,allocate,10,"// Try to allocate a VGPR at the end of the argument list, or if no argument; // VGPRs are left allocating a stack slot.; // If \p Mask is is given it indicates bitfield position in the register.; // If \p Arg is given use it with new ]p Mask instead of allocating new.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:49,Energy Efficiency,allocate,allocate,49,"// If this has a fixed position, we still should allocate the register in the; // CCInfo state. Technically we could get away with this for values passed; // outside of the normal argument range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:4,Energy Efficiency,Allocate,Allocate,4,/// Allocate implicit function VGPR arguments at the end of allocated user; /// arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:60,Energy Efficiency,allocate,allocated,60,/// Allocate implicit function VGPR arguments at the end of allocated user; /// arguments.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:4,Energy Efficiency,Allocate,Allocate,4,/// Allocate implicit function VGPR arguments in fixed registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special inputs passed in user SGPRs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate pre-loaded kernel arguemtns. Arguments to be preloading must be; // sequential starting from the first argument.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:16,Performance,load,loaded,16,// Allocate pre-loaded kernel arguemtns. Arguments to be preloading must be; // sequential starting from the first argument.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Energy Efficiency,allocate,allocate,10,// Always allocate this last since it is a synthetic preload.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate special input registers that are initialized per-wave.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Security,access,access,24,"// For now assume stack access is needed in any callee functions, so we need; // the scratch registers to pass in.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:234,Energy Efficiency,allocate,allocated,234,"// We tentatively reserve the last registers (skipping the last registers; // which may contain VCC, FLAT_SCR, and XNACK). After register allocation,; // we'll replace these with the ones immediately after those which were; // really allocated. In the prologue copies will be inserted from the; // argument to these reserved registers.; // Without HSA, relocations are used for the scratch pointer and the; // buffer resource setup is always inserted in the prologue. Scratch wave; // offset is still in an input SGPR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:655,Safety,avoid,avoid,655,"// For entry functions we have to set up the stack pointer if we use it,; // whereas non-entry functions get this ""for free"". This means there is no; // intrinsic advantage to using S32 over S34 in cases where we do not have; // calls but do need a frame pointer (i.e. if we are requested to have one; // because frame pointer elimination is disabled). To keep things simple we; // only ever use S32 as the call ABI stack pointer, and so using it does not; // imply we need a separate frame pointer.; //; // Try to use s32 as the SP, but move it if it would interfere with input; // arguments. This won't work with calls though.; //; // FIXME: Move SP to avoid any possible inputs, or find a way to spill input; // registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:368,Usability,simpl,simple,368,"// For entry functions we have to set up the stack pointer if we use it,; // whereas non-entry functions get this ""for free"". This means there is no; // intrinsic advantage to using S32 over S34 in cases where we do not have; // calls but do need a frame pointer (i.e. if we are requested to have one; // because frame pointer elimination is disabled). To keep things simple we; // only ever use S32 as the call ABI stack pointer, and so using it does not; // imply we need a separate frame pointer.; //; // Try to use s32 as the SP, but move it if it would interfere with input; // arguments. This won't work with calls though.; //; // FIXME: Move SP to avoid any possible inputs, or find a way to spill input; // registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:171,Modifiability,variab,variable,171,"// hasFP should be accurate for entry functions even before the frame is; // finalized, because it does not rely on the known stack size, only; // properties like whether variable sized objects are present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:25,Energy Efficiency,allocate,allocateSpecialInputSGPRs,25,// FIXME: Sink this into allocateSpecialInputSGPRs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:91,Testability,assert,assert,91,"// If this is an 8 or 16-bit value, it is really passed promoted; // to 32 bits. Insert an assert[sz]ext to capture this, then; // truncate to the right size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Deployability,update,updated,23,// Operand #0 = Chain (updated below); // Copy the result values into the output registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain and glue.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:44,Integrability,depend,depending,44,// Add code to pass special inputs required depending on used features separate; // from the explicit user arguments present in the IR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Energy Efficiency,allocate,allocate,101,"// We may have proven the input wasn't needed, although the ABI is; // requiring it. We just need to allocate the register appropriately.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Usability,simpl,simple,125,"// For a divergent call target, we need to do a waterfall loop over the; // possible callees which precludes us from using a simple jump.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Energy Efficiency,allocate,allocate,21,"// With a fixed ABI, allocate fixed registers before user arguments.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:98,Availability,avail,available,98,"// Since we're not changing the ABI to make this a tail call, the memory; // operands are already available in the caller's incoming argument space.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:78,Performance,load,loaded,78,"// Make sure any stack arguments overlapping with where we're storing; // are loaded before this eventual operation. Otherwise they'll be; // clobbered.; // FIXME: Why is this really necessary? This seems to just result in a; // lot of code to copy the stack and write them back to the same; // locations, which are supposed to be immutable?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Availability,redundant,redundant,9,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Safety,redund,redundant,9,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Security,access,access,96,"// Add a redundant copy of the callee global which will not be legalized, as; // we need direct access to the callee later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Energy Efficiency,consumption,consumption,148,"// Each tail call may have to adjust the stack by a different amount, so; // this information must travel along with the operation for eventual; // consumption by emitEpilogue.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:26,Availability,error,error,26,// Defer to cannot select error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:27,Safety,avoid,avoid,27,// TODO: We could possibly avoid a 64-bit shift and use a simpler table if we; // knew only one mode was demanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Usability,simpl,simpler,58,// TODO: We could possibly avoid a 64-bit shift and use a simpler table if we; // knew only one mode was demanded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:93,Modifiability,extend,extended,93,"// There's a gap in the 4-bit encoded table and actual enum values, so offset; // if it's an extended value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Usability,Clear,Clear,3,// Clear TRAP_STS.MEM_VIOL,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Load,Load,3,// Load and check TRAP_STS.MEM_VIOL,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,"// Update EXEC, save the original EXEC value to VCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,"// Update EXEC, switch all done bits to 0 and all todo bits to 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:181,Availability,alive,alive,181,"// This has slightly sub-optimal regalloc when the source vector is killed by; // the read. The register allocator does not understand that the kill is; // per-workitem, so is kept alive for the whole loop so we end up not re-using a; // subregister from it, using 1 more VGPR than necessary. This was saved when; // this was expanded after register allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Availability,mask,mask,17,// Save the EXEC mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Performance,optimiz,optimization,148,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:29,Safety,avoid,avoid,29,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:82,Safety,avoid,avoid,82,"// TODO: Look at the uses to avoid the copy. This may require rescheduling; // to avoid interfering with other uses, so probably requires a new; // optimization pass.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Integrability,depend,depend,24,// Reduction operations depend on whether the input operand is SGPR or VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Energy Efficiency,Reduce,Reduced,71,// These operations with a uniform value i.e. SGPR are idempotent.; // Reduced value will be same as given sgpr.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:189,Energy Efficiency,reduce,reduce,189,"// TODO: Implement DPP Strategy and switch based on immediate strategy; // operand. For now, for all the cases (default, Iterative and DPP we use; // iterative approach by default.); // To reduce the VGPR using iterative approach, we need to iterate; // over all the active lanes. Lowering consists of ComputeLoop,; // which iterate over only active lanes. We use copy of EXEC register; // as induction variable and every active lane modifies it using bitset0; // so that we will get the next active lane for next iteration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:403,Modifiability,variab,variable,403,"// TODO: Implement DPP Strategy and switch based on immediate strategy; // operand. For now, for all the cases (default, Iterative and DPP we use; // iterative approach by default.); // To reduce the VGPR using iterative approach, we need to iterate; // over all the active lanes. Lowering consists of ComputeLoop,; // which iterate over only active lanes. We use copy of EXEC register; // as induction variable and every active lane modifies it using bitset0; // so that we will get the next active lane for next iteration.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Modifiability,variab,variable,38,"// Create initail values of induction variable from Exec, Accumulator and; // insert branch instr to newly created ComputeBlockk",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Perform,Perform,3,// Perform the computations,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,optimiz,optimize,10,"// Try to optimize cases that only set the denormal mode or rounding mode.; //; // If the s_setreg_b32 fully sets all of the bits in the rounding mode or; // denormal mode to a constant, we can use s_round_mode or s_denorm_mode; // instead.; //; // FIXME: This could be predicates on the immediate, but tablegen doesn't; // allow you to have a no side effect instruction in the output of a; // sideeffecting pattern.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:190,Performance,perform,perform,190,"// This currently forces unfolding various combinations of fsub into fma with; // free fneg'd operands. As long as we have fast FMA (controlled by; // isFMAFasterThanFMulAndFAdd), we should perform these.; // When fma is quarter rate, for f64 where add / sub are at best half rate,; // most of these combines appear to be cycle neutral but save on instruction; // count / code size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:41,Integrability,depend,depends,41,"// Answering this is somewhat tricky and depends on the specific device which; // have different rates for fma or all f64 operations.; //; // v_fma_f64 and v_mul_f64 always take the same number of cycles as each other; // regardless of which device (although the number of cycles differs between; // devices), so it is always profitable for f64.; //; // v_fma_f32 takes 4 or 16 cycles depending on the device, so it is profitable; // only on full rate devices. Normally, we should prefer selecting v_mad_f32; // which we can always do even without fused FP ops since it returns the same; // result as the separate operations and since it is always full; // rate. Therefore, we lie and report that it is not faster for f32. v_mad_f32; // however does not support denormals, so we do report fma as faster if we have; // a fast fma device and require denormals.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:385,Integrability,depend,depending,385,"// Answering this is somewhat tricky and depends on the specific device which; // have different rates for fma or all f64 operations.; //; // v_fma_f64 and v_mul_f64 always take the same number of cycles as each other; // regardless of which device (although the number of cycles differs between; // devices), so it is always profitable for f64.; //; // v_fma_f32 takes 4 or 16 cycles depending on the device, so it is profitable; // only on full rate devices. Normally, we should prefer selecting v_mad_f32; // which we can always do even without fused FP ops since it returns the same; // result as the separate operations and since it is always full; // rate. Therefore, we lie and report that it is not faster for f32. v_mad_f32; // however does not support denormals, so we do report fma as faster if we have; // a fast fma device and require denormals.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Availability,avail,available,17,// If mad is not available this depends only on if f32 fma is full rate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:32,Integrability,depend,depends,32,// If mad is not available this depends only on if f32 fma is full rate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Performance,load,loads,95,"// Used for D16: Casts the result of an instruction into the right vector,; // packs values if loads return unpacked values.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:32,Performance,load,loads,32,// TODO: Support non-format TFE loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,load,load,30,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:120,Performance,load,load,120,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:227,Performance,perform,performSignExtendInRegCombine,227,"// Lower llvm.amdgcn.s.buffer.load.(i8, u8) intrinsics. First, we generate; // s_buffer_load_u8 for signed and unsigned load instructions. Next, DAG; // combiner tries to merge the s_buffer_load_u8 with a sext instruction; // (performSignExtendInRegCombine()) and it replaces s_buffer_load_u8 with; // s_buffer_load_i8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:17,Safety,avoid,avoid,17,// FIXME: Either avoid relying on address space here or change the default; // address space for functions to avoid the explicit check.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Safety,avoid,avoid,110,// FIXME: Either avoid relying on address space here or change the default; // address space for functions to avoid the explicit check.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Testability,Assert,Assert,10,"// FIXME: Assert during selection that this is only selected for; // ieee_mode. Currently a combine can produce the ieee version for non-ieee; // mode functions, but this happens to be OK since it's only done in cases; // where there is known no sNaN.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:326,Energy Efficiency,efficient,efficient,326,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:585,Modifiability,extend,extended,585,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:599,Modifiability,extend,extended,599,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:762,Modifiability,extend,extended,762,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:779,Modifiability,extend,extended,779,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:912,Modifiability,extend,extended,912,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1002,Modifiability,extend,extended,1002,"// There are four ways to lower s_mul_u64:; //; // 1. If all the operands are uniform, then we lower it as it is.; //; // 2. If the operands are divergent, then we have to split s_mul_u64 in 32-bit; // multiplications because there is not a vector equivalent of s_mul_u64.; //; // 3. If the cost model decides that it is more efficient to use vector; // registers, then we have to split s_mul_u64 in 32-bit multiplications.; // This happens in splitScalarSMULU64() in SIInstrInfo.cpp .; //; // 4. If the cost model decides to use vector registers and both of the; // operands are zero-extended/sign-extended from 32-bits, then we split the; // s_mul_u64 in two 32-bit multiplications. The problem is that it is not; // possible to check if the operands are zero-extended or sign-extended in; // SIInstrInfo.cpp. For this reason, here, we replace s_mul_u64 with; // s_mul_u64_u32_pseudo if both operands are zero-extended and we replace; // s_mul_u64 with s_mul_i64_i32_pseudo if both operands are sign-extended.; // If the cost model decides that we have to use vector registers, then; // splitScalarSMulPseudo() (in SIInstrInfo.cpp) split s_mul_u64_u32/; // s_mul_i64_i32_pseudo in two vector multiplications. If the cost model; // decides that we should use scalar registers, then s_mul_u64_u32_pseudo/; // s_mul_i64_i32_pseudo is lowered as s_mul_u64 in expandPostRAPseudo() in; // SIInstrInfo.cpp .",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:138,Modifiability,extend,extended,138,"// If all the operands are zero-enteted to 32-bits, then we replace s_mul_u64; // with s_mul_u64_u32_pseudo. If all the operands are sign-extended to; // 32-bits, then we replace s_mul_u64 with s_mul_i64_i32_pseudo.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,Queue,QueuePtr,30,"// For code object version 5, QueuePtr is passed through implicit kernarg.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Performance,queue,queue-ptr,71,"// We probably are in a function incorrectly marked with; // amdgpu-no-queue-ptr. This is undefined. We don't want to delete the; // trap, so just use a null pointer.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:345,Usability,simpl,simply,345,"// Note: this feature (register) is broken. When used as a 32-bit operand,; // it returns a wrong value (all zeroes?). The real value is in the upper 32; // bits.; //; // To work around the issue, directly emit a 64 bit mov from this register; // then extract the high bits. Note that this shouldn't even result in a; // shift being emitted and simply become a pair of registers (e.g.):; // s_mov_b64 s[6:7], src_shared_base; // v_mov_b32_e32 v1, s7; //; // FIXME: It would be more natural to emit a CopyFromReg here, but then copy; // coalescing would kick in and it would think it's okay to use the ""HI""; // subregister directly (instead of extracting the HI 32 bits) which is an; // artificial (unusable) register.; // Register TableGen definitions would need an overhaul to get rid of the; // artificial ""HI"" aperture registers and prevent this kind of issue from; // happening.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:71,Performance,queue,queue-ptr,71,// We probably are in a function incorrectly marked with; // amdgpu-no-queue-ptr. This is undefined.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:132,Availability,avail,available,132,"// TODO: Use custom target PseudoSourceValue.; // TODO: We should use the value from the IR intrinsic call, but it might not; // be available and how do we get it?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:57,Performance,load,loads,57,"// TODO: Search through arithmetic, handle arguments and loads; // marked nonnull.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:266,Performance,optimiz,optimized,266,"// This lowers an INSERT_SUBVECTOR by extracting the individual elements from; // the small vector and inserting them into the big vector. That is better than; // the default expansion of doing it via a stack slot. Even though the use of; // the stack slot would be optimized away afterwards, the stack slot itself; // remains.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:113,Safety,avoid,avoid,113,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:43,Security,access,access,43,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Security,access,access,125,"// Static indexing does not lower to stack access, and hence there is no need; // for special custom lowering to avoid stack access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid stack access for dynamic indexing by custom lowering to; // v_bfi_b32 (v_bfm_b32 16, (shl idx, 16)), val, vec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Security,access,access,15,"// Avoid stack access for dynamic indexing by custom lowering to; // v_bfi_b32 (v_bfm_b32 16, (shl idx, 16)), val, vec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Availability,mask,mask,62,// Convert vector index to bit-index and get the required bit mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Availability,Mask,Mask,6,// 2. Mask off all other indicies except the required index within (1).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Availability,Mask,Mask,6,// 3. Mask off the required index within the target vector.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Performance,optimiz,optimizations,23,// Make sure we do any optimizations that will make it easier to fold; // source modifiers before obscuring it with bit operations.; // XXX - Why doesn't this get called when vector_shuffle is expanded?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:356,Safety,Avoid,Avoid,356,"// vector_shuffle <0,1,6,7> lhs, rhs; // -> concat_vectors (extract_subvector lhs, 0), (extract_subvector rhs, 2); //; // vector_shuffle <6,7,2,3> lhs, rhs; // -> concat_vectors (extract_subvector rhs, 2), (extract_subvector lhs, 2); //; // vector_shuffle <6,7,0,1> lhs, rhs; // -> concat_vectors (extract_subvector rhs, 2), (extract_subvector lhs, 0); // Avoid scalarizing when both halves are reading from consecutive elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid adding defined bits with the zero_extend.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:489,Modifiability,variab,variable,489,"// In order to support pc-relative addressing, the PC_ADD_REL_OFFSET SDNode is; // lowered to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:971,Modifiability,variab,variable,971,"// In order to support pc-relative addressing, the PC_ADD_REL_OFFSET SDNode is; // lowered to the following code sequence:; //; // For constant address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol; // s_addc_u32 s1, s1, 0; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // a fixup or relocation is emitted to replace $symbol with a literal; // constant, which is a pc-relative offset from the encoding of the $symbol; // operand to the global variable.; //; // For global address space:; // s_getpc_b64 s[0:1]; // s_add_u32 s0, s0, $symbol@{gotpc}rel32@lo; // s_addc_u32 s1, s1, $symbol@{gotpc}rel32@hi; //; // s_getpc_b64 returns the address of the s_add_u32 instruction and then; // fixups or relocations are emitted to replace $symbol@*@lo and; // $symbol@*@hi with lower 32 bits and higher 32 bits of a literal constant,; // which is a 64-bit pc-relative offset from the encoding of the $symbol; // operand to the global variable.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:208,Energy Efficiency,allocate,allocated,208,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:274,Energy Efficiency,allocate,allocated,274,// HIP uses an unsized array `extern __shared__ T s[]` or similar; // zero-sized type in other languages to declare the dynamic shared; // memory which size is not known at the compile time. They will be; // allocated by the runtime and placed directly after the static; // allocated ones. They all share the same offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:218,Availability,redundant,redundant,218,"// We can't use S_MOV_B32 directly, because there is no way to specify m0 as; // the destination register.; //; // We can't use CopyToReg, because MachineCSE won't combine COPY instructions,; // so we will end up with redundant moves to m0.; //; // We use a pseudo to ensure we emit s_mov_b32 with m0 as the direct result.; // A Null SDValue creates a glue result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:218,Safety,redund,redundant,218,"// We can't use S_MOV_B32 directly, because there is no way to specify m0 as; // the destination register.; //; // We can't use CopyToReg, because MachineCSE won't combine COPY instructions,; // so we will end up with redundant moves to m0.; //; // We use a pseudo to ensure we emit s_mov_b32 with m0 as the direct result.; // A Null SDValue creates a glue result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:54,Performance,load,load,54,// Re-construct the required return value for a image load intrinsic.; // This is more complicated due to the optional use TexFailCtrl which means the required; // return type is an aggregate,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Availability,error,error,23,// Expecting to get an error flag since TFC is on - and dmask is 0; // Force dmask to be at least 1 otherwise the instruction will fail,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Performance,load,load,110,// Has something earlier tagged that the return type needs adjusting; // This happens if the instruction is a load or has set TexFailCtrl flags,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,load,load,19,// This is a no-op load. This can be eliminated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:18,Performance,optimiz,optimization,18,// TODO no-return optimization,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,load,load,30,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:151,Performance,load,loads,151,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:260,Performance,perform,performSignExtendInRegCombine,260,"// Lower llvm.amdgcn.s.buffer.load.{i16, u16} intrinsics. Initially, the; // s_buffer_load_u16 instruction is emitted for both signed and unsigned; // loads. Later, DAG combiner tries to combine s_buffer_load_u16 with sext; // and generates s_buffer_load_i16 (performSignExtendInRegCombine).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Performance,load,load,14,// Widen vec3 load to vec4.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Performance,load,load,51,// We have a divergent offset. Emit a MUBUF buffer load instead. We can; // assume that the buffer is unswizzled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:81,Availability,mask,masking,81,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:26,Testability,Assert,AssertZext,26,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:131,Testability,assert,assert,131,// Don't bother inserting AssertZext for packed IDs since we're emitting the; // masking operations anyway.; //; // TODO: We could assert the top bit is 0 for the source copy.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Performance,optimiz,optimized,38,"// s_buffer_load, because of how it's optimized, can't be volatile; // so reject ones with the volatile bit set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:86,Safety,avoid,avoid,86,"// On targets not supporting constant in soffset field, turn zero to; // SGPR_NULL to avoid generating an extra s_mov with zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Performance,load,load,38,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI and handle TFE loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:114,Performance,load,loads,114,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI and handle TFE loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,// cachepolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachepolicy,3,"// cachepolicy, swizzled buffer",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Performance,Perform,Perform,101,// If reference to barrier id is not an inline constant then it must be; // referenced with M0[4:0]. Perform an OR with the member count to; // include it in M0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:353,Availability,down,down,353,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:206,Energy Efficiency,power,power,206,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:304,Performance,load,load,304,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Integrability,Wrap,Wrap,3,// Wrap a global or flat pointer into a buffer intrinsic using the flags; // specified in the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:34,Performance,load,loads,34,// Handle 8 bit and 16 bit buffer loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:35,Performance,load,loads,35,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Performance,load,loads,67,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:136,Performance,load,loads,136,// Try to turn 8 and 16-bit scalar loads into SMEM eligible 32-bit loads.; // TODO: Skip this on GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Performance,load,loads,19,// FIXME: Constant loads should all be marked invariant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:61,Performance,load,load,61,"// Don't do this early, since it may interfere with adjacent load merging for; // illegal types. We can avoid losing alignment information for exotic types; // pre-legalize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:104,Safety,avoid,avoid,104,"// Don't do this early, since it may interfere with adjacent load merging for; // illegal types. We can avoid losing alignment information for exotic types; // pre-legalize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:122,Performance,load,load,122,"// We may need to handle exotic cases, such as i16->i64 extloads, so insert; // the appropriate extension from the 32-bit load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:56,Security,access,access,56,// TODO: Should check if the address can definitely not access stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Performance,load,load,37,"// FIXME: Copied from PPC; // First, load into 32 bits, then truncate to 1 bit.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Security,access,access,51,// If there is a possibility that flat instruction access scratch memory; // then we need to use the same legalization rules we use for private.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Performance,load,loads,15,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Performance,load,loads,139,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:15,Performance,load,loads,15,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Performance,load,loads,139,"// Non-uniform loads will be selected to MUBUF instructions, so they; // have the same legalization requirements as global and private; // loads.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Performance,load,loads,6,// v3 loads not supported on SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:13,Performance,load,loads,13,// v3 and v4 loads are supported for private and global memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Integrability,Depend,Depending,3,"// Depending on the setting of the private_element_size field in the; // resource descriptor, we can only make private accesses up to a certain; // size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:119,Security,access,accesses,119,"// Depending on the setting of the private_element_size field in the; // resource descriptor, we can only make private accesses up to a certain; // size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Performance,load,loads,6,// v3 loads not supported on SI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:112,Availability,error,error,112,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:429,Availability,error,error,429,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:365,Safety,Unsafe,UnsafeFPMath,365,"// v_rcp_f32 and v_rsq_f32 do not support denormals, and according to; // the CI documentation has a worst case error of 1 ulp.; // OpenCL requires <= 2.5 ulp for 1.0 / x, so it should always be OK to; // use it as long as we aren't trying to use denormals.; //; // v_rcp_f16 and v_rsq_f16 DO support denormals and 0.51ulp.; // 1.0 / sqrt(x) -> rsq(x); // XXX - Is UnsafeFPMath sufficient to do this for f64? The maximum ULP; // error seems really high at 2^29 ULP.; // 1.0 / x -> rcp(x)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Integrability,depend,dependence,96,"// Note we can't use the STRICT_FMA/STRICT_FMUL for the non-strict FDIV; // lowering. The chain dependence is insufficient, and we need glue. We do; // not need the glue variants in a strictfp function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Usability,usab,usable,88,// Workaround a hardware bug on SI where the condition output from div_scale; // is not usable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:51,Security,access,access,51,// If there is a possibility that flat instruction access scratch memory; // then we need to use the same legalization rules we use for private.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:70,Availability,error,error,70,// Probably an invalid store. If so we'll end up emitting a selection error.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid the full correct expansion for f32 sqrt when promoting from f16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Performance,optimiz,optimizations,96,//===----------------------------------------------------------------------===//; // Custom DAG optimizations; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Energy Efficiency,Reduce,Reduce,3,"// Reduce width of sign operand, we only need the highest bit.; //; // fcopysign f64:x, f64:y ->; // fcopysign f64:x, (extract_vector_elt (bitcast f64:y to v2f32), 1); // TODO: In some cases it might make sense to go all the way to f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:741,Usability,simpl,simplified,741,"// (shl (add x, c1), c2) -> add (shl x, c2), (shl c1, c2); // (shl (or x, c1), c2) -> add (shl x, c2), (shl c1, c2) iff x and c1 share no; // bits; // This is a variant of; // (mul (add x, c1), c2) -> add (mul x, c2), (mul c1, c2),; //; // The normal DAG combiner will do this, but only if the add has one use since; // that would increase the number of instructions.; //; // This prevents us from seeing a constant offset that can be folded into a; // memory instruction's addressing mode. If we know the resulting add offset of; // a pointer can be folded into an addressing offset, we can replace the pointer; // operand with the add of new constant offset. This eliminates one of the uses,; // and may allow the remaining use to also be simplified.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:141,Security,expose,exposes,141,// Break up 64-bit bit operation of a constant into two 32-bit and/or/xor. This; // will typically happen anyway for a VALU 64-bit and. This exposes other 32-bit; // integer combine opportunities since most 64-bit operations are decomposed; // this way. TODO: We won't want this for SALU especially if it is an inline; // immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:87,Safety,Avoid,Avoid,87,"// If we need to materialize a 64-bit immediate, it will be split up later; // anyway. Avoid creating the harder to understand 64-bit immediate; // materialization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:33,Availability,mask,mask,33,// 0xff for any zero byte in the mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:96,Availability,mask,masking,96,// Check if a node selects whole bytes from its operand 0 starting at a byte; // boundary while masking the rest. Returns select mask as in the v_perm_b32; // or -1 if not succeeded.; // Note byte select encoding:; // value 0-3 selects corresponding source byte;; // value 0xc selects zero;; // value 0xff selects 0xff.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:129,Availability,mask,mask,129,// Check if a node selects whole bytes from its operand 0 starting at a byte; // boundary while masking the rest. Returns select mask as in the v_perm_b32; // or -1 if not succeeded.; // Note byte select encoding:; // value 0-3 selects corresponding source byte;; // value 0xc selects zero;; // value 0xff selects 0xff.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Availability,mask,mask,19,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:47,Availability,mask,mask,47,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Availability,mask,mask,101,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:120,Performance,optimiz,optimized,120,"// and (srl x, c), mask => shl (bfe x, nb + c, mask >> nb), nb; // nb = number of trailing zeroes in mask; // It can be optimized out using SDWA for GFX8+ in the SDWA peephole pass,; // given that we are selecting 8 or 16 bit fields starting at byte boundary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:33,Availability,mask,mask,33,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:54,Availability,mask,mask,54,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:113,Availability,mask,mask,113,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:134,Availability,mask,mask,134,"// and (fcmp seto), (fp_class x, mask) -> fp_class x, mask & ~(p_nan | n_nan); // and (fcmp setuo), (fp_class x, mask) -> fp_class x, mask & (p_nan | n_nan)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:66,Availability,mask,masks,66,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Availability,mask,masks,123,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Availability,mask,mask,67,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:103,Availability,mask,mask,103,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:21,Availability,mask,mask,21,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:45,Availability,mask,mask,45,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Availability,mask,masks,95,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:172,Availability,mask,mask,172,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:216,Availability,mask,mask,216,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:260,Availability,mask,masks,260,"// Each byte in each mask is either selector mask 0-3, or has higher; // bits set in either of masks, which can be 0xff for 0xff or 0x0c for; // zero. If 0x0c is in either mask it shall always be 0x0c. Otherwise; // mask which is not 0xff wins. By anding both masks we have a correct; // result except that 0x0c shall be corrected to give 0x0c only.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:478,Performance,Load,LoadCombine,478,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:509,Performance,load,loads,509,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1478,Performance,perform,performed,1478,"// A key component of v_perm is a mapping between byte position of the src; // operands, and the byte position of the dest. To provide such, we need: 1. the; // node that provides x byte of the dest of the OR, and 2. the byte of the node; // used to provide that x byte. calculateByteProvider finds which node provides; // a certain byte of the dest of the OR, and calculateSrcByte takes that node,; // and finds an ultimate src and byte position For example: The supported; // LoadCombine pattern for vector loads is as follows; // t1; // or; // / \; // t2 t3; // zext shl; // | | \; // t4 t5 16; // or anyext; // / \ |; // t6 t7 t8; // srl shl or; // / | / \ / \; // t9 t10 t11 t12 t13 t14; // trunc* 8 trunc* 8 and and; // | | / | | \; // t15 t16 t17 t18 t19 t20; // trunc* 255 srl -256; // | / \; // t15 t15 16; //; // *In this example, the truncs are from i32->i16; //; // calculateByteProvider would find t6, t7, t13, and t14 for bytes 0-3; // respectively. calculateSrcByte would find (given node) -> ultimate src &; // byteposition: t6 -> t15 & 1, t7 -> t16 & 0, t13 -> t15 & 0, t14 -> t15 & 3.; // After finding the mapping, we can combine the tree into vperm t15, t16,; // 0x05000407; // Find the source and byte position from a node.; // \p DestByte is the byte position of the dest of the or that the src; // ultimately provides. \p SrcIndex is the byte of the src that maps to this; // dest of the or byte. \p Depth tracks how many recursive iterations we have; // performed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:386,Performance,perform,performed,386,"// For a byte position in the result of an Or, traverse the tree and find the; // node (and the byte of the node) which ultimately provides this {Or,; // BytePosition}. \p Op is the operand we are currently examining. \p Index is; // the byte position of the Op that corresponds with the originally requested; // byte of the Or \p Depth tracks how many recursive iterations we have; // performed. \p StartingIndex is the originally requested byte of the Or",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Performance,load,load,23,"// If the width of the load does not reach byte we are trying to provide for; // and it is not a ZEXTLOAD, then the load does not provide for the byte in; // question",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:116,Performance,load,load,116,"// If the width of the load does not reach byte we are trying to provide for; // and it is not a ZEXTLOAD, then the load does not provide for the byte in; // question",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Availability,mask,mask,23,"// Returns true if the mask matches consecutive bytes, and the first byte; // begins at a power of 2 byte offset from 0th byte",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:90,Energy Efficiency,power,power,90,"// Returns true if the mask matches consecutive bytes, and the first byte; // begins at a power of 2 byte offset from 0th byte",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:13,Availability,mask,mask,13,"// Since the mask is applied to Src1:Src2, Src1 bytes must be offset; // by sizeof(Src2) = 4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:284,Modifiability,extend,extended,284,"// If the ultimate src is less than 32 bits, then we will only be; // using bytes 0: Op.getValueSizeInBytes() - 1 in the or.; // CalculateByteProvider would not have returned Op as source if we; // used a byte that is outside its ValueType. Thus, we are free to; // ANY_EXTEND as the extended bits are dont-cares.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:66,Availability,mask,masks,66,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:123,Availability,mask,masks,123,// Canonicalize the expression in an attempt to have fewer unique masks; // and therefore fewer registers used to hold the masks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:67,Availability,mask,mask,67,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:103,Availability,mask,mask,103,"// Select 0xc for each lane used from source operand. Zero has 0xc mask; // set, 0xff have 0xff in the mask, actual lanes are in the 0-3 range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,Availability,mask,mask,37,// Kill zero bytes selected by other mask. Zero value is 0xc.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:11,Availability,mask,masks,11,// Combine masks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,cache,cachePolicy,3,// cachePolicy,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:148,Safety,unsafe,unsafe,148,"// If it's free to do so, push canonicalizes further up the source, which may; // find a canonical source.; //; // TODO: More opcodes. Note this is unsafe for the _ieee minnum/maxnum for; // sNaNs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:77,Availability,avail,available,77,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:23,Modifiability,extend,extend,23,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:180,Modifiability,extend,extended,180,"// Note: we could also extend to i32 and use i32 med3 if i16 med3 is; // not available, but this is unlikely to be profitable as constants; // will often need to be materialized & extended, especially on; // pre-GFX10 where VOP3 instructions couldn't take literal operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:24,Availability,avail,available,24,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:52,Availability,avail,available,52,"// med3 for f16 is only available on gfx9+, and not available for v2f16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Safety,safe,safe,14,"// This isn't safe with signaling NaNs because in IEEE mode, min/max on a; // signaling NaN gives a quiet NaN. The quiet NaN input to the min would; // then give the other result, which is different from med3 with a NaN; // input.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:35,Safety,safe,safe,35,"// const_a, const_b, x -> clamp is safe in all cases including signaling; // nans.; // FIXME: Should this be allowing -0.0?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:46,Availability,avail,available,46,// On some architectures (GFX9) movrel is not available and it's better; // to expand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:16,Availability,avail,available,16,"// If movrel is available, use it instead of expanding for vector of 8; // elements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,Performance,load,load,110,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:25,Security,access,accesses,25,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:50,Security,access,accesses,50,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:97,Security,expose,exposes,97,// Try to turn sub-dword accesses of vectors into accesses of the same 32-bit; // elements. This exposes more load reduction opportunities by replacing; // multiple small extract_vector_elements with a single 32-bit extract.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize f16 fmed3 pattern performed on f32. On gfx8 there is no f16 fmed3,; // and expanding it with min/max saves 1 instruction vs. casting to f32 and; // casting back.; // fptrunc (f32 (fmed3 (fpext f16:a, fpext f16:b, fpext f16:c))) =>; // fmin(fmax(a, b), fmax(fmin(a, b), c))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Performance,perform,performed,30,"// Optimize f16 fmed3 pattern performed on f32. On gfx8 there is no f16 fmed3,; // and expanding it with min/max saves 1 instruction vs. casting to f32 and; // casting back.; // fptrunc (f32 (fmed3 (fpext f16:a, fpext f16:b, fpext f16:c))) =>; // fmin(fmax(a, b), fmax(fmin(a, b), c))",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:31,Performance,perform,perform,31,"// For a reassociatable opcode perform:; // op x, (op y, z) -> op (op x, z), y, if x and z are uniform",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Avoid,Avoid,3,"// Avoid the fold if it would unduly increase the number of multiplies due to; // multiple uses, except on hardware with full-rate multiply-add (which is; // part of full-rate 64-bit ops).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:94,Modifiability,extend,extended,94,"// The operands and final result all have the same number of bits. If; // operands need to be extended, they can be extended with garbage. The; // resulting garbage in the high bits of the mad_[iu]64_[iu]32 result is; // truncated away in the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:116,Modifiability,extend,extended,116,"// The operands and final result all have the same number of bits. If; // operands need to be extended, they can be extended with garbage. The; // resulting garbage in the high bits of the mad_[iu]64_[iu]32 result is; // truncated away in the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:271,Integrability,depend,depending,271,"// The basic code generated is conceptually straightforward. Pseudo code:; //; // accum = mad_64_32 lhs.lo, rhs.lo, accum; // accum.hi = add (mul lhs.hi, rhs.lo), accum.hi; // accum.hi = add (mul lhs.lo, rhs.hi), accum.hi; //; // The second and third lines are optional, depending on whether the factors; // are {sign,zero}-extended or not.; //; // The actual DAG is noisier than the pseudo code, but only due to; // instructions that disassemble values into low and high parts, and; // assemble the final result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:324,Modifiability,extend,extended,324,"// The basic code generated is conceptually straightforward. Pseudo code:; //; // accum = mad_64_32 lhs.lo, rhs.lo, accum; // accum.hi = add (mul lhs.hi, rhs.lo), accum.hi; // accum.hi = add (mul lhs.lo, rhs.hi), accum.hi; //; // The second and third lines are optional, depending on whether the factors; // are {sign,zero}-extended or not.; //; // The actual DAG is noisier than the pseudo code, but only due to; // instructions that disassemble values into low and high parts, and; // assemble the final result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:82,Availability,mask,mask,82,"// Attempt to find Src vector which contains our SDValue, if so, add our; // perm mask to the existing one. If we are unable to find a match for the; // first SDValue, attempt to find match for the second.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:95,Availability,mask,mask,95,"// If we have multiple sources in the chain, combine them via perms (using; // calculated perm mask) and Ors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:182,Usability,simpl,simply,182,"// There are 9 possible permutations of; // {S0IsUnsigned, S0IsSigned, S1IsUnsigned, S1IsSigned}; // In two permutations, the sign bits are known to be the same for both Ops,; // so simply return Signed / Unsigned corresponding to the MSB",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:269,Modifiability,extend,extended,269,"// In the remaining five permutations, we don't know the value of the sign; // bit for at least one Op. Since we have a valid ByteProvider, we know that; // the upper bits must be extension bits. Thus, the only ways for the sign; // bit to be unknown is if it was sign extended from unknown value, or if it; // was any extended. In either case, it is correct to use the signed; // version of the signedness semantics of dot4; // In two of such permutations, we known the sign bit is set for; // one op, and the other is unknown. It is okay to used signed version of; // dot4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:319,Modifiability,extend,extended,319,"// In the remaining five permutations, we don't know the value of the sign; // bit for at least one Op. Since we have a valid ByteProvider, we know that; // the upper bits must be extension bits. Thus, the only ways for the sign; // bit to be unknown is if it was sign extended from unknown value, or if it; // was any extended. In either case, it is correct to use the signed; // version of the signedness semantics of dot4; // In two of such permutations, we known the sign bit is set for; // one op, and the other is unknown. It is okay to used signed version of; // dot4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Availability,Mask,Masks,3,"// Masks were constructed with assumption that we would find a chain of; // length 4. If not, then we need to 0 out the MSB bits (via perm mask of; // 0x0c) so they do not affect dot calculation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:139,Availability,mask,mask,139,"// Masks were constructed with assumption that we would find a chain of; // length 4. If not, then we need to 0 out the MSB bits (via perm mask of; // 0x0c) so they do not affect dot calculation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:62,Usability,undo,undoes,62,// Try to get the fneg to fold into the source modifier. This undoes generic; // DAG combines and folds them into the mad.; //; // Only do this if we are not trying to support denormals. v_mad_f32 does; // not support denormals ever.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:8,Integrability,contract,contract,8,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:28,Integrability,contract,contract,28,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Integrability,contract,contract,64,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:85,Integrability,contract,contract,85,"// fdiv contract 1.0, (sqrt contract x) -> rsq for f16; // fdiv contract -1.0, (sqrt contract x) -> fneg(rsq) for f16",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:150,Integrability,contract,contract,150,"// fdot2_f32_f16 always flushes fp32 denormal operand and output to zero,; // regardless of the denorm mode setting. Therefore,; // unsafe-fp-math/fp-contract is sufficient to allow generating fdot2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:132,Safety,unsafe,unsafe-fp-math,132,"// fdot2_f32_f16 always flushes fp32 denormal operand and output to zero,; // regardless of the denorm mode setting. Therefore,; // unsafe-fp-math/fp-contract is sufficient to allow generating fdot2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Modifiability,Extend,Extend,9,// TODO: Extend type shouldn't matter (assuming legal types).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:6,Usability,simpl,simplified,6,"// We simplified Src. If this node is not dead, visit it again so it is; // folded properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:60,Testability,assert,assert,60,"// These are folded out, but on the chance it happens don't assert.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if we can't understand the usage,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if we have more than one user per component.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Safety,Abort,Abort,3,// Abort if there's no change,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:3,Deployability,Update,Update,3,// Update the users of the node with the new indices,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Deployability,update,updated,88,/// Fold the instructions after selecting them.; /// Returns null if users were already updated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:150,Availability,failure,failure,150,// Any MIMG instructions that use tfe or lwe require an initialization of the; // result register that will be written in the case of a memory access failure.; // The required code is also added to tie this init code to the result of the; // img instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:143,Security,access,access,143,// Any MIMG instructions that use tfe or lwe require an initialization of the; // result register that will be written in the case of a memory access failure.; // The required code is also added to tie this init code to the result of the; // img instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:79,Availability,error,error,79,// Abandon attempt if the dst size isn't large enough; // - this is in fact an error but this is picked up elsewhere and; // reported correctly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:167,Availability,error,error,167,"// Final initialized value will be in here; // If PRTStrictNull feature is enabled (the default) then initialize; // all the result registers to 0, otherwise just the error indication; // register (VGPRn+1)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Integrability,depend,depending,30,/// Assign the register class depending on the number of; /// bits set in the writemask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:59,Security,access,access,59,// Figure out which registers should be reserved for stack access. Only after; // the function is legalized do we know all of the non-spill stack objects or if; // calls are present.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:58,Security,access,access,58,// Callable functions have fixed registers used for stack access.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Testability,log,logic,19,// TODO: Move this logic to getReservedRegs(); // Reserve the SGPR(s) to save/restore EXEC for WWM spill/copy handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:88,Testability,test,testcases,88,// We need to worry about replacing the default register with itself in case; // of MIR testcases missing the MFI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:31,Performance,cache,cache,31,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:80,Performance,cache,cache,80,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:360,Performance,cache,cache,360,"// On GFX10 I$ is 4 x 64 bytes cache lines.; // By default prefetcher keeps one cache line behind and reads two ahead.; // We can modify it with S_INST_PREFETCH for larger loops to have two lines; // behind and one ahead.; // Therefor we can benefit from aligning loop headers if loop fits 192 bytes.; // If loop fits 64 bytes it always spans no more than two cache lines and; // does not need an alignment.; // Else if loop is less or equal 128 bytes we do not need to modify prefetch,; // Else if loop is less or equal 192 bytes we need two lines behind.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:10,Performance,load,load,10,// A flat load may access private memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,Security,access,access,19,// A flat load may access private memory.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:129,Energy Efficiency,efficient,efficient,129,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:14,Safety,unsafe,unsafe-fp-atomics,14,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:64,Safety,unsafe,unsafe,64,"// The amdgpu-unsafe-fp-atomics attribute enables generation of unsafe; // floating point atomic instructions. May generate more efficient code,; // but may not respect rounding and denormal modes, and may give incorrect; // results for certain memory destinations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:118,Availability,mask,mask,118,// FIXME: This is a workaround for DivergenceAnalysis not understanding always; // uniform values (as produced by the mask results of control flow intrinsics); // used outside of divergent blocks. The phi users need to also be treated as; // always uniform.; //; // FIXME: DA is no longer in-use. Does this still apply to UniformityAnalysis?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:38,Availability,mask,mask,38,// FIXME: We assume we never cast the mask results of a control flow; // intrinsic.; // Early exit if the type won't be consistent as a compile time hack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:53,Security,access,access,53,// Check if we have a good chance to form the memory access pattern with the; // base and offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:73,Performance,load,load,73,// Propagate metadata set by AMDGPUAnnotateUniformValues to the MMO of a load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:425,Performance,load,loaded,425,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:805,Performance,load,loaded,805,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:822,Performance,load,load,822,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:892,Performance,load,loaded,892,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1092,Performance,load,loaded,1092,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1234,Performance,load,loaded,1234,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1260,Performance,load,loaded,1260,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1303,Performance,load,loaded,1303,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:1348,Performance,load,loaded,1348,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:125,Deployability,release,release,125,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:181,Deployability,release,release,181,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:271,Deployability,release,release,271,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:7,Performance,optimiz,optimization,7,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:70,Performance,cache,cache,70,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:232,Safety,Avoid,Avoid,232,"// The optimization removes store aspect of the atomicrmw. Therefore, cache; // must be flushed if the atomic ordering had a release semantics. This is; // not necessary a fence, a release fence just coincides to do that flush.; // Avoid replacing of an atomicrmw with a release semantics.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:43,Integrability,Interface,Interface,43,"//===-- SIISelLowering.h - SI DAG Lowering Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:411,Integrability,interface,interface,411,"//===-- SIISelLowering.h - SI DAG Lowering Interface ------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI DAG Lowering interface definition; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:38,Performance,load,load,38,"// Call DAG.getMemIntrinsicNode for a load, but first widen a dwordx3 type to; // dwordx4 if on SI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:107,Modifiability,extend,extending,107,"/// Converts \p Op, which must be of floating point type, to the; /// floating point type \p VT, by either extending or truncating it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:213,Testability,log,logic,213,"// Convert the i128 that an addrspace(8) pointer is natively represented as; // into the v4i32 that all the buffer intrinsics expect to receive. We can't; // add register classes for i128 on pain of the promotion logic going haywire,; // so this slightly ugly hack is what we've got. If passed a non-pointer; // argument (as would be seen in older buffer intrinsics), does nothing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:3,Integrability,Wrap,Wrap,3,// Wrap a 64-bit pointer into a v4i32 (which is how all SelectionDAG code; // represents ptr addrspace(8)) using the flags specified in the intrinsic.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:34,Performance,load,loads,34,// Handle 8 bit and 16 bit buffer loads,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:2,Availability,Mask,Mask,2,/*Mask*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h:2,Security,Access,AccessTy,2,/*AccessTy*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:30,Modifiability,config,configured,30,// Check if hardware has been configured to expect color or depth exports.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:3,Deployability,Update,Update,3,// Update dominator tree,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize out branches to the next block.; // This only occurs in -O0 when BranchFolding is not executed.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILateBranchLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1797,Energy Efficiency,schedul,scheduling,1797,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:2309,Energy Efficiency,reduce,reduced,2309,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1687,Performance,load,loading,1687,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:2144,Performance,load,loads,2144,"nstructions with close by immediate offsets.; // This will fuse operations such as; // ds_read_b32 v0, v2 offset:16; // ds_read_b32 v1, v2 offset:32; // ==>; // ds_read2_b32 v[0:1], v2, offset0:4 offset1:8; //; // The same is done for certain SMEM and VMEM opcodes, e.g.:; // s_buffer_load_dword s4, s[0:3], 4; // s_buffer_load_dword s5, s[0:3], 8; // ==>; // s_buffer_load_dwordx2 s[4:5], s[0:3], 4; //; // This pass also tries to promote constant offset to the immediate by; // adjusting the base. It tries to use a base from the nearby instructions that; // allows it to have a 13bit constant offset and then promotes the 13bit offset; // to the immediate.; // E.g.; // s_movk_i32 s0, 0x1800; // v_add_co_u32_e32 v0, vcc, s0, v2; // v_addc_co_u32_e32 v1, vcc, 0, v6, vcc; //; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[0:1], off; // =>; // s_movk_i32 s0, 0x1000; // v_add_co_u32_e32 v5, vcc, s0, v2; // v_addc_co_u32_e32 v6, vcc, 0, v6, vcc; // global_load_dwordx2 v[5:6], v[5:6], off; // global_load_dwordx2 v[0:1], v[5:6], off offset:2048; //; // Future improvements:; //; // - This is currently missing stores of constants because loading; // the constant into the data register is placed between the stores, although; // this is arguably a scheduling problem.; //; // - Live interval recomputing seems inefficient. This currently only matches; // one pair, and recomputes live intervals and moves on to the next pair. It; // would be better to compute a list of all merges that need to occur.; //; // - With a list of instructions to process, we can also merge more. If a; // cluster of loads have offsets that are too large to fit in the 8-bit; // offsets, but are close enough to fit in the 8 bits, we can add to the base; // pointer and use the new reduced offsets.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:10,Performance,load,loads,10,"// GLOBAL loads and stores are classified as FLAT initially. If both combined; // instructions are FLAT GLOBAL adjust the class to GLOBAL_LOAD or GLOBAL_STORE.; // If either or both instructions are non segment specific FLAT the resulting; // combined operation will be FLAT, potentially promoting one of the GLOBAL; // operations to FLAT.; // For other instructions return the original unmodified class.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:125,Security,access,access,125,// Given that \p CI and \p Paired are adjacent memory operations produce a new; // MMO for the combined operation with a new access size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:86,Energy Efficiency,power,power,86,"// Return the value in the inclusive range [Lo,Hi] that is aligned to the; // highest power of two. Note that the result is well defined for all inputs; // including corner cases like:; // - if Lo == Hi, return that value; // - if Lo == 0, return 0 (even though the ""- 1"" below underflows; // - if Lo > Hi, return 0 (as if the range wrapped around)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:333,Integrability,wrap,wrapped,333,"// Return the value in the inclusive range [Lo,Hi] that is aligned to the; // highest power of two. Note that the result is well defined for all inputs; // including corner cases like:; // - if Lo == Hi, return that value; // - if Lo == 0, return 0 (even though the ""- 1"" below underflows; // - if Lo > Hi, return 0 (as if the range wrapped around)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:67,Performance,load,loads,67,"// TODO: Should be possible to support more formats, but if format loads; // are not dword-aligned, the merged load might not be valid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:111,Performance,load,load,111,"// TODO: Should be possible to support more formats, but if format loads; // are not dword-aligned, the merged load might not be valid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:191,Performance,load,load,191,"// Reject cases like:; // dword + dwordx2 -> dwordx3; // dword + dwordx3 -> dwordx4; // If we tried to combine these cases, we would fail to extract a subreg; // for the result of the second load due to SGPR alignment requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:39,Energy Efficiency,reduce,reduced,39,// Check if the new offsets fit in the reduced 8-bit range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:104,Energy Efficiency,power,power,104,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:190,Performance,load,load,190,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:104,Energy Efficiency,power,power,104,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:190,Performance,load,load,190,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:144,Availability,failure,failure,144,/// This function assumes that CI comes before Paired in a basic block. Return; /// an insertion point for the merged instruction or nullptr on failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:26,Availability,mask,masks,26,// Check both offsets (or masks for MIMG) can be combined and fit in the; // reduced range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:77,Energy Efficiency,reduce,reduced,77,// Check both offsets (or masks for MIMG) can be combined and fit in the; // reduced range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:18,Availability,down,down,18,// Try to sink CI down to Paired.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:3,Deployability,Update,Update,3,// Update base and offset with the NewBase and NewOffset in MI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:167,Performance,load,loads,167,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:213,Performance,load,load,213,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:259,Performance,load,load,259,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:305,Performance,load,load,305,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:352,Performance,load,load,352,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:399,Performance,load,load,399,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:446,Performance,load,load,446,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:456,Performance,optimiz,optimization,456,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:921,Performance,load,load,921,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:951,Performance,load,load,951,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:981,Performance,load,load,981,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1007,Performance,load,load,1007,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:1056,Performance,load,load,1056,"// Step2: Traverse through MI's basic block and find an anchor(that has the; // same base-registers) with the highest 13bit distance from MI's offset.; // E.g. (64bit loads); // bb:; // addr1 = &a + 4096; load1 = load(addr1, 0); // addr2 = &a + 6144; load2 = load(addr2, 0); // addr3 = &a + 8192; load3 = load(addr3, 0); // addr4 = &a + 10240; load4 = load(addr4, 0); // addr5 = &a + 12288; load5 = load(addr5, 0); //; // Starting from the first load, the optimization will try to find a new base; // from which (&a + 4096) has 13 bit distance. Both &a + 6144 and &a + 8192; // has 13bit distance from &a + 4096. The heuristic considers &a + 8192; // as the new-base(anchor) because of the maximum distance which can; // accommodate more intermediate bases presumably.; //; // Step3: move (&a + 8192) above load1. Compute and promote offsets from; // (&a + 8192) for load1, load2, load4.; // addr = &a + 8192; // load1 = load(addr, -4096); // load2 = load(addr, -2048); // load3 = load(addr, 0); // load4 = load(addr, 2048); // addr5 = &a + 12288; load5 = load(addr5, 0); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:89,Performance,load,load,89,// TODO: Support finding an anchor(with same base) from store addresses or; // any other load addresses where the opcodes are different.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:18,Security,access,accesses,18,"// Treat volatile accesses, ordered accesses and unmodeled side effects as; // barriers. We can look after this barrier for separate merges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:36,Security,access,accesses,36,"// Treat volatile accesses, ordered accesses and unmodeled side effects as; // barriers. We can look after this barrier for separate merges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:15,Usability,resume,resume,15,// Search will resume after this instruction in a separate merge list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:122,Energy Efficiency,schedul,scheduler,122,"// Scan through looking for adjacent LDS operations with constant offsets from; // the same base register. We rely on the scheduler to do the hard work of; // clustering nearby loads, and assume these are all adjacent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:177,Performance,load,loads,177,"// Scan through looking for adjacent LDS operations with constant offsets from; // the same base register. We rely on the scheduler to do the hard work of; // clustering nearby loads, and assume these are all adjacent.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:129,Performance,optimiz,optimize,129,"// We weren't able to make any changes, so delete the list so we don't; // process the same instructions the next time we try to optimize this; // block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:64,Performance,optimiz,optimization,64,"// We made changes, but also determined that there were no more optimization; // opportunities, so we don't need to reprocess the list",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1352,Availability,mask,mask,1352,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1430,Availability,mask,mask,1430,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1744,Availability,mask,mask,1744,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1832,Availability,mask,mask,1832,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:2054,Availability,mask,mask,2054,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:663,Deployability,update,update,663,"//===-- SILowerControlFlow.cpp - Use predicates for control flow ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %e",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1022,Deployability,update,update,1022,"//===-- SILowerControlFlow.cpp - Use predicates for control flow ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %e",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1336,Deployability,update,update,1336,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1816,Deployability,Update,Update,1816,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1506,Performance,optimiz,optimization,1506,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1882,Performance,optimiz,optimization,1882,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:1398,Usability,Clear,Clear,1398,"with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass lowers the pseudo control flow instructions to real; /// machine instructions.; ///; /// All control flow is handled using predicated instructions and; /// a predicate stack. Each Scalar ALU controls the operations of 64 Vector; /// ALUs. The Scalar ALU can update the predicate for any of the Vector ALUs; /// by writing to the 64-bit EXEC register (each bit corresponds to a; /// single vector ALU). Typically, for predicates, a vector ALU will write; /// to its bit of the VCC register (like EXEC VCC is 64-bits, one for each; /// Vector ALU) and then the ScalarALU will AND the VCC register with the; /// EXEC to update the predicates.; ///; /// For example:; /// %vcc = V_CMP_GT_F32 %vgpr1, %vgpr2; /// %sgpr0 = SI_IF %vcc; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0; /// %sgpr0 = SI_ELSE %sgpr0; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr0; /// SI_END_CF %sgpr0; ///; /// becomes:; ///; /// %sgpr0 = S_AND_SAVEEXEC_B64 %vcc // Save and update the exec mask; /// %sgpr0 = S_XOR_B64 %sgpr0, %exec // Clear live bits from saved exec mask; /// S_CBRANCH_EXECZ label0 // This instruction is an optional; /// // optimization which allows us to; /// // branch if all the bits of; /// // EXEC are zero.; /// %vgpr0 = V_ADD_F32 %vgpr0, %vgpr0 // Do the IF block of the branch; ///; /// label0:; /// %sgpr0 = S_OR_SAVEEXEC_B64 %sgpr0 // Restore the exec mask for the Then; /// // block; /// %exec = S_XOR_B64 %sgpr0, %exec // Update the exec mask; /// S_BRANCH_EXECZ label1 // Use our branch optimization; /// // instruction again.; /// %vgpr0 = V_SUB_F32 %vgpr0, %vgpr // Do the THEN block; /// label1:; /// %exec = S_OR_B64 %exec, %sgpr0 // Re-enable saved exec mask bits; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:10,Availability,redundant,redundant,10,// Remove redundant SI_END_CF instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:10,Safety,redund,redundant,10,// Remove redundant SI_END_CF instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:136,Availability,mask,mask,136,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:88,Performance,optimiz,optimize,88,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:161,Usability,clear,cleared,161,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:120,Safety,safe,safe,120,// Check for SI_KILL_*_TERMINATOR on path from if to endif.; // if there is any such terminator simplifications are not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:96,Usability,simpl,simplifications,96,// Check for SI_KILL_*_TERMINATOR on path from if to endif.; // if there is any such terminator simplifications are not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:45,Energy Efficiency,schedul,scheduling,45,// Add an implicit def of exec to discourage scheduling VALU after this which; // will interfere with trying to form s_and_saveexec_b64 later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:56,Performance,optimiz,optimized,56,// Insert the S_CBRANCH_EXECZ instruction which will be optimized later; // during SIRemoveShortExecBranches.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:142,Deployability,update,update,142,// FIXME: Is there a better way of adjusting the liveness? It shouldn't be; // hard to add another def here but I'm not sure how to correctly update the; // valno.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:50,Availability,mask,mask,50,// This accounts for any modification of the EXEC mask within the block and; // can be optimized out pre-RA when not required.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:87,Performance,optimiz,optimized,87,// This accounts for any modification of the EXEC mask within the block and; // can be optimized out pre-RA when not required.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:59,Availability,mask,masked,59,"// Skip ANDing with exec if the break condition is already masked by exec; // because it is a V_CMP in the same basic block. (We know the break; // condition operand was an i1 in IR, so if it is a VALU instruction it must; // be one with a carry-out.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:84,Availability,mask,mask,84,"// AND the break condition operand with exec, then OR that into the ""loop; // exit"" mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:115,Availability,Alive,AliveBlocks,115,"// Track the set of registers defined in the original block so we don't; // accidentally add the original block to AliveBlocks. AliveBlocks only; // includes blocks which are live through, which excludes live outs and; // local defs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:128,Availability,Alive,AliveBlocks,128,"// Track the set of registers defined in the original block so we don't; // accidentally add the original block to AliveBlocks. AliveBlocks only; // includes blocks which are live through, which excludes live outs and; // local defs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:34,Testability,log,logical,34,"// Returns replace operands for a logical operation, either single result; // for exec or two operands if source was another equivalent operation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:194,Availability,mask,mask,194,"// Search and combine pairs of equivalent instructions, like; // S_AND_B64 x, (S_AND_B64 x, y) => S_AND_B64 x, y; // S_OR_B64 x, (S_OR_B64 x, y) => S_OR_B64 x, y; // One of the operands is exec mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:132,Availability,mask,mask,132,// If the only instruction immediately following this END_CF is another; // END_CF in the only successor we can avoid emitting exec mask restore here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:112,Safety,avoid,avoid,112,// If the only instruction immediately following this END_CF is another; // END_CF in the only successor we can avoid emitting exec mask restore here.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:101,Availability,mask,mask,101,// Only skip inner END_CF if outer ENDCF belongs to SI_IF.; // If that belongs to SI_ELSE then saved mask has an inverted value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:37,Availability,mask,mask,37,// Cleanup bit manipulations on exec mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:19,Deployability,update,update,19,// Note: we cannot update block layout and preserve live intervals;; // hence we must insert a branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:472,Availability,mask,masks,472,"//===-- SILowerI1Copies.cpp - Lower I1 Copies -----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass lowers all occurrences of i1 values (with a vreg_1 register class); // to lane masks (32 / 64-bit scalar registers). The pass assumes machine SSA; // form and a wave-level control flow graph.; //; // Before this pass, values that are semantically i1 and are defined and used; // within the same basic block are already represented as lane masks in scalar; // registers. However, values that cross basic blocks are always transferred; // between basic blocks in vreg_1 virtual registers and are lowered by this; // pass.; //; // The only instructions that use or define vreg_1 virtual registers are COPY,; // PHI, and IMPLICIT_DEF.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:732,Availability,mask,masks,732,"//===-- SILowerI1Copies.cpp - Lower I1 Copies -----------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass lowers all occurrences of i1 values (with a vreg_1 register class); // to lane masks (32 / 64-bit scalar registers). The pass assumes machine SSA; // form and a wave-level control flow graph.; //; // Before this pass, values that are semantically i1 and are defined and used; // within the same basic block are already represented as lane masks in scalar; // registers. However, values that cross basic blocks are always transferred; // between basic blocks in vreg_1 virtual registers and are lowered by this; // pass.; //; // The only instructions that use or define vreg_1 virtual registers are COPY,; // PHI, and IMPLICIT_DEF.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:193,Availability,mask,mask,193,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:280,Availability,mask,mask,280,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:603,Availability,mask,mask,603,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:777,Availability,mask,mask,777,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:926,Availability,mask,mask,926,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:964,Availability,avail,available,964,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:160,Usability,simpl,simply,160,"/// Helper class that determines the relationship between incoming values of a; /// phi in the control flow graph to determine where an incoming value can; /// simply be taken as a scalar lane mask as-is, and where it needs to be; /// merged with another, previously defined lane mask.; ///; /// The approach is as follows:; /// - Determine all basic blocks which, starting from the incoming blocks,; /// a wave may reach before entering the def block (the block containing the; /// phi).; /// - If an incoming block has no predecessors in this set, we can take the; /// incoming value as a scalar lane mask as-is.; /// -- A special case of this is when the def block has a self-loop.; /// - Otherwise, the incoming value needs to be merged with a previously; /// defined lane mask.; /// - If there is a path into the set of reachable blocks that does _not_ go; /// through an incoming block where we can take the scalar lane mask as-is,; /// we need to invent an available value for the SSAUpdater. Choices are; /// 0 and undef, with differing consequences for how to merge values etc.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:1133,Performance,cache,cache,1133,"/// Helper class that detects loops which require us to lower an i1 COPY into; /// bitwise manipulation.; ///; /// Unfortunately, we cannot use LoopInfo because LoopInfo does not distinguish; /// between loops with the same header. Consider this example:; ///; /// A-+-+; /// | | |; /// B-+ |; /// | |; /// C---+; ///; /// A is the header of a loop containing A, B, and C as far as LoopInfo is; /// concerned. However, an i1 COPY in B that is used in C must be lowered to; /// bitwise operations to combine results from different loop iterations when; /// B has a divergent branch (since by default we will compile this code such; /// that threads in a wave are merged at the entry of C).; ///; /// The following rule is implemented to determine whether bitwise operations; /// are required: use the bitwise lowering for a def in block B if a backward; /// edge to B is reachable without going through the nearest common; /// post-dominator of B and all uses of the def.; ///; /// TODO: This rule is conservative because it does not check whether the; /// relevant branches are actually divergent.; ///; /// The class is designed to cache the CFG traversal so that it can be re-used; /// for multiple defs within the same basic block.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:22,Safety,detect,detects,22,"/// Helper class that detects loops which require us to lower an i1 COPY into; /// bitwise manipulation.; ///; /// Unfortunately, we cannot use LoopInfo because LoopInfo does not distinguish; /// between loops with the same header. Consider this example:; ///; /// A-+-+; /// | | |; /// B-+ |; /// | |; /// C---+; ///; /// A is the header of a loop containing A, B, and C as far as LoopInfo is; /// concerned. However, an i1 COPY in B that is used in C must be lowered to; /// bitwise operations to combine results from different loop iterations when; /// B has a divergent branch (since by default we will compile this code such; /// that threads in a wave are merged at the entry of C).; ///; /// The following rule is implemented to determine whether bitwise operations; /// are required: use the bitwise lowering for a def in block B if a backward; /// edge to B is reachable without going through the nearest common; /// post-dominator of B and all uses of the def.; ///; /// TODO: This rule is conservative because it does not check whether the; /// relevant branches are actually divergent.; ///; /// The class is designed to cache the CFG traversal so that it can be re-used; /// for multiple defs within the same basic block.; ///; /// TODO: We could use region analysis to quickly skip over SESE regions during; /// the traversal.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:106,Deployability,update,updater,106,"/// Add undef values dominating the loop and the optionally given additional; /// blocks, so that the SSA updater doesn't have to search all the way to the; /// function entry.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:247,Availability,mask,mask,247,"/// Lower all instructions that def or use vreg_1 registers.; ///; /// In a first pass, we lower COPYs from vreg_1 to vector registers, as can; /// occur around inline assembly. We do this first, before vreg_1 registers; /// are changed to scalar mask registers.; ///; /// Then we lower all defs of vreg_1 registers. Phi nodes are lowered before; /// all others, because phi lowering looks through copies and can therefore; /// often make copy lowering unnecessary.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:63,Usability,simpl,simple,63,// Phis in a loop that are observed outside the loop receive a simple but; // conservatively correct treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:92,Availability,mask,mask,92,/// Return a point at the end of the given \p MBB to insert SALU instructions; /// for lane mask calculation. Take terminators and SCC into account.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp:41,Availability,mask,masked,41,// TODO: check whether CurReg is already masked by EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:473,Availability,mask,mask,473,"//===-- SILowerI1Copies.h --------------------------------------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the PhiLoweringHelper class that implements lane; /// mask merging algorithm for divergent i1 phis.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:395,Integrability,Interface,Interface,395,"//===-- SILowerI1Copies.h --------------------------------------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition of the PhiLoweringHelper class that implements lane; /// mask merging algorithm for divergent i1 phis.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:194,Availability,mask,mask,194,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:224,Availability,mask,mask,224,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h:157,Deployability,Update,UpdatedReg,157,"/// Incoming for lane maks phi as machine instruction, incoming register \p Reg; /// and incoming block \p Block are taken from machine instruction.; /// \p UpdatedReg (if valid) is \p Reg lane mask merged with another lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerI1Copies.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:28,Performance,load,loadRegFromStackSlot,28,// Insert in reverse order. loadRegFromStackSlot can insert; // multiple instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:183,Integrability,wrap,wrapping,183,"// Even when we do not change any CSR, we still want to insert the; // prologue and epilogue of the function.; // So set the save points for those.; // Use the points found by shrink-wrapping, if any.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:88,Deployability,update,updateLiveness,88,"// TODO: To support shrink wrapping, this would need to copy; // PrologEpilogInserter's updateLiveness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:27,Integrability,wrap,wrapping,27,"// TODO: To support shrink wrapping, this would need to copy; // PrologEpilogInserter's updateLiveness.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:115,Energy Efficiency,allocate,allocated,115,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:679,Modifiability,extend,extend,679,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:33,Safety,avoid,avoid,33,"// TODO: This is a workaround to avoid the unmodelled liveness computed with; // whole-wave virtual registers when allocated together with the regular VGPR; // virtual registers. Presently, the liveness computed during the regalloc is; // only uniform (or single lane aware) and it doesn't take account of the; // divergent control flow that exists for our GPUs. Since the WWM registers; // can modify inactive lanes, the wave-aware liveness should be computed for; // the virtual registers to accurately plot their interferences. Without; // having the divergent CFG for the function, it is difficult to implement the; // wave-aware liveness info. Until then, we conservatively extend the liveness; // of the wwm registers into the entire function so that they won't be reused; // without first spilling/splitting their liveranges.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:43,Modifiability,extend,extend,43,// Insert the KILL in the return blocks to extend their liveness untill the; // end of function. Insert a separate KILL for each VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:10,Security,expose,expose,10,"// First, expose any CSR SGPR spills. This is mostly the same as what PEI; // does, but somewhat simpler.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:97,Usability,simpl,simpler,97,"// First, expose any CSR SGPR spills. This is mostly the same as what PEI; // does, but somewhat simpler.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:106,Energy Efficiency,efficient,efficient,106,// Spill callee-saved SGPRs into physical VGPR lanes.; // TODO: This is to ensure the CFIs are static for efficient frame; // unwinding in the debugger. Spilling them into virtual VGPR lanes; // involve regalloc to allocate the physical VGPRs and that might; // cause intermediate spill/split of such liveranges for successful; // allocation. This would result in broken CFI encoding unless the; // regalloc aware CFI generation to insert new CFIs along with the; // intermediate spills is implemented. There is no such support; // currently exist in the LLVM compiler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:215,Energy Efficiency,allocate,allocate,215,// Spill callee-saved SGPRs into physical VGPR lanes.; // TODO: This is to ensure the CFIs are static for efficient frame; // unwinding in the debugger. Spilling them into virtual VGPR lanes; // involve regalloc to allocate the physical VGPRs and that might; // cause intermediate spill/split of such liveranges for successful; // allocation. This would result in broken CFI encoding unless the; // regalloc aware CFI generation to insert new CFIs along with the; // intermediate spills is implemented. There is no such support; // currently exist in the LLVM compiler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp:125,Deployability,update,update,125,"// FIXME: The dead frame indices are replaced with a null register from; // the debug value instructions. We should instead, update it with the; // correct register value. But not sure the register value alone is; // adequate to lower the DIExpression. It should be worked out later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerSGPRSpills.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:578,Availability,mask,mask,578,"//===-- SILowerWWMCopies.cpp - Lower Copies after regalloc ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Lowering the WWM_COPY instructions for various register classes.; /// AMDGPU target generates WWM_COPY instruction to differentiate WWM; /// copy from COPY. This pass generates the necessary exec mask manipulation; /// instructions to replicate 'Whole Wave Mode' and lowers WWM_COPY back to; /// COPY.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:53,Availability,avail,available,53,// We can't determine the liveness info if LIS isn't available. Early return; // in that case and always assume SCC is live.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp:46,Availability,mask,mask,46,"// For WWM vector copies, manipulate the exec mask around the copy; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerWWMCopies.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:41,Availability,down,down,41,"// TODO: Pick a high register, and shift down, similar to a kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:96,Security,access,access,96,"// Non-entry functions have no special inputs for now, other registers; // required for scratch access.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:86,Availability,avail,available,86,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:135,Availability,avail,available,135,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:188,Availability,avail,available,188,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times. For now, reserve highest available VGPR. After; // RA, shift it to the lowest available unused VGPR if the one exist.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:10,Availability,avail,available,10,"// If the available register tuples are aligned with the kernarg to be; // preloaded use that register, otherwise we need to use a set of SGPRs and; // merge them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:159,Energy Efficiency,allocate,allocate,159,// Skip if this is a function with the amdgpu_cs_chain or; // amdgpu_cs_chain_preserve calling convention and this is a scratch register.; // We never need to allocate a spill for these because we don't even need to; // restore the inactive lanes for them (they're scratchier than the usual; // scratch registers).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:3,Deployability,Update,Update,3,// Update various tables with the new VGPR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:20,Availability,avail,available,20,"// Find the highest available register if called before RA to ensure the; // lowest registers are available for allocation. The LaneVGPR, in that; // case, will be shifted back to the lowest range after VGPR allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:98,Availability,avail,available,98,"// Find the highest available register if called before RA to ensure the; // lowest registers are available for allocation. The LaneVGPR, in that; // case, will be shifted back to the lowest range after VGPR allocation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:25,Energy Efficiency,allocate,allocated,25,// This has already been allocated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:25,Energy Efficiency,allocate,allocated,25,// This has already been allocated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:26,Testability,log,logic,26,// FIXME: Move allocation logic out of MachineFunctionInfo and initialize; // once.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:27,Energy Efficiency,allocate,allocated,27,"// All other SGPRs must be allocated on the default stack, so reset the; // stack ID.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:33,Availability,mask,mask,33,// Check and update the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp:13,Deployability,update,update,13,// Check and update the optional mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:54,Integrability,interface,interface,54,"//==- SIMachineFunctionInfo.h - SIMachineFunctionInfo interface --*- C++ -*-==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:52,Modifiability,config,config,52,"/// This class keeps track of the SPI_SP_INPUT_ADDR config register, which; /// tells the hardware which interpolation parameters to load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:133,Performance,load,load,133,"/// This class keeps track of the SPI_SP_INPUT_ADDR config register, which; /// tells the hardware which interpolation parameters to load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:50,Energy Efficiency,allocate,allocate,50,// Registers that may be reserved when RA doesn't allocate enough; // registers to plan for the case where an indirect branch ends up; // being needed during branch relaxation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:168,Usability,usab,usable,168,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:491,Usability,undo,undone,491,"/// Number of bytes of arguments this function has on the stack. If the callee; /// is expected to restore the argument stack this should be a multiple of 16,; /// all usable during a tail call.; ///; /// The alternative would forbid tail call optimisation in some cases: if we; /// want to transfer control from a function with 8-bytes of stack-argument; /// space to a function with 16-bytes then misalignment of this value would; /// make a stack adjustment necessary, which could not be undone by the; /// callee.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:314,Safety,avoid,avoid,314,"// To track the registers used in instructions that can potentially modify the; // inactive lanes. The WWM instructions and the writelane instructions for; // spilling SGPRs to VGPRs fall under such category of operations. The VGPRs; // modified by them should be spilled/restored at function prolog/epilog to; // avoid any undesired outcome. Each entry in this map holds a pair of values,; // the VGPR and its stack slot index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:24,Availability,MASK,MASK,24,// To save/restore EXEC MASK around WWM spills and copies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:27,Energy Efficiency,allocate,allocated,27,// Get the scratch SGPR if allocated to save/restore \p Reg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:25,Energy Efficiency,allocate,allocated,25,// Get all scratch SGPRs allocated to copy/restore the SGPR spills.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:21,Energy Efficiency,allocate,allocated,21,// Check if \p FI is allocated for any SGPR spill to a VGPR lane during PEI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:104,Availability,avail,available,104,// To bring the Physical VGPRs in the highest range allocated for CSR SGPR; // spilling into the lowest available range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:52,Energy Efficiency,allocate,allocated,52,// To bring the Physical VGPRs in the highest range allocated for CSR SGPR; // spilling into the lowest available range.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:95,Security,access,accesses,95,/// Returns the physical register reserved for use as the resource; /// descriptor for scratch accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h:115,Testability,test,tests,115,// Note the unset value for this is AMDGPU::SP_REG rather than; // NoRegister. This is mostly a workaround for MIR tests where state that; // can't be directly computed from the function is not preserved in serialized; // MIR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineFunctionInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Energy Efficiency,Schedul,Scheduler,36,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:406,Energy Efficiency,Schedul,Scheduler,406,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:46,Integrability,Interface,Interface,46,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:416,Integrability,interface,interface,416,"//===-- SIMachineScheduler.cpp - SI Scheduler Interface -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Energy Efficiency,schedul,scheduler,8,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:41,Energy Efficiency,schedul,scheduling,41,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:763,Energy Efficiency,schedul,scheduler,763,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1225,Energy Efficiency,schedul,scheduling,1225,"ction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typica",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1692,Energy Efficiency,schedul,scheduler,1692,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1721,Energy Efficiency,schedul,scheduling,1721,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1827,Energy Efficiency,schedul,schedules,1827,"ge to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2858,Energy Efficiency,schedul,scheduling,2858,"ons into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefro",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5147,Energy Efficiency,schedul,scheduling,5147,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:731,Integrability,depend,dependencies,731,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2636,Integrability,depend,dependencies,2636,"W can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is hi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:240,Performance,load,load,240,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:296,Performance,load,load,296,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:411,Performance,load,load,411,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:467,Performance,load,load,467,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:506,Performance,load,load,506,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:607,Performance,load,load,607,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1107,Performance,load,loading,1107,"tecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1379,Performance,load,loading,1379,"/ . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be co",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1437,Performance,latency,latency,1437,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1468,Performance,perform,performance,1468,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1522,Performance,perform,performance,1522,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1564,Performance,cache,cache,1564,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3032,Performance,latency,latency,3032,"; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3293,Performance,latency,latency,3293,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3311,Performance,latency,latency,3311,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3345,Performance,latency,latency,3345,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3364,Performance,cache,cache,3364,"/ right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instructions inside the block to have a minimal set of dependencies; // on high latencies. It will make it easy to pick blocks to hide specific; // high latencies.; // The block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3711,Performance,latency,latency,3711,"he block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3898,Performance,cache,caches,3898,"cond the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_load",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3965,Performance,cache,cache,3965," do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload th",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4129,Performance,latency,latency,4129,"is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4532,Performance,latency,latency,4532,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4558,Performance,load,loading,4558,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4571,Performance,cache,cache,4571,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4619,Performance,throughput,throughput,4619,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4718,Performance,cache,cache,4718,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4911,Performance,cache,cache,4911,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4987,Performance,cache,cache,4987,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5013,Performance,load,loads,5013,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5029,Performance,latency,latency,5029,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1551,Safety,predict,predict,1551,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:208,Security,access,accessing,208,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:379,Security,access,accessing,379,"// This scheduler implements a different scheduling algorithm than; // GenericScheduler.; //; // There are several specific architecture behaviours that can't be modelled; // for GenericScheduler:; // . When accessing the result of an SGPR load instruction, you have to wait; // for all the SGPR load instructions before your current instruction to; // have finished.; // . When accessing the result of an VGPR load instruction, you have to wait; // for all the VGPR load instructions previous to the VGPR load instruction; // you are interested in to finish.; // . The less the register pressure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3754,Security,access,access,3754,"he block creation algorithm is divided into several steps, and several; // variants can be tried during the scheduling process.; //; // Second the order of the instructions inside the blocks is chosen.; // At that step we do take into account only register usage and hiding; // low latency instructions; //; // Third the block order is chosen, there we try to hide high latencies; // and keep register usage low.; //; // After the third step, a pass is done to improve the hiding of low; // latencies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:5065,Security,access,access,5065,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1764,Usability,simpl,simpler,1764,"ssure, the best load latencies are hidden; //; // Moreover some specifities (like the fact a lot of instructions in the shader; // have few dependencies) makes the generic scheduler have some unpredictable; // behaviours. For example when register pressure becomes high, it can either; // manage to prevent register pressure from going too high, or it can; // increase register pressure even more than if it hadn't taken register; // pressure into account.; //; // Also some other bad behaviours are generated, like loading at the beginning; // of the shader a constant in VGPR you won't need until the end of the shader.; //; // The scheduling problem for SI can distinguish three main parts:; // . Hiding high latencies (texture sampling, etc); // . Hiding low latencies (SGPR constant loading, etc); // . Keeping register usage low for better latency hiding and general; // performance; //; // Some other things can also affect performance, but are hard to predict; // (cache usage, the fact the HW can issue several instructions from different; // wavefronts if different types, etc); //; // This scheduler tries to solve the scheduling problem by dividing it into; // simpler sub-problems. It divides the instructions into blocks, schedules; // locally inside the blocks where it takes care of low latencies, and then; // chooses the order of the blocks by taking care of high latencies.; // Dividing the instructions into blocks helps control keeping register; // usage low.; //; // First the instructions are put into blocks.; // We want the blocks help control register usage and hide high latencies; // later. To help control register usage, we typically want all local; // computations, when for example you create a result that can be consumed; // right away, to be contained in a block. Block inputs and outputs would; // typically be important results that are needed in several locations of; // the shader. Since we do want blocks to help hide high latencies, we want; // the instruction",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4767,Usability,simpl,simple,4767,"cies.; //; // Actually when talking about 'low latency' or 'high latency' it includes; // both the latency to get the cache (or global mem) data go to the register,; // and the bandwidth limitations.; // Increasing the number of active wavefronts helps hide the former, but it; // doesn't solve the latter, thus why even if wavefront count is high, we have; // to try have as many instructions hiding high latencies as possible.; // The OpenCL doc says for example latency of 400 cycles for a global mem; // access, which is hidden by 10 instructions if the wavefront count is 10.; // Some figures taken from AMD docs:; // Both texture and constant L1 caches are 4-way associative with 64 bytes; // lines.; // Constant cache is shared with 4 CUs.; // For texture sampling, the address generation unit receives 4 texture; // addresses per cycle, thus we could expect texture sampling latency to be; // equivalent to 4 instructions in the very best case (a VGPR is 64 work items,; // instructions in a wavefront group are executed every 4 cycles),; // or 16 instructions if the other wavefronts associated to the 3 other VALUs; // of the CU do texture sampling too. (Don't take these figures too seriously,; // as I'm not 100% sure of the computation); // Data exports should get similar latency.; // For constant loading, the cache is shader with 4 CUs.; // The doc says ""a throughput of 16B/cycle for each of the 4 Compute Unit""; // I guess if the other CU don't read the cache, it can go up to 64B/cycle.; // It means a simple s_buffer_load should take one instruction to hide, as; // well as a s_buffer_loadx2 and potentially a s_buffer_loadx8 if on the same; // cache line.; //; // As of today the driver doesn't preload the constants in cache, thus the; // first loads get extra latency. The doc says global memory access can be; // 300-600 cycles. We do not specially take that into account when scheduling; // As we expect the driver to be able to preload the constants soon.; // common code //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:734,Deployability,release,release,734,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:127,Integrability,depend,depend,127,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:232,Integrability,depend,depend,232,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:492,Integrability,depend,depend,492,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Performance,latency,latency,16,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:93,Performance,latency,latency,93,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:147,Performance,latency,latency,147,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:246,Performance,latency,latency,246,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:366,Performance,latency,latency,366,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:446,Performance,latency,latency,446,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:516,Performance,latency,latency,516,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:585,Performance,load,loads,585,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:704,Performance,load,loaded,704,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:765,Performance,load,loading,765,"// Schedule low latency instructions as top as possible.; // Order of priority is:; // . Low latency instructions which do not depend on other low latency; // instructions we haven't waited for; // . Other instructions which do not depend on low latency instructions; // we haven't waited for; // . Low latencies; // . All other instructions; // Goal is to get: low latency instructions - independent instructions; // - (eventually some more low latency instructions); // - instructions that depend on the first low latency instructions.; // If in the block there is a lot of constant loads, the SGPR usage; // could go quite high, thus above the arbitrary limit of 60 will encourage; // use the already loaded constants (in order to release some SGPRs) before; // loading more.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Safety,Predict,Predict,3,// Predict register usage after this instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule something valid.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:57,Availability,alive,alive,57,"// Goes though all SU. RPTracker captures what had to be alive for the SUs; // to execute, and what is still alive at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:109,Availability,alive,alive,109,"// Goes though all SU. RPTracker captures what had to be alive for the SUs; // to execute, and what is still alive at the end.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:1056,Energy Efficiency,schedul,scheduling,1056,"// There is several possibilities to distinguish:; // 1) Reg is not input to any instruction in the block, but is output of one; // 2) 1) + read in the block and not needed after it; // 3) 1) + read in the block but needed in another block; // 4) Reg is input of an instruction but another block will read it too; // 5) Reg is input of an instruction and then rewritten in the block.; // result is not read in the block (implies used in another block); // 6) Reg is input of an instruction and then rewritten in the block.; // result is read in the block and not needed in another block; // 7) Reg is input of an instruction and then rewritten in the block.; // result is read in the block but also needed in another block; // LiveInRegs will contains all the regs in situation 4, 5, 6, 7; // We want LiveOutRegs to contain only Regs whose content will be read after; // in another block, and whose content was written in the current block,; // that is we want it to get 1, 3, 5, 7; // Since we made the MIs of a block to be packed all together before; // scheduling, then the LiveIntervals were correct, and the RPTracker was; // able to correctly handle 5 vs 6, 2 vs 3.; // (Note: This is not sufficient for RPTracker to not do mistakes for case 4); // The RPTracker's LiveOutRegs has 1, 3, (some correct or incorrect)4, 5, 7; // Comparing to LiveInRegs is not sufficient to differentiate 4 vs 5, 7; // The use of findDefBetween removes the case 4.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:33,Availability,down,down,33,// Prepares TopRPTracker for top down scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:38,Energy Efficiency,schedul,scheduling,38,// Prepares TopRPTracker for top down scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule for real now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:4,Deployability,Release,Release,4,/// Release Successors of the SU that are in the block or not.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Scheduling,3,"// Scheduling this node will trigger a wait,; // thus propagate to other instructions that they do not need to wait either.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:49,Energy Efficiency,schedul,scheduling,49,// We remove links from outside blocks to enable scheduling inside the block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:81,Integrability,depend,depend,81,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:300,Integrability,depend,dependencies,300,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:386,Integrability,depend,dependency,386,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:55,Performance,latency,latency,55,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:232,Performance,latency,latency,232,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:334,Performance,latency,latency,334,"// We don't want to put in the same block; // two high latency instructions that depend; // on each other.; // One way would be to check canAddEdge; // in both directions, but that currently is not; // enough because there the high latency order is; // enforced (via links).; // Instead, look at the dependencies between the; // high latency instructions and deduce if it is; // a data dependency or not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:6,Integrability,depend,dependencies,6,// No dependencies between each other,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:21,Integrability,depend,dependency,21,// Check the type of dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:74,Performance,latency,latency,74,"// If in the path to join the two instructions,; // there is another high latency instruction,; // or instructions colored for another block; // abort the merge.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:145,Safety,abort,abort,145,"// If in the path to join the two instructions,; // there is another high latency instruction,; // or instructions colored for another block; // abort the merge.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Integrability,depend,depends,36,"// If one of the SU in the subgraph depends on the result of SU j,; // there'll be a data dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:90,Integrability,depend,dependency,90,"// If one of the SU in the subgraph depends on the result of SU j,; // there'll be a data dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:107,Integrability,depend,depend,107,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:121,Integrability,depend,dependency,121,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:211,Performance,latency,latency,211,"// Add all the required instructions to the block; // These cannot live in another block (because they; // depend (order dependency) on one of the; // instruction in the block, and are required for the; // high latency instruction we add.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:54,Integrability,depend,depending,54,"// Traverse TopDown, and give different colors to SUs depending; // on which combination of High Latencies they depend on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:112,Integrability,depend,depend,112,"// Traverse TopDown, and give different colors to SUs depending; // on which combination of High Latencies they depend on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:48,Availability,down,down,48,// Every combination of colors given by the top down; // and bottom up Reserved node dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:85,Integrability,depend,dependency,85,// Every combination of colors given by the top down; // and bottom up Reserved node dependency,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Performance,latency,latency,8,// High latency instructions: already given.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:30,Integrability,depend,depending,30,// TODO: Attribute new colors depending on color; // combination of children.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:33,Performance,load,loading,33,// No predecessor: Vgpr constant loading.; // Low latency instructions usually have a predecessor (the address),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:50,Performance,latency,latency,50,// No predecessor: Vgpr constant loading.; // Low latency instructions usually have a predecessor (the address),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:82,Energy Efficiency,schedul,scheduled,82,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:141,Energy Efficiency,schedul,schedule,141,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:354,Energy Efficiency,schedul,scheduling,354,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:175,Performance,perform,performance,175,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:220,Safety,safe,safety,220,"// Put all exports together in a block.; // The block will naturally end up being scheduled last,; // thus putting exports at the end of the schedule, which; // is better for performance.; // However we must ensure, for safety, the exports can be put; // together in the same block without any other instruction.; // This could happen, for example, when scheduling after regalloc; // if reloading a spilled register from memory using the same; // register than used in a previous export.; // If that happens, do not regroup the exports.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:71,Integrability,depend,dependencies,71,"// SU is an export instruction. Check whether one of its successor; // dependencies is a non-export, in which case we skip export grouping.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Integrability,depend,dependencies,16,// Ignore these dependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:16,Integrability,depend,depends,16,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:150,Integrability,depend,depended,150,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:304,Integrability,depend,dependencies,304,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:206,Usability,Simpl,Simply,206,"// A non-export depends on us. Skip export grouping.; // Note that this is a bit pessimistic: We could still group all other; // exports that are not depended on by non-exports, directly or; // indirectly. Simply skipping this particular export but grouping all; // others would not account for indirect dependencies.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:26,Energy Efficiency,schedul,scheduling,26,// Restore links previous scheduling variant has overridden.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:9,Integrability,depend,dependencies,9,// Build dependencies between blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:47,Energy Efficiency,schedul,scheduling,47,// Free root and leafs of all blocks to enable scheduling inside them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:9,Energy Efficiency,schedul,schedule,9,// We do schedule a valid scheduling such that a Block corresponds; // to a range of instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:26,Energy Efficiency,schedul,scheduling,26,// We do schedule a valid scheduling such that a Block corresponds; // to a range of instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:142,Deployability,update,update,142,"// Note: the following code, and the part restoring previous position; // is by far the most expensive operation of the Scheduler.; // Do not update CurrentTop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:120,Energy Efficiency,Schedul,Scheduler,120,"// Note: the following code, and the part restoring previous position; // is by far the most expensive operation of the Scheduler.; // Do not update CurrentTop.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update the instruction stream.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:140,Energy Efficiency,schedul,scheduler,140,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:244,Energy Efficiency,schedul,scheduling,244,// Update LiveIntervals.; // Note: Moving all instructions and calling handleMove every time; // is the most cpu intensive operation of the scheduler.; // It would gain a lot if there was a way to recompute the; // LiveIntervals for the entire scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2,Deployability,Update,UpdateFlags,2,/*UpdateFlags=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:64,Energy Efficiency,schedul,schedule,64,// Now we have Block of SUs == Block of MI.; // We do the final schedule for the instructions inside the block.; // The property that all the SUs of the Block are grouped together as MI; // is used for correct reg usage tracking.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update the instruction stream.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Deployability,Update,Update,3,// Update LiveIntervals.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:2,Deployability,Update,UpdateFlags,2,/*UpdateFlags=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:92,Energy Efficiency,schedul,scheduling,92,// Increase LiveOutRegsNumUsages for blocks; // producing registers consumed in another; // scheduling region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:72,Energy Efficiency,schedul,scheduling,72,// Fill LiveRegsConsumers for regs that were already; // defined before scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:3,Energy Efficiency,Schedul,Schedule,3,// Schedule high latencies early so you can hide them better.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:25,Availability,alive,alive,25,// Tracking of currently alive registers to determine VGPR Usage.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:60,Availability,alive,alive,60,"// We produce this register, thus it must not be previously alive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Energy Efficiency,adapt,adapted,8,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:21,Energy Efficiency,schedul,scheduleDAG,21,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,Modifiability,adapt,adapted,8,// Code adapted from scheduleDAG.cpp; // Does a topological sort over the SUs.; // Both TopDown and BottomUp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:284,Performance,load,loads,284,"// Move low latencies further from their user without; // increasing SGPR usage (in general); // This is to be replaced by a better pass that would; // take into account SGPR usage (based on VGPR Usage; // and the corresponding wavefront count), that would; // try to merge groups of loads if it make sense, etc",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:36,Integrability,depend,depends,36,// Moves COPY instructions on which depends; // the low latency instructions too.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:56,Performance,latency,latency,56,// Moves COPY instructions on which depends; // the low latency instructions too.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:20,Energy Efficiency,Schedul,ScheduleDAGMI,20,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:38,Energy Efficiency,Schedul,ScheduleDAGMILive,38,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:129,Energy Efficiency,Schedul,Scheduler,129,"// We reuse several ScheduleDAGMI and ScheduleDAGMILive; // functions, but to make them happy we must initialize; // the default Scheduler implementation (even if we do not; // run it)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:27,Energy Efficiency,schedul,scheduling,27,// Fill some stats to help scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:51,Performance,perform,performing,51,"// if VGPR usage is extremely high, try other good performing variants; // which could lead to lower VGPR usage",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:94,Performance,perform,performing,94,"// if VGPR usage is still extremely high, we may spill. Try other variants; // which are less performing, but that could lead to lower VGPR usage.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:50,Energy Efficiency,schedul,scheduling,50,// Tell the outside world about the result of the scheduling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:34,Energy Efficiency,Schedul,Scheduler,34,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:406,Energy Efficiency,Schedul,Scheduler,406,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:44,Integrability,Interface,Interface,44,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:416,Integrability,interface,interface,416,"//===-- SIMachineScheduler.h - SI Scheduler Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// SI Machine Scheduler interface; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:100,Energy Efficiency,schedul,schedule,100,// The block Predecessors and Successors must be all registered; // before fastSchedule().; // Fast schedule with no particular requirement.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:12,Energy Efficiency,schedul,schedule,12,"// Complete schedule that will try to minimize reg pressure and; // low latencies, and will fill liveins and liveouts.; // Needs all MIs to be grouped between BeginBlock and EndBlock.; // The MIs can be moved after the scheduling,; // it is just used to allow correct track of live registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:219,Energy Efficiency,schedul,scheduling,219,"// Complete schedule that will try to minimize reg pressure and; // low latencies, and will fill liveins and liveouts.; // Needs all MIs to be grouped between BeginBlock and EndBlock.; // The MIs can be moved after the scheduling,; // it is just used to allow correct track of live registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:25,Energy Efficiency,schedul,scheduled,25,// Needs the block to be scheduled inside; // TODO: find a way to compute it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:39,Performance,latency,latency,39,// Give a Reserved color to every high latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:80,Integrability,depend,depending,80,// Compute coloring for topdown and bottom traversals with; // different colors depending on dependencies on Reserved colors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:93,Integrability,depend,dependencies,93,// Compute coloring for topdown and bottom traversals with; // different colors depending on dependencies on Reserved colors.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:66,Integrability,depend,dependencies,66,// Give color to all non-colored SUs according to Reserved groups dependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:45,Availability,down,down,45,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:50,Integrability,depend,dependencies,50,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:132,Integrability,depend,dependencies,132,// Divides Blocks having no bottom up or top down dependencies on Reserved groups.; // The new colors are computed according to the dependencies on the other blocks; // formed with colorAccordingToReservedDependencies.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:118,Integrability,depend,depend,118,"// Merge Constant loads that have all their users into another group to the group.; // (TODO: else if all their users depend on the same group, put them there)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:18,Performance,load,loads,18,"// Merge Constant loads that have all their users into another group to the group.; // (TODO: else if all their users depend on the same group, put them there)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:88,Integrability,depend,depending,88,// Divides Blocks with important size.; // Idea of implementation: attribute new colors depending on topdown and; // bottom up links to other blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:59,Energy Efficiency,schedul,scheduling,59,// Put in one group all instructions with no users in this scheduling region; // (we'd want these groups be at the end).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:10,Energy Efficiency,schedul,schedulable,10,// Num of schedulable unscheduled blocks reading the register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:41,Energy Efficiency,schedul,scheduling,41,// Check register pressure change; // by scheduling a block with these LiveIn and LiveOut.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:35,Energy Efficiency,Schedul,Scheduling,35,// For moveLowLatencies. After all Scheduling variants are tested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:59,Testability,test,tested,59,// For moveLowLatencies. After all Scheduling variants are tested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:23,Energy Efficiency,schedul,schedule,23,// Entry point for the schedule.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:9,Energy Efficiency,schedul,scheduling,9,"// After scheduling is done, improve low latency placements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:41,Performance,latency,latency,41,"// After scheduling is done, improve low latency placements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:18,Energy Efficiency,schedul,scheduling,18,// Some stats for scheduling inside blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Integrability,synchroniz,synchronization,15,/// The atomic synchronization scopes supported by the AMDGPU target.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:35,Security,access,accessed,35,/// The address spaces that can be accessed by a FLAT instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:20,Integrability,synchroniz,synchronization,20,/// \returns Atomic synchronization scope of the machine instruction used to; /// create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Availability,Failure,Failure,13,/// \returns Failure ordering constraint of the machine instruction used to; /// create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:35,Security,access,accessed,35,/// \returns The address spaces be accessed by the machine; /// instruction used to create this SIMemOpInfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Security,access,access,28,"/// \returns True if memory access of the machine instruction used to; /// create this SIMemOpInfo is volatile, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Security,access,access,28,"/// \returns True if memory access of the machine instruction used to; /// create this SIMemOpInfo is nontemporal, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Integrability,message,message,24,/// Reports unsupported message \p Msg for \p MI to LLVM context.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Integrability,synchroniz,synchronization,24,"/// Inspects the target synchronization scope \p SSID and determines; /// the SI atomic scope it corresponds to, the address spaces it; /// covers, and whether the memory ordering applies between address; /// spaces.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Security,access,accessed,51,/// \return Return a bit set of the address spaces accessed by \p AS.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:31,Security,access,accessing,31,/// Construct class to support accessing the machine memory operands; /// of instructions in the machine function \p MF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Performance,Load,Load,13,"/// \returns Load info if \p MI is a load operation, ""std::nullopt"" otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:37,Performance,load,load,37,"/// \returns Load info if \p MI is a load operation, ""std::nullopt"" otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:22,Performance,cache,cache,22,/// Whether to insert cache invalidating instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:13,Performance,cache,cache,13,/// Create a cache control for the subtarget \p ST.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:55,Performance,cache,caches,55,/// Update \p MI memory load instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory store instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:56,Performance,cache,caches,56,/// Update \p MI memory store instruction to bypass any caches up to; /// the \p Scope memory scope for address spaces \p; /// AddrSpace. Return true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory read-modify-write instruction to bypass any caches up; /// to the \p Scope memory scope for address spaces \p AddrSpace. Return true; /// iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:68,Performance,cache,caches,68,/// Update \p MI memory read-modify-write instruction to bypass any caches up; /// to the \p Scope memory scope for address spaces \p AddrSpace. Return true; /// iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Deployability,Update,Update,4,/// Update \p MI memory instruction of kind \p Op associated with address; /// spaces \p AddrSpace to indicate it is volatile and/or nontemporal. Return; /// true iff the instruction was modified.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:4,Performance,Cache,Cache,4,/// Cache Control.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:12,Performance,load,load,12,"/// Expands load operation \p MI. Returns true if instructions are; /// added/deleted or \p MI is modified, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:3,Security,Validat,Validator,3,// Validator should check whether or not MMOs cover the entire set of; // locations accessed by the memory instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:84,Security,access,accessed,84,// Validator should check whether or not MMOs cover the entire set of; // locations accessed by the memory instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to MISS_EVICT.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:63,Performance,cache,cache,63,// Set L1 cache policy to MISS_EVICT.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:11,Performance,cache,cache,11,/// The L1 cache is write through so does not need to be bypassed. There is no; /// bypass control for the L2 cache at the isa level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:110,Performance,cache,cache,110,/// The L1 cache is write through so does not need to be bypassed. There is no; /// bypass control for the L2 cache at the isa level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:54,Performance,cache,cache,54,"/// Do not set GLC for RMW atomic operations as L0/L1 cache is automatically; /// bypassed, and the GLC bit is instead used to indicate if they are; /// return or no-return.; /// Note: there is no L2 cache coherent bypass control at the ISA level.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:200,Performance,cache,cache,200,"/// Do not set GLC for RMW atomic operations as L0/L1 cache is automatically; /// bypassed, and the GLC bit is instead used to indicate if they are; /// return or no-return.; /// Note: there is no L2 cache coherent bypass control at the ISA level.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:44,Performance,load,load,44,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:128,Performance,cache,cache,128,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Modifiability,config,configures,28,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:42,Performance,cache,cache,42,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:82,Performance,load,loads,82,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:111,Performance,cache,cache,111,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L1 cache keeps all memory operations in order for; // wavefronts in the same work-group.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:214,Integrability,synchroniz,synchronizing,214,"// If no cross address space ordering then an GDS ""S_WAITCNT lgkmcnt(0)""; // is not needed as GDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/LDS memory as GDS operations could be; // reordered with respect to later global/LDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:14,Performance,cache,cache,14,// Set the L1 cache policy to MISS_LRU.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:65,Performance,cache,cache,65,// Set the L1 cache policy to MISS_LRU.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:92,Performance,cache,cache,92,/// Do not set glc for store atomic operations as they implicitly write; /// through the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass. Store atomics implicitly write through the L1; // cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:73,Performance,cache,cache,73,// No cache to bypass. Store atomics implicitly write through the L1; // cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,"/// Do not set glc for RMW atomic operations as they implicitly bypass; /// the L1 cache, and the glc bit is instead used to indicate if they are; /// return or no-return.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass. RMW atomics implicitly bypass the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:60,Performance,cache,cache,60,// No cache to bypass. RMW atomics implicitly bypass the L1 cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:44,Performance,load,load,44,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:128,Performance,cache,cache,128,// Set L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache bypass policy at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:28,Modifiability,config,configures,28,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:42,Performance,cache,cache,42,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:82,Performance,load,loads,82,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:111,Performance,cache,cache,111,"// Setting both GLC and SLC configures L1 cache policy to MISS_EVICT; // for both loads and stores, and the L2 cache policy to STREAM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:389,Security,access,access,389,"// In threadgroup split mode the waves of a work-group can be executing on; // different CUs. Therefore need to wait for global or GDS memory operations; // to complete to ensure they are visible to waves in the other CUs.; // Otherwise in non-threadgroup split mode all waves of a work-group are on; // the same CU, so no need to wait for global memory as all waves in the; // work-group access the same the L1, nor wait for GDS as access are ordered; // on a CU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:433,Security,access,access,433,"// In threadgroup split mode the waves of a work-group can be executing on; // different CUs. Therefore need to wait for global or GDS memory operations; // to complete to ensure they are visible to waves in the other CUs.; // Otherwise in non-threadgroup split mode all waves of a work-group are on; // the same CU, so no need to wait for global memory as all waves in the; // work-group access the same the L1, nor wait for GDS as access are ordered; // on a CU.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:43,Energy Efficiency,allocate,allocated,43,// In threadgroup split mode LDS cannot be allocated so no need to wait for; // LDS memory operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote VMEM data or; // stale local VMEM data with MTYPE NC. Local VMEM data with MTYPE RW and; // CC will never be stale due to the local memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:230,Performance,cache,cache,230,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INVL2"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:339,Performance,cache,cache,339,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INVL2"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:254,Performance,cache,cache,254,"// Inserting a ""S_WAITCNT vmcnt(0)"" before is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a following ""BUFFER_WBL2"". The ""BUFFER_WBL2"" is guaranteed; // to initiate writeback of any dirty cache lines of earlier writes by the; // same wave. A ""S_WAITCNT vmcnt(0)"" is needed after to ensure the; // writeback has completed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:50,Performance,cache,cache,50,// RMW atomic operations implicitly bypass the L1 cache and only use SC1; // to indicate system or agent scope. The SC0 bit is used to indicate if; // they are return or no-return. Leave SC1 bit unset to indicate agent; // scope.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote VMEM data or; // stale local VMEM data with MTYPE NC. Local VMEM data with MTYPE RW and; // CC will never be stale due to the local memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:228,Performance,cache,cache,228,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INV"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:337,Performance,cache,cache,337,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INV"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,Performance,load,loads,26,// Ensures that following loads will not see stale remote date or local; // MTYPE NC global data. Local MTYPE RW and CC memory will never be stale; // due to the memory probes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:203,Performance,cache,cache,203,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:279,Performance,cache,cache,279,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:203,Performance,cache,cache,203,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:279,Performance,cache,cache,279,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:75,Performance,cache,caches,75,"// Could generate ""BUFFER_INV"" but it would do nothing as there are no; // caches to invalidate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:254,Performance,cache,cache,254,"// Inserting a ""S_WAITCNT vmcnt(0)"" before is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a following ""BUFFER_WBL2"". The ""BUFFER_WBL2"" is guaranteed; // to initiate writeback of any dirty cache lines of earlier writes by the; // same wave. A ""S_WAITCNT vmcnt(0)"" is needed after to ensure the; // writeback has completed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:49,Performance,cache,caches,49,"// Do not generate ""BUFFER_WBL2"" as there are no caches it would; // writeback, and would require an otherwise unnecessary; // ""S_WAITCNT vmcnt(0)"".",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:21,Performance,cache,cache,21,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:76,Performance,cache,cache,76,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Performance,cache,cache,17,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Performance,load,load,51,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:135,Performance,cache,cache,135,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:25,Modifiability,config,configures,25,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:147,Modifiability,config,configures,147,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:7,Performance,load,loads,7,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:46,Performance,cache,cache,46,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:168,Performance,cache,cache,168,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Performance,cache,cache,210,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L0 cache keeps all memory operations in order for; // work-items in the same wavefront.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:214,Integrability,synchroniz,synchronizing,214,"// If no cross address space ordering then an GDS ""S_WAITCNT lgkmcnt(0)""; // is not needed as GDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/LDS memory as GDS operations could be; // reordered with respect to later global/LDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:21,Performance,cache,cache,21,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:76,Performance,cache,cache,76,// Set the L0 and L1 cache policies to MISS_EVICT.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to bypass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,caches,62,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:259,Performance,cache,cache,259,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Security,access,access,191,"/// The scratch address space does not need the global memory caches; /// to be bypassed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:171,Performance,cache,cache,171,"// Only handle load and store, not atomic read-modify-write insructions. The; // latter use glc to indicate if the atomic returns a result and so must not; // be used for cache control.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Performance,cache,cache,17,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:51,Performance,load,load,51,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:135,Performance,cache,cache,135,// Set L0 and L1 cache policy to be MISS_EVICT for load instructions; // and MISS_LRU for store instructions.; // Note: there is no L2 cache coherent bypass control at the ISA level.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,// Set MALL NOALLOC for load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:25,Modifiability,config,configures,25,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:147,Modifiability,config,configures,147,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:7,Performance,load,loads,7,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:46,Performance,cache,cache,46,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:83,Performance,cache,cache,83,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:168,Performance,cache,cache,168,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Performance,cache,cache,210,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:24,Performance,load,load,24,// Set MALL NOALLOC for load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:10,Performance,cache,cache,10,// The L0 cache keeps all memory operations in order for; // work-items in the same wavefront.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:210,Integrability,synchroniz,synchronizing,210,"// If no cross address space ordering then an ""S_WAITCNT lgkmcnt(0)"" is; // not needed as LDS operations for all waves are executed in a total; // global ordering as observed by all waves. Required if also; // synchronizing with global/GDS memory as LDS operations could be; // reordered with respect to later global/GDS memory operations of the; // same wave.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:62,Performance,cache,cache,62,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:257,Performance,cache,cache,257,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:189,Security,access,access,189,"/// The scratch address space does not need the global memory cache; /// to be flushed as all memory operations by the same thread are; /// sequentially consistent, and no other thread can access scratch; /// memory.; /// Other address spaces do not have a cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:6,Performance,cache,cache,6,// No cache to invalidate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only handle load and store, not atomic read-modify-write instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:8,Deployability,update,update,8,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:15,Performance,load,load,15,"// Only update load and store, not LLVM IR atomic read-modify-write; // instructions. The latter are always marked as volatile so cannot sensibly; // handle it as do not want to pessimize all atomics. Also they do not support; // the nontemporal attribute.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:33,Performance,cache,cache,33,// Set non-temporal hint for all cache levels.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:38,Performance,cache,caches,38,// Atomic instructions already bypass caches to the scope specified by the; // SyncScope operand. Only non-atomic volatile and nontemporal instructions; // need additional treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:38,Performance,cache,caches,38,// Atomic instructions already bypass caches to the scope specified by the; // SyncScope operand. Only non-atomic volatile and nontemporal instructions; // need additional treatment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:17,Deployability,release,release,17,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:191,Performance,optimiz,optimizing,191,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:236,Performance,cache,cache,236,"// TODO: If both release and invalidate are happening they could be combined; // to use the single ""BUFFER_WBINV*"" instruction. This could be done by; // reorganizing this code or as part of optimizing SIInsertWaitcnt pass to; // track cache invalidate and write back instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:43,Energy Efficiency,schedul,scheduler,43,// Unbundle instructions after the post-RA scheduler.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:3,Availability,Mask,Mask,3,// Mask is a bitmask where a '1' indicates the corresponding Mode bit has a; // known value,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:55,Availability,mask,mask,55,// merge an unknown value by using the unknown value's mask to remove bits; // from the result,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:53,Availability,mask,mask,53,// intersect two Status values to produce a mode and mask that is a subset; // of both values,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:34,Deployability,update,update,34,// Insert a setreg instruction to update the Mode register.; // It is possible (though unlikely) for an instruction to require a change to; // the value of disjoint parts of the Mode register when we don't know the; // value of the intervening bits. In that case we need to use more than one; // setreg instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:171,Deployability,update,update,171,"// In Phase 1 we iterate through the instructions of the block and for each; // instruction we get its mode usage. If the instruction uses the Mode register; // we:; // - update the Change status, which tracks the changes to the Mode register; // made by this block; // - if this instruction's requirements are compatible with the current setting; // of the Mode register we merge the modes; // - if it isn't compatible and an InsertionPoint isn't set, then we set the; // InsertionPoint to the current instruction, and we remember the current; // mode; // - if it isn't compatible and InsertionPoint is set we insert a seteg before; // that instruction (unless this instruction forms part of the block's; // entry requirements in which case the insertion is deferred until Phase 3; // when predecessor exit values are known), and move the insertion point to; // this instruction; // - if this is a setreg instruction we treat it as an incompatible instruction.; // This is sub-optimal but avoids some nasty corner cases, and is expected to; // occur very rarely.; // - on exit we have set the Require, Change, and initial Exit modes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:990,Safety,avoid,avoids,990,"// In Phase 1 we iterate through the instructions of the block and for each; // instruction we get its mode usage. If the instruction uses the Mode register; // we:; // - update the Change status, which tracks the changes to the Mode register; // made by this block; // - if this instruction's requirements are compatible with the current setting; // of the Mode register we merge the modes; // - if it isn't compatible and an InsertionPoint isn't set, then we set the; // InsertionPoint to the current instruction, and we remember the current; // mode; // - if it isn't compatible and InsertionPoint is set we insert a seteg before; // that instruction (unless this instruction forms part of the block's; // entry requirements in which case the insertion is deferred until Phase 3; // when predecessor exit values are known), and move the insertion point to; // this instruction; // - if this is a setreg instruction we treat it as an incompatible instruction.; // This is sub-optimal but avoids some nasty corner cases, and is expected to; // occur very rarely.; // - on exit we have set the Require, Change, and initial Exit modes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:121,Availability,Mask,Mask,121,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:307,Availability,Mask,Mask,307,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:499,Availability,Mask,Mask,499,"// Build a status that is common to all the predecessors by intersecting; // all the predecessor exit status values.; // Mask bits (which represent the Mode bits with a known value) can only be; // added by explicit SETREG instructions or the initial default value -; // the intersection process may remove Mask bits.; // If we find a predecessor that has not yet had an exit value determined; // (this can happen for example if a block is its own predecessor) we defer; // use of that value as the Mask will be all zero, and we will revisit this; // block again later (unless the only predecessor without an exit value is; // this block).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp:17,Performance,perform,performed,17,"// Processing is performed in a number of phases; // Phase 1 - determine the initial mode required by each block, and add setreg; // instructions for intra block requirements.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegister.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h:166,Modifiability,extend,extended,166,"/// Return values used for llvm.get.rounding; ///; /// When both the F32 and F64/F16 modes are the same, returns the standard; /// values. If they differ, returns an extended mode starting at 8.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h:3,Modifiability,Inherit,Inherit,3,// Inherit everything from RoundingMode,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIModeRegisterDefaults.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:18,Testability,log,logical,18,"/// If \p MI is a logical operation on an exec value,; /// return the register copied to.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:153,Availability,mask,mask,153,// XXX - Seems LivePhysRegs doesn't work correctly since it will incorrectly; // report the register as unavailable because a super-register with a lane mask; // is unavailable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:87,Safety,detect,detected,87,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:99,Usability,clear,clearKillFlags,99,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:242,Usability,clear,clear,242,"// Check for kills that appear after the terminator instruction, that; // would not be detected by clearKillFlags, since they will cause the; // register to be dead at a later place, causing the verifier to fail.; // We use the candidates to clear the kill flags later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequences emitted for control flow lowering. They are originally; // emitted as the separate operations because spill code may need to be; // inserted for the saved copy of exec.; //; // x = copy exec; // z = s_<op>_b64 x, y; // exec = copy z; // =>; // x = s_<op>_saveexec_b64 y; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:72,Energy Efficiency,schedul,scheduled,72,// Make sure this is inserted after any VALU ops that may have been; // scheduled in between.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:15,Performance,optimiz,optimized,15,"// Inserts the optimized s_mov_b32 / v_cmpx sequence based on the; // operands extracted from a v_cmp ..., s_and_saveexec pattern.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:218,Deployability,pipeline,pipeline,218,"// Record (on GFX10.3 and later) occurences of; // v_cmp_* SGPR, IMM, VGPR; // s_and_saveexec_b32 EXEC_SGPR_DEST, SGPR; // to be replaced with; // s_mov_b32 EXEC_SGPR_DEST, exec_lo; // v_cmpx_* IMM, VGPR; // to reduce pipeline stalls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:211,Energy Efficiency,reduce,reduce,211,"// Record (on GFX10.3 and later) occurences of; // v_cmp_* SGPR, IMM, VGPR; // s_and_saveexec_b32 EXEC_SGPR_DEST, SGPR; // to be replaced with; // s_mov_b32 EXEC_SGPR_DEST, exec_lo; // v_cmpx_* IMM, VGPR; // to reduce pipeline stalls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:341,Integrability,depend,dependencies,341,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:34,Performance,optimiz,optimize,34,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:200,Safety,safe,safe,200,"// Tries to find a possibility to optimize a v_cmp ..., s_and_saveexec; // sequence by looking at an instance of an s_and_saveexec instruction.; // Returns a pointer to the v_cmp instruction if it is safe to replace the; // sequence (see the conditions in the function body). This is after register; // allocation, so some checks on operand dependencies need to be considered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:108,Performance,optimiz,optimization,108,"// If the v_cmp target is in use between v_cmp and s_and_saveexec or after the; // s_and_saveexec, skip the optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:419,Availability,mask,mask,419,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:514,Energy Efficiency,reduce,reduce,514,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:405,Performance,perform,performs,405,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:442,Performance,optimiz,optimizations,442,"//===-- SIOptimizeExecMaskingPreRA.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs exec mask handling peephole optimizations which needs; /// to be done before register allocation to reduce register pressure.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequence; // %sel = V_CNDMASK_B32_e64 0, 1, %cc; // %cmp = V_CMP_NE_U32 1, %sel; // $vcc = S_AND_B64 $exec, %cmp; // S_CBRANCH_VCC[N]Z; // =>; // $vcc = S_ANDN2_B64 $exec, %cc; // S_CBRANCH_VCC[N]Z; //; // It is the negation pattern inserted by DAGCombiner::visitBRCOND() in the; // rebuildSetCC(). We start with S_CBRANCH to avoid exhaustive search, but; // only 3 first instructions are really needed. S_AND_B64 with exec is a; // required part of the pattern since V_CNDMASK_B32 writes zeroes for inactive; // lanes.; //; // Returns true on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:338,Safety,avoid,avoid,338,"// Optimize sequence; // %sel = V_CNDMASK_B32_e64 0, 1, %cc; // %cmp = V_CMP_NE_U32 1, %sel; // $vcc = S_AND_B64 $exec, %cmp; // S_CBRANCH_VCC[N]Z; // =>; // $vcc = S_ANDN2_B64 $exec, %cc; // S_CBRANCH_VCC[N]Z; //; // It is the negation pattern inserted by DAGCombiner::visitBRCOND() in the; // rebuildSetCC(). We start with S_CBRANCH to avoid exhaustive search, but; // only 3 first instructions are really needed. S_AND_B64 with exec is a; // required part of the pattern since V_CNDMASK_B32 writes zeroes for inactive; // lanes.; //; // Returns true on success.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:85,Performance,optimiz,optimization,85,"// Cannot safely mirror live intervals with PHI nodes, so check for these; // before optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:10,Safety,safe,safely,10,"// Cannot safely mirror live intervals with PHI nodes, so check for these; // before optimization.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Deployability,Update,Update,3,"// Update live intervals for CCReg before potentially removing CmpReg/SelReg,; // and their associated liveness information.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:3,Performance,Optimiz,Optimize,3,"// Optimize sequence; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // %tmp = S_AND $exec, %dst; // $exec = S_XOR_term $exec, %tmp; // =>; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // $exec = S_XOR_term $exec, %dst; //; // Clean up potentially unnecessary code added for safety during; // control flow lowering.; //; // Return whether any changes were made to MBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:331,Safety,safe,safety,331,"// Optimize sequence; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // %tmp = S_AND $exec, %dst; // $exec = S_XOR_term $exec, %tmp; // =>; // %dst = S_OR_SAVEEXEC %src; // ... instructions not modifying exec ...; // $exec = S_XOR_term $exec, %dst; //; // Clean up potentially unnecessary code added for safety during; // control flow lowering.; //; // Return whether any changes were made to MBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:95,Deployability,update,update,95,"// Skip this if the endpgm has any implicit uses, otherwise we would need; // to be careful to update / remove them.; // S_ENDPGM always has a single imm operand that is not used other than to; // end up in the encoding",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp:25,Testability,log,logical,25,"// If the only user of a logical operation is move to exec, fold it now; // to prevent forming of saveexec. I.e.:; //; // %0:sreg_64 = COPY $exec; // %1:sreg_64 = S_AND_B64 %0:sreg_64, %2:sreg_64; // =>; // %1 = S_AND_B64 $exec, %2:sreg_64",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMaskingPreRA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1105,Availability,alive,alive,1105,"-----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There sh",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1252,Availability,mask,mask,1252,"ormation.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can onl",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1369,Safety,safe,safe,1369," This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can only be executed with a uniform value. For example an indirect call; /// to a divergent address:; /// bb.start:; /// %a = ...; /// %fun = ...; /// ...; /// bb.loop:; /// call %f",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1217,Security,access,accessed,1217,"ormation.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can onl",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:1558,Security,access,accessed,1558,"ation, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the second loop iteration still get correct data.; /// 2.) There should be no further uses after the IF-ELSE region.; ///; ///; /// Waterfall loops get inserted around instructions that use divergent values; /// but can only be executed with a uniform value. For example an indirect call; /// to a divergent address:; /// bb.start:; /// %a = ...; /// %fun = ...; /// ...; /// bb.loop:; /// call %fun (%a); /// ... // %a can be dead here; /// loop %bb.loop; ///; /// The loop block is executed multiple times, but it is run exactly once for; /",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:673,Usability,simpl,simple,673,"//===--------------------- SIOptimizeVGPRLiveRange.cpp -------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass tries to remove unnecessary VGPR live ranges in divergent if-else; /// structures and waterfall loops.; ///; /// When we do structurization, we usually transform an if-else into two; /// successive if-then (with a flow block to do predicate inversion). Consider a; /// simple case after structurization: A divergent value %a was defined before; /// if-else and used in both THEN (use in THEN is optional) and ELSE part:; /// bb.if:; /// %a = ...; /// ...; /// bb.then:; /// ... = op %a; /// ... // %a can be dead here; /// bb.flow:; /// ...; /// bb.else:; /// ... = %a; /// ...; /// bb.endif; ///; /// As register allocator has no idea of the thread-control-flow, it will just; /// assume %a would be alive in the whole range of bb.then because of a later; /// use in bb.else. On AMDGPU architecture, the VGPR is accessed with respect; /// to exec mask. For this if-else case, the lanes active in bb.then will be; /// inactive in bb.else, and vice-versa. So we are safe to say that %a was dead; /// after the last use in bb.then until the end of the block. The reason is; /// the instructions in bb.then will only overwrite lanes that will never be; /// accessed in bb.else.; ///; /// This pass aims to tell register allocator that %a is in-fact dead,; /// through inserting a phi-node in bb.flow saying that %a is undef when coming; /// from bb.then, and then replace the uses in the bb.else with the result of; /// newly inserted phi.; ///; /// Two key conditions must be met to ensure correctness:; /// 1.) The def-point should be in the same loop-level as if-else-endif to make; /// sure the s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:66,Availability,alive,alive,66,/// Collect the killed registers in the ELSE region which are not alive through; /// the whole THEN region.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:15,Performance,optimiz,optimize,15,// We can only optimize AGPR/VGPR virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:84,Performance,optimiz,optimize,84,// The register is live through the path If->Flow or Flow->Endif.; // we should not optimize for such cases.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:15,Performance,optimiz,optimize,15,// We can only optimize AGPR/VGPR virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:10,Modifiability,variab,variable,10,"// If the variable is used after the loop, the register coalescer will; // merge the newly created register and remove the phi node again.; // Just do nothing in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Usability,Clear,Clear,3,"// Clear Live bit, as we will recalculate afterwards",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:36,Availability,alive,alive,36,// Get the blocks the Reg should be alive through,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:12,Availability,alive,alive,12,// Mark Reg alive through the block if this is a PHI incoming block,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:12,Availability,alive,aliveBlocks,12,// Transfer aliveBlocks from Reg to NewReg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:118,Deployability,update,update,118,// Replace all uses in the ELSE region or the PHIs in ENDIF block; // Use early increment range because setReg() will update the linked list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Safety,Detect,DetectDeadLanes,3,"// DetectDeadLanes may mark register uses as undef without removing; // them, in which case a non-phi instruction using the original register; // may exist in the Endif block even though the register is not live; // into it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:28,Availability,alive,alive,28,// The optimized Reg is not alive through Flow blocks anymore.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:7,Performance,optimiz,optimized,7,// The optimized Reg is not alive through Flow blocks anymore.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:91,Deployability,update,update,91,// Replace all uses in the LOOP region; // Use early increment range because setReg() will update the linked list.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:3,Safety,Detect,Detect,3,// Detect the if-else blocks,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:32,Performance,optimiz,optimized,32,// Collect the registers can be optimized,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:22,Performance,optimiz,optimize,22,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:14,Safety,safe,safe,14,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:22,Performance,optimiz,optimize,22,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:14,Safety,safe,safe,14,// Now we are safe to optimize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:37,Performance,optimiz,optimization,37,"//===- SIPeepholeSDWA.cpp - Peephole optimization for SDWA instructions ---===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file This pass tries to apply several peephole SDWA patterns.; ///; /// E.g. original:; /// V_LSHRREV_B32_e32 %0, 16, %1; /// V_ADD_CO_U32_e32 %2, %0, %3; /// V_LSHLREV_B32_e32 %4, 16, %2; ///; /// Replace:; /// V_ADD_CO_U32_sdwa %4, %1, %3; /// dst_sel:WORD_1 dst_unused:UNUSED_PAD src0_sel:WORD_1 src1_sel:DWORD; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:37,Security,access,accessing,37,"// This will work if the tied src is accessing WORD_0, and the dst is; // writing WORD_1. Modifiers don't matter because all the bits that; // would be impacted are being overwritten by the dst.; // Any other case will not work.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:68,Usability,clear,clear,68,// MI should be moved right before v_or_b32.; // For this we should clear all kill flags on uses of MI src-operands or else; // we can encounter problem with use of killed operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:94,Performance,perform,perform,94,"// Convert the V_ADD_CO_U32_e64 into V_ADD_CO_U32_e32. This allows; // isConvertibleToSDWA to perform its transformation on V_ADD_CO_U32_e32 into; // V_ADD_CO_U32_sdwa.; //; // We are transforming from a VOP3 into a VOP2 form of the instruction.; // %19:vgpr_32 = V_AND_B32_e32 255,; // killed %16:vgpr_32, implicit $exec; // %47:vgpr_32, %49:sreg_64_xexec = V_ADD_CO_U32_e64; // %26.sub0:vreg_64, %19:vgpr_32, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed %49:sreg_64_xexec, implicit $exec; //; // becomes; // %47:vgpr_32 = V_ADD_CO_U32_sdwa; // 0, %26.sub0:vreg_64, 0, killed %16:vgpr_32, 0, 6, 0, 6, 0,; // implicit-def $vcc, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed $vcc, implicit $exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:44,Deployability,update,update,44,"// Since the carry output of MI is now VCC, update its use in MISucc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp:464,Performance,load,loads,464,"//===-- SIPostRABundler.cpp -----------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass creates bundles of memory instructions to protect adjacent loads; /// and stores from being rescheduled apart from each other post-RA.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPostRABundler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp:407,Energy Efficiency,allocate,allocated,407,"//===- SIPreAllocateWWMRegs.cpp - WWM Register Pre-allocation -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Pass to pre-allocated WWM registers; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp:3,Deployability,Update,Update,3,// Update the set of reserved registers to include WWM ones.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreAllocateWWMRegs.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:396,Performance,perform,performs,396,"//===-- SIPreEmitPeephole.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs the peephole optimizations before code emission.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:418,Performance,optimiz,optimizations,418,"//===-- SIPreEmitPeephole.cpp ------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass performs the peephole optimizations before code emission.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:281,Availability,mask,mask,281,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:334,Availability,mask,mask,334,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:392,Performance,perform,performs,392,"// Match:; // sreg = -1 or 0; // vcc = S_AND_B64 exec, sreg or S_ANDN2_B64 exec, sreg; // S_CBRANCH_VCC[N]Z; // =>; // S_CBRANCH_EXEC[N]Z; // We end up with this pattern sometimes after basic block placement.; // It happens while combining a block which assigns -1 or 0 to a saved mask; // and another block which consumes that saved mask and then a branch.; //; // While searching this also performs the following substitution:; // vcc = V_CMP; // vcc = S_AND exec, vcc; // S_CBRANCH_VCC[N]Z; // =>; // vcc = V_CMP; // S_CBRANCH_VCC[N]Z",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:10,Availability,mask,mask,10,// Invert mask for s_andn2,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:11,Deployability,update,updated,11,// EXEC is updated directly,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:3,Modifiability,Rewrite,Rewrite,3,// Rewrite to unconditional branch,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:3,Integrability,Depend,Depends,3,// Depends only on EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:42,Performance,optimiz,optimize,42,// Check first terminator for branches to optimize,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp:121,Performance,optimiz,optimizeSetGPR,121,// Scan the block for two S_SET_GPR_IDX_ON instructions to see if a; // second is not needed. Do expensive checks in the optimizeSetGPR(); // and limit the distance to 20 instructions for compile time purposes.; // Note: this needs to work on bundles as S_SET_GPR_IDX* instructions; // may be bundled with the instructions they modify.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPreEmitPeephole.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:294,Performance,load,loads,294,"// A temporary struct to spill SGPRs.; // This is mostly to spill SGPRs to memory. Spilling SGPRs into VGPR lanes emits; // just v_writelane and v_readlane.; //; // When spilling to memory, the SGPRs are written into VGPR lanes and the VGPR; // is saved to scratch (or the other way around for loads).; // For this, a VGPR is required where the needed lanes can be clobbered. The; // RegScavenger can provide a VGPR where currently active lanes can be; // clobbered, but we still need to save inactive lanes.; // The high-level steps are:; // - Try to scavenge SGPR(s) to save exec; // - Try to scavenge VGPR; // - Save needed, all or inactive lanes of a TmpVGPR; // - Spill/Restore SGPRs using TmpVGPR; // - Restore TmpVGPR; //; // To save all lanes of TmpVGPR, exec needs to be saved and modified. If we; // cannot scavenge temporary SGPRs to save exec, we use the following code:; // buffer_store_dword TmpVGPR ; only if active lanes need to be saved; // s_not exec, exec; // buffer_store_dword TmpVGPR ; save inactive lanes; // s_not exec, exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:127,Performance,load,loads,127,"/* When spilling to stack */; // The SGPRs are written into this VGPR, which is then written to scratch; // (or vice versa for loads).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:30,Performance,load,load,30,// Add an implicit use of the load so it is not dead.; // FIXME This inserts an unnecessary waitcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:129,Availability,mask,mask,129,"// Write TmpVGPR to memory or read TmpVGPR from memory.; // Either using a single buffer_load/store if exec is set to the needed mask; // or using; // buffer_load; // s_not exec, exec; // buffer_load; // s_not exec, exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:139,Security,access,accessing,139,"// During ISel lowering we always reserve the stack pointer in entry and chain; // functions, but never actually want to reference it when accessing our own; // frame. If we need a frame pointer we use it, but otherwise we can just use; // an immediate ""0"" which we represent by returning NoRegister.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:75,Energy Efficiency,allocate,allocated,75,"// Reserve special purpose registers.; //; // EXEC_LO and EXEC_HI could be allocated and used as regular register, but; // this seems likely to result in bugs, so I'm marking them as reserved.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:45,Energy Efficiency,allocate,allocated,45,// Reserve null register - it shall never be allocated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:51,Energy Efficiency,allocate,allocated,51,// Disallow vcc_hi allocation in wave32. It may be allocated but most likely; // will result in bugs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:92,Safety,detect,detected,92,"// We have to assume the SP is needed in case there are calls in the function,; // which is detected after the function is lowered. If we aren't really going; // to need SP, don't bother reserving it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:90,Availability,MASK,MASK,90,// FIXME: Use same reserved register introduced in D149775; // SGPR used to preserve EXEC MASK around WWM spill/copy instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:86,Availability,avail,available,86,"// On GFX908, in order to guarantee copying between AGPRs, we need a scratch; // VGPR available at all times.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:223,Safety,avoid,avoid,223,"// Do not use frame virtual registers. They used to be used for SGPRs, but; // once we reach PrologEpilogInserter, we can no longer spill SGPRs. If the; // scavenger fails, we can increment/decrement the necessary SGPRs to avoid a; // spill.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:26,Integrability,depend,depends,26,"// This is inaccurate. It depends on the instruction and address space. The; // only place where we should hit this is for dealing with frame indexes /; // private accesses, so this is correct in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:164,Security,access,accesses,164,"// This is inaccurate. It depends on the instruction and address space. The; // only place where we should hit this is for dealing with frame indexes /; // private accesses, so this is correct in that case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:52,Performance,load,loads,52,// On gfx90a+ AGPR is a regular VGPR acceptable for loads and stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:83,Performance,perform,perform,83,"// We only have 1 VGPR offset, or 1 SGPR offset. We don't have a free; // SGPR, so perform the add as vector.; // We don't need a base SGPR in the kernel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:17,Security,access,access,17,// We don't have access to the register scavenger if this function is called; // during PEI::scavengeFrameVirtualRegs() so use LiveUnits in this case.; // TODO: Clobbering SCC is not necessary for scratch instructions in the; // entry.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:86,Usability,simpl,simplify,86,"// We currently only support spilling VGPRs to EltSize boundaries, meaning; // we can simplify the adjustment of Offset here to just scale with; // WavefrontSize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:43,Energy Efficiency,allocate,allocated,43,"// AGPRs to spill VGPRs and vice versa are allocated in a reverse order,; // starting from the last lane. In case if a register cannot be completely; // spilled into another register that will ensure its alignment does not; // change. For targets with VGPR alignment requirement this is important; // in case of flat scratch usage as we might get a scratch_load or; // scratch_store of an unaligned register otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:72,Performance,optimiz,optimization,72,"// The epilog restore of a wwm-scratch register can cause undesired; // optimization during machine-cp post PrologEpilogInserter if the same; // register was assigned for return value ABI lowering with a COPY; // instruction. As given below, with the epilog reload, the earlier COPY; // appeared to be dead during machine-cp.; // ...; // v0 in WWM operation, needs the WWM spill at prolog/epilog.; // $vgpr0 = V_WRITELANE_B32 $sgpr20, 0, $vgpr0; // ...; // Epilog block:; // $vgpr0 = COPY $vgpr1 // outgoing value moved to v0; // ...; // WWM spill restore to preserve the inactive lanes of v0.; // $sgpr4_sgpr5 = S_XOR_SAVEEXEC_B64 -1; // $vgpr0 = BUFFER_LOAD $sgpr0_sgpr1_sgpr2_sgpr3, $sgpr32, 0, 0, 0; // $exec = S_MOV_B64 killed $sgpr4_sgpr5; // ...; // SI_RETURN implicit $vgpr0; // ...; // To fix it, mark the same reg as a tied op for such restore instructions; // so that it marks a usage for the preceding COPY.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Performance,Load,Load,3,// Load/store VGPR,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:81,Safety,detect,detect,81,// There could be undef components of a spilled super register.; // TODO: Can we detect this and skip the spill?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Performance,Load,Load,3,// Load in VGPR data,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:81,Safety,detect,detect,81,// There could be undef components of a spilled super register.; // TODO: Can we detect this and skip the spill?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:17,Performance,load,load,17,// Don't need to load VGPR in.; // Unpack lanes,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:119,Safety,safe,safely,119,/// Special case of eliminateFrameIndex. Returns true if the SGPR was spilled to; /// a VGPR and the stack slot can be safely eliminated when all other users are; /// handled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:9,Security,access,access,9,// Other access to frame index,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:74,Testability,assert,asserts,74,"// removeOperand doesn't fixup tied operand indexes as it goes, so; // it asserts. Untie vdst_in for now and retie them afterwards.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:3,Usability,Undo,Undo,3,// Undo frame register modification.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:129,Safety,avoid,avoid,129,"// We have to produce a carry out, and there isn't a free SGPR pair; // for it. We can keep the whole computation on the SALU to avoid; // clobbering an additional register at the cost of an extra mov.; // We may have 1 free scratch SGPR even though a carry out is; // unavailable. Only one additional mov is needed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:49,Usability,undo,undo,49,"// If there were truly no free SGPRs, we need to undo everything.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:20,Usability,simpl,simply,20,"// If the offset is simply too big, don't convert to a scratch wave offset; // relative index.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:92,Modifiability,rewrite,rewrite,92,"// We want to prefer the smallest register class possible, so we don't want to; // stop and rewrite on anything that looks like a subregister; // extract. Operations mostly don't care about the super register class, so we; // only want to stop on the most basic of copies between the same register; // class.; //; // e.g. if we have something like; // %0 = ...; // %1 = ...; // %2 = REG_SEQUENCE %0, sub0, %1, sub1, %2, sub2; // %3 = COPY %2, sub0; //; // We want to look through the COPY to find:; // => %3 = COPY %0; // Plain copy.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:30,Modifiability,extend,extending,30,// TODO: 64-bit operands have extending behavior from 32-bit literal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:68,Energy Efficiency,allocate,allocate,68,"// Do not increase size of registers beyond dword, we would need to allocate; // adjacent registers and constraint regalloc more than needed.; // Always allow dword coalescing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:44,Integrability,Interface,Interface,44,"//===-- SIRegisterInfo.h - SI Register Info Interface ----------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIRegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:395,Integrability,Interface,Interface,395,"//===-- SIRegisterInfo.h - SI Register Info Interface ----------*- C++ -*--===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// Interface definition for SIRegisterInfo; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:278,Availability,mask,mask,278,"/// Sub reg indexes for getRegSplitParts.; /// First index represents subreg size from 1 to 16 DWORDs.; /// The inner vector is sorted by bit offset.; /// Provided a register can be fully split with given subregs,; /// all elements of the inner vector combined give a full lane mask.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:23,Availability,avail,available,23,/// Return the largest available SGPR aligned to \p Align for the register; /// class \p RC.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:9,Security,access,access,9,"// Stack access is very expensive. CSRs are also the high registers, and we; // want to minimize the number of used registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h:191,Availability,avail,available,191,"// Insert spill or restore instructions.; // When lowering spill pseudos, the RegScavenger should be set.; // For creating spill instructions during frame lowering, where no scavenger; // is available, LiveUnits can be used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:3,Safety,Detect,Detect,3,"// Detect ""Dst = VSrc * VGPR + Imm"" and convert to AK form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:3,Safety,Detect,Detect,3,"// Detect ""Dst = VSrc * Imm + VGPR"" and convert to MK form.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:126,Usability,clear,clear,126,"/// Attempt to shrink AND/OR/XOR operations requiring non-inlineable literals.; /// For AND or OR, try using S_BITSET{0,1} to clear or set bits.; /// If the inverse of the immediate is legal, use ANDN2, ORN2 or; /// XNOR (as a ^ b == ~(a ^ ~b)).; /// \returns true if the caller should continue the machine function iterator",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:383,Deployability,release,release,383,"// Match:; // mov t, x; // mov x, y; // mov y, t; //; // =>; //; // mov t, x (t is potentially dead and move eliminated); // v_swap_b32 x, y; //; // Returns next valid instruction pointer if was able to create v_swap_b32.; //; // This shall not be done too early not to prevent possible folding which may; // remove matched moves, and this should preferably be done before RA to; // release saved registers and also possibly after RA which can insert copies; // too.; //; // This is really just a generic peephole that is not a canonical shrinking,; // although requirements match the pass placement and it reduces code size too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:607,Energy Efficiency,reduce,reduces,607,"// Match:; // mov t, x; // mov x, y; // mov y, t; //; // =>; //; // mov t, x (t is potentially dead and move eliminated); // v_swap_b32 x, y; //; // Returns next valid instruction pointer if was able to create v_swap_b32.; //; // This shall not be done too early not to prevent possible folding which may; // remove matched moves, and this should preferably be done before RA to; // release saved registers and also possibly after RA which can insert copies; // too.; //; // This is really just a generic peephole that is not a canonical shrinking,; // although requirements match the pass placement and it reduces code size too.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:298,Performance,optimiz,optimizations,298,"// If this has a literal constant source that is the same as the; // reversed bits of an inline immediate, replace with a bitreverse of; // that constant. This saves 4 bytes in the common case of materializing; // sign bits.; // Test if we are after regalloc. We only want to do this after any; // optimizations happen because this will confuse them.; // XXX - not exactly a check for post-regalloc run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:229,Testability,Test,Test,229,"// If this has a literal constant source that is the same as the; // reversed bits of an inline immediate, replace with a bitreverse of; // that constant. This saves 4 bytes in the common case of materializing; // sign bits.; // Test if we are after regalloc. We only want to do this after any; // optimizations happen because this will confuse them.; // XXX - not exactly a check for post-regalloc run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:17,Testability,log,logic,17,// Shrink scalar logic operations.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2429,Availability,recover,recovering,2429,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:1910,Safety,avoid,avoid,1910,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2429,Safety,recover,recovering,2429,"; ///; /// Whole quad mode is required for derivative computations, but it interferes; /// with shader side effects (stores and atomics). It ensures that WQM is; /// enabled when necessary, but disabled around stores and atomics.; ///; /// When necessary, this pass creates a function prolog; ///; /// S_MOV_B64 LiveMask, EXEC; /// S_WQM_B64 EXEC, EXEC; ///; /// to enter WQM at the top of the function and surrounds blocks of Exact; /// instructions by; ///; /// S_AND_SAVEEXEC_B64 Tmp, LiveMask; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// We also compute when a sequence of instructions requires strict whole; /// wavefront mode (StrictWWM) and insert instructions to save and restore it:; ///; /// S_OR_SAVEEXEC_B64 Tmp, -1; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// When a sequence of instructions requires strict whole quad mode (StrictWQM); /// we use a similar save and restore mechanism and force whole quad mode for; /// those instructions:; ///; /// S_MOV_B64 Tmp, EXEC; /// S_WQM_B64 EXEC, EXEC; /// ...; /// S_MOV_B64 EXEC, Tmp; ///; /// In order to avoid excessive switching during sequences of Exact; /// instructions, the pass first analyzes which instructions must be run in WQM; /// (aka which instructions produce values that lead to derivative; /// computations).; ///; /// Basic blocks are always exited in WQM as long as some successor needs WQM.; ///; /// There is room for improvement given better control flow analysis:; ///; /// (1) at the top level (outside of control flow statements, and as long as; /// kill hasn't been used), one SGPR can be saved by recovering WQM from; /// the LiveMask (this is implemented for the entry block).; ///; /// (2) when entire regions (e.g. if-else blocks or entire loops) only; /// consist of exact and don't-care instructions, the switch only has to; /// be done at the entry and exit points rather than potentially in each; /// block of the region.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:37,Availability,mask,masks,37,// Note: this code assumes that lane masks on AMDGPU completely; // cover registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Performance,Perform,Perform,3,// Perform a depth-first iteration of the LiveRange graph marking defs.; // Stop processing of a given branch when all use lanes have been defined.; // The first definition stops processing for a physical register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:26,Usability,simpl,simply,26,// For physical registers simply mark the defining instruction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:77,Safety,avoid,avoids,77,// Only generate implicit WQM if implicit derivatives are required.; // This avoids inserting unintended WQM if a shader type without; // implicit derivatives uses an image sampling instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:82,Safety,avoid,avoid,82,"// Mark these STRICTWQM, but only for the instruction, not its operands.; // This avoid unnecessarily marking M0 as requiring WQM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:2,Deployability,Update,UpdateLiveIns,2,/*UpdateLiveIns*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update dominator trees,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:166,Availability,mask,mask,166,// Comparison is for live lanes; however here we compute the inverse; // (killed lanes). This is because VCMP will always generate 0 bits; // for inactive lanes so a mask of live lanes would not be correct; // inside control flow.; // Invert the comparison by swapping the operands and adjusting; // the comparison codes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:57,Availability,mask,mask,57,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:101,Availability,alive,alive,101,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update live intervals,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:52,Availability,mask,mask,52,"// Op represents live lanes after kill,; // so exec mask needs to be factored in.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:57,Availability,mask,mask,57,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:101,Availability,alive,alive,101,"// State of SCC represents whether any lanes are live in mask,; // if SCC is 0 then no lanes will be alive anymore.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:62,Deployability,update,update,62,"// In the case we got this far some lanes are still live,; // update EXEC to deactivate lanes as appropriate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:45,Availability,mask,mask,45,// Kill - deactivate lanes no longer in live mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Deployability,Update,Update,3,// Update live intervals,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:146,Availability,mask,mask,146,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:79,Energy Efficiency,allocate,allocates,79,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:130,Safety,avoid,avoids,130,"// Convert a strict mode transition to a pseudo transition.; // This still pre-allocates registers to prevent clobbering,; // but avoids any EXEC mask changes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:55,Availability,mask,mask,55,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:104,Availability,mask,mask,104,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:40,Security,access,accessing,40,// Replace (or supplement) instructions accessing live mask.; // This can only happen once all the live mask registers have been created; // and the execute state (WQM/StrictWWM/Exact) of instructions is known.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:38,Safety,detect,detected,38,// Transition WQM -> StrictWQM -> WQM detected.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:3,Performance,Perform,Perform,3,// Perform splitting after instruction scan to simplify iteration.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:47,Usability,simpl,simplify,47,// Perform splitting after instruction scan to simplify iteration.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:94,Safety,safe,safely,94,"// Return an iterator in the (inclusive) range [First, Last] at which; // instructions can be safely inserted, keeping in mind that some of the; // instructions we want to add necessarily clobber SCC.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:48,Safety,safe,safe,48,// This stores the first instruction where it's safe to switch from WQM to; // Exact or vice versa.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:48,Safety,safe,safe,48,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:194,Safety,safe,safe,194,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:240,Safety,safe,safe,240,"// This stores the first instruction where it's safe to switch from Strict; // mode to Exact/WQM or to switch to Strict mode. It must always be the same; // as, or after, FirstWQM since if it's safe to switch to/from Strict, it must; // be safe to switch to/from WQM as well.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:76,Safety,safe,safely,76,"// If the instruction doesn't actually need a correct EXEC, then we can; // safely leave Strict mode enabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:31,Integrability,depend,depends,31,// Whether we need to save SCC depends on start and end states.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:77,Availability,mask,mask,77,// Exact/Strict -> Strict: save SCC; // Exact/Strict -> WQM: save SCC if WQM mask is generated from exec; // Exact/Strict -> Exact: no save,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:36,Integrability,depend,depends,36,// Check that it already implicitly depends on exec (like all VALU movs; // should do).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:33,Integrability,depend,dependency,33,// Remove early-clobber and exec dependency from simple SGPR copies.; // This allows some to be eliminated during/post RA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:49,Usability,simpl,simple,49,// Remove early-clobber and exec dependency from simple SGPR copies.; // This allows some to be eliminated during/post RA.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:107,Usability,simpl,simple,107,// the only reason we should be here is V_SET_INACTIVE has; // an undef input so it is being replaced by a simple copy.; // There should be a second undef source that we should remove.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:13,Usability,simpl,simple,13,// Shader is simple does not need any state changes or any complex lowering,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:37,Availability,mask,mask,37,// Store a copy of the original live mask when required,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:45,Performance,perform,perform,45,// Lowering blocks causes block splitting so perform as a second pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:31,Availability,mask,mask,31,// Compute live range for live mask,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:116,Usability,simpl,simplest,116,"// Physical registers like SCC aren't tracked by default anyway, so just; // removing the ranges we computed is the simplest option for maintaining; // the analysis results.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:9,Performance,perform,performed,9,// If we performed any kills then recompute EXEC,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:98,Availability,error,error,98,"// Immediate operand kind.; // It helps to identify the location of an offending operand after an error.; // Note that regular literals and mandatory literals (KImm) must be handled; // differently. When looking for an offending operand, we should usually; // ignore mandatory literals because they are part of the instruction and; // cannot be changed. Report location of mandatory operands only for VOPD,; // when both OpX and OpY have a KImm and there are no other literals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:20,Availability,error,error,20,// Instruction will error in AMDGPUAsmParser::MatchAndEmitInstruction,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Deployability,update,update,8,// Also update vgpr_count (dependent on agpr_count for gfx908/gfx90a),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:27,Integrability,depend,dependent,27,// Also update vgpr_count (dependent on agpr_count for gfx908/gfx90a),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:32,Modifiability,variab,variables,32,"// TODO: make those pre-defined variables read-only.; // Currently there is none suitable machinery in the core llvm-mc for this.; // MCSymbol::isRedefinable is intended for another purpose, and; // AsmParser::parseDirectiveSet() cannot be specialized for specific target.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:156,Availability,failure,failures,156,// TODO: We should avoid using host float here. It would be better to; // check the float bit values which is what a few other places do.; // We've had bot failures before due to weird NaN support on mips hosts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:19,Safety,avoid,avoid,19,// TODO: We should avoid using host float here. It would be better to; // check the float bit values which is what a few other places do.; // We've had bot failures before due to weird NaN support on mips hosts.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:135,Safety,avoid,avoid,135,"// Cannot apply fp modifiers to int literals preserving the same semantics; // for VOP1/2/C and VOP3 because of integer truncation. To avoid ambiguity,; // disable these cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:35,Modifiability,extend,extend,35,"// FIXME: 64-bit operands can zero extend, sign extend, or pad zeroes for FP; // types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:48,Modifiability,extend,extend,48,"// FIXME: 64-bit operands can zero extend, sign extend, or pad zeroes for FP; // types.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:43,Modifiability,extend,extend,43,// We got int literal token.; // Only sign extend inline immediates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:113,Availability,error,error,113,"// Currently all regular registers have their .l and .h subregisters, so; // we should never need to generate an error here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:129,Safety,avoid,avoid,129,// Check if this is an operand modifier or an opcode modifier; // which may look like an expression but it is not. We should; // avoid parsing these modifiers as expressions. Currently; // recognized sequences are:; // |...|; // abs(...); // neg(...); // sext(...); // -reg; // -|...|; // -abs(...); // name:...; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:556,Usability,intuit,intuitive,556,"// Check if the current token is an SP3 'neg' modifier.; // Currently this modifier is allowed in the following context:; //; // 1. Before a register, e.g. ""-v0"", ""-v[...]"" or ""-[v0,v1]"".; // 2. Before an 'abs' modifier: -abs(...); // 3. Before an SP3 'abs' modifier: -|...|; //; // In all other cases ""-"" is handled as a part; // of an expression that follows the sign.; //; // Note: When ""-"" is followed by an integer literal,; // this is interpreted as integer negation rather; // than a floating-point NEG modifier applied to N.; // Beside being contr-intuitive, such use of floating-point; // NEG modifier would have resulted in different meaning; // of integer literals used with VOP1/2/C and VOP3,; // for example:; // v_exp_f32_e32 v5, -1 // VOP1: src0 = 0xFFFFFFFF; // v_exp_f32_e64 v5, -1 // VOP3: src0 = 0x80000001; // Negative fp literals with preceding ""-"" are; // handled likewise for uniformity; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:105,Security,validat,validated,105,"// special operand like VINTERP attr_chan; // An instruction may use only one literal.; // This has been validated on the previous step.; // See validateVOPLiteral.; // This literal may be used as more than one operand.; // If all these operands are of the same size,; // this literal counts as one scalar value.; // Otherwise it counts as 2 scalar values.; // See ""GFX10 Shader Programming"", section 3.6.2.3.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:145,Security,validat,validateVOPLiteral,145,"// special operand like VINTERP attr_chan; // An instruction may use only one literal.; // This has been validated on the previous step.; // See validateVOPLiteral.; // This literal may be used as more than one operand.; // If all these operands are of the same size,; // this literal counts as one scalar value.; // Otherwise it counts as 2 scalar values.; // See ""GFX10 Shader Programming"", section 3.6.2.3.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:72,Performance,cache,cache,72,// On GFX12 if both OpX and OpY are V_MOV_B32 then OPY uses SRC2 source-cache.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:144,Energy Efficiency,green,green,144,"// GATHER4 instructions use dmask in a different fashion compared to; // other MIMG instructions. The only useful DMASK values are; // 1=red, 2=green, 4=blue, 8=alpha. (e.g. 1 returns; // (red,red,red,red) etc.) The ISA document doesn't mention; // this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Security,validat,validate,8,// Only validate GDS for non-GWS instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:17,Usability,undo,undocumented,17,// gfx90a has an undocumented limitation:; // DS_GWS opcodes must use even aligned registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:67,Security,validat,validate,67,"// For MUBUF/MTBUF d16 is a part of opcode, so there is nothing to validate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:66,Availability,error,errors,66,// This instruction is not supported.; // Clear any other pending errors because they are no longer relevant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:42,Usability,Clear,Clear,42,// This instruction is not supported.; // Clear any other pending errors because they are no longer relevant.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:104,Security,access,accessed,104,// No flat_scr on SI.; // On GFX10Plus flat scratch is not a valid register operand and can only be; // accessed with s_setreg/s_getreg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:59,Availability,error,error,59,"// If we successfully parsed the operand or if there as an error parsing,; // we are done.; //; // If we are parsing after we reach EndOfStatement then this means we; // are appending default values to the Operands list. This is only done; // by custom parser, so we shouldn't continue on to the generic parsing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:3,Usability,Clear,Clear,3,// Clear any forced encodings from the previous instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:71,Availability,error,error,71,"// We are expecting an soffset operand,; // but let matcher handle the error.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:8,Integrability,message,message,8,// skip message name,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:25,Integrability,depend,depends,25,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:44,Integrability,message,message,44,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:3,Security,Validat,Validation,3,// Validation strictness depends on whether message is specified; // in a symbolic or in a numeric form. In the latter case; // only encoding possibility is checked.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:121,Availability,error,error,121,// Make sure we are not parsing something; // that looks like a label or an expression but is not.; // This will improve error messages.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:127,Integrability,message,messages,127,// Make sure we are not parsing something; // that looks like a label or an expression but is not.; // This will improve error messages.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:36,Security,validat,validator,36,// Offset range is checked later by validator.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp:144,Security,validat,validate,144,"// Tokens like ""glc"" would be parsed as immediate operands in ParseOperand().; // But MatchInstructionImpl() expects to meet token and fails to validate; // operand. This method checks if we are given immediate operand but expect to; // get corresponding token.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AsmParser/AMDGPUAsmParser.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:3,Usability,clear,clear,3,// clear,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:123,Availability,error,error,123,// The NSA encoding does not contain enough operands for the; // combination of base opcode / dimension. Should this be an error?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:25,Availability,error,error,25,// ToDo: add support for error operands to MCInst.h; // return MCOperand::createError(V);,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:34,Availability,avail,available,34,// ToDo: unclear if s[100:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Availability,avail,available,33,// ToDo: unclear if s[96:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Availability,avail,available,33,// ToDo: unclear if s[88:104] is available on VI. Can we use VCC as SGPR in; // this bundle?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:236,Availability,error,error,236,"// ToDo: case 248: 1/(2*PI) - is allowed only on VI; // ImmWidth 0 is a default case where operand should not allow immediates.; // Imm value is still decoded into 32 bit immediate operand, inst printer will; // use it to print verbose error message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:242,Integrability,message,message,242,"// ToDo: case 248: 1/(2*PI) - is allowed only on VI; // ImmWidth 0 is a default case where operand should not allow immediates.; // Imm value is still decoded into 32 bit immediate operand, inst printer will; // use it to print verbose error message.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:33,Safety,avoid,avoid,33,// XXX: cast to int is needed to avoid stupid warning:; // compare with unsigned is always true,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:218,Usability,simpl,simply,218,// We cannot accurately backward compute #VGPRs used from; // GRANULATED_WORKITEM_VGPR_COUNT. But we are concerned with getting the same; // value of GRANULATED_WORKITEM_VGPR_COUNT in the reassembled binary. So we; // simply calculate the inverse of what the assembler does.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:681,Availability,recover,recover,681,// We cannot backward compute values used to calculate; // GRANULATED_WAVEFRONT_SGPR_COUNT. Hence the original values for following; // directives can't be computed:; // .amdhsa_reserve_vcc; // .amdhsa_reserve_flat_scratch; // .amdhsa_reserve_xnack_mask; // They take their respective default values if not specified in the assembly.; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + VCC + FLAT_SCRATCH + XNACK_MASK); //; // We compute the inverse as though all directives apart from NEXT_FREE_SGPR; // are set to 0. So while disassembling we consider that:; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + 0 + 0 + 0); //; // The disassembler cannot recover the original values of those 3 directives.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:681,Safety,recover,recover,681,// We cannot backward compute values used to calculate; // GRANULATED_WAVEFRONT_SGPR_COUNT. Hence the original values for following; // directives can't be computed:; // .amdhsa_reserve_vcc; // .amdhsa_reserve_flat_scratch; // .amdhsa_reserve_xnack_mask; // They take their respective default values if not specified in the assembly.; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + VCC + FLAT_SCRATCH + XNACK_MASK); //; // We compute the inverse as though all directives apart from NEXT_FREE_SGPR; // are set to 0. So while disassembling we consider that:; //; // GRANULATED_WAVEFRONT_SGPR_COUNT; // = f(NEXT_FREE_SGPR + 0 + 0 + 0); //; // The disassembler cannot recover the original values of those 3 directives.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:94,Usability,simpl,simply,94,"// KERNEL_CODE_ENTRY_BYTE_OFFSET; // So far no directive controls this for Code Object V3, so simply skip for; // disassembly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp:38,Availability,failure,failure,38,// Size = 64 regardless of success or failure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h:14,Integrability,interface,interface,14,// Exposes an interface expected by autogenerated code in; // FixedLenDecoderEmitter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h:3,Security,Expose,Exposes,3,// Exposes an interface expected by autogenerated code in; // FixedLenDecoderEmitter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Disassembler/AMDGPUDisassembler.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:201,Deployability,pipeline,pipeline,201,"// s_endpgm also behaves as if there is an implicit; // s_waitcnt 0, but I'm not sure if it would be appropriate; // to model this in llvm-mca based on how the iterations work; // while simulating the pipeline over and over.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:3,Availability,Redundant,Redundant,3,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:3,Safety,Redund,Redundant,3,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:110,Safety,avoid,avoid,110,// Redundant switch so I don't have to repeat the code above; // for each case. There are more clever ways to avoid this; // extra switch and anyone can feel free to implement one of them.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:73,Deployability,update,updateEventWaitcntAfter,73,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:211,Security,access,access,211,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:12,Testability,log,logic,12,"// The core logic from this function is taken from; // SIInsertWaitcnts::updateEventWaitcntAfter() In that pass, the instructions; // that are being looked at are in the MachineInstr format, whereas we have; // access to the MCInst format. The side effects of this are that we can't use; // the mayAccessVMEMThroughFlat(Inst) or mayAccessLDSThroughFlat(Inst); // functions. Therefore, we conservatively assume that these functions will; // return true. This may cause a few instructions to be incorrectly tagged; // with an extra CNT. However, these are instructions that do interact with at; // least one CNT so giving them an extra CNT shouldn't cause issues in most; // scenarios.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp:259,Security,access,access,259,// We conservatively assume that mayAccessVMEMThroughFlat(Inst); // and mayAccessLDSThroughFlat(Inst) would both return true for this; // instruction. We have to do this because those functions use; // information about the memory operands that we don't have access to.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:616,Energy Efficiency,schedul,scheduling,616,"//===------------------- AMDGPUCustomBehaviour.h ----------------*-C++ -* -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; ///; /// This file defines the AMDGPUCustomBehaviour class which inherits from; /// CustomBehaviour. This class is used by the tool llvm-mca to enforce; /// target specific behaviour that is not expressed well enough in the; /// scheduling model for mca to enforce it automatically.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:452,Modifiability,inherit,inherits,452,"//===------------------- AMDGPUCustomBehaviour.h ----------------*-C++ -* -===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; /// \file; ///; /// This file defines the AMDGPUCustomBehaviour class which inherits from; /// CustomBehaviour. This class is used by the tool llvm-mca to enforce; /// target specific behaviour that is not expressed well enough in the; /// scheduling model for mca to enforce it automatically.; ///; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:310,Availability,avail,available,310,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:219,Deployability,update,updateEventWaitcntAfter,219,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:119,Testability,log,logic,119,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:278,Testability,log,logic,278,"/// This method gets called from the constructor and is; /// where we setup the InstrWaitCntInfo vector.; /// The core logic for determining which CNTs an instruction; /// interacts with is taken from SIInsertWaitcnts::updateEventWaitcntAfter().; /// Unfortunately, some of the logic from that function is not available to us; /// in this scope so we conservatively end up assuming that some; /// instructions interact with more CNTs than they do in reality.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:224,Deployability,pipeline,pipeline,224,/// This method gets called from checkCustomHazard when mca is attempting to; /// dispatch an s_waitcnt instruction (or one of its variants). The method; /// looks at each of the instructions that are still executing in the pipeline; /// to determine if the waitcnt should force a wait.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:266,Integrability,depend,dependencies,266,/// This method is used to determine if an instruction; /// should be allowed to be dispatched. The return value is; /// how many cycles until the instruction can be dispatched.; /// This method is called after MCA has already checked for; /// register and hardware dependencies so this method should only; /// implement custom behaviour and dependencies that are not picked up; /// by MCA naturally.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h:342,Integrability,depend,dependencies,342,/// This method is used to determine if an instruction; /// should be allowed to be dispatched. The return value is; /// how many cycles until the instruction can be dispatched.; /// This method is called after MCA has already checked for; /// register and hardware dependencies so this method should only; /// implement custom behaviour and dependencies that are not picked up; /// by MCA naturally.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCA/AMDGPUCustomBehaviour.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp:57,Availability,mask,mask,57,"// For each byte of the fragment that the fixup touches, mask in the bits from; // the fixup value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp:46,Modifiability,variab,variable,46,// SCRATCH_RSRC_DWORD[01] is a special global variable that represents; // the scratch buffer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUELFObjectWriter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:667,Modifiability,extend,extend,667,"// FIXME: The current implementation of; // AsmParser::parseRegisterOrRegisterNumber in MC implies we either emit this; // as an integer or we provide a name which represents a physical register.; // For CFI instructions we really want to emit a name for the DWARF register; // instead, because there may be multiple DWARF registers corresponding to a; // single physical register. One case where this problem manifests is with; // wave32/wave64 where using the physical register name is ambiguous: if we; // write e.g. `.cfi_undefined v0` we lose information about the wavefront; // size which we need to encode the register in the final DWARF. Ideally we; // would extend MC to support parsing DWARF register names so we could do; // something like `.cfi_undefined dwarf_wave32_v0`. For now we just live with; // non-pretty DWARF register names in assembly text.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:33,Performance,load,load,33,// This will default to printing load variants when neither MayStore nor; // MayLoad flag is present which is the case with instructions like; // image_get_resinfo.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:93,Integrability,message,message,93,"// Check if operand register class contains register used.; // Intention: print disassembler message when invalid code is decoded,; // for example sgpr register used in VReg or VISrc(VReg or imm) operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp:36,Safety,avoid,avoid,36,// Use 'neg(...)' instead of '-' to avoid ambiguity.; // This is important for integer literals because; // -1 is not the same value as neg(1).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h:52,Integrability,interface,interface,52,"//===-- AMDGPUInstPrinter.h - AMDGPU MC Inst -> ASM interface ---*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUInstPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp:97,Energy Efficiency,reduce,reduced,97,"// This is the maximum instruction encoded size for gfx10. With a known; // subtarget, it can be reduced to 8 bytes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp:16,Modifiability,Variab,Variable,16,//===--- Global Variable Emission Directives --------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h:54,Integrability,Interface,Interface,54,"//===-- MCTargetDesc/AMDGPUMCAsmInfo.h - AMDGPU MCAsm Interface -*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h:56,Modifiability,inherit,inherits,56,"// If you need to create another MCAsmInfo class, which inherits from MCAsmInfo,; // you will need to make sure your new class sets PrivateGlobalPrefix to; // a prefix that won't appear in a function name. The default value; // for PrivateGlobalPrefix is 'L', so it will consider any function starting; // with 'L' as a local symbol.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCAsmInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp:59,Integrability,depend,depend,59,"// FIXME: If this is expression is PCRel or not should not depend on what; // the expression looks like. Given that this is just a general expression,; // it should probably be FK_Data_4 and whatever is producing; //; // s_add_u32 s2, s2, (extern_const_addrspace+16; //; // And expecting a PCRel should instead produce; //; // .Ltmp1:; // s_add_u32 s2, s2, (extern_const_addrspace+16)-.Ltmp1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCCodeEmitter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:15,Performance,cache,cache,15,// Instruction cache line size in bytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:22,Testability,test,tests,22,"// TODO: Why are some tests have ""mingw"" listed as OS?; // llvm_unreachable(""Unsupported OS"");",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:22,Testability,test,tests,22,"// TODO: Why are some tests have ""mingw"" listed as OS?; // assert(STI.getTargetTriple().getOS() == Triple::UnknownOS);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:59,Testability,assert,assert,59,"// TODO: Why are some tests have ""mingw"" listed as OS?; // assert(STI.getTargetTriple().getOS() == Triple::UnknownOS);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp:15,Performance,cache,cache,15,// Instruction cache line size in bytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:81,Deployability,update,update,81,"// Assume the default COV for now, EmitDirectiveAMDHSACodeObjectVersion; // will update this if it is encountered.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:281,Availability,failure,failure,281,"/// Emit HSA Metadata; ///; /// When \p Strict is true, known metadata elements must already be; /// well-typed. When \p Strict is false, known types are inferred and; /// the \p HSAMetadata structure is updated with the correct types.; ///; /// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:204,Deployability,update,updated,204,"/// Emit HSA Metadata; ///; /// When \p Strict is true, known metadata elements must already be; /// well-typed. When \p Strict is false, known types are inferred and; /// the \p HSAMetadata structure is updated with the correct types.; ///; /// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h:39,Availability,failure,failure,39,"/// \returns True on success, false on failure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/AMDGPUTargetStreamer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h:50,Integrability,interface,interface,50,"//===-- R600InstPrinter.h - AMDGPU MC Inst -> ASM interface -----*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/MCTargetDesc/R600InstPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:17,Availability,mask,mask,17,/// \returns Bit mask for given bit \p Shift and bit \p Width.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:13,Performance,Load,Loadcnt,13,/// \returns Loadcnt bit width,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:42,Integrability,depend,depending,42,"/// \returns Storecnt or Vscnt bit width, depending on VersionMajor.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:23,Performance,Load,Loadcnt,23,/// \returns shift for Loadcnt/Storecnt in combined S_WAIT instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:3,Integrability,Wrap,Wrapper,3,"// Wrapper for Tablegen'd function. enum Subtarget is not defined in any; // header files, so we need to wrap it in a function that takes unsigned; // instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:105,Integrability,wrap,wrap,105,"// Wrapper for Tablegen'd function. enum Subtarget is not defined in any; // header files, so we need to wrap it in a function that takes unsigned; // instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:55,Testability,test,tested,55,"// Some subtargets allow encoding 2048, but this isn't tested or supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:43,Energy Efficiency,power,powers,43,"// These alignment values are specified in powers of two, so alignment =; // 2^n. The minimum alignment is 2^4 = 16.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:377,Energy Efficiency,efficient,efficient,377,"//===----------------------------------------------------------------------===//; // Custom Operands.; //; // A table of custom operands shall describe ""primary"" operand names; // first followed by aliases if any. It is not required but recommended; // to arrange operands so that operand encoding match operand position; // in the table. This will make disassembly a bit more efficient.; // Unused slots in the table shall have an empty name.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:14,Performance,optimiz,optimization,14,"// This is an optimization that should work in most cases.; // As a side effect, it may cause selection of an alias; // instead of a primary operand name in case of sparse tables.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:8,Safety,safe,safe,8,// As a safe default always respond as if PS has color exports.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:3,Safety,Avoid,Avoid,3,"// Avoid using MCRegisterClass::getSize, since that function will go away; // (move from MC* level to Target* level). Return size in bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:268,Modifiability,extend,extended,268,"// Unfortunately, the Instruction Set Architecture Reference Guide is; // misleading about how the inline operands work for (packed) 16-bit; // instructions. In a nutshell, the actual HW behavior is:; //; // - integer encodings (-16 .. 64) are always produced as sign-extended; // 32-bit values; // - float encodings are produced as:; // - for F16 instructions: corresponding half-precision float values in; // the LSBs, 0 in the MSBs; // - for UI16 instructions: corresponding single-precision float value",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp:61,Usability,Guid,Guide,61,"// Unfortunately, the Instruction Set Architecture Reference Guide is; // misleading about how the inline operands work for (packed) 16-bit; // instructions. In a nutshell, the actual HW behavior is:; //; // - integer encodings (-16 .. 64) are always produced as sign-extended; // 32-bit values; // - float encodings are produced as:; // - for F16 instructions: corresponding half-precision float values in; // the LSBs, 0 in the MSBs; // - for UI16 instructions: corresponding single-precision float value",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:7,Availability,mask,mask,7,"// LSB mask for VGPR banks per VOPD component operand.; // 4 banks result in a mask 3, setting 2 lower bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:79,Availability,mask,mask,79,"// LSB mask for VGPR banks per VOPD component operand.; // 4 banks result in a mask 3, setting 2 lower bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:3,Integrability,Interface,Interface,3,"// Interface functions of this class map VOPD component operand indices; // to indices of operands in MachineInstr/MCInst or parsed operands array.; //; // Note that this class operates with 3 kinds of indices:; // - VOPD component operand indices (Component::DST, Component::SRC0, etc.);; // - MC operand indices (they refer operands in a MachineInstr/MCInst);; // - parsed operand indices (they refer operands in parsed operands array).; //; // For SINGLE components mapping between these indices is trivial.; // But things get more complicated for COMPONENT_X and; // COMPONENT_Y because these components share the same; // MachineInstr/MCInst and the same parsed operands array.; // Below is an example of component operand to parsed operand; // mapping for the following instruction:; //; // v_dual_add_f32 v255, v4, v5 :: v_dual_mov_b32 v6, v1; //; // PARSED COMPONENT PARSED; // COMPONENT OPERANDS OPERAND INDEX OPERAND INDEX; // -------------------------------------------------------------------; // ""v_dual_add_f32"" 0; // v_dual_add_f32 v255 0 (DST) --> 1; // v4 1 (SRC0) --> 2; // v5 2 (SRC1) --> 3; // ""::"" 4; // ""v_dual_mov_b32"" 5; // v_dual_mov_b32 v6 0 (DST) --> 6; // v1 1 (SRC0) --> 7; // -------------------------------------------------------------------; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:39,Integrability,depend,depends,39,// Create layout for COMPONENT_Y which depends on COMPONENT_X layout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:46,Integrability,depend,depends,46,// Create ComponentInfo for COMPONENT_Y which depends on COMPONENT_X layout.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:168,Availability,error,error,168,/// \returns Integer value requested using \p F's \p Name attribute.; ///; /// \returns \p Default if attribute is not present.; ///; /// \returns \p Default and emits error if requested value cannot be converted; /// to integer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:276,Availability,error,error,276,"/// \returns A pair of integer values requested using \p F's \p Name attribute; /// in ""first[,second]"" format (""second"" is optional unless \p OnlyFirstRequired; /// is false).; ///; /// \returns \p Default if attribute is not present.; ///; /// \returns \p Default and emits error if one of the requested values cannot be; /// converted to integer, or \p OnlyFirstRequired is false and ""second"" value is; /// not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:108,Availability,mask,mask,108,// The following methods are only meaningful on targets that support; // S_WAITCNT.; /// \returns Vmcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:24,Availability,mask,mask,24,/// \returns Expcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:25,Availability,mask,mask,25,/// \returns Lgkmcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:25,Availability,mask,mask,25,/// \returns Waitcnt bit mask for given isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:135,Availability,mask,mask,135,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:123,Performance,Load,Loadcnt,123,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:214,Performance,LOAD,LOADcnt,214,"// The following methods are only meaningful on targets that support; // S_WAIT_*CNT, introduced with gfx12.; /// \returns Loadcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support LOADcnt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:27,Availability,mask,mask,27,/// \returns Samplecnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support SAMPLEcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:24,Availability,mask,mask,24,/// \returns Bvhcnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support BVHcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:23,Availability,mask,mask,23,/// \returns Dscnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support DScnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:23,Availability,mask,mask,23,/// \returns Dscnt bit mask for given isa \p Version.; /// Returns 0 for versions that do not support KMcnt,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:34,Availability,mask,mask,34,"/// \return STOREcnt or VScnt bit mask for given isa \p Version.; /// returns 0 for versions that do not support STOREcnt or VScnt.; /// STOREcnt and VScnt are the same counter, the name used; /// depends on the ISA version.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:197,Integrability,depend,depends,197,"/// \return STOREcnt or VScnt bit mask for given isa \p Version.; /// returns 0 for versions that do not support STOREcnt or VScnt.; /// STOREcnt and VScnt are the same counter, the name used; /// depends on the ISA version.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:167,Performance,Load,LoadcntDscnt,167,// The following are only meaningful on targets that support; // S_WAIT_LOADCNT_DSCNT and S_WAIT_STORECNT_DSCNT.; /// \returns Decoded Waitcnt structure from given \p LoadcntDscnt for given; /// isa \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:16,Performance,Load,Loadcnt,16,/// \returns \p Loadcnt and \p Dscnt components of \p Decoded encoded as an; /// immediate that can be used with S_WAIT_LOADCNT_DSCNT for given isa; /// \p Version.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:459,Energy Efficiency,allocate,allocate,459,"// These functions are considered entrypoints into the current module, i.e. they; // are allowed to be called from outside the current module. This is different; // from isEntryFunctionCC, which is only true for functions that are entered by; // the hardware. Module entry points include all entry functions but also; // include functions that can be called from other functions inside or outside; // the current module. Module entry functions are allowed to allocate LDS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h:142,Availability,avail,available,142,/// For pre-GFX12 FLAT instructions the offset must be positive;; /// MSB is ignored and forced to zero.; ///; /// \return The number of bits available for the signed offset field in flat; /// instructions. Note that some forms of the instruction disallow negative; /// offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUBaseInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:119,Modifiability,variab,variable,119,// external zero size addrspace(3) without initializer implies cuda/hip extern; // __shared__ the semantics for such a variable appears to be that all extern; // __shared__ variables alias one another. This hits different handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:173,Modifiability,variab,variables,173,// external zero size addrspace(3) without initializer implies cuda/hip extern; // __shared__ the semantics for such a variable appears to be that all extern; // __shared__ variables alias one another. This hits different handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:20,Modifiability,variab,variable,20,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:58,Performance,load,load,58,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:111,Performance,optimiz,optimizer,111,"// A constant undef variable can't be written to, and any load is; // undef, so it should be eliminated by the optimizer. It could be; // dropped by the back end if not. This pass skips over it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:106,Availability,error,error,106,// Initializers are unimplemented for LDS address space.; // Leave such variables in place for consistent error reporting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:72,Modifiability,variab,variables,72,// Initializers are unimplemented for LDS address space.; // Leave such variables in place for consistent error reporting.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:49,Performance,load,load,49,"// Ignore atomics not aliasing with the original load, any atomic is a; // universal MemoryDef from MSSA's point of view too, just like a fence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:106,Performance,load,load,106,"// Start with a nearest dominating clobbering access, it will be either; // live on entry (nothing to do, load is not clobbered), MemoryDef, or; // MemoryPhi if several MemoryDefs can define this memory state. In that; // case add all Defs to WorkList and continue going up and checking all; // the definitions of this memory location until the root. When all the; // defs are exhausted and came to the entry state we have no clobber.; // Along the scan ignore barriers and fences which are considered clobbers; // by the MemorySSA, but not really writing anything into the memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:46,Security,access,access,46,"// Start with a nearest dominating clobbering access, it will be either; // live on entry (nothing to do, load is not clobbered), MemoryDef, or; // MemoryPhi if several MemoryDefs can define this memory state. In that; // case add all Defs to WorkList and continue going up and checking all; // the definitions of this memory location until the root. When all the; // defs are exhausted and came to the entry state we have no clobber.; // Along the scan ignore barriers and fences which are considered clobbers; // by the MemorySSA, but not really writing anything into the memory.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:111,Deployability,update,update,111,/// Given a \p Def clobbering a load from \p Ptr according to the MSSA check; /// if this is actually a memory update or an artificial clobber to facilitate; /// ordering constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:32,Performance,load,load,32,/// Given a \p Def clobbering a load from \p Ptr according to the MSSA check; /// if this is actually a memory update or an artificial clobber to facilitate; /// ordering constraints.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h:18,Performance,Load,Load,18,/// Check is a \p Load is clobbered in its function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:100,Security,access,access,100,// Calculate the PAL metadata key for *S_SCRATCH_SIZE. It can be used; // with a constant offset to access any non-register shader-specific PAL; // metadata key.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:218,Energy Efficiency,allocate,allocate,218,// Set the number of used vgprs in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the number of vgprs to; // allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the number of vgprs to; // allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used agprs in the metadata. This is an optional advisory; // record for logging etc;,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:219,Energy Efficiency,allocate,allocate,219,// Set the amount of LDS used in bytes in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the amount of LDS to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:100,Testability,log,logging,100,// Set the amount of LDS used in bytes in the metadata. This is an optional; // advisory record for logging etc; wave dispatch actually uses the rsrc1; // register for the shader stage to determine the amount of LDS to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used vgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of vgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:214,Energy Efficiency,allocate,allocate,214,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:93,Testability,log,logging,93,// Set the number of used sgprs in the metadata. This is an optional advisory; // record for logging etc; wave dispatch actually uses the rsrc1 register for; // the shader stage to determine the number of sgprs to allocate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h:101,Integrability,wrap,wrapper,101,// Get the PAL version major (idx 0) or minor (idx 1). This is an internal; // helper for the public wrapper functions that request Major or Minor,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUPALMetadata.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h:25,Integrability,interface,interface,25,"//===- ARC.h - Top-level interface for ARC representation -------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the entry points for global functions defined in the LLVM; // ARC back-end.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARC.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp:474,Integrability,depend,dependent,474,"//===- ARCAsmPrinter.cpp - ARC LLVM assembly writer -------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a printer that converts from our internal representation; // of machine-dependent LLVM code to GNU format ARC assembly language.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp:48,Performance,load,loads,48,"//===- ARCExpandPseudosPass - ARC expand pseudo loads -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass expands stores with large offsets into an appropriate sequence.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCExpandPseudos.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:113,Availability,down,downward,113,// Allocate by adjusting by the negative of what the record holder tracked; // it tracked a positive offset in a downward growing stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate by adjusting by the negative of what the record holder tracked; // it tracked a positive offset in a downward growing stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:14,Modifiability,variab,variable,14,"// If we have variable sized frame objects, then we have to move; // the stack pointer to a known spot (fp - StackSize).; // Then, replace the frame pointer by (new) [sp,StackSize-4].; // Then, move the stack pointer the rest of the way (sp = sp + StackSize).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:25,Availability,down,down,25,// Create slots for last down to r13.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:13,Integrability,rout,routines,13,"// There are routines for saving at least 3 registers (r13 to r15, etc.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:13,Integrability,rout,routines,13,"// There are routines for saving at least 3 registers (r13 to r15, etc.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp:16,Modifiability,variab,variables,16,// Adjust local variables that are 4-bytes or larger to 4-byte boundary,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:58,Performance,load,load,58,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:186,Performance,load,loaded,186,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:313,Performance,load,loading,313,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:184,Performance,load,loaded,184,"/// If the specified machine instruction is a direct; /// store to a stack slot, return the virtual or physical register number of; /// the source reg along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than storing to the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp:1294,Integrability,rout,routine,1294,"/// Analyze the branching code at the end of MBB, returning; /// true if it cannot be understood (e.g. it's a switch dispatch or isn't; /// implemented for a target). Upon success, this returns false and returns; /// with the following information in various cases:; ///; /// 1. If this block ends with no branches (it just falls through to its succ); /// just return false, leaving TBB/FBB null.; /// 2. If this block ends with only an unconditional branch, it sets TBB to be; /// the destination block.; /// 3. If this block ends with a conditional branch and it falls through to a; /// successor block, it sets TBB to be the branch destination block and a; /// list of operands that evaluate the condition. These operands can be; /// passed to other TargetInstrInfo methods to create new branches.; /// 4. If this block ends with a conditional branch followed by an; /// unconditional branch, it returns the 'true' destination in TBB, the; /// 'false' destination in FBB, and a list of operands that evaluate the; /// condition. These operands can be passed to other TargetInstrInfo; /// methods to create new branches.; ///; /// Note that RemoveBranch and insertBranch must be implemented to support; /// cases where this method returns success.; ///; /// If AllowModify is true, then this routine is allowed to modify the basic; /// block (e.g. delete instructions after the unconditional branch).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:58,Performance,load,load,58,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:186,Performance,load,loaded,186,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:313,Performance,load,loading,313,"/// If the specified machine instruction is a direct; /// load from a stack slot, return the virtual or physical register number of; /// the destination along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than loading from the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:184,Performance,load,loaded,184,"/// If the specified machine instruction is a direct; /// store to a stack slot, return the virtual or physical register number of; /// the source reg along with the FrameIndex of the loaded stack slot. If; /// not, return 0. This predicate must return 0 if the instruction has; /// any side effects other than storing to the stack slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:28,Performance,load,load,28,// Emit code before MBBI to load immediate value into physical register Reg.; // Returns an iterator to the new instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp:91,Energy Efficiency,schedul,scheduling,91,"/// This pass converts a legalized DAG into a ARC-specific DAG, ready for; /// instruction scheduling.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:31,Modifiability,extend,extend,31,// We read the TIMER0 and zero-extend it to 64-bits as the intrinsic; // requires.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:8,Modifiability,extend,extend,8,// Sign extend inreg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:107,Availability,avail,available,107,// TODO: Predicate these with `options.hasBitScan() ? Legal : Expand`; // when the HasBitScan predicate is available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:17,Performance,load,loads,17,// Transform all loads nodes into one single node because; // all load nodes are independent of each other.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:66,Performance,load,load,66,// Transform all loads nodes into one single node because; // all load nodes are independent of each other.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:70,Performance,load,load,70,"/// Transform physical registers into virtual registers, and generate load; /// operations for argument places on the stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:73,Energy Efficiency,schedul,scheduler,73,// All getCopyFromReg ops must precede any getMemcpys to prevent the; // scheduler clobbering a register before it has been copied.; // The stages are:; // 1. CopyFromReg (and load) arg & vararg registers.; // 2. Chain CopyFromReg nodes into a TokenFactor.; // 3. Memcpy 'byVal' args & push final InVals.; // 4. Chain mem ops nodes into a TokenFactor.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:176,Performance,load,load,176,// All getCopyFromReg ops must precede any getMemcpys to prevent the; // scheduler clobbering a register before it has been copied.; // The stages are:; // 1. CopyFromReg (and load) arg & vararg registers.; // 2. Chain CopyFromReg nodes into a TokenFactor.; // 3. Memcpy 'byVal' args & push final InVals.; // 4. Chain mem ops nodes into a TokenFactor.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:24,Performance,load,load,24,// 1a. CopyFromReg (and load) arg registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:3,Performance,Load,Load,3,// Load the argument to a virtual register,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:52,Performance,load,load,52,// Create the SelectionDAG nodes corresponding to a load; // from this parameter,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:61,Safety,avoid,avoiding,61,"// guarantee that all emitted copies are; // stuck together, avoiding something bad",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain.; // Add the glue if we have it.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:92,Performance,Optimiz,Optimization,92,//===----------------------------------------------------------------------===//; // Target Optimization Hooks; //===----------------------------------------------------------------------===//,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:296,Performance,load,load,296,"//===----------------------------------------------------------------------===//; // Addressing mode description hooks; //===----------------------------------------------------------------------===//; /// Return true if the addressing mode represented by AM is legal for this; /// target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:44,Integrability,Interface,Interface,44,"//===- ARCISelLowering.h - ARC DAG Lowering Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that ARC uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:405,Integrability,interface,interfaces,405,"//===- ARCISelLowering.h - ARC DAG Lowering Interface -----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that ARC uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:18,Integrability,Wrap,Wrapper,18,// Global Address Wrapper,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h:94,Performance,load,load,94,"/// Return true if the addressing mode represented by AM is legal for this; /// target, for a load/store of the specified type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:462,Performance,load,load,462,"//===- ARCOptAddrMode.cpp ---------------------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; /// \file; /// This pass folds LD/ST + ADD pairs into Pre/Post-increment form of; /// load/store instructions.; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:19,Performance,load,load,19,// Returns true if load/store instruction \p Ldst can be hoisted up to; // instruction \p To,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:65,Availability,down,down,65,"// // Returns true if load/store instruction \p Ldst can be sunk down; // // to instruction \p To; // bool canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To);; // Check if instructions \p Ldst and \p Add can be moved to become adjacent; // If they can return instruction which need not to move.; // If \p Uses is not null, fill it with instructions after \p Ldst which use; // \p Ldst's base register",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:22,Performance,load,load,22,"// // Returns true if load/store instruction \p Ldst can be sunk down; // // to instruction \p To; // bool canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To);; // Check if instructions \p Ldst and \p Add can be moved to become adjacent; // If they can return instruction which need not to move.; // If \p Uses is not null, fill it with instructions after \p Ldst which use; // \p Ldst's base register",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:3,Deployability,Update,Update,3,// Update all instructions in \p Uses to accomodate increment; // of \p BaseReg by \p Offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:88,Deployability,update,update,88,// Change instruction \p Ldst to postincrement form.; // \p NewBase is register to hold update base value; // \p NewOffset is instruction's new offset,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:72,Performance,load,load,72,// Return true if \p Off can be used as immediate offset; // operand of load/store instruction (S9 literal),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:27,Performance,load,load,27,// Return true if \p MI is load/store instruction with immediate offset; // which can be adjusted by \p Disp,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp:104,Performance,load,load,104,"// bool ARCOptAddrMode::canSinkLoadStoreTo(MachineInstr *Ldst, MachineInstr *To) {; // // Can only sink load/store within same BB; // if (Ldst->getParent() != To->getParent()); // return false;; // MachineBasicBlock::const_iterator MI(Ldst), ME(To),; // End(Ldst->getParent()->end());; // bool IsStore = Ldst->mayStore();; // bool IsLoad = Ldst->mayLoad();; // Register ValReg = IsLoad ? Ldst->getOperand(0).getReg() : Register();; // for (; MI != ME && MI != End; ++MI) {; // if (MI->isDebugValue()); // continue;; // if (MI->mayStore() || MI->isCall() || MI->isInlineAsm() ||; // MI->hasUnmodeledSideEffects()); // return false;; // if (IsStore && MI->mayLoad()); // return false;; // if (ValReg && MI->readsVirtualRegister(ValReg)); // return false;; // }; // return true;; // }",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCOptAddrMode.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:3,Performance,Load,Loads,3,// Loads can always be reached with LD_rlimm.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:82,Performance,load,load,82,// We can be sure that the scavenged-register slot is within the range; // of the load offset.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:29,Performance,load,load,29,"// TODO: assert based on the load type:; // ldb needs no alignment,; // ldh needs 2 byte alignment; // ld needs 4 byte alignment",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp:9,Testability,assert,assert,9,"// TODO: assert based on the load type:; // ldb needs no alignment,; // ldh needs 2 byte alignment; // ld needs 4 byte alignment",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCSubtarget.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp:28,Deployability,Configurat,Configuration,28,/// ARC Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp:28,Modifiability,Config,Configuration,28,/// ARC Code Generator Pass Configuration Options.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:8,Deployability,Pipeline,Pipeline,8,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:17,Deployability,Configurat,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h:17,Modifiability,Config,Configuration,17,// Pass Pipeline Configuration,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCTargetMachine.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp:2,Performance,Tune,TuneCPU,2,/*TuneCPU=*/,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/MCTargetDesc/ARCMCTargetDesc.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:31,Performance,Optimiz,Optimize,31,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:879,Performance,perform,performance,879,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:1236,Performance,optimiz,optimized,1236,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:61,Security,access,accesses,61,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:1246,Security,access,access,1246,"//=== A15SDOptimizerPass.cpp - Optimize DPR and SPR register accesses on A15==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // The Cortex-A15 processor employs a tracking scheme in its register renaming; // in order to process each instruction's micro-ops speculatively and; // out-of-order with appropriate forwarding. The ARM architecture allows VFP; // instructions to read and write 32-bit S-registers. Each S-register; // corresponds to one half (upper or lower) of an overlaid 64-bit D-register.; //; // There are several instruction patterns which can be used to provide this; // capability which can provide higher performance than other, potentially more; // direct patterns, specifically around when one micro-op reads a D-register; // operand that has recently been written as one or more S-register results.; //; // This file defines a pre-regalloc pass which looks for SPR producers which; // are going to be used by a DPR (or QPR) consumers and creates the more; // optimized access pattern.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:15,Performance,optimiz,optimization,15,//; // Pattern optimization methods; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:7,Security,Sanitiz,Sanitizing,7,//; // Sanitizing method - used to make sure if don't leave dead code around.; //,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:20,Performance,optimiz,optimized,20,// Creates the more optimized patterns and generally does all the code; // transformations in this pass.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:74,Performance,optimiz,optimizer,74,// See if all bar one of the operands are IMPLICIT_DEF and insert the; // optimizer pattern accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:45,Deployability,update,update,45,"// The only way we can do a partial register update is through a COPY,; // INSERT_SUBREG or REG_SEQUENCE.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:50,Performance,optimiz,optimize,50,"// This function inserts instructions in order to optimize interactions between; // SPR registers and DPR/QPR registers. It does so by performing VDUPs on all; // lanes, and the using VEXT instructions to recompose the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:135,Performance,perform,performing,135,"// This function inserts instructions in order to optimize interactions between; // SPR registers and DPR/QPR registers. It does so by performing VDUPs on all; // lanes, and the using VEXT instructions to recompose the result.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:618,Energy Efficiency,efficient,efficiently,618,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:499,Integrability,depend,depends,499,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:631,Integrability,depend,depending,631,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:275,Safety,avoid,avoid,275,"// We look for instructions that write S registers that are then read as; // D/Q registers. These can only be caused by COPY, INSERT_SUBREG and; // REG_SEQUENCE pseudos that insert an SPR value into a DPR register or; // merge two SPR values to form a DPR register. In order avoid false; // positives we make sure that there is an SPR producer so we look past; // COPY and PHI nodes to find it.; //; // The best code pattern for when an SPR producer is going to be used by a; // DPR or QPR consumer depends on whether the other lanes of the; // corresponding DPR/QPR are currently defined.; //; // We can handle these efficiently, depending on the type of; // pseudo-instruction that is producing the pattern; //; // * COPY: * VDUP all lanes and merge the results together; // using VEXTs.; //; // * INSERT_SUBREG: * If the SPR value was originally in another DPR/QPR; // lane, and the other lane(s) of the DPR/QPR register; // that we are inserting in are undefined, use the; // original DPR/QPR value.; // * Otherwise, fall back on the same stategy as COPY.; //; // * REG_SEQUENCE: * If all except one of the input operands are; // IMPLICIT_DEFs, insert the VDUP pattern for just the; // defined input operand; // * Otherwise, fall back on the same stategy as COPY.; //; // First, get all the reads of D-registers done by this instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:54,Integrability,depend,dependency,54,"// Now, work out if the instruction causes a SPR->DPR dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:10,Performance,optimiz,optimize,10,// We can optimize this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:117,Performance,optimiz,optimize,117,"// Make sure to constrain the register class of the new register to; // match what we're replacing. Otherwise we can optimize a DPR_VFP2; // reference into a plain DPR, and that will end poorly. NewReg is; // always virtual here, so there will always be a matching subclass; // to find.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:103,Availability,avail,available,103,"// Since the A15SDOptimizer pass can insert VDUP instructions, it can only be; // enabled when NEON is available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h:26,Integrability,interface,interface,26,"//===-- ARM.h - Top-level interface for ARM representation ------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the entry points for global functions defined in the LLVM; // ARM back-end.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARM.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:474,Integrability,depend,dependent,474,"//===-- ARMAsmPrinter.cpp - Print machine code to an ARM .s file ----------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a printer that converts from our internal representation; // of machine-dependent LLVM code to GAS-format ARM assembly language.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:28,Security,secur,secure,28,// Emit symbol for CMSE non-secure entry point,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:108,Modifiability,variab,variables,108,"// Collect all globals that had their storage promoted to a constant pool.; // Functions are emitted before variables, so this accumulates promoted; // globals from all functions in PromotedGlobals.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:29,Performance,optimiz,optimization,29,// Calculate this function's optimization goal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:17,Performance,optimiz,optimization,17,// Combine a new optimization goal with existing ones.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:224,Availability,down,down,224,"// FIXME: The register allocator not only may not have given us the; // registers in sequence, but may not be in ascending registers. This; // will require changes in the register allocator that'll need to be; // propagated down here if the operands change.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:147,Integrability,depend,depends,147,// 'Q' should correspond to the low order register and 'R' to the high; // order register. Whether this corresponds to the upper or lower half; // depends on the endianess mode.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:9,Testability,stub,stub,9,// L_foo$stub:,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:59,Modifiability,variab,variables,59,// Output non-lazy-pointers for external and common global variables.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:245,Performance,perform,perform,245,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:238,Safety,safe,safely,238,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:339,Safety,safe,safe,339,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:92,Integrability,rout,routines,92,"//===----------------------------------------------------------------------===//; // Helper routines for emitStartOfAsmFile() and emitEndOfAsmFile(); // FIXME:; // The following seem like one-off assembler flags, but they actually need; // to appear in the .ARM.attributes section in ELF.; // Instead of subclassing the MCELFStreamer, we do the work here.; // Returns true if all functions have the same function attribute value.; // It also returns true when the module has no functions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:33,Availability,avail,available,33,// Emit build attributes for the available hardware.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:167,Security,expose,exposed,167,"// FIXME: To support emitting this build attribute as GCC does, the; // -mfp16-format option and associated plumbing must be; // supported. For now the __fp16 type is exposed by default, so this; // attribute should be emitted with value 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:97,Performance,load,load,97,"// special cases:; // 1) for Thumb1 code we sometimes materialize the constant via constpool; // load.; // 2) for Thumb1 execute only code we materialize the constant via the; // following pattern:; // movs r3, #:upper8_15:<const>; // lsls r3, #8; // adds r3, #:upper0_7:<const>; // lsls r3, #8; // adds r3, #:lower8_15:<const>; // lsls r3, #8; // adds r3, #:lower0_7:<const>; // So we need to special-case MOVS, ADDS and LSLS, and keep track of; // where we are in the sequence with the simplest of state machines.; // 3) for Thumb2 execute only code we materialize the constant via; // immediate constants in 2 separate instructions (MOVW/MOVT).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:488,Usability,simpl,simplest,488,"// special cases:; // 1) for Thumb1 code we sometimes materialize the constant via constpool; // load.; // 2) for Thumb1 execute only code we materialize the constant via the; // following pattern:; // movs r3, #:upper8_15:<const>; // lsls r3, #8; // adds r3, #:upper0_7:<const>; // lsls r3, #8; // adds r3, #:lower8_15:<const>; // lsls r3, #8; // adds r3, #:lower0_7:<const>; // So we need to special-case MOVS, ADDS and LSLS, and keep track of; // where we are in the sequence with the simplest of state machines.; // 3) for Thumb2 execute only code we materialize the constant via; // immediate constants in 2 separate instructions (MOVW/MOVT).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:48,Deployability,update,update,48,"// Registers, pushed as a part of folding an SP update into the; // push instruction are marked as undef and should not be; // restored when unwinding, because the function can modify the; // corresponding stack slots.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:3,Usability,Simpl,Simple,3,// Simple pseudo-instructions have their lowering (with expansion to real; // instructions) auto-generated.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:61,Testability,test,test,61,"// TODOD FIXME: Enable feature predicate checks once all the test pass.; // ARM_MC::verifyInstructionPredicates(MI->getOpcode(),; // getSubtargetInfo().getFeatureBits());",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:21,Performance,load,load,21,// Form and emit the load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:489,Performance,load,load,489,"// TBB [base, idx] =; // ADDS idx, idx, base; // LDRB idx, [idx, #4] ; or LDRH if TBH; // LSLS idx, #1; // ADDS pc, pc, idx; // When using PC as the base, it's important that there is no padding; // between the last ADDS and the start of the jump table. The jump table; // is 4-byte aligned, so we ensure we're 4 byte aligned here too.; //; // FIXME: Ideally we could vary the LDRB index based on the padding; // between the sequence and jump table, however that relies on MCExprs; // for load indexes which are currently not supported.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h:4,Performance,Optimiz,OptimizationGoals,4,"/// OptimizationGoals - Maintain a combined optimization goal for all; /// functions in a module: one of Tag_ABI_optimization_goals values,; /// -1 if uninitialized, 0 if conflicting goals",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h:44,Performance,optimiz,optimization,44,"/// OptimizationGoals - Maintain a combined optimization goal for all; /// functions in a module: one of Tag_ABI_optimization_goals values,; /// -1 if uninitialized, 0 if conflicting goals",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Energy Efficiency,schedul,scheduling,52,// Use a ScoreboardHazardRecognizer for prepass ARM scheduling. TargetInstrImpl; // currently defaults to no prepass hazard recognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:117,Safety,hazard,hazard,117,// Use a ScoreboardHazardRecognizer for prepass ARM scheduling. TargetInstrImpl; // currently defaults to no prepass hazard recognizer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:31,Energy Efficiency,schedul,scheduling,31,// Called during:; // - pre-RA scheduling; // - post-RA scheduling when FeatureUseMISched is set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:56,Energy Efficiency,schedul,scheduling,56,// Called during:; // - pre-RA scheduling; // - post-RA scheduling when FeatureUseMISched is set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:72,Energy Efficiency,schedul,scheduling,72,// We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.; // Cortex-M7: TRM indicates that there is a single ITCM bank and two DTCM; // banks banked on bit 2. Assume that TCMs are in use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:34,Safety,hazard,hazard,34,// We would like to restrict this hazard recognizer to only; // post-RA scheduling; we can tell that we're post-RA because we don't; // track VRegLiveness.; // Cortex-M7: TRM indicates that there is a single ITCM bank and two DTCM; // banks banked on bit 2. Assume that TCMs are in use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Energy Efficiency,schedul,scheduling,25,// Called during post-RA scheduling when FeatureUseMISched is not set,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:28,Performance,load,load,28,// Try splitting an indexed load/store to an un-indexed one plus an add/sub; // operation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:305,Testability,test,test,305,"// Branch analysis.; // Cond vector output format:; // 0 elements indicates an unconditional branch; // 2 elements indicates a conditional branch; the elements are; // the condition to check and the CPSR.; // 3 elements indicates a hardware loop end; the elements; // are the opcode, the operand value to test, and a dummy; // operand used to pad out to 3 operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:224,Integrability,interface,interface,224,// VMOVRRD is also a copy instruction but it requires; // special way of handling. It is more complex copy version; // and since that we are not considering it. For recognition; // of such instruction isExtractSubregLike MI interface fuction; // could be used.; // VORRq is considered as a move only if two inputs are; // the same register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Security,access,access,30,// FIXME: don't use t2STRs to access frame.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Security,access,access,30,// FIXME: don't use t2LDRs to access frame.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:69,Integrability,depend,depending,69,/// Expands MEMCPY to either LDMIA/STMIA or LDMIA_UPD/STMID_UPD; /// depending on whether the result is used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:248,Availability,down,down,248,// This hook gets to expand COPY instructions before they become; // copyPhysReg() calls. Look for VMOVS instructions that can legally be; // widened to VMOVD. We prefer the VMOVD when possible because it may be; // changed into a VORR that can go down the NEON pipeline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:262,Deployability,pipeline,pipeline,262,// This hook gets to expand COPY instructions before they become; // copyPhysReg() calls. Look for VMOVS instructions that can legally be; // widened to VMOVD. We prefer the VMOVD when possible because it may be; // changed into a VORR that can go down the NEON pipeline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:7,Usability,clear,clear,7,"// All clear, widen the COPY.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Deployability,Update,Update,41,/// Create a copy of a const pool value. Update CPI to the new index and return; /// the label UID.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:16,Performance,load,loaded,16,"// Check if the loaded value, e.g. a constantpool of a global address, are; // the same.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:63,Energy Efficiency,schedul,scheduler,63,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:386,Integrability,interface,interface,386,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:98,Performance,load,loads,98,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:108,Performance,load,loading,108,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only differences; /// between the two addresses is the offset. It also returns the offsets by; /// reference.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Energy Efficiency,schedul,scheduler,65,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:167,Energy Efficiency,schedul,scheduled,167,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:295,Energy Efficiency,schedul,scheduled,295,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:473,Energy Efficiency,schedul,schedule,473,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:568,Energy Efficiency,schedul,scheduled,568,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:649,Integrability,interface,interface,649,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:146,Performance,load,loads,146,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:209,Performance,load,loads,209,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:219,Performance,load,loading,219,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:259,Performance,cache,cache,259,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:372,Performance,load,load,372,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:490,Performance,load,loads,490,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:534,Performance,load,loads,534,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads should; /// be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.; ///; /// FIXME: remove this in favor of the MachineInstr interface once pre-RA-sched; /// is permanently disabled.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:169,Performance,load,loads,169,"// Check if the machine opcodes are different. If they are different; // then we consider them to not be of the same base address,; // EXCEPT in the case of Thumb2 byte loads where one is LDRBi8 and the other LDRBi12.; // In this case, they are considered to be the same because they are different; // encoding forms of the same basic instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Performance,load,loads,40,// FIXME: overly conservative?; // Four loads in a row should be sufficient.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Energy Efficiency,schedul,scheduling,25,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:236,Energy Efficiency,schedul,scheduling,236,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:247,Safety,hazard,hazard,247,"// Debug info is never a scheduling boundary. It's necessary to be explicit; // due to the special treatment of IT instructions below, otherwise a; // dbg_value followed by an IT will result in the IT instruction being; // considered a scheduling hazard, which is wrong. It should be the actual; // instruction preceding the dbg_value instruction(s), just like it is; // when debug info is not present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:35,Energy Efficiency,schedul,scheduled,35,// Terminators and labels can't be scheduled around.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Energy Efficiency,schedul,scheduling,40,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Energy Efficiency,schedul,schedule,65,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:220,Integrability,depend,dependencies,220,"// Treat the start of the IT block as a scheduling boundary, but schedule; // t2IT along with all instructions following it.; // FIXME: This is a big hammer. But the alternative is to add all potential; // true and anti dependencies to IT block instructions as implicit operands; // to the t2IT instruction. The added compile time and complexity does not; // seem worth it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:20,Energy Efficiency,schedul,schedule,20,"// Don't attempt to schedule around any instruction that defines; // a stack-oriented pointer, as it's unlikely to be profitable. This; // saves compile time, because it doesn't require every single; // stack slot reference to depend on the instruction that does the; // modification.; // Calls don't actually change the stack pointer, even if they have imp-defs.; // No ARM calling conventions change the stack pointer. (X86 calling; // conventions sometimes do).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:227,Integrability,depend,depend,227,"// Don't attempt to schedule around any instruction that defines; // a stack-oriented pointer, as it's unlikely to be profitable. This; // saves compile time, because it doesn't require every single; // stack slot reference to depend on the instruction that does the; // modification.; // Calls don't actually change the stack pointer, even if they have imp-defs.; // No ARM calling conventions change the stack pointer. (X86 calling; // conventions sometimes do).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:13,Performance,optimiz,optimizing,13,"// If we are optimizing for size, see if the branch in the predecessor can be; // lowered to cbn?z by the constant island lowering pass, and return false if; // so. This results in a shorter instruction sequence.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:128,Safety,avoid,avoid,128,// Attempt to estimate the relative costs of predication versus branching.; // Here we scale up each component of UnpredCost to avoid precision issue when; // scaling TCycles/FCycles by Probability.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:31,Safety,predict,predictor,31,"// When we don't have a branch predictor it's always cheaper to not take a; // branch than take it, so we have to take that into account.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:106,Energy Efficiency,reduce,reduce,106,"// If this branch is likely to be folded into the comparison to form a; // CB(N)Z, then removing it won't reduce code size at all, because that will; // just replace the CB(N)Z with a CMP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Energy Efficiency,Reduce,Reduce,3,// Reduce false anti-dependencies to let the target's out-of-order execution; // engine do its thing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:21,Integrability,depend,dependencies,21,// Reduce false anti-dependencies to let the target's out-of-order execution; // engine do its thing.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:64,Safety,detect,detects,64,// Check if MI has any non-dead defs or physreg uses. This also detects; // predicated instructions which will be reading CPSR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:180,Energy Efficiency,allocate,allocated,180,// The output register value when the predicate is false is an implicit; // register operand tied to the first def.; // The tie makes the register allocator ensure the FalseReg is allocated the; // same register as operand 0.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Deployability,Update,Update,3,// Update SeenMIs set: register newly created MI and erase removed DefMI.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:42,Usability,clear,clear,42,"// We will handle these bits from offset, clear them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:46,Performance,load,load,46,"// This optimisation potentially adds lots of load and store; // micro-operations, it's only really a great benefit to code-size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:50,Energy Efficiency,allocate,allocate,50,// Now try to find enough space in the reglist to allocate NumBytes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,perform,perform,37,// Finally we know we can profitably perform the optimisation so go; // ahead: strip all existing registers off and add them back again; // in the right order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:42,Usability,clear,clear,42,"// We will handle these bits from offset, clear them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:46,Usability,simpl,simplify,46,"// FIXME: When addrmode2 goes away, this will simplify (like the; // T2 version), as the LDR.i12 versions don't need the encoding; // tricks for the offset value.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Usability,simpl,simplify,52,"// Otherwise, it didn't fit. Pull in what we can to simplify the immed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:131,Availability,mask,mask,131,"/// isSuitableForMask - Identify a suitable 'and' instruction that; /// operates on the given source register and applies the same mask; /// as a 'tst' instruction. Provide a limited look-through for copies.; /// When successful, MI will hold the found instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:120,Availability,redundant,redundant,120,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:154,Availability,redundant,redundant,154,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:226,Availability,redundant,redundant,226,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:306,Availability,redundant,redundant,306,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:94,Deployability,update,update,94,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:366,Modifiability,extend,extended,366,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:120,Safety,redund,redundant,120,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:154,Safety,redund,redundant,154,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:226,Safety,redund,redundant,226,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:306,Safety,redund,redundant,306,"/// isRedundantFlagInstr - check whether the first instruction, whose only; /// purpose is to update flags, can be made redundant.; /// CMPrr can be made redundant by SUBrr if the operands are the same.; /// CMPri can be made redundant by SUBri if the operands are the same.; /// CMPrr(r0, r1) can be made redundant by ADDr[ri](r0, r1, X).; /// This function can be extended later on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:166,Availability,redundant,redundant,166,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:4,Performance,optimiz,optimizeCompareInstr,4,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:166,Safety,redund,redundant,166,"/// optimizeCompareInstr - Convert the instruction supplying the argument to the; /// comparison into one that sets the zero bit in the flags register;; /// Remove a redundant Compare instruction if an earlier instruction can set the; /// flags in the same way as Compare.; /// E.g. SUBrr(r1,r2) and CMPrr(r1,r2). We also handle the case where two; /// operands are swapped: SUBrr(r1,r2) and CMPrr(r2,r1), by updating the; /// condition code of instructions which use the flags.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Availability,Mask,Masked,3,// Masked compares sometimes use the same register as the corresponding 'and'.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:202,Performance,load,load,202,"// We also want to do this peephole for cases like this: if (a*b == 0),; // and optimise away the CMP instruction from the generated code sequence:; // MULS, MOVS, MOVS, CMP. Here the MOVS instructions load the boolean values; // resulting from the select instruction, but these MOVS instructions for; // Thumb1 (V6M) are flag setting and are thus preventing this optimisation.; // However, if we only have MOVS instructions in between the CMP and the; // other instruction (the MULS in this example), then the CPSR is dead so we; // can safely reorder the sequence into: MOVS, MOVS, MULS, CMP. We do this; // reordering and then continue the analysis hoping we can eliminate the; // CMP. This peephole works on the vregs, so is still in SSA form. As a; // consequence, the movs won't redefine/kill the MUL operands which would; // make this reordering illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:538,Safety,safe,safely,538,"// We also want to do this peephole for cases like this: if (a*b == 0),; // and optimise away the CMP instruction from the generated code sequence:; // MULS, MOVS, MOVS, CMP. Here the MOVS instructions load the boolean values; // resulting from the select instruction, but these MOVS instructions for; // Thumb1 (V6M) are flag setting and are thus preventing this optimisation.; // However, if we only have MOVS instructions in between the CMP and the; // other instruction (the MULS in this example), then the CPSR is dead so we; // can safely reorder the sequence into: MOVS, MOVS, MULS, CMP. We do this; // reordering and then continue the analysis hoping we can eliminate the; // CMP. This peephole works on the vregs, so is still in SSA form. As a; // consequence, the movs won't redefine/kill the MUL operands which would; // make this reordering illegal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Availability,redundant,redundant,38,// Check whether CmpInstr can be made redundant by the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Safety,redund,redundant,38,// Check whether CmpInstr can be made redundant by the current instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:116,Energy Efficiency,schedul,scheduled,116,"// In some cases, we scan the use-list of an instruction for an AND;; // that AND is in the same BB, but may not be scheduled before the; // corresponding TST. In that case, bail out.; //; // FIXME: We could try to reschedule the AND.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:160,Safety,safe,safe,160,"// Scan forward for the use of CPSR; // When checking against MI: if it's a conditional code that requires; // checking of the V bit or C bit, then this is not safe to do.; // It is safe to remove CmpInstr if CPSR is redefined or killed.; // If we are done with the basic block, we need to check whether CPSR is; // live-out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:182,Safety,safe,safe,182,"// Scan forward for the use of CPSR; // When checking against MI: if it's a conditional code that requires; // checking of the V bit or C bit, then this is not safe to do.; // It is safe to remove CmpInstr if CPSR is redefined or killed.; // If we are done with the basic block, we need to check whether CPSR is; // live-out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:91,Deployability,update,updated,91,"// If we have SUB(r1, r2) and CMP(r2, r1), the condition code based; // on CMP needs to be updated to be based on SUB.; // If we have ADD(r1, r2, X) and CMP(r1, r2), the condition code also; // needs to be modified.; // Push the condition code operands to OperandsToUpdate.; // If it is safe to remove CmpInstr, the condition code of these; // operands will be modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:287,Safety,safe,safe,287,"// If we have SUB(r1, r2) and CMP(r2, r1), the condition code based; // on CMP needs to be updated to be based on SUB.; // If we have ADD(r1, r2, X) and CMP(r1, r2), the condition code also; // needs to be modified.; // Push the condition code operands to OperandsToUpdate.; // If it is safe to remove CmpInstr, the condition code of these; // operands will be modified.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:39,Deployability,update,update,39,// VSel doesn't support condition code update.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:66,Safety,safe,safe,66,// Z N V; // The instruction uses the V bit or C bit which is not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:111,Performance,optimiz,optimize,111,"// If CPSR is not killed nor re-defined, we should check whether it is; // live-out. If it is live-out, do not optimize.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Deployability,Toggle,Toggle,3,// Toggle the optional operand to CPSR (if it exists - in Thumb1 we always; // set CPSR so this is represented as an explicit output),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Availability,redundant,redundant,52,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Performance,optimiz,optimize,41,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Safety,redund,redundant,52,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:153,Safety,avoid,avoid,153,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:64,Safety,safe,safe,64,"// If DefMI defines CPSR and it is not dead, it's obviously not safe; // to delete DefMI.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Performance,optimiz,optimization,57,"// If the instruction sets the flag, do not attempt this optimization; // since it may change the semantics of the code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:877,Energy Efficiency,schedul,scheduling,877,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,load,loaded,37,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:281,Security,access,access,281,"// Return the number of 32-bit words loaded by LDM or stored by STM. If this; // can't be easily determined return 0 (missing MachineMemOperand).; //; // FIXME: The current MachineInstr design does not support relying on machine; // mem operands to determine the width of a memory access. Instead, we expect; // the target to provide this information based on the instruction opcode and; // operands. However, using MachineMemOperand is the best solution now for; // two reasons:; //; // 1) getNumMicroOps tries to infer LDM memory width from the total number of MI; // operands. This is much more dangerous than using the MachineMemOperand; // sizes because CodeGen passes can insert/remove optional machine operands. In; // fact, it's totally incorrect for preRA passes and appears to be wrong for; // postRA passes as well.; //; // 2) getNumLDMAddresses is only used by the scheduling machine model and any; // machine model that calls this should handle the unknown (zero size) case.; //; // Long term, we should require a target hook that verifies MachineMemOperand; // sizes during MC lowering. That target hook should be local to MC lowering; // because we can't ensure that it is aware of other MI forms. Doing this will; // ensure that MachineMemOperands are correctly propagated through all passes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:14,Energy Efficiency,schedul,scheduler,14,"// FIXME: The scheduler currently can't handle values larger than 16. But; // the values can actually go up to 32 for floating-point load/store; // multiple (VLDMIA etc.). Also, the way this code is reasoning about memory; // operations isn't right; we could end up with ""extra"" memory operands for; // various reasons, like tail merge merging two memory operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:133,Performance,load,load,133,"// FIXME: The scheduler currently can't handle values larger than 16. But; // the values can actually go up to 32 for floating-point load/store; // multiple (VLDMIA etc.). Also, the way this code is reasoning about memory; // operations isn't right; we could end up with ""extra"" memory operands for; // various reasons, like tail merge merging two memory operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:158,Energy Efficiency,schedul,scheduled,158,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:195,Energy Efficiency,schedul,scheduling,195,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:26,Performance,load,load,26,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:136,Performance,load,loads,136,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:220,Performance,load,load,220,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:474,Performance,load,load,474,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:345,Usability,simpl,simply,345,"// The number of uOps for load / store multiple are determined by the number; // registers.; //; // On Cortex-A8, each pair of register loads / stores can be scheduled on the; // same cycle. The scheduling for the first load / store must be done; // separately by assuming the address is not 64-bit aligned.; //; // On Cortex-A9, the formula is simply (#reg / 2) + (#reg % 2). If the address; // is not 64-bit aligned, then AGU would take an extra cycle. For VFP / NEON; // load / store multiple, the formula is (#reg / 2) + (#reg % 2) + 1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Performance,latency,latency,10,// Result latency is issue cycle + 2: E2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Performance,latency,latency,10,// Result latency is AGU cycles + 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:74,Performance,latency,latency,74,"// This may be a def / use of a variable_ops instruction, the operand; // latency might be determinable dynamically. Let the target try to; // figure it out.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,Performance,latency,latency,41,"// We can't seem to determine the result latency of the def, assume it's 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:168,Performance,latency,latency,168,/// Return the number of cycles to add to (or subtract from) the static; /// itinerary based on the def opcode and alignment. The caller will ensure that; /// adjusted latency is at least one cycle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,latency,latency,37,// FIXME: Properly handle all of the latency adjustments for address; // writeback.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:14,Performance,latency,latency,14,// No operand latency. The caller may fall back to getInstrLatency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:38,Performance,latency,latency,38,// Otherwise it takes the instruction latency (generally one).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:30,Energy Efficiency,schedul,scheduling,30,"// For Thumb2 and -Os, prefer scheduling CPSR setting instruction close to; // its uses. Instructions which are otherwise scheduled between them may; // incur a code size penalty (not able to use the CPSR setting 16-bit; // instructions).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:122,Energy Efficiency,schedul,scheduled,122,"// For Thumb2 and -Os, prefer scheduling CPSR setting instruction close to; // its uses. Instructions which are otherwise scheduled between them may; // incur a code size penalty (not able to use the CPSR setting 16-bit; // instructions).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:23,Performance,latency,latency,23,"// Get the itinerary's latency if possible, and handle variable_ops.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:26,Performance,latency,latency,26,// Unable to find operand latency. The caller may resort to getInstrLatency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Performance,latency,latency,24,"// Return the itinerary latency, which may be zero but not less than zero.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,Performance,latency,latency,37,// FIXME: Properly handle all of the latency adjustments for address; // writeback.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:18,Energy Efficiency,schedul,scheduler,18,"// An instruction scheduler typically runs on unbundled instructions, however; // other passes may query the latency of a bundled instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:109,Performance,latency,latency,109,"// An instruction scheduler typically runs on unbundled instructions, however; // other passes may query the latency of a bundled instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Modifiability,variab,variable,25,"// For instructions with variable uops, use uops as latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,Performance,latency,latency,52,"// For instructions with variable uops, use uops as latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,Performance,latency,latency,53,"// For the common case, fall back on the itinerary's latency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:50,Performance,latency,latency,50,// Hoist VFP / NEON instructions with 4 or higher latency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:215,Availability,down,down,215,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:247,Availability,down,down,247,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:290,Availability,down,down,290,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:326,Availability,down,down,326,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:229,Deployability,pipeline,pipeline,229,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:260,Deployability,pipeline,pipeline,260,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:339,Deployability,pipeline,pipeline,339,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:431,Deployability,pipeline,pipeline,431,"//===----------------------------------------------------------------------===//; // Execution domains.; //===----------------------------------------------------------------------===//; //; // Some instructions go down the NEON pipeline, some go down the VFP pipeline,; // and some can go down both. The vmov instructions go down the VFP pipeline,; // but they can be changed to vorr equivalents that are executed by the NEON; // pipeline.; //; // We use the following execution domain numbering:; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:20,Security,access,access,20,// If we don't have access to NEON instructions then we won't be able; // to swizzle anything to the NEON domain. Check to make sure.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,Performance,perform,perform,53,"// In general there's no single instruction that can perform an S <-> S; // move in NEON space, but a pair of VEXT instructions *can* do the; // job. It turns out that the VEXTs needed will only use DSrc once, with; // the position based purely on the combination of lane-0 and lane-1; // involved. For example; // vmov s0, s2 -> vext.32 d0, d0, d1, #1 vext.32 d0, d0, d0, #1; // vmov s1, s3 -> vext.32 d0, d1, d0, #1 vext.32 d0, d0, d0, #1; // vmov s0, s3 -> vext.32 d0, d0, d0, #1 vext.32 d0, d1, d0, #1; // vmov s1, s2 -> vext.32 d0, d0, d0, #1 vext.32 d0, d0, d1, #1; //; // Pattern of the MachineInstrs is:; // %DDst = VEXTd32 %DSrc1, %DSrc2, Lane, 14, %noreg (;implicits)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:102,Deployability,update,updates,102,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:586,Deployability,update,update,586,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:648,Deployability,update,update,648,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:715,Deployability,update,update,715,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:764,Integrability,depend,dependency-breaking,764,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:453,Performance,load,loads,453,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:511,Performance,load,load,511,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:367,Safety,avoid,avoided,367,"//===----------------------------------------------------------------------===//; // Partial register updates; //===----------------------------------------------------------------------===//; //; // Swift renames NEON registers with 64-bit granularity. That means any; // instruction writing an S-reg implicitly reads the containing D-reg. The; // problem is mostly avoided by translating f32 operations to v2f32 operations; // on D-registers, but f32 loads are still a problem.; //; // These instructions can load an f32 into a NEON register:; //; // VLDRS - Only writes S, partial D update.; // VLD1LNd32 - Writes all D-regs, explicit partial D update, 2 uops.; // VLD1DUPd32 - Writes all D-regs, no partial reg update, 2 uops.; //; // FCONSTD can be used as a dependency-breaking instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Integrability,depend,dependency,24,// Explicitly reads the dependency.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:81,Integrability,depend,dependency,81,"// If this instruction actually reads a value from Reg, there is no unwanted; // dependency.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:33,Integrability,depend,dependency,33,// MI has an unwanted D-register dependency.; // Avoid defs in the previous N instructrions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:49,Safety,Avoid,Avoid,49,// MI has an unwanted D-register dependency.; // Avoid defs in the previous N instructrions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:28,Integrability,depend,dependency,28,// Break a partial register dependency after getPartialRegUpdateClearance; // returned non-zero.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:232,Energy Efficiency,schedul,schedule,232,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:335,Integrability,depend,dependency-breaking,335,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:103,Performance,load,loading,103,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:16,Performance,load,load,16,// Literal pool load,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:2150,Availability,avail,available,2150,"nction is; /// called with a BL instruction, and the outlined function tail-calls the; /// original call destination.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// BL f I2; /// B f; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 0 | 0 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerDefault implies that the function should be called with; /// a save and restore of LR to the stack.; ///; /// That is,; ///; /// I1 Save LR OUTLIN",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:2188,Safety,avoid,avoid,2188," ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 0 | 0 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerNoLRSave implies that the function should be called using; /// a BL instruction, but doesn't require LR to be saved and restored. This; /// happens when LR is known to be dead.; ///; /// That is,; ///; /// I1 OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 4 | 4 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerRegSave implies that the function should be called with a; /// save and restore of LR to an available register. This allows us to avoid; /// stack fixups. Note that this outlining variant is compatible with the; /// NoLRSave case.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |; /// | Stack fixup required | No | No |; /// +-------------------------+--------+-----+; ///; /// \p MachineOutlinerDefault implies that the function should be called with; /// a save and restore of LR to the stack.; ///; /// That is,; ///; /// I1 Save LR OUTLINED_FUNCTION:; /// I2 --> BL OUTLINED_FUNCTION I1; /// I3 Restore LR I2; /// I3; /// BX LR; ///; /// +-------------------------+--------+-----+; /// | | Thumb2 | ARM |; /// +-------------------------+--------+-----+; /// | Call overhead in Bytes | 8 | 12 |; /// | Frame overhead in Bytes | 2 | 4 |;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Availability,avail,available,24,// Check if there is an available register across the sequence that we can; // use.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:10,Safety,unsafe,unsafe,10,"// If the unsafe registers in this block are all dead, then we don't need; // to compute liveness here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:116,Security,authenticat,authentication,116,"// We expect the majority of the outlining candidates to be in consensus with; // regard to return address sign and authentication, and branch target; // enforcement, in other words, partitioning according to all the four; // possible combinations of PAC-RET and BTI is going to yield one big subset; // and three small (likely empty) subsets. That allows us to cull incompatible; // candidates separately for PAC-RET and BTI.; // Partition the candidates in two sets: one with BTI enabled and one with BTI; // disabled. Remove the candidates from the smaller set. If they are the same; // number prefer the non-BTI ones for outlining, since they have less; // overhead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:32,Safety,safe,safe,32,"// At this point, we have only ""safe"" candidates to outline. Figure out; // frame + call instruction information.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,Security,authenticat,authentication,40,// Adjust costs to account for sign and authentication instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:25,Availability,avail,available,25,"// Is an unused register available? If so, we won't modify the stack, so; // we can outline with the same frame type as those that don't save LR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:88,Deployability,update,update,88,"// If there are no places where we have to save LR, then note that we don't; // have to update the stack. Otherwise, give every candidate the default; // call type",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Performance,Load,Load,3,// Load/Store Multiple,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:8,Performance,Load,Load,8,// Neon Load/Store Multiple,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:9,Security,access,access,9,// PCrel access,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:8,Safety,safe,safe,8,// It's safe to outline from MF.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:18,Availability,avail,available,18,"// Check if LR is available through all of the MBB. If it's not, then set; // a flag.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:45,Availability,avail,available,45,// Check if each of the unsafe registers are available...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Safety,unsafe,unsafe,24,// Check if each of the unsafe registers are available...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:32,Availability,avail,available,32,"// If any of these registers is available in the MBB, but also a live out of; // the block, then we know outlining is unsafe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:118,Safety,unsafe,unsafe,118,"// If any of these registers is available in the MBB, but also a live out of; // the block, then we know outlining is unsafe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Integrability,depend,depends,57,"// If we don't know anything about the callee, assume it depends on the; // stack layout of the caller. In that case, it's only legal to outline; // as a tail-call. Explicitly list the call instructions we know about so; // we don't get unexpected results with call pseudo-instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:84,Safety,safe,safely,84,// We have a function we have information about. Check if it's something we; // can safely outline.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:132,Availability,avail,available,132,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:344,Security,authenticat,authentication,344,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:380,Security,authenticat,authentication,380,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:609,Security,authenticat,authentication,609,"// True if there is no chance that any outlined candidate from this range; // could require stack fixups. That is, both; // * LR is available in the range (No save/restore around call); // * The range doesn't include calls (No save/restore in outlined frame); // are true.; // These conditions also ensure correctness of the return address; // authentication - we insert sign and authentication instructions only if; // we save/restore LR on stack, but then this condition ensures that the; // outlined range does not modify the SP, therefore the SP value used for; // signing is the same as the one used for authentication.; // FIXME: This is very restrictive; the flags check the whole block,; // not just the bit we will try to outline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:110,Performance,load,load,110,"// At this point, we have a stack instruction that we might need to fix up.; // up. We'll handle it if it's a load or store.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:6,Security,authenticat,authentication,6,"// LR authentication is after the CFI instructions, below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:24,Modifiability,rewrite,rewrite,24,"// For thunk outlining, rewrite the last instruction from a call to a; // tail-call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:81,Security,access,accesses,81,// We modified the stack.; // Walk over the basic block and fix up all the stack accesses.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:101,Energy Efficiency,schedul,schedule,101,// Bitset[0 .. MAX_STAGES-1] ... iterations needed; // [LAST_IS_USE] : last reference to register in schedule is a use; // [SEEN_AS_LIVE] : Normal pressure algorithm believes register is live,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,Energy Efficiency,schedul,schedule,57,// Determine which values will be loop-carried after the schedule is; // applied,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:44,Energy Efficiency,schedul,schedule,44,// Determine more-or-less what the proposed schedule (reversed) is going to; // be; it might not be quite the same because the within-cycle ordering; // created by SMSchedule depends upon changes to help with address offsets and; // the like.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:175,Integrability,depend,depends,175,// Determine more-or-less what the proposed schedule (reversed) is going to; // be; it might not be quite the same because the within-cycle ordering; // created by SMSchedule depends upon changes to help with address offsets and; // the like.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:3,Usability,Learn,Learn,3,"// Learn whether the last use/def of each cross-iteration register is a use or; // def. If it is a def, RegisterPressure will implicitly increase max pressure; // and we do not have to add the pressure.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:176,Deployability,pipeline,pipelineable,176,"// If the branch is a Bcc, then the CPSR should be set somewhere within the; // block. We need to determine the reaching definition of CPSR so that; // it can be marked as non-pipelineable, allowing the pipeliner to force; // it into stage 0 or give up if it cannot or will not do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:203,Deployability,pipeline,pipeliner,203,"// If the branch is a Bcc, then the CPSR should be set somewhere within the; // block. We need to determine the reaching definition of CPSR so that; // it can be marked as non-pipelineable, allowing the pipeliner to force; // it into stage 0 or give up if it cannot or will not do so.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:65,Deployability,pipeline,pipeline,65,"// Unable to find the CC setter, so unable to guarantee; // that pipeline will work",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:78,Modifiability,enhance,enhance,78,"/// Specialization of \ref TargetInstrInfo::describeLoadedValue, used to; /// enhance debug entry value descriptions for ARM targets.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:63,Energy Efficiency,schedul,scheduler,63,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:98,Performance,load,loads,98,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:108,Performance,load,loading,108,/// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler to; /// determine if two loads are loading from the same base address. It should; /// only return true if the base pointers are the same and the only; /// differences between the two addresses is the offset. It also returns the; /// offsets by reference.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:65,Energy Efficiency,schedul,scheduler,65,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:167,Energy Efficiency,schedul,scheduled,167,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:295,Energy Efficiency,schedul,scheduled,295,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:473,Energy Efficiency,schedul,schedule,473,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:568,Energy Efficiency,schedul,scheduled,568,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:146,Performance,load,loads,146,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:209,Performance,load,loads,209,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:219,Performance,load,loading,219,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:259,Performance,cache,cache,259,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:372,Performance,load,load,372,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:490,Performance,load,loads,490,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:534,Performance,load,loads,534,"/// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to; /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads; /// should be scheduled togther. On some targets if two loads are loading from; /// addresses in the same cache line, it's better if they are scheduled; /// together. This function takes two integers that represent the load offsets; /// from the common base address. It returns true if it decides it's desirable; /// to schedule the two loads together. ""NumLoads"" is the number of loads that; /// have already been scheduled after Load1.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:134,Availability,redundant,redundant,134,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:185,Deployability,update,updated,185,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:4,Performance,optimiz,optimizeCompareInstr,4,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:134,Safety,redund,redundant,134,"/// optimizeCompareInstr - Convert the instruction to set the zero flag so; /// that we can remove a ""comparison with zero""; Remove a redundant CMP; /// instruction if the flags can be updated in the same way by an earlier; /// instruction such as SUB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:123,Deployability,Pipeline,PipelinerLoopInfo,123,"/// Analyze loop L, which must be a single-basic-block loop, and if the; /// conditions can be understood enough produce a PipelinerLoopInfo object.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:178,Security,authenticat,authentication,178,"/// Adds an instruction which saves the link register on top of the stack into; /// the MachineBasicBlock \p MBB at position \p It. If \p Auth is true,; /// compute and store an authentication code alongiside the link register.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:174,Security,authenticat,authentication,174,"/// Adds an instruction which restores the link register from the top the; /// stack into the MachineBasicBlock \p MBB at position \p It. If \p Auth is; /// true, restore an authentication code and authenticate LR.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:198,Security,authenticat,authenticate,198,"/// Adds an instruction which restores the link register from the top the; /// stack into the MachineBasicBlock \p MBB at position \p It. If \p Auth is; /// true, restore an authentication code and authenticate LR.; /// If \p CFI is true, emit CFI instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:87,Deployability,update,updates,87,/// Returns true if the machine instruction offset can handle the stack fixup; /// and updates it if requested.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:24,Performance,Perform,Perform,24,/// verifyInstruction - Perform target specific instruction verification.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:45,Safety,hazard,hazards,45,/// Modeling special VFP / NEON fp MLA / MLS hazards.; /// MLxEntryMap - Map fp MLA / MLS to the corresponding entry in the internal; /// MLx table.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:95,Energy Efficiency,schedul,scheduled,95,/// MLxHazardOpcodes - Set of add / sub and multiply opcodes that would cause; /// stalls when scheduled together with fp MLA / MLS opcodes.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:107,Energy Efficiency,schedul,scheduled,107,/// canCauseFpMLxStall - Return true if an instruction of the specified opcode; /// will cause stalls when scheduled after (within 4-cycle window) a fp; /// MLA / MLS instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:73,Availability,mask,mask,73,"// This table shows the VPT instruction variants, i.e. the different; // mask field encodings, see also B5.6. Predication/conditional execution in; // the ArmARM.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:4,Modifiability,rewrite,rewriteARMFrameIndex,4,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:54,Modifiability,Rewrite,Rewrite,54,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:68,Security,access,access,68,"/// rewriteARMFrameIndex / rewriteT2FrameIndex -; /// Rewrite MI to access 'Offset' bytes from the FP. Return false if the; /// offset could not be handled directly in MI, and return the left-over; /// portion by reference.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:125,Performance,load,load,125,"/// Returns the number of instructions required to materialize the given; /// constant in a register, or 3 if a literal pool load is needed.; /// If ForCodesize is specified, an approximate cost in bytes is returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h:18,Security,access,access,18,"// Given a memory access Opcode, check that the give Imm would be a valid Offset; // for this instruction using its addressing mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:33,Availability,mask,mask,33,"// This should return a register mask that is the same as that returned by; // getCallPreservedMask but that additionally preserves the register used for; // the first i32 argument (which must also be the register used to return a; // single i32 return value); //; // In case that the calling convention does not use the same register for; // both or otherwise does not want to enable this optimization, the function; // should return NULL",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:390,Performance,optimiz,optimization,390,"// This should return a register mask that is the same as that returned by; // getCallPreservedMask but that additionally preserves the register used for; // the first i32 argument (which must also be the register used to return a; // single i32 return value); //; // In case that the calling convention does not use the same register for; // both or otherwise does not want to enable this optimization, the function; // should return NULL",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:10,Safety,avoid,avoid,10,// FIXME: avoid re-calculating this every time.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:72,Availability,avail,available,72,// hasFP ends up calling getMaxCallFrameComputed() which may not be; // available when getPressureLimit() is called as part of; // ScheduleDAGRRList.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:131,Energy Efficiency,Schedul,ScheduleDAGRRList,131,// hasFP ends up calling getMaxCallFrameComputed() which may not be; // available when getPressureLimit() is called as part of; // ScheduleDAGRRList.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:179,Deployability,update,updated,179,// If 'Reg' is one of the even / odd register pair and it's now changed; // (e.g. coalesced) into a different register. The other register of the; // pair allocation hint must be updated to reflect the relationship; // change.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:171,Energy Efficiency,allocate,allocate,171,"// If we have stack realignment and VLAs, we have no pointer to use to; // access the stack. If we have stack realignment, and a large call frame,; // we have no place to allocate the emergency spill slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:75,Security,access,access,75,"// If we have stack realignment and VLAs, we have no pointer to use to; // access the stack. If we have stack realignment, and a large call frame,; // we have no place to allocate the emergency spill slot.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:242,Modifiability,variab,variable,242,"// Thumb has trouble with negative offsets from the FP. Thumb2 has a limited; // negative range for ldr/str (255), and Thumb1 is positive offsets only.; //; // It's going to be better to use the SP or Base Pointer instead. When there; // are variable sized objects, we can't reference off of the SP, so we; // reserve a Base Pointer.; //; // For Thumb2, estimate whether a negative offset from the frame pointer; // will be sufficient to reach the whole stack frame. If a function has a; // smallish frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, the scavenger will still enable access to work, it just; // won't be optimal. (We should always be able to reach the emergency; // spill slot from the frame pointer.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:691,Security,access,access,691,"// Thumb has trouble with negative offsets from the FP. Thumb2 has a limited; // negative range for ldr/str (255), and Thumb1 is positive offsets only.; //; // It's going to be better to use the SP or Base Pointer instead. When there; // are variable sized objects, we can't reference off of the SP, so we; // reserve a Base Pointer.; //; // For Thumb2, estimate whether a negative offset from the frame pointer; // will be sufficient to reach the whole stack frame. If a function has a; // smallish frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, the scavenger will still enable access to work, it just; // won't be optimal. (We should always be able to reach the emergency; // spill slot from the frame pointer.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:32,Performance,load,load,32,/// emitLoadConstPool - Emits a load from constpool to materialize the; /// specified immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:12,Performance,load,load,12,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:485,Performance,load,loads,485,"// It's the load/store FI references that cause issues, as it can be difficult; // to materialize the offset if it won't fit in the literal field. Estimate; // based on the size of the local frame and some conservative assumptions; // about the rest of the stack frame (note, this is pre-regalloc, so; // we don't know everything for certain yet) whether this offset is likely; // to be out of range of the immediate. Return true if so.; // We only generate virtual base registers for loads and stores, so; // return false for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:56,Modifiability,variab,variable,56,"// Without a virtual base register, if the function has variable sized; // objects, all fixed-size local references will be via the frame pointer,; // Approximate the offset and see if it's legal for the instruction.; // Note that the incoming offset is based on the SP value at function entry,; // so it'll be negative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:138,Security,access,access,138,"// Estimate an offset from the stack pointer.; // The incoming offset is relating to the SP at the start of the function,; // but when we access the local it'll be relative to the SP after local; // allocation, so adjust our SP-relative offset by that allocation size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:52,Energy Efficiency,allocate,allocated,52,// Assume that we'll have at least some spill slots allocated.; // FIXME: This is a total SWAG number. We should run some statistics; // and pick a real one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:127,Availability,avail,available,127,"// 128 bytes of spill slots; // If there's a frame pointer and the addressing mode allows it, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:282,Modifiability,variab,variables,282,"// 128 bytes of spill slots; // If there's a frame pointer and the addressing mode allows it, try using it.; // The FP is only available if there is no dynamic realignment. We; // don't know for sure yet whether we'll need that, so we guess based; // on whether there are any local variables that would trigger it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:45,Energy Efficiency,allocate,allocate,45,"// The offset likely isn't legal, we want to allocate a virtual base register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:200,Security,access,access,200,// PEI::scavengeFrameVirtualRegs() cannot accurately track SPAdj because the; // call frame setup/destroy instructions have already been eliminated. That; // means the stack pointer cannot be used to access the emergency spill slot; // when !hasReservedCallFrame().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:3,Deployability,Update,Update,3,// Update the original instruction to use the scratch register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:138,Testability,test,test,138,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:200,Testability,test,test,200,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp:221,Testability,test,test-suite,221,"// This number is the largest round number that which meets the criteria:; // (1) addresses PR18825; // (2) generates better code in some test cases (like vldm-shed-a9.ll); // (3) Doesn't regress any test cases (in-tree, test-suite, and SPEC); // In practice the SizeMultiplier will only factor in for straight line code; // that uses a lot of NEON vectors, which isn't terribly common.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:148,Modifiability,variab,variable,148,"/// BasePtr - ARM physical register used as a base ptr in complex stack; /// frames. I.e., when we need a 3rd base, not just SP and FP, due to; /// variable size stack objects.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:58,Availability,mask,mask,58,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i32 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i32 argument and an i32 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:230,Availability,mask,mask,230,/// getThisReturnPreservedMask - Returns a call preserved mask specific to the; /// case that 'returned' is on an i32 first argument if the calling convention; /// is one that can (partially) model this attribute with a preserved mask; /// (i.e. it is a calling convention that uses the same register for the first; /// i32 argument and an i32 return value); ///; /// Should return NULL in the case that the calling convention does not have; /// this property,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h:32,Performance,load,load,32,/// emitLoadConstPool - Emits a load from constpool to materialize the; /// specified immediate.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseRegisterInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp:88,Deployability,update,updated,88,"// This is where block i begins. Stop if the offset is already correct,; // and we have updated 2 blocks. This is the maximum number of blocks; // changed before calling this function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h:105,Safety,predict,predict,105,/// Compute the number of known offset bits internally to this block.; /// This number should be used to predict worst case padding when; /// splitting the block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h:235,Testability,Log,LogAlign,235,"/// Compute the number of known low bits of postOffset. If this block; /// contains inline asm, the number of known bits drops to the; /// instruction alignment. An aligned terminator may increase the number; /// of know bits.; /// If LogAlign is given, also consider the alignment of the next block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBasicBlockInfo.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp:3,Usability,Clear,Clear,3,"// Clear the kill flags, as the cmp/bcc will no longer kill any operands.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp:4,Deployability,Update,Updates,4,/// Updates ordering (of WLS BB and their loopExits) in inner loops first; /// Returns true if any change was made in any of the loops,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBlockPlacement.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBranchTargets.cpp:715,Security,attack,attacks,715,"//===-- ARMBranchTargets.cpp -- Harden code using v8.1-M BTI extension -----==//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This pass inserts BTI instructions at the start of every function and basic; // block which could be indirectly called. The hardware will (when enabled); // trap when an indirect branch or call instruction targets an instruction; // which is not a valid BTI instruction. This is intended to guard against; // control-flow hijacking attacks.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBranchTargets.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBranchTargets.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:41,Integrability,Rout,Routines,41,"//=== ARMCallingConv.cpp - ARM Custom CC Routines ---------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the custom routines for the ARM Calling Convention that; // aren't done by tablegen, and includes the table generated implementations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:413,Integrability,rout,routines,413,"//=== ARMCallingConv.cpp - ARM Custom CC Routines ---------------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the custom routines for the ARM Calling Convention that; // aren't done by tablegen, and includes the table generated implementations.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:3,Energy Efficiency,Allocate,Allocate,3,"// Allocate part of an AAPCS HFA or HVA. We assume that each member of the HA; // has InConsecutiveRegs set, and that the last member also has; // InConsecutiveRegsLast set. We must process all members of the HA before; // we can allocate it, as we need to know the total number of registers that; // will be needed in order to (attempt to) allocate a contiguous block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:230,Energy Efficiency,allocate,allocate,230,"// Allocate part of an AAPCS HFA or HVA. We assume that each member of the HA; // has InConsecutiveRegs set, and that the last member also has; // InConsecutiveRegsLast set. We must process all members of the HA before; // we can allocate it, as we need to know the total number of registers that; // will be needed in order to (attempt to) allocate a contiguous block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:341,Energy Efficiency,allocate,allocate,341,"// Allocate part of an AAPCS HFA or HVA. We assume that each member of the HA; // has InConsecutiveRegs set, and that the last member also has; // InConsecutiveRegsLast set. We must process all members of the HA before; // we can allocate it, as we need to know the total number of registers that; // will be needed in order to (attempt to) allocate a contiguous block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:38,Energy Efficiency,allocate,allocated,38,// Add the argument to the list to be allocated once we know the size of the; // aggregate. Store the type's required alignment as extra info for later: in; // the [N x i64] case all trace has been removed by the time we actually get; // to do allocation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:10,Energy Efficiency,allocate,allocate,10,"// Try to allocate a contiguous block of registers, each of the correct; // size to hold one member.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:33,Energy Efficiency,allocate,allocated,33,"// After the first item has been allocated, the rest are packed as tightly as; // possible. (E.g. an incoming i64 would have starting Align of 8, but we'll; // be allocating a bunch of i32 slots).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:37,Energy Efficiency,allocate,allocated,37,// All pending members have now been allocated,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:16,Energy Efficiency,allocate,allocated,16,// This will be allocated by the last member of the aggregate,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:21,Modifiability,extend,extended,21,"// f16 arguments are extended to i32 and assigned to a register in [r0, r3]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp:21,Modifiability,extend,extended,21,"// f16 arguments are extended to f32 and assigned to a register in [s0, s15]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.h:55,Integrability,Rout,Routines,55,"//=== ARMCallingConv.h - ARM Custom Calling Convention Routines -*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file declares the entry points for ARM calling convention analysis.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallingConv.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:33,Modifiability,extend,extended,33,"// If the value is zero- or sign-extended, its size becomes 4 bytes, so; // that's what we should load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:98,Performance,load,load,98,"// If the value is zero- or sign-extended, its size becomes 4 bytes, so; // that's what we should load.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:23,Modifiability,extend,extended,23,"// If the value is not extended, a simple load will suffice.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:42,Performance,load,load,42,"// If the value is not extended, a simple load will suffice.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:35,Usability,simpl,simple,35,"// If the value is not extended, a simple load will suffice.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:39,Deployability,update,update,39,// We now know the size of the stack - update the ADJCALLSTACKDOWN; // accordingly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:742,Usability,resume,resume,742,"/// CPUser - One user of a constant pool, keeping the machine instruction; /// pointer, the constant pool being referenced, and the max displacement; /// allowed from the instruction to the CP. The HighWaterMark records the; /// highest basic block where a new CPEntry can be placed. To ensure this; /// pass terminates, the CP entries are initially placed at the end of the; /// function and then move monotonically to lower addresses. The; /// exception to this rule is when the current CP entry for a particular; /// CPUser is out of range, but there is another CP entry for the same; /// constant value in range. We want to use the existing in-range CP; /// entry, but if it later moves out of range, the search for new water; /// should resume where it left off. The HighWaterMark is used to record; /// that point.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:61,Safety,safe,safety,61,// Verify offset using the real max displacement without the safety; // adjustment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:96,Deployability,pipeline,pipeline,96,"// For LOB's, the ARMLowOverheadLoops pass may remove the unconditional; // branch later in the pipeline.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:65,Energy Efficiency,adapt,adapted,65,// TBB generation code in this constant island pass has not been adapted to; // deal with speculation barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:65,Modifiability,adapt,adapted,65,// TBB generation code in this constant island pass has not been adapted to; // deal with speculation barriers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:27,Usability,clear,clear,27,"// Data is out of date, so clear it. It'll be re-computed later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Performance,Perform,Perform,3,"// Perform the initial placement of the constant pool entries. To start with,; // we put them all at the end of the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Usability,Clear,Clear,3,"// Clear NewWaterList now. If we split a block for branches, it should; // appear as ""new water"" for the next iteration of constant pool placement.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:24,Performance,load,load,24,// Shrink 32-bit Thumb2 load and store instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Performance,Optimiz,Optimize,3,// Optimize jump tables using TBB / TBH.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:4,Performance,Perform,Perform,4,"/// Perform the initial placement of the regular constant pool entries.; /// To start with, we put them all at the end of the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:109,Energy Efficiency,efficient,efficient,109,"/// Do initial placement of the jump tables. Because Thumb2's TBB and TBH; /// instructions can be made more efficient if the jump table immediately; /// follows the instruction, it's best to place them immediately next to their; /// jumps to begin with. In almost all cases they'll never be moved from that; /// position.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:4,Deployability,update,updateForInsertedWaterBlock,4,"/// updateForInsertedWaterBlock - When a block is newly inserted into the; /// machine function, it upsets all of the block numbers. Renumber the blocks; /// and update the arrays that parallel this numbering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:162,Deployability,update,update,162,"/// updateForInsertedWaterBlock - When a block is newly inserted into the; /// machine function, it upsets all of the block numbers. Renumber the blocks; /// and update the arrays that parallel this numbering.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:77,Availability,avail,available,77,"// Next, update WaterList. Specifically, we need to add NewMBB as having; // available water after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:9,Deployability,update,update,9,"// Next, update WaterList. Specifically, we need to add NewMBB as having; // available water after it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:107,Deployability,Update,Update,107,"/// Split the basic block containing MI into two blocks, which are joined by; /// an unconditional branch. Update data structures and renumber blocks to; /// account for this change and returns the newly created block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:164,Availability,avail,available,164,// Add an unconditional branch from OrigBB to NewBB.; // Note the new unconditional branch is not being recorded.; // There doesn't seem to be meaningful DebugInfo available; this doesn't; // correspond to anything in the source.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update the CFG. All succs of OrigBB are now succs of NewBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update live-in information in the new block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,"// Update internal data structures to account for the newly inserted MBB.; // This is almost the same as updateForInsertedWaterBlock, except that; // the Water goes after OrigBB, not NewBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:105,Deployability,update,updateForInsertedWaterBlock,105,"// Update internal data structures to account for the newly inserted MBB.; // This is almost the same as updateForInsertedWaterBlock, except that; // the Water goes after OrigBB, not NewBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:78,Availability,avail,available,78,"// Next, update WaterList. Specifically, we need to add OrigMBB as having; // available water after it (but not if it's already there, which happens; // when splitting before a conditional branch that is followed by an; // unconditional branch - in that case we want to insert NewBB).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:9,Deployability,update,update,9,"// Next, update WaterList. Specifically, we need to add OrigMBB as having; // available water after it (but not if it's already there, which happens; // when splitting before a conditional branch that is followed by an; // unconditional branch - in that case we want to insert NewBB).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:102,Deployability,Update,Update,102,/// getUserOffset - Compute the offset of U.MI as seen by the hardware; /// displacement computation. Update U.KnownAlignment to match its current; /// basic block location.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:42,Availability,down,down,42,"// On Thumb, offsets==2 mod 4 are rounded down by the hardware for; // purposes of the displacement computation; compensate for that here.; // For unknown alignments, getMaxDisp() constrains the range instead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:313,Security,access,access,313,"// The nearest water without splitting the UserBB is right after it.; // If the distance is still large (we have a big BB), then we need to split it; // if we don't converge after certain iterations. This helps the following; // situation to converge:; // BB0:; // Big BB; // BB1:; // Constant Pool; // When a CP access is out of range, BB0 may be used as water. However,; // inserting islands between BB0 and BB1 makes other accesses out of range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:426,Security,access,accesses,426,"// The nearest water without splitting the UserBB is right after it.; // If the distance is still large (we have a big BB), then we need to split it; // if we don't converge after certain iterations. This helps the following; // situation to converge:; // BB0:; // Big BB; // BB1:; // Constant Pool; // When a CP access is out of range, BB0 may be used as water. However,; // inserting islands between BB0 and BB1 makes other accesses out of range.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Safety,Avoid,Avoid,3,// Avoid splitting an IT block.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:310,Deployability,update,update,310,"// Avoid splitting a MOVW+MOVT pair with a relocation on Windows.; // On Windows, this instruction pair is covered by one single; // IMAGE_REL_ARM_MOV32T relocation which covers both instructions. If a; // constant island is injected inbetween them, the relocation will clobber; // the instruction and fail to update the MOVT instruction.; // (These instructions are bundled up until right before the ConstantIslands; // pass.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:225,Integrability,inject,injected,225,"// Avoid splitting a MOVW+MOVT pair with a relocation on Windows.; // On Windows, this instruction pair is covered by one single; // IMAGE_REL_ARM_MOV32T relocation which covers both instructions. If a; // constant island is injected inbetween them, the relocation will clobber; // the instruction and fail to update the MOVT instruction.; // (These instructions are bundled up until right before the ConstantIslands; // pass.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Safety,Avoid,Avoid,3,"// Avoid splitting a MOVW+MOVT pair with a relocation on Windows.; // On Windows, this instruction pair is covered by one single; // IMAGE_REL_ARM_MOV32T relocation which covers both instructions. If a; // constant island is injected inbetween them, the relocation will clobber; // the instruction and fail to update the MOVT instruction.; // (These instructions are bundled up until right before the ConstantIslands; // pass.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:225,Security,inject,injected,225,"// Avoid splitting a MOVW+MOVT pair with a relocation on Windows.; // On Windows, this instruction pair is covered by one single; // IMAGE_REL_ARM_MOV32T relocation which covers both instructions. If a; // constant island is injected inbetween them, the relocation will clobber; // the instruction and fail to update the MOVT instruction.; // (These instructions are bundled up until right before the ConstantIslands; // pass.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:146,Deployability,update,updated,146,"// If the original WaterList entry was ""new water"" on this iteration,; // propagate that to the new island. This is just keeping NewWaterList; // updated to match the WaterList, which will be updated below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:192,Deployability,update,updated,192,"// If the original WaterList entry was ""new water"" on this iteration,; // propagate that to the new island. This is just keeping NewWaterList; // updated to match the WaterList, which will be updated below.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:28,Deployability,Update,Update,28,// We are adding new water. Update NewWaterList.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:156,Energy Efficiency,reduce,reduces,156,// Remove the original WaterList entry; we want subsequent insertions in; // this vicinity to go after the one we're about to insert. This; // considerably reduces the number of times we have to move the same CPE; // more than once and is also important to ensure the algorithm terminates.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update internal data structures to account for the newly inserted MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:69,Deployability,Update,Update,69,/// removeDeadCPEMI - Remove a dead constant pool entry instruction. Update; /// sizes and offsets of impacted basic blocks.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:92,Deployability,update,updated,92,"// If the branch is at the end of its MBB and that has a fall-through block,; // direct the updated conditional branch to the fall-through block. Otherwise,; // split the MBB before the next instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:56,Usability,simpl,simply,56,// Last MI in the BB is an unconditional branch. Can we simply invert the; // condition and swap destinations:; // beq L1; // b L2; // =>; // bne L2; // b L1,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:80,Deployability,update,update,80,"// The conditional successor will be swapped between the BBs after this, so; // update CFG.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:76,Deployability,update,update,76,// Insert a new conditional branch and a new unconditional branch.; // Also update the ImmBranch as well as adding a new entry for the new branch.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:107,Safety,safe,safe,107,"// If the conditional branch doesn't kill CPSR, then CPSR can be liveout; // so this transformation is not safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:45,Deployability,update,update,45,"// Swapped a t2Bcc for a t2LE, so no need to update the size of the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:149,Energy Efficiency,reduce,reduce,149,"// The order in which branches appear in ImmBranches is approximately their; // order within the function body. By visiting later branches first, we reduce; // the distance between earlier forward branches and their targets, making it; // more likely that the cbn?z optimization, which can only apply to forward; // branches, will succeed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:266,Performance,optimiz,optimization,266,"// The order in which branches appear in ImmBranches is approximately their; // order within the function body. By visiting later branches first, we reduce; // the distance between earlier forward branches and their targets, making it; // more likely that the cbn?z optimization, which can only apply to forward; // branches, will succeed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:264,Availability,redundant,redundant,264,"// An LE has been generated, but it's not the terminator - that is an; // unconditional branch. However, the logic has now been reversed with the; // CBN?Z being the conditional branch and the LE being the unconditional; // branch. So this means we can remove the redundant unconditional branch; // at the end of the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:264,Safety,redund,redundant,264,"// An LE has been generated, but it's not the terminator - that is an; // unconditional branch. However, the logic has now been reversed with the; // CBN?Z being the conditional branch and the LE being the unconditional; // branch. So this means we can remove the redundant unconditional branch; // at the end of the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:109,Testability,log,logic,109,"// An LE has been generated, but it's not the terminator - that is an; // unconditional branch. However, the logic has now been reversed with the; // CBN?Z being the conditional branch and the LE being the unconditional; // branch. So this means we can remove the redundant unconditional branch; // at the end of the block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:117,Security,access,access,117,"/// While trying to form a TBB/TBH instruction, we may (if the table; /// doesn't immediately follow the BR_JT) need access to the start of the; /// jump-table. We know one instruction that produces such a register; this; /// function works out whether that definition can be preserved to the BR_JT,; /// possibly by removing an intervening addition (which is usually needed to; /// calculate the actual entry to jump to).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:39,Performance,load,load,39,"// %base cannot be redefined after the load as it will appear before; // TBB/TBH like:; // %base =; // %base =; // tBB %base, %idx",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:26,Performance,load,load,26,// Now safe to delete the load and lsl. The LEA will be removed later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:7,Safety,safe,safe,7,// Now safe to delete the load and lsl. The LEA will be removed later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:205,Usability,simpl,simple,205,"// If the destination block is terminated by an unconditional branch,; // try to move it; otherwise, create a new block following the jump; // table that branches back to the actual target. This is a very simple; // heuristic. FIXME: We can definitely improve it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update numbering to account for the block being moved.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:99,Availability,avail,available,99,// Add an unconditional branch from NewBB to BB.; // There doesn't seem to be meaningful DebugInfo available; this doesn't; // correspond directly to anything in the source.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update internal data structures to account for the newly inserted MBB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp:3,Deployability,Update,Update,3,// Update the CFG.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantIslandPass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:167,Performance,load,load,167,"// end namespace ARMCP; /// ARMConstantPoolValue - ARM specific constantpool value. This is used to; /// represent PC-relative displacement between the address of the load; /// instruction and the constant being loaded, i.e. (&GV-(LPIC+8)).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:212,Performance,load,loaded,212,"// end namespace ARMCP; /// ARMConstantPoolValue - ARM specific constantpool value. This is used to; /// represent PC-relative displacement between the address of the load; /// instruction and the constant being loaded, i.e. (&GV-(LPIC+8)).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:19,Performance,load,load,19,// Label id of the load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:18,Performance,load,loaded,18,// Constant being loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:19,Performance,load,loaded,19,// ExtSymbol being loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:487,Energy Efficiency,schedul,scheduling,487,"//===-- ARMExpandPseudoInsts.cpp - Expand pseudo instructions -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling, if-conversion, and other late; // optimizations. This pass should be run after register allocation but before; // the post-regalloc scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:631,Energy Efficiency,schedul,scheduling,631,"//===-- ARMExpandPseudoInsts.cpp - Expand pseudo instructions -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling, if-conversion, and other late; // optimizations. This pass should be run after register allocation but before; // the post-regalloc scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:533,Performance,optimiz,optimizations,533,"//===-- ARMExpandPseudoInsts.cpp - Expand pseudo instructions -------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains a pass that expands pseudo instructions into target; // instructions to allow proper scheduling, if-conversion, and other late; // optimizations. This pass should be run after register allocation but before; // the post-regalloc scheduling pass.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:226,Integrability,depend,depending,226,"// Constants for register spacing in NEON load/store instructions.; // For quad-register load-lane and store-lane pseudo instructors, the; // spacing is initially assumed to be EvenDblSpc, and that is changed to; // OddDblSpc depending on the lane number operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:42,Performance,load,load,42,"// Constants for register spacing in NEON load/store instructions.; // For quad-register load-lane and store-lane pseudo instructors, the; // spacing is initially assumed to be EvenDblSpc, and that is changed to; // OddDblSpc depending on the lane number operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:89,Performance,load,load-lane,89,"// Constants for register spacing in NEON load/store instructions.; // For quad-register load-lane and store-lane pseudo instructors, the; // spacing is initially assumed to be EvenDblSpc, and that is changed to; // OddDblSpc depending on the lane number operand.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:20,Performance,load,load,20,// Entries for NEON load/store information table. The table is sorted by; // PseudoOpc for fast binary-search lookups.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:15,Performance,load,loaded,15,// D registers loaded or stored,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:80,Performance,load,load,80,/// LookupNEONLdSt - Search the NEONLdStTable for information about a NEON; /// load or store pseudo instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:186,Performance,load,load,186,"// This check is overly conservative. Unless we are certain that the machine; // operand is not a symbol reference, we return that it is a symbol reference.; // This is important as the load pair may not be split up Windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:89,Safety,avoid,avoid,89,"// Expand the mov into a sequence of mov/add+lsl of the individual bytes. We; // want to avoid emitting any zero bytes, as they won't change the result, and; // also don't want any pointless shifts, so instead of immediately emitting; // the shift for a byte we keep track of how much we will need to shift and do; // it before the next nonzero byte.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:25,Security,access,accessed,25,"// The size of the area, accessed by that VLSTM/VLLDM; // S0-S31 + FPSCR + 8 more bytes (VPR + pad, or just pad)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear the registers using the CLRM instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear the registers and flags by copying ClobberReg into them.; // (Baseline can't do a high register clear in one instruction).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:105,Usability,clear,clear,105,// Clear the registers and flags by copying ClobberReg into them.; // (Baseline can't do a high register clear in one instruction).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:38,Usability,clear,cleared,38,"// Find which FP registers need to be cleared. The parameter `ClearRegs` is; // initialised with all elements set to true, and this function resets all the; // bits, which correspond to register uses. Returns true if any floating point; // register is defined, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:62,Usability,Clear,ClearRegs,62,"// Find which FP registers need to be cleared. The parameter `ClearRegs` is; // initialised with all elements set to true, and this function resets all the; // bits, which correspond to register uses. Returns true if any floating point; // register is defined, false otherwise.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,"// Clear the FP registers for v8.0-M, by copying over the content; // of LR. Uses R12 as a scratch register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:110,Security,Secur,Secure,110,"// If optimising for minimum size, clear FP registers unconditionally.; // Otherwise, check the CONTROL.SFPA (Secure Floating-Point Active) bit and; // don't clear them if they belong to the non-secure state.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:195,Security,secur,secure,195,"// If optimising for minimum size, clear FP registers unconditionally.; // Otherwise, check the CONTROL.SFPA (Secure Floating-Point Active) bit and; // don't clear them if they belong to the non-secure state.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:35,Usability,clear,clear,35,"// If optimising for minimum size, clear FP registers unconditionally.; // Otherwise, check the CONTROL.SFPA (Secure Floating-Point Active) bit and; // don't clear them if they belong to the non-secure state.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:158,Usability,clear,clear,158,"// If optimising for minimum size, clear FP registers unconditionally.; // Otherwise, check the CONTROL.SFPA (Secure Floating-Point Active) bit and; // don't clear them if they belong to the non-secure state.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:119,Usability,clear,clear,119,"// At the new basic blocks we need to have live-in the registers, used; // for the return value as well as LR, used to clear registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:14,Usability,clear,clear,14,"// If SFPA is clear, jump over ClearBB to DoneBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:31,Usability,Clear,ClearBB,31,"// If SFPA is clear, jump over ClearBB to DoneBB.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:12,Usability,clear,clearing,12,// Emit the clearing sequence,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:14,Usability,clear,clear,14,// Attempt to clear as double,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear first part as single,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear second part as single,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,"// Clear FPSCR bits 0-4, 7, 28-31; // The other bits are program global according to the AAPCS",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Modifiability,extend,extend,3,// extend range,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:12,Usability,clear,clear,12,// Save and clear FP registers if present,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:12,Availability,avail,available,12,// Store an available register for FPSCR clearing,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:41,Usability,clear,clearing,41,// Store an available register for FPSCR clearing,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:37,Performance,load,load,37,// For big-endian targets we need to load the two subregisters of Reg; // manually because VLDRD would load them in wrong order,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:103,Performance,load,load,103,// For big-endian targets we need to load the two subregisters of Reg; // manually because VLDRD would load them in wrong order,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:32,Usability,clear,clear,32,"// restore FPSCR from stack and clear bits 0-4, 7, 28-31; // The other bits are program global according to the AAPCS",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:86,Energy Efficiency,schedul,scheduler,86,"// The ldr must happen after a floating point instruction. To prevent the; // post-ra scheduler to mess with the order, we create a bundle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear FP registers with a VSCCLRM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:7,Availability,Avail,AvailableRegs,7,// Use AvailableRegs to store the fp regs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:8,Performance,load,load,8,// Lazy load fp regs from stack.; // This executes as NOP in the absence of floating-point support.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:14,Usability,clear,clear,14,"// If SFPA is clear jump over to VLLDM, otherwise execute an instruction; // which has no functional effect apart from causing context creation:; // vmovne s0, s0. In the absence of FPU we emit .inst.w 0xeeb00a40,; // which is defined as NOP if not executed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Performance,Load,Load,3,// Load FP registers from stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:60,Usability,simpl,simply,60,/// Expand a CMP_SWAP pseudo-inst to an ldrex/strex loop as simply as; /// possible. This only gets used at -O0 so we don't care about efficiency of; /// the generated code.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Deployability,Update,Update,3,// Update call site info and delete the pseudo instruction TCRETURN.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:30,Security,authenticat,authenticate,30,"// For v8.0-M.Main we need to authenticate LR before clearing FPRs, which; // uses R12 as a scratch register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:53,Usability,clear,clearing,53,"// For v8.0-M.Main we need to authenticate LR before clearing FPRs, which; // uses R12 as a scratch register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:19,Security,secur,secure,19,// Restore the non-secure floating point context.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear all GPR that are not a use of the return instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:17,Usability,clear,cleared,17,// Get the first cleared register as a scratch (to use later with tBIC).; // We need to use the first so we can ensure it is a low register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Usability,Clear,Clear,3,// Clear LSB of JumpReg,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:8,Usability,clear,clear,8,// save+clear FP regs with ClearRegs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:27,Usability,Clear,ClearRegs,27,// save+clear FP regs with ClearRegs,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:3,Deployability,Update,Update,3,// Update the call site info.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:37,Performance,load,load,37,// We need a new const-pool entry to load from.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:15,Modifiability,variab,variables,15,// Convenience variables to avoid some queries.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:28,Safety,avoid,avoid,28,// Convenience variables to avoid some queries.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:25,Integrability,rout,routines,25,// Instruction selection routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Integrability,rout,routines,11,// Utility routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:17,Integrability,rout,routines,17,// Call handling routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:24,Integrability,rout,routines,24,// OptionalDef handling routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:20,Performance,load,loading,20,// Require VFP2 for loading fp constants.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Performance,Load,Load,3,// Load from constant pool. For now 32-bit only.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:35,Safety,avoid,avoids,35,"// Use movw+movt when possible, it avoids constant pool entries.; // Non-darwin targets only support static movt relocations in FastISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Performance,Load,Load,3,// Load value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:78,Modifiability,rewrite,rewriteXFrameIndex,78,// This will get lowered later into the correct offsets and registers; // via rewriteXFrameIndex.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:46,Modifiability,extend,extended,46,// If this is a type than can be sign or zero-extended to a basic operation; // go ahead and accept it now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Performance,load,loads,11,// Integer loads/stores handle 12-bit offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:16,Performance,load,load,16,// ARM halfword load/stores and signed byte loads use +/-imm8 offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:44,Performance,load,loads,44,// ARM halfword load/stores and signed byte loads use +/-imm8 offsets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:57,Usability,simpl,simplified,57,"// If this is a stack pointer and the offset needs to be simplified then; // put the alloca address into a register, set the base type back to; // register and continue. This should almost never happen.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:41,Performance,load,load,41,// Since the offset is too large for the load/store instruction; // get the reg+offset into a register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:20,Integrability,depend,depends,20,// addrmode5 output depends on the selection dag addressing dividing the; // offset by 4 that it then later multiplies. Do this here as well.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:16,Performance,load,load,16,// ARM halfword load/stores and signed byte loads need an additional; // operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:44,Performance,load,loads,44,// ARM halfword load/stores and signed byte loads need an additional; // operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:16,Performance,load,load,16,// ARM halfword load/stores and signed byte loads need an additional; // operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:44,Performance,load,loads,44,// ARM halfword load/stores and signed byte loads need an additional; // operand.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:13,Performance,load,loads,13,// Unaligned loads need special handling. Floats require word-alignment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:7,Performance,load,load,7,// Can load and store double precision even without FeatureFP64,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:20,Performance,load,loads,20,// FIXME: Unaligned loads need special handling. Doublewords require; // word-alignment.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:17,Availability,down,down,17,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:26,Performance,load,load,26,// If we had an unaligned load of a float we've converted it to an regular; // load. Now we must move from the GRP to the FP register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:79,Performance,load,load,79,// If we had an unaligned load of a float we've converted it to an regular; // load. Now we must move from the GRP to the FP register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:10,Performance,load,loads,10,// Atomic loads need special handling.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:7,Performance,load,load,7,// Can load and store double precision even without FeatureFP64,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:17,Availability,down,down,17,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Usability,Simpl,Simplify,3,// Simplify this down to something we can handle.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:41,Safety,avoid,avoid,41,"// Simple branch support.; // If we can, avoid recomputing the compare - redoing it could lead to wonky; // behavior.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Usability,Simpl,Simple,3,"// Simple branch support.; // If we can, avoid recomputing the compare - redoing it could lead to wonky; // behavior.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:391,Testability,test,test,391,"// We've been divorced from our compare! Our block was split, and; // now our compare lives in a predecessor block. We musn't; // re-compare here, as the children of the compare aren't guaranteed; // live across the block boundary (we *could* check for this).; // Regardless, the compare has been done in the predecessor block,; // and it left a value for us in a virtual register. Ergo, we test; // the one-bit value left in the virtual register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:50,Modifiability,extend,extend,50,"// We have i1, i8, or i16, we need to either zero extend or sign extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:65,Modifiability,extend,extend,65,"// We have i1, i8, or i16, we need to either zero extend or sign extend.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:60,Safety,safe,safe,60,"// ARMEmitCmp emits a FMSTAT when necessary, so it's always safe to use CPSR.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:40,Modifiability,extend,extending,40,// Make sure we have VFP and that we're extending float to double.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:18,Integrability,rout,routine,18,"// The conversion routine works on fp-reg to fp-reg and the operand above; // was an integer, move it to the fp registers if possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:27,Modifiability,extend,extend,27,// FIXME: ArgLocs[++i] may extend beyond ArgLocs.size(),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Deployability,update,update,11,// Finally update the result.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:24,Modifiability,extend,extended,24,// Special handling for extended integers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Deployability,update,update,11,// Finally update the result.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:24,Modifiability,extend,extended,24,// Special handling for extended integers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Performance,Perform,Perform,3,"// Perform extension if flagged as either zext or sext. Otherwise, do; // nothing.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:3,Safety,Avoid,Avoid,3,// Avoid a cross-class copy. This is very unlikely.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:41,Safety,avoid,avoid,41,// Manually compute the global's type to avoid building it when unnecessary.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Usability,simpl,simple,11,// Handle *simple* calls for now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:18,Availability,mask,mask,18,// Add a register mask with the call-preserved registers.; // Proper defs for return values will be added by setPhysRegsDeadExcept().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:9,Safety,Avoid,Avoid,9,// TODO: Avoid some calling conventions?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Usability,simpl,simple,11,// Handle *simple* calls for now.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:18,Availability,mask,mask,18,// Add a register mask with the call-preserved registers.; // Proper defs for return values will be added by setPhysRegsDeadExcept().,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:15,Performance,load,load,15,// Recursively load frame address; // ldr r0 [fp]; // ldr r0 [r0]; // ldr r0 [r0]; // ...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:45,Availability,mask,mask,45,// All instructions have either a shift or a mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:194,Availability,avail,available,194,"// Either one or two instructions are emitted.; // They're always of the form:; // dst = in OP imm; // CPSR is set only by 16-bit Thumb instructions.; // Predicate, if any, is AL.; // S bit, if available, is always 0.; // When two are emitted the first's result will feed as the second's input,; // that value is then dead.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:162,Availability,mask,mask,162,// This table describes sign- and zero-extend instructions which can be; // folded into a preceding load. All of these extends have an immediate; // (sometimes a mask and sometimes a shift) that's applied after; // extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:39,Modifiability,extend,extend,39,// This table describes sign- and zero-extend instructions which can be; // folded into a preceding load. All of these extends have an immediate; // (sometimes a mask and sometimes a shift) that's applied after; // extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:119,Modifiability,extend,extends,119,// This table describes sign- and zero-extend instructions which can be; // folded into a preceding load. All of these extends have an immediate; // (sometimes a mask and sometimes a shift) that's applied after; // extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:100,Performance,load,load,100,// This table describes sign- and zero-extend instructions which can be; // folded into a preceding load. All of these extends have an immediate; // (sometimes a mask and sometimes a shift) that's applied after; // extension.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:105,Performance,load,load,105,"/// The specified machine instr operand is a vreg, and that; /// vreg is being provided by the specified load instruction. If possible,; /// try to fold the load as an operand to the instruction, returning true if; /// successful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:157,Performance,load,load,157,"/// The specified machine instr operand is a vreg, and that; /// vreg is being provided by the specified load instruction. If possible,; /// try to fold the load as an operand to the instruction, returning true if; /// successful.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:42,Modifiability,extend,extend,42,"// Combine load followed by zero- or sign-extend.; // ldrb r1, [r0] ldrb r1, [r0]; // uxtb r2, r1 =>; // mov r3, r2 mov r3, r1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,Performance,load,load,11,"// Combine load followed by zero- or sign-extend.; // ldrb r1, [r0] ldrb r1, [r0]; // uxtb r2, r1 =>; // mov r3, r2 mov r3, r1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:86,Usability,ux,uxtb,86,"// Combine load followed by zero- or sign-extend.; // ldrb r1, [r0] ldrb r1, [r0]; // uxtb r2, r1 =>; // mov r3, r2 mov r3, r1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:15,Usability,simpl,simple,15,// Only handle simple cases. i.e. Up to 4 i8/i16/i32 scalar arguments; // which are passed in r0 - r3.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:631,Deployability,update,updated,631,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:744,Deployability,update,update,744,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:806,Deployability,update,updates,806,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1132,Deployability,update,updated,1132,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1223,Deployability,update,updated,1223,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:929,Safety,safe,safe,929,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1150,Safety,unsafe,unsafe,1150,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1351,Safety,safe,safe,1351,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1509,Safety,safe,safe,1509,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:1544,Safety,unsafe,unsafe,1544,"//===-- ARMFixCortexA57AES1742098Pass.cpp ---------------------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; // This pass works around a Cortex Core Fused AES erratum:; // - Cortex-A57 Erratum 1742098; // - Cortex-A72 Erratum 1655431; //; // The erratum may be triggered if an input vector register to AESE or AESD was; // last written by an instruction that only updated 32 bits of it. This can; // occur for either of the input registers.; //; // The workaround chosen is to update the input register using `r = VORRq r, r`,; // as this updates all 128 bits of the register unconditionally, but does not; // change the values observed in `r`, making the input safe.; //; // This pass has to be conservative in a few cases:; // - an input vector register to the AES instruction is defined outside the; // current function, where we have to assume the register was updated in an; // unsafe way; and; // - an input vector register to the AES instruction is updated along multiple; // different control-flow paths, where we have to ensure all the register; // updating instructions are safe.; //; // Both of these cases may apply to a input vector register. In either case, we; // need to ensure that, when the pass is finished, there exists a safe; // instruction between every unsafe register updating instruction and the AES; // instruction.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:23,Safety,safe,safe,23,// Unknown: Assume not safe.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:3,Performance,Load,Loads,3,"// Loads (when condition = al); // VLD Dn, [Rn, #imm]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:153,Safety,safe,safe,153,"// If the register doesn't have defining instructions, and is not a; // live-in, then something is wrong and the fixup must always be; // inserted to be safe.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:19,Safety,unsafe,unsafe,19,// If there are no unsafe definitions...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:23,Safety,unsafe,unsafe,23,"// Otherwise, the only unsafe ""definition"" is a live-in, so insert the; // fixup at the start of the function.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:16,Safety,unsafe,unsafe,16,"// There is one unsafe defining instruction, which needs a fixup. It is; // generally good to hoist the fixup to be adjacent to the defining; // instruction rather than the using instruction, as the using; // instruction may be inside a loop when the defining instruction is; // not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:232,Safety,safe,safe,232,"// Insert the new `VORRq qN, qN, qN`. There are a few details here:; //; // The uses are marked as killed, even if the original use of OperandToFixup; // is not killed, as the new instruction is clobbering the register. This is; // safe even if there are other uses of `qN`, as the VORRq value-wise a no-op; // (it is inserted for microarchitectural reasons).; //; // The def and the uses are still marked as Renamable if the original register; // was, to avoid having to rummage through all the other uses and defs and; // unset their renamable bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp:456,Safety,avoid,avoid,456,"// Insert the new `VORRq qN, qN, qN`. There are a few details here:; //; // The uses are marked as killed, even if the original use of OperandToFixup; // is not killed, as the new instruction is clobbering the register. This is; // safe even if there are other uses of `qN`, as the VORRq value-wise a no-op; // (it is inserted for microarchitectural reasons).; //; // The def and the uses are still marked as Renamable if the original register; // was, to avoid having to rummage through all the other uses and defs and; // unset their renamable bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFixCortexA57AES1742098Pass.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:701,Availability,down,downward,701,"//===- ARMFrameLowering.cpp - ARM Frame Information -----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains the ARM implementation of TargetFrameLowering class.; //; //===----------------------------------------------------------------------===//; //; // This file contains the ARM implementation of TargetFrameLowering class.; //; // On ARM, stack frames are structured as follows:; //; // The stack grows downward.; //; // All of the individual frame areas on the frame below are optional, i.e. it's; // possible to create a function so that the particular area isn't present; // in the frame.; //; // At function entry, the ""frame"" looks as follows:; //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- sp; // | | Lower address; //; //; // After the prologue has run, the frame has the following general structure.; // Technically the last frame area (VLAs) doesn't get created until in the; // main function body, after the prologue is run. However, it's depicted here; // for completeness.; //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- (sp at function entry); // | |; // | varargs from registers |; // | |; // |-----------------------------------|; // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:4089,Energy Efficiency,allocate,allocate,4089,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3801,Integrability,depend,depending,3801,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:2256,Modifiability,variab,variables,2256," created until in the; // main function body, after the prologue is run. However, it's depicted here; // for completeness.; //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- (sp at function entry); // | |; // | varargs from registers |; // | |; // |-----------------------------------|; // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are bo",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:2397,Modifiability,variab,variable-sized,2397," //; // | | Higher address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- (sp at function entry); // | |; // | varargs from registers |; // | |; // |-----------------------------------|; // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed w",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:2418,Modifiability,variab,variables,2418,"address; // |-----------------------------------|; // | |; // | arguments passed on the stack |; // | |; // |-----------------------------------| <- (sp at function entry); // | |; // | varargs from registers |; // | |; // |-----------------------------------|; // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are loca",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3304,Modifiability,variab,variables,3304,"variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3425,Modifiability,variab,variables,3425,"ables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===---------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3631,Modifiability,variab,variables,3631,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:4048,Modifiability,variab,variable-sized,4048,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:4158,Modifiability,variab,variable,4158,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3721,Performance,load,loads,3721,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:2666,Security,access,access,2666," // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't b",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:2793,Security,access,access,2793," // | |; // | prev_lr |; // | prev_fp |; // | (a.k.a. ""frame record"") |; // | |; // |- - - - - - - - - - - - - - - - - -| <- fp (r7 or r11); // | |; // | callee-saved gpr registers |; // | |; // |-----------------------------------|; // | |; // | callee-saved fp/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't b",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3000,Security,access,access,3000,"/simd regs |; // | |; // |-----------------------------------|; // |.empty.space.to.make.part.below....|; // |.aligned.in.case.it.needs.more.than| (size of this area is unknown at; // |.the.standard.8-byte.alignment.....| compile time; if present); // |-----------------------------------|; // | |; // | local variables of fixed size |; // | including spill slots |; // |-----------------------------------| <- base pointer (not defined by ABI,; // |.variable-sized.local.variables....| LLVM chooses r6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arg",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3618,Security,access,access,3618,"6); // |.(VLAs)............................| (size of this area is unknown at; // |...................................| compile time); // |-----------------------------------| <- sp; // | | Lower address; //; //; // To access the data in a frame, at-compile time, a constant offset must be; // computable from one of the pointers (fp, bp, sp) to access it. The size; // of the areas with a dotted background cannot be computed at compile-time; // if they are present, making it required to have all three of fp, bp and; // sp to be set up to be able to access all contents in the frame areas,; // assuming all of the frame areas are non-empty.; //; // For most functions, some of the frame areas are empty. For those functions,; // it may not be necessary to set up fp or bp:; // * A base pointer is definitely needed when there are both VLAs and local; // variables with more-than-default alignment requirements.; // * A frame pointer is definitely needed when there are local variables with; // more-than-default alignment requirements.; //; // In some cases when a base pointer is not strictly needed, it is generated; // anyway when offsets from the frame pointer to access local variables become; // so large that the offset can't be encoded in the immediate fields of loads; // or stores.; //; // The frame pointer might be chosen to be r7 or r11, depending on the target; // architecture and operating system. See ARMSubtarget::getFramePointerReg for; // details.; //; // Outgoing function arguments must be at the bottom of the stack frame when; // calling another function. If we do not have variable-sized stack objects, we; // can allocate a ""reserved call frame"" area at the bottom of the local; // variable area, large enough for all outgoing calls. If we do have VLAs, then; // the stack pointer must be decremented and incremented around each call to; // make space for the arguments below the VLAs.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:165,Testability,test,test-suite,165,"// iOS always has a FP for backtracking, force other targets to keep their FP; // when doing FastISel. The emitted code is currently superior, and in cases; // like test-suite's lencod FastISel isn't quite correct when FP is eliminated.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:35,Safety,safe,safely,35,/// Returns true if the target can safely skip saving callee-saved registers; /// for noreturn nounwind functions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:136,Modifiability,variab,variable,136,/// hasFP - Return true if the specified function should have a dedicated frame; /// pointer register. This is true if the function has variable sized allocas; /// or if frame pointer elimination is disabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:247,Availability,avail,available,247,"/// canSimplifyCallFramePseudos - If there is a reserved call frame, the; /// call frame pseudos can be simplified. Unlike most targets, having a FP; /// is not sufficient here since we still may reference some objects via SP; /// even when FP is available in Thumb2 mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:104,Usability,simpl,simplified,104,"/// canSimplifyCallFramePseudos - If there is a reserved call frame, the; /// call frame pseudos can be simplified. Unlike most targets, having a FP; /// is not sufficient here since we still may reference some objects via SP; /// even when FP is available in Thumb2 mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:11,Performance,load,load,11,"// Given a load or a store instruction, generate an appropriate unwinding SEH; // code on Windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:362,Performance,optimiz,optimizing,362,"// end anonymous namespace; /// Emit an instruction sequence that will align the address in; /// register Reg by zero-ing out the lower bits. For versions of the; /// architecture that support Neon, this must be done in a single; /// instruction, since skipAlignedDPRCS2Spills assumes it is done in a; /// single instruction. That function only gets called when optimizing; /// spilling of D registers on a core with the Neon instruction set; /// present.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:29,Availability,avail,available,29,"// if the BFC instruction is available, use that to zero the lower; // bits:; // bfc Reg, #0, log2(Alignment); // otherwise use BIC, if the mask to zero the required number of bits; // can be encoded in the bic immediate field; // bic Reg, Reg, Alignment-1; // otherwise, emit; // lsr Reg, Reg, log2(Alignment); // lsl Reg, Reg, log2(Alignment)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:140,Availability,mask,mask,140,"// if the BFC instruction is available, use that to zero the lower; // bits:; // bfc Reg, #0, log2(Alignment); // otherwise use BIC, if the mask to zero the required number of bits; // can be encoded in the bic immediate field; // bic Reg, Reg, Alignment-1; // otherwise, emit; // lsr Reg, Reg, log2(Alignment); // lsl Reg, Reg, log2(Alignment)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:92,Availability,avail,available,92,"// Since this is only reached for Thumb-2 targets, the BFC instruction; // should always be available.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:423,Testability,assert,assert,423,/// We need the offset of the frame pointer relative to other MachineFrameInfo; /// offsets which are encoded relative to SP at function begin.; /// See also emitPrologue() for how the FP is set up.; /// Unfortunately we cannot determine this value in determineCalleeSaves() yet; /// as assignCalleeSavedSpillSlots() hasn't run at this point. Instead we use; /// this to produce a conservative estimate that we check in an assert() later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:28,Availability,avail,available,28,"// For Thumb1, push.w isn't available, so the first push will always push; // r7 and lr onto the stack first.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3,Energy Efficiency,Allocate,Allocate,3,// Allocate the vararg register save area.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:76,Safety,safe,safe,76,"// Restore from fp only in ARM mode: e.g. sub sp, r7, #24; // Note it's not safe to do this in Thumb2 mode because it would have; // taken two instructions:; // mov sp, r7; // sub sp, #24; // If an interrupt is taken between the two instructions, then sp is in; // an inconsistent state (pointing to the middle of callee-saved area).; // The interrupt handler can end up clobbering the registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:138,Security,access,access,138,"// If we need dynamic stack realignment, do it here. Be paranoid and make; // sure if we also have VLAs, we have a base pointer for frame access.; // If aligned NEON registers were spilled, the stack has already been; // realigned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:77,Performance,perform,perform,77,"// We cannot use sp as source/dest register here, thus we're using r4 to; // perform the calculations. We're emitting the following sequence:; // mov r4, sp; // -- use emitAligningInstructions to produce best sequence to zero; // -- out lower bits in r4; // mov sp, r4; // FIXME: It will be better just to find spare register here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:151,Energy Efficiency,allocate,allocated,151,"// If we need a base pointer, set it up here. It's whatever the value; // of the stack pointer is at this point. Any variable size objects; // will be allocated after this, so we can still use the base pointer; // to reference locals.; // FIXME: Clarify FrameSetup flags here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:117,Modifiability,variab,variable,117,"// If we need a base pointer, set it up here. It's whatever the value; // of the stack pointer is at this point. Any variable size objects; // will be allocated after this, so we can still use the base pointer; // to reference locals.; // FIXME: Clarify FrameSetup flags here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:20,Modifiability,variab,variable,20,// If the frame has variable sized objects then the epilogue must restore; // the sp from fp. We can assume there's an FP here since hasFP already; // checks for hasVarSizedObjects.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:59,Modifiability,extend,extends,59,// Reset SP based on frame pointer only if the stack frame extends beyond; // frame pointer stack slot or target is ELF and the function has FP.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3,Security,Validat,Validate,3,"// Validate PAC, It should have been already popped into R12. For CMSE entry; // function, the validation instruction is emitted during expansion of the; // tBXNS_RET, since the validation must use the value of SP at function; // entry, before saving, resp. after restoring, FPCXTNS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:95,Security,validat,validation,95,"// Validate PAC, It should have been already popped into R12. For CMSE entry; // function, the validation instruction is emitted during expansion of the; // tBXNS_RET, since the validation must use the value of SP at function; // entry, before saving, resp. after restoring, FPCXTNS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:178,Security,validat,validation,178,"// Validate PAC, It should have been already popped into R12. For CMSE entry; // function, the validation instruction is emitted during expansion of the; // tBXNS_RET, since the validation must use the value of SP at function; // entry, before saving, resp. after restoring, FPCXTNS.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:243,Usability,simpl,simple,243,/// getFrameIndexReference - Provide a base+offset reference to an FI slot for; /// debug info. It's the same as what we use for resolving the code-gen; /// references for now. FIXME: This can go wrong when references are; /// SP-relative and simple call frames aren't used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:112,Availability,reliab,reliable,112,// Use frame pointer to reference fixed objects. Use it for locals if; // there are VLAs (and thus the SP isn't reliable as a base).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:84,Availability,avail,available,84,"// Try to use the frame pointer if we can, else use the base pointer; // since it's available. This is handy for the emergency spill slot, in; // particular.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:63,Safety,avoid,avoid,63,"// In Thumb2 mode, the negative offset is very limited. Try to avoid; // out of range references. ldr <rt>,[<rn>, #-<imm8>]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:82,Modifiability,refactor,refactoring,82,// ARM mode needs an extra reg0 here due to addrmode2. Will go away once; // that refactoring is complete (eventually).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:126,Performance,perform,perform,126,"// We must set parameter MustBeSingleInstruction to true, since; // skipAlignedDPRCS2Spills expects exactly 3 instructions to perform; // stack alignment. Luckily, this can always be done since all ARM; // architecture versions that support Neon also support the BFC; // instruction.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:16,Security,secur,secure,16,// Save the non-secure floating point context.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:27,Performance,load,load,27,// Addressing modes 4 & 6 (load/store) instructions can't encode an; // immediate offset for stack references.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:211,Performance,perform,performance,211,"// In functions that realign the stack, it can be an advantage to spill the; // callee-saved vector registers after realigning the stack. The vst1 and vld1; // instructions take alignment hints that can improve performance.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:162,Deployability,patch,patch,162,"// We are disabling shrinkwrapping for now when PAC is enabled, as; // shrinkwrapping can cause clobbering of r12 when the PAC code is; // generated. A follow-up patch will fix this in a more performant manner.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:192,Performance,perform,performant,192,"// We are disabling shrinkwrapping for now when PAC is enabled, as; // shrinkwrapping can cause clobbering of r12 when the PAC code is; // generated. A follow-up patch will fix this in a more performant manner.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:83,Security,access,access,83,"// Thumb1 may require a spill when storing to a frame index through FP (or any; // access with execute-only), for cases where FP is a high register (R11). This; // scans the function for cases where this may happen.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:259,Performance,load,loads,259,// This tells PEI to spill the FP as if it is any other callee-save register; // to take advantage the eliminateFrameIndex machinery. This also ensures it; // is spilled in the order specified by getCalleeSavedRegs() to make it easier; // to combine multiple loads / stores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:33,Testability,assert,assert,33,// Silence unused warning in non-assert builds.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:138,Safety,safe,safe,138,"// If a stack probe will be emitted, spill R4 and LR, since they are; // clobbered by the stack probe call.; // This estimate should be a safe, conservative estimate. The actual; // stack probe is enabled based on the size of the local objects;; // this estimate also includes the varargs store size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:36,Modifiability,variab,variable,36,// Spill LR if Thumb1 function uses variable length argument lists.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:129,Availability,avail,available,129,"// If any of the stack slot references may be out of range of an immediate; // offset, make sure a register (or a spill slot) is available for the; // register scavenger. Note that if we're indexing off the frame pointer, the; // effective stack size is 4 bytes larger since the FP points to the stack; // slot of the previous FP. Also, if we have variable sized objects in the; // function, stack slot references will often be negative, and some of; // our instructions are positive-offset only, so conservatively consider; // that case to want a spill slot (or register) as well. Similarly, if; // the function adjusts the stack pointer during execution and the; // adjustments aren't already part of our stack size estimate, our offset; // calculations may be off, so be conservative.; // FIXME: We could add logic to be more precise about negative offsets; // and which instructions will need a scratch register for them. Is it; // worth the effort and added fragility?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:348,Modifiability,variab,variable,348,"// If any of the stack slot references may be out of range of an immediate; // offset, make sure a register (or a spill slot) is available for the; // register scavenger. Note that if we're indexing off the frame pointer, the; // effective stack size is 4 bytes larger since the FP points to the stack; // slot of the previous FP. Also, if we have variable sized objects in the; // function, stack slot references will often be negative, and some of; // our instructions are positive-offset only, so conservatively consider; // that case to want a spill slot (or register) as well. Similarly, if; // the function adjusts the stack pointer during execution and the; // adjustments aren't already part of our stack size estimate, our offset; // calculations may be off, so be conservative.; // FIXME: We could add logic to be more precise about negative offsets; // and which instructions will need a scratch register for them. Is it; // worth the effort and added fragility?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:812,Testability,log,logic,812,"// If any of the stack slot references may be out of range of an immediate; // offset, make sure a register (or a spill slot) is available for the; // register scavenger. Note that if we're indexing off the frame pointer, the; // effective stack size is 4 bytes larger since the FP points to the stack; // slot of the previous FP. Also, if we have variable sized objects in the; // function, stack slot references will often be negative, and some of; // our instructions are positive-offset only, so conservatively consider; // that case to want a spill slot (or register) as well. Similarly, if; // the function adjusts the stack pointer during execution and the; // adjustments aren't already part of our stack size estimate, our offset; // calculations may be off, so be conservative.; // FIXME: We could add logic to be more precise about negative offsets; // and which instructions will need a scratch register for them. Is it; // worth the effort and added fragility?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:41,Security,access,access,41,"// If FP is not used, SP will be used to access arguments, so count the; // size of arguments into the estimation.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:201,Security,access,accesses,201,"// For Thumb1, don't bother to iterate over the function. The only; // instruction that requires an emergency spill slot is a store to a; // frame index.; //; // tSTRspi, which is used for sp-relative accesses, has an 8-bit unsigned; // immediate. tSTRi, which is used for bp- and fp-relative accesses, has; // a 5-bit unsigned immediate.; //; // We could try to check if the function actually contains a tSTRspi; // that might need the spill slot, but it's not really important.; // Functions with VLAs or extremely large call frames are rare, and; // if a function is allocating more than 1KB of stack, an extra 4-byte; // slot probably isn't relevant.; //; // A special case is the scenario where r11 is used as FP, where accesses; // to a frame index will require its value to be moved into a low reg.; // This is handled later on, once we are able to determine if we have any; // fp-relative accesses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:293,Security,access,accesses,293,"// For Thumb1, don't bother to iterate over the function. The only; // instruction that requires an emergency spill slot is a store to a; // frame index.; //; // tSTRspi, which is used for sp-relative accesses, has an 8-bit unsigned; // immediate. tSTRi, which is used for bp- and fp-relative accesses, has; // a 5-bit unsigned immediate.; //; // We could try to check if the function actually contains a tSTRspi; // that might need the spill slot, but it's not really important.; // Functions with VLAs or extremely large call frames are rare, and; // if a function is allocating more than 1KB of stack, an extra 4-byte; // slot probably isn't relevant.; //; // A special case is the scenario where r11 is used as FP, where accesses; // to a frame index will require its value to be moved into a low reg.; // This is handled later on, once we are able to determine if we have any; // fp-relative accesses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:725,Security,access,accesses,725,"// For Thumb1, don't bother to iterate over the function. The only; // instruction that requires an emergency spill slot is a store to a; // frame index.; //; // tSTRspi, which is used for sp-relative accesses, has an 8-bit unsigned; // immediate. tSTRi, which is used for bp- and fp-relative accesses, has; // a 5-bit unsigned immediate.; //; // We could try to check if the function actually contains a tSTRspi; // that might need the spill slot, but it's not really important.; // Functions with VLAs or extremely large call frames are rare, and; // if a function is allocating more than 1KB of stack, an extra 4-byte; // slot probably isn't relevant.; //; // A special case is the scenario where r11 is used as FP, where accesses; // to a frame index will require its value to be moved into a low reg.; // This is handled later on, once we are able to determine if we have any; // fp-relative accesses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:897,Security,access,accesses,897,"// For Thumb1, don't bother to iterate over the function. The only; // instruction that requires an emergency spill slot is a store to a; // frame index.; //; // tSTRspi, which is used for sp-relative accesses, has an 8-bit unsigned; // immediate. tSTRi, which is used for bp- and fp-relative accesses, has; // a 5-bit unsigned immediate.; //; // We could try to check if the function actually contains a tSTRspi; // that might need the spill slot, but it's not really important.; // Functions with VLAs or extremely large call frames are rare, and; // if a function is allocating more than 1KB of stack, an extra 4-byte; // slot probably isn't relevant.; //; // A special case is the scenario where r11 is used as FP, where accesses; // to a frame index will require its value to be moved into a low reg.; // This is handled later on, once we are able to determine if we have any; // fp-relative accesses.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:47,Security,access,accesses,47,// Final estimate of whether sp or bp-relative accesses might require; // scavenging.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:81,Testability,log,logic,81,"// If the stack pointer moves and we don't have a base pointer, the; // estimate logic doesn't work. The actual offsets might be larger when; // we're constructing a call frame, or we might need to use negative; // offsets from fp.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:59,Security,access,accessed,59,"// If we have a frame pointer, we assume arguments will be accessed; // relative to the frame pointer. Check whether fp-relative accesses to; // arguments require scavenging.; //; // We could do slightly better on Thumb1; in some cases, an sp-relative; // offset would be legal even though an fp-relative offset is not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:129,Security,access,accesses,129,"// If we have a frame pointer, we assume arguments will be accessed; // relative to the frame pointer. Check whether fp-relative accesses to; // arguments require scavenging.; //; // We could do slightly better on Thumb1; in some cases, an sp-relative; // offset would be legal even though an fp-relative offset is not.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:432,Energy Efficiency,allocate,allocate,432,"// This is the number of extra spills inserted for callee-save GPRs which; // would not otherwise be used by the function. When greater than zero it; // guaranteees that it is possible to scavenge a register to hold the; // address of a stack slot. On Thumb1, the register must be a valid operand; // to tSTRi, i.e. r4-r7. For other subtargets, this is any GPR, i.e. r4-r11; // or lr.; //; // If we don't insert a spill, we instead allocate an emergency spill; // slot, which can be used by scavenging to spill an arbitrary register.; //; // We currently don't try to figure out whether any specific instruction; // requires scavening an additional register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:458,Availability,avail,available,458,"// For Thumb1-only targets, we need some low registers when we save and; // restore the high registers (which aren't allocatable, but could be; // used by inline assembly) because the push/pop instructions can not; // access high registers. If necessary, we might need to push more low; // registers to ensure that there is at least one free that can be used; // for the saving & restoring, and preferably we should ensure that as; // many as are needed are available so that fewer push/pop instructions; // are required.; // Low registers which are not currently pushed, but could be (r4-r7).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:218,Security,access,access,218,"// For Thumb1-only targets, we need some low registers when we save and; // restore the high registers (which aren't allocatable, but could be; // used by inline assembly) because the push/pop instructions can not; // access high registers. If necessary, we might need to push more low; // registers to ensure that there is at least one free that can be used; // for the saving & restoring, and preferably we should ensure that as; // many as are needed are available so that fewer push/pop instructions; // are required.; // Low registers which are not currently pushed, but could be (r4-r7).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:77,Availability,avail,available,77,"// If there are more high registers that need pushing than low registers; // available, push some more low registers so that we can use fewer push; // instructions. This might not reduce RegDeficit all the way to zero,; // because we can only guarantee that r4-r6 are available, but r8-r11 may; // need saving.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:268,Availability,avail,available,268,"// If there are more high registers that need pushing than low registers; // available, push some more low registers so that we can use fewer push; // instructions. This might not reduce RegDeficit all the way to zero,; // because we can only guarantee that r4-r6 are available, but r8-r11 may; // need saving.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:180,Energy Efficiency,reduce,reduce,180,"// If there are more high registers that need pushing than low registers; // available, push some more low registers so that we can use fewer push; // instructions. This might not reduce RegDeficit all the way to zero,; // because we can only guarantee that r4-r6 are available, but r8-r11 may; // need saving.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3,Safety,Avoid,Avoid,3,// Avoid spilling LR in Thumb1 if there's a tail call: it's expensive to; // restore LR in that case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:321,Security,access,accesses,321,"// Estimate if we might need to scavenge registers at some point in order; // to materialize a stack offset. If so, either spill one additional; // callee-saved register or reserve a special spill slot to facilitate; // register scavenging. Thumb1 needs a spill slot for stack pointer; // adjustments and for frame index accesses when FP is high register,; // even when the frame itself is small.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:134,Usability,Clear,Clear,134,// Check if all terminators do not implicitly use LR. Then we can 'restore' LR; // into PC so it is not live out of the return block: Clear the Restored bit; // in that case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:826,Energy Efficiency,allocate,allocate,826,"// Adjust the function prologue to enable split stacks. This currently only; // supports android and linux.; //; // The ABI of the segmented stack prologue is a little arbitrarily chosen, but; // must be well defined in order to allow for consistent implementations of the; // __morestack helper function. The ABI is also not a normal ABI in that it; // doesn't follow the normal calling conventions because this allows the; // prologue of each function to be optimized further.; //; // Currently, the ABI looks like (when calling __morestack); //; // * r4 holds the minimum stack size requested for this function call; // * r5 holds the stack size of the arguments to the function; // * the beginning of the function is 3 instructions after the call to; // __morestack; //; // Implementations of __morestack should use r4 to allocate a new stack, r5 to; // place the arguments on to the new stack, and the 3-instruction knowledge to; // jump directly to the body of the function when working on the new stack.; //; // An old (and possibly no longer compatible) implementation of __morestack for; // ARM can be found at [1].; //; // [1] - https://github.com/mozilla/rust/blob/86efd9/src/rt/arch/arm/morestack.S",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:460,Performance,optimiz,optimized,460,"// Adjust the function prologue to enable split stacks. This currently only; // supports android and linux.; //; // The ABI of the segmented stack prologue is a little arbitrarily chosen, but; // must be well defined in order to allow for consistent implementations of the; // __morestack helper function. The ABI is also not a normal ABI in that it; // doesn't follow the normal calling conventions because this allows the; // prologue of each function to be optimized further.; //; // Currently, the ABI looks like (when calling __morestack); //; // * r4 holds the minimum stack size requested for this function call; // * r5 holds the stack size of the arguments to the function; // * the beginning of the function is 3 instructions after the call to; // __morestack; //; // Implementations of __morestack should use r4 to allocate a new stack, r5 to; // place the arguments on to the new stack, and the 3-instruction knowledge to; // jump directly to the body of the function when working on the new stack.; //; // An old (and possibly no longer compatible) implementation of __morestack for; // ARM can be found at [1].; //; // [1] - https://github.com/mozilla/rust/blob/86efd9/src/rt/arch/arm/morestack.S",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:47,Deployability,update,update,47,// Grab everything that reaches PrologueMBB to update there liveness as well.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:101,Deployability,update,updates,101,"// Remove the newly added blocks from the list, since we know; // we do not have to do the following updates for them.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:96,Deployability,update,update,96,"// Replace the edges to PrologueMBB by edges to the sequences; // we are about to add, but only update for immediate predecessors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3,Deployability,Update,Update,3,// Update the CFA offset now that we've popped,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:3,Deployability,Update,Update,3,// Update the CFA offset now that we've popped,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h:4,Deployability,Update,Update,4,"/// Update the IsRestored flag on LR if it is spilled, based on the return; /// instructions.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h:60,Integrability,wrap,wrapping,60,/// Returns true if the target will correctly handle shrink wrapping.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:45,Safety,hazard,hazard,45,"//===-- ARMHazardRecognizer.cpp - ARM postra hazard recognizer ------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:10,Safety,Detect,Detect,10,// FIXME: Detect integer instructions properly.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:32,Safety,hazard,hazards,32,// Look for special VMLA / VMLS hazards. A VMUL / VADD / VSUB following; // a VMLA / VMLS will cause 4 cycle stall.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:10,Energy Efficiency,schedul,schedule,10,// Try to schedule another instruction for the next 4 cycles.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:40,Energy Efficiency,schedul,schedule,40,// Stalled for 4 cycles but still can't schedule any other instructions.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:36,Safety,hazard,hazards,36,///////// Bank conflicts handled as hazards //////////////,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp:36,Security,access,access,36,"// Is this a stack pointer-relative access? We could in general try to; // use ""is this the same register and is it unchanged?"", but the; // memory operand tracking is highly likely to have already found that.; // What we're after here is bank conflicts between different objects in; // the stack frame.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h:424,Energy Efficiency,schedul,scheduling,424,"//===-- ARMHazardRecognizer.h - ARM Hazard Recognizers ----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling ARM functions.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h:36,Safety,Hazard,Hazard,36,"//===-- ARMHazardRecognizer.h - ARM Hazard Recognizers ----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling ARM functions.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h:401,Safety,hazard,hazard,401,"//===-- ARMHazardRecognizer.h - ARM Hazard Recognizers ----------*- C++ -*-===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines hazard recognizers for scheduling ARM functions.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h:3,Safety,Hazard,Hazards,3,// Hazards related to FP MLx instructions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h:3,Safety,Hazard,Hazards,3,// Hazards related to bank conflicts,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMHazardRecognizer.h
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:25,Usability,simpl,simple,25,// Select the opcode for simple extensions (that translate to a single SXT/UXT; // instruction). Extension operations more complicated than that should not; // invoke this. Returns the original opcode if it doesn't know how to select a; // better one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:75,Usability,UX,UXT,75,// Select the opcode for simple extensions (that translate to a single SXT/UXT; // instruction). Extension operations more complicated than that should not; // invoke this. Returns the original opcode if it doesn't know how to select a; // better one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:32,Performance,load,loads,32,// Select the opcode for simple loads and stores. Returns the original opcode; // if it doesn't know how to select a better one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:25,Usability,simpl,simple,25,// Select the opcode for simple loads and stores. Returns the original opcode; // if it doesn't know how to select a better one.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:51,Performance,perform,perform,51,"// When lowering comparisons, we sometimes need to perform two compares instead; // of just one. Get the condition codes for both comparisons. If only one is; // needed, the second member of the pair is ARMCC::AL.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:23,Performance,perform,performing,23,// The opcode used for performing the comparison.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:3,Usability,Simpl,Simple,3,"// Simple case, we only need one comparison and we're done.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:10,Usability,simpl,simple,10,"// Not so simple, we need two successive comparisons.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:3,Performance,Perform,Perform,3,// Perform the comparison.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:152,Performance,load,load,152,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:297,Performance,load,load,297,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:65,Security,access,accesses,65,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:91,Security,access,accesses,91,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:127,Security,access,accesses,127,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:250,Security,access,accesses,250,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:3,Performance,Load,Load,3,// Load the offset from the constant pool.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:3,Performance,Load,Load,3,// Load the global's address from the constant pool.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:14,Availability,down,down,14,// ZExt boils down to & 0x1; for SExt we also subtract that from 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:3,Performance,Load,Load,3,// Load from constant pool,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:31,Performance,load,load,31,/// Indexed (pre/post inc/dec) load matching code for ARM.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:28,Performance,load,load,28,"/// SelectVLD - Select NEON load intrinsics. NumVecs should be; /// 1, 2, 3 or 4. The opcode arrays specify the instructions used for; /// loads of D registers and even subregs and odd subregs of Q registers.; /// For NumVecs <= 2, QOpcodes1 is not used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:139,Performance,load,loads,139,"/// SelectVLD - Select NEON load intrinsics. NumVecs should be; /// 1, 2, 3 or 4. The opcode arrays specify the instructions used for; /// loads of D registers and even subregs and odd subregs of Q registers.; /// For NumVecs <= 2, QOpcodes1 is not used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:34,Performance,load,load,34,"/// SelectVLDSTLane - Select NEON load/store lane intrinsics. NumVecs should; /// be 2, 3 or 4. The opcode arrays specify the instructions used for; /// load/store of D registers and Q registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:153,Performance,load,load,153,"/// SelectVLDSTLane - Select NEON load/store lane intrinsics. NumVecs should; /// be 2, 3 or 4. The opcode arrays specify the instructions used for; /// load/store of D registers and Q registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:40,Performance,load,load,40,/// SelectMVE_WB - Select MVE writeback load/store intrinsics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:44,Performance,load,load,44,"/// SelectMVE_VLD - Select MVE interleaving load intrinsics. NumVecs; /// should be 2 or 4. The opcode array specifies the instructions; /// used for 8, 16 and 32-bit lane sizes respectively, and each; /// pointer points to a set of NumVecs sub-opcodes used for the; /// different stages (e.g. VLD20 versus VLD21) of each load family.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:322,Performance,load,load,322,"/// SelectMVE_VLD - Select MVE interleaving load intrinsics. NumVecs; /// should be 2 or 4. The opcode array specifies the instructions; /// used for 8, 16 and 32-bit lane sizes respectively, and each; /// pointer points to a set of NumVecs sub-opcodes used for the; /// different stages (e.g. VLD20 versus VLD21) of each load family.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:31,Performance,load,load-duplicate,31,"/// SelectVLDDup - Select NEON load-duplicate intrinsics. NumVecs; /// should be 1, 2, 3 or 4. The opcode array specifies the instructions used; /// for loading D registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:153,Performance,load,loading,153,"/// SelectVLDDup - Select NEON load-duplicate intrinsics. NumVecs; /// should be 1, 2, 3 or 4. The opcode array specifies the instructions used; /// for loading D registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:82,Energy Efficiency,power,power,82,"/// Checks if N is a multiplication by a constant where we can extract out a; /// power of two from the constant so that it can be used in a shift, but only; /// if it simplifies the materialization of the constant. Returns true if it; /// is, and assigns to PowerOfTwo the power of two that should be extracted; /// out and to NewMulConst the new constant to be multiplied by.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:259,Energy Efficiency,Power,PowerOfTwo,259,"/// Checks if N is a multiplication by a constant where we can extract out a; /// power of two from the constant so that it can be used in a shift, but only; /// if it simplifies the materialization of the constant. Returns true if it; /// is, and assigns to PowerOfTwo the power of two that should be extracted; /// out and to NewMulConst the new constant to be multiplied by.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:274,Energy Efficiency,power,power,274,"/// Checks if N is a multiplication by a constant where we can extract out a; /// power of two from the constant so that it can be used in a shift, but only; /// if it simplifies the materialization of the constant. Returns true if it; /// is, and assigns to PowerOfTwo the power of two that should be extracted; /// out and to NewMulConst the new constant to be multiplied by.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:168,Usability,simpl,simplifies,168,"/// Checks if N is a multiplication by a constant where we can extract out a; /// power of two from the constant so that it can be used in a shift, but only; /// if it simplifies the materialization of the constant. Returns true if it; /// is, and assigns to PowerOfTwo the power of two that should be extracted; /// out and to NewMulConst the new constant to be multiplied by.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:35,Testability,test,tests,35,/// isInt32Immediate - This method tests to see if the node is a 32-bit constant; /// operand. If so Imm will receive the 32-bit value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:34,Testability,test,tests,34,// isInt32Immediate - This method tests to see if a constant operand.; // If so Imm will receive the 32 bit value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:39,Testability,test,tests,39,// isOpcWithIntImmediate - This method tests to see if the node is a specific; // opcode and that it has a immediate integer right operand.; // If so Imm will receive the 32 bit value.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:34,Safety,avoid,avoid,34,// We use make_early_inc_range to avoid invalidation issues.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:20,Availability,mask,mask,20,// Check if the AND mask is an immediate of the form: 000.....1111111100,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:147,Safety,hazard,hazards,147,/// hasNoVMLxHazardUse - Return true if it's desirable to select a FP MLA / MLS; /// node. VFP / NEON fp VMLA / VMLS instructions have special RAW hazards (at; /// least on current ARM implementations) which should be avoidded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:218,Safety,avoid,avoidded,218,/// hasNoVMLxHazardUse - Return true if it's desirable to select a FP MLA / MLS; /// node. VFP / NEON fp VMLA / VMLS instructions have special RAW hazards (at; /// least on current ARM implementations) which should be avoidded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:20,Energy Efficiency,power,power,20,// Find the largest power of 2 that MulConstVal is a multiple of,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:9,Usability,simpl,simple,9,// Match simple R + imm12 operands.; // Base only.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:9,Usability,simpl,simple,9,// Leave simple R +/- imm12 operands for LDRi12,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:70,Energy Efficiency,allocate,allocate,70,"// Make sure the offset is inside the object, or we might fail to; // allocate an emergency spill slot. (An out-of-range access is UB, but; // it could show up anyway.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:121,Security,access,access,121,"// Make sure the offset is inside the object, or we might fail to; // allocate an emergency spill slot. (An out-of-range access is UB, but; // it could show up anyway.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:9,Usability,simpl,simple,9,// Match simple R + imm12 operands.; // Base only.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:9,Usability,simpl,simple,9,// Match simple R - imm8 operands.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:21,Performance,load,load,21,"// A T1 post-indexed load is just a single register LDM: LDM r0!, {r1}.; // The encoding of LDM is not how the rest of ISel expects a post-inc load to; // look however, so we use a pseudo here and switch it for a tLDMIA_UPD after; // ISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:143,Performance,load,load,143,"// A T1 post-indexed load is just a single register LDM: LDM r0!, {r1}.; // The encoding of LDM is not how the rest of ISel expects a post-inc load to; // look however, so we use a pseudo here and switch it for a tLDMIA_UPD after; // ISel.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:19,Availability,mask,masked,19,// We allow LE non-masked loads to change the type (for example use a vldrb.8; // as opposed to a vldrw.32). This can allow extra addressing modes or; // alignments for what is otherwise an equivalent instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:26,Performance,load,loads,26,// We allow LE non-masked loads to change the type (for example use a vldrb.8; // as opposed to a vldrw.32). This can allow extra addressing modes or; // alignments for what is otherwise an equivalent instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:135,Integrability,depend,depend,135,/// GetVLDSTAlign - Get the alignment (in bytes) for the alignment operand; /// of a NEON VLD or VST instruction. The supported values depend on the; /// number of registers being loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:180,Performance,load,loaded,180,/// GetVLDSTAlign - Get the alignment (in bytes) for the alignment operand; /// of a NEON VLD or VST instruction. The supported values depend on the; /// number of registers being loaded.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:27,Deployability,update,update,27,// Get the register stride update opcode of a VLD/VST instruction that; // is otherwise equivalent to the given fixed stride updating instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:96,Performance,perform,performed,96,"/// Returns true if the given increment is a Constant known to be equal to the; /// access size performed by a NEON load/store. This means the ""[rN]!"" form can; /// be used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:116,Performance,load,load,116,"/// Returns true if the given increment is a Constant known to be equal to the; /// access size performed by a NEON load/store. This means the ""[rN]!"" form can; /// be used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:84,Security,access,access,84,"/// Returns true if the given increment is a Constant known to be equal to the; /// access size performed by a NEON load/store. This means the ""[rN]!"" form can; /// be used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:33,Performance,load,loaded,33,"// Otherwise, quad registers are loaded with two separate instructions,; // where one loads the even registers and the other loads the odd registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:86,Performance,load,loads,86,"// Otherwise, quad registers are loaded with two separate instructions,; // where one loads the even registers and the other loads the odd registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:125,Performance,load,loads,125,"// Otherwise, quad registers are loaded with two separate instructions,; // where one loads the even registers and the other loads the odd registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:3,Performance,Load,Load,3,"// Load the even subregs. This is always an updating load, so that it; // provides the address to the second load for the odd subregs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:53,Performance,load,load,53,"// Load the even subregs. This is always an updating load, so that it; // provides the address to the second load for the odd subregs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:109,Performance,load,load,109,"// Load the even subregs. This is always an updating load, so that it; // provides the address to the second load for the odd subregs.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:3,Performance,Load,Load,3,// Load the odd subregs.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:23,Energy Efficiency,power,power,23,// Alignment must be a power of two; make sure of that.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:23,Energy Efficiency,power,power,23,// Alignment must be a power of two; make sure of that.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:48,Energy Efficiency,efficient,efficiently,48,// We are trying to use VMOV/VMOVX/VINS to more efficiently lower insert and; // extracts of v8f16 and v8i16 vectors. Check that we have two adjacent; // inserts of the correct type:,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:77,Usability,simpl,simplifies,77,"// If the two extracted lanes are from the same place and adjacent, this; // simplifies into a f32 lane move.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:54,Availability,mask,mask,54,"// For unsigned extracts, check for a shift right and mask",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:22,Availability,mask,mask,22,// The immediate is a mask of the low bits iff imm & (imm+1) == 0,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:3,Availability,Mask,Mask,3,"// Mask off the unnecessary bits of the AND immediate; normally; // DAGCombine will do this, but that might not happen if; // targetShrinkDemandedConstant chooses a different immediate.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:51,Availability,mask,mask,51,"// Or we are looking for a shift of an and, with a mask operand",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:375,Safety,detect,detects,375,"/// Target-specific DAG combining for ISD::SUB.; /// Target-independent combining lowers SELECT_CC nodes of the form; /// select_cc setg[ge] X, 0, X, -X; /// select_cc setgt X, -1, X, -X; /// select_cc setl[te] X, 0, -X, X; /// select_cc setlt X, 1, -X, X; /// which represent Integer ABS into:; /// Y = sra (X, size(X)-1); sub (xor (X, Y), Y); /// ARM instruction selection detects the latter and matches it to; /// ARM::ABS or ARM::t2ABS machine node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:6,Availability,Mask,Mask,6,// 1. Mask includes the LSB -> Simply shift the top N bits off,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:31,Usability,Simpl,Simply,31,// 1. Mask includes the LSB -> Simply shift the top N bits off,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:6,Availability,Mask,Mask,6,// 2. Mask includes the MSB -> Simply shift the bottom N bits off,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:31,Usability,Simpl,Simply,31,// 2. Mask includes the MSB -> Simply shift the bottom N bits off,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:27,Usability,clear,clear,27,"// 4. Do a double shift to clear bottom and top bits, but only in; // thumb-1 mode as in thumb-2 we can use UBFX.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:50,Safety,avoid,avoid,50,"// Set the alignment of the frame object to 4, to avoid having to generate; // more than one ADD",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:70,Energy Efficiency,allocate,allocated,70,// The register-offset variant of LDRD mandates that the register; // allocated to RegOffset is not reused in any of the remaining operands.; // This restriction is currently not enforced. Therefore emitting this; // variant is explicitly avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:239,Safety,avoid,avoided,239,// The register-offset variant of LDRD mandates that the register; // allocated to RegOffset is not reused in any of the remaining operands.; // This restriction is currently not enforced. Therefore emitting this; // variant is explicitly avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:70,Energy Efficiency,allocate,allocated,70,// The register-offset variant of STRD mandates that the register; // allocated to RegOffset is not reused in any of the remaining operands.; // This restriction is currently not enforced. Therefore emitting this; // variant is explicitly avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:239,Safety,avoid,avoided,239,// The register-offset variant of STRD mandates that the register; // allocated to RegOffset is not reused in any of the remaining operands.; // This restriction is currently not enforced. Therefore emitting this; // variant is explicitly avoided.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:73,Safety,avoid,avoid,73,"// select (CMPZ X, #-C) -> (CMPZ (ADDS X, #C), #0); // This allows us to avoid materializing the expensive negative constant.; // The CMPZ #0 is useless and will be peepholed away but we need to keep it; // for its glue output.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:40,Availability,mask,mask,40,"// Maps a Banked Register string to its mask value. The mask value returned is; // for use in the MRSbanked / MSRbanked instruction nodes as the Banked Register; // mask operand, which expresses which register is to be used, e.g. r8, and in; // which mode it is to be used, e.g. usr. Returns -1 to signify that the string; // was invalid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:56,Availability,mask,mask,56,"// Maps a Banked Register string to its mask value. The mask value returned is; // for use in the MRSbanked / MSRbanked instruction nodes as the Banked Register; // mask operand, which expresses which register is to be used, e.g. r8, and in; // which mode it is to be used, e.g. usr. Returns -1 to signify that the string; // was invalid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:165,Availability,mask,mask,165,"// Maps a Banked Register string to its mask value. The mask value returned is; // for use in the MRSbanked / MSRbanked instruction nodes as the Banked Register; // mask operand, which expresses which register is to be used, e.g. r8, and in; // which mode it is to be used, e.g. usr. Returns -1 to signify that the string; // was invalid.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:7,Availability,mask,mask,7,"// The mask operand contains the special register (R Bit) in bit 4, whether; // the register is spsr (R bit is 1) or one of cpsr/apsr (R bit is 0), and; // bits 3-0 contains the fields to be accessed in the special register, set by; // the flags provided with the register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:191,Security,access,accessed,191,"// The mask operand contains the special register (R Bit) in bit 4, whether; // the register is spsr (R bit is 1) or one of cpsr/apsr (R bit is 0), and; // bits 3-0 contains the fields to be accessed in the special register, set by; // the flags provided with the register.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:188,Availability,mask,mask,188,// The flags permitted for apsr are the same flags that are allowed in; // M class registers. We get the flag value and then shift the flags into; // the correct place to combine with the mask.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:61,Availability,mask,mask,61,// Inspect the supplied flags string and set the bits in the mask for; // the relevant and valid flags allowed for cpsr and spsr.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:8,Safety,avoid,avoids,8,// This avoids allowing strings where the same flag bit appears twice.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:162,Availability,mask,masks,162,// Lower the read_register intrinsic to ARM specific DAG nodes; // using the supplied metadata string to select the instruction node to use; // and the registers/masks to construct as operands for the node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:119,Availability,mask,mask,119,"// If the target is M Class then need to validate that the register string; // is an acceptable value, so check that a mask can be constructed from the; // string.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:41,Security,validat,validate,41,"// If the target is M Class then need to validate that the register string; // is an acceptable value, so check that a mask can be constructed from the; // string.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:163,Availability,mask,masks,163,// Lower the write_register intrinsic to ARM specific DAG nodes; // using the supplied metadata string to select the instruction node to use; // and the registers/masks to use in the nodes,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:99,Availability,mask,mask,99,// If the target was M Class then need to validate the special register value; // and retrieve the mask for use in the instruction node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:42,Security,validat,validate,42,// If the target was M Class then need to validate the special register value; // and retrieve the mask for use in the instruction node.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:35,Availability,mask,mask,35,"// We then check to see if a valid mask can be constructed for one of the; // register string values permitted for the A and R class cores. These values; // are apsr, spsr and cpsr; these are also valid on older cores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:326,Deployability,update,update,326,"// Memory operands to inline asm in the SelectionDAG are modeled with two; // operands: a constant of value InlineAsm::Kind::Mem followed by the input; // operand. If we get here and we have a Kind::Mem, skip the next operand; // (so it doesn't get misinterpreted), and continue. We do this here because; // it's important to update the OpChanged array correctly before moving on.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:3,Deployability,Update,Update,3,// Update the original glue user.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:52,Safety,safe,safe,52,// Require the address to be in a register. That is safe for all ARM; // variants and it is hard to do anything much smarter without knowing; // how the operand is used.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp:110,Energy Efficiency,schedul,scheduling,110,"/// createARMISelDag - This pass converts a legalized DAG into a; /// ARM-specific DAG, ready for instruction scheduling.; ///",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelDAGToDAG.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:405,Integrability,interface,interfaces,405,"//===- ARMISelLowering.cpp - ARM DAG Lowering Implementation --------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file defines the interfaces that ARM uses to lower LLVM code into a; // selection DAG.; //; //===----------------------------------------------------------------------===//",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:112,Availability,down,down,112,// We support these really simple operations even on types where all; // the actual arithmetic has to be broken down into simpler; // operations or turned into library calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:27,Usability,simpl,simple,27,// We support these really simple operations even on types where all; // the actual arithmetic has to be broken down into simpler; // operations or turned into library calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:122,Usability,simpl,simpler,122,// We support these really simple operations even on types where all; // the actual arithmetic has to be broken down into simpler; // operations or turned into library calls.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:37,Performance,load,loads,37,// Pre and Post inc are supported on loads and stores,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:37,Performance,load,loads,37,// Pre and Post inc are supported on loads and stores,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:42,Performance,load,load,42,"// We 'support' these types up to bitcast/load/store level, regardless of; // MVE integer-only / float support. Only doing FP data processing on the FP; // vector types is inhibited at integer-only level.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:23,Modifiability,extend,extend,23,// It is legal to sign extend from v4i8/v4i16 to v4i32 or v8i8 to v8i16.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:58,Modifiability,extend,extends,58,"// Pre and Post inc on these are legal, given the correct extends",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:34,Availability,avail,available,34,// Uses VFP for Thumb libfuncs if available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:83,Integrability,rout,routines,83,"// Floating-point to integer conversions.; // i64 conversions are done via library routines even when generating VFP; // instructions, so use the same ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:83,Integrability,rout,routines,83,"// Integer to floating-point conversions.; // i64 conversions are done via library routines even when generating VFP; // instructions, so use the same ones.; // FIXME: There appears to be some naming inconsistency in ARM libgcc:; // e.g., __floatunsidf vs. __floatunssidfvfp.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:26,Availability,avail,available,26,// These libcalls are not available in 32-bit.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:8,Integrability,depend,dependent,8,// EABI dependent RTLIB,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:49,Safety,detect,detect,49,// Custom handling for some quad-vector types to detect VMULL.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:44,Safety,avoid,avoid,44,// Custom handling for some vector types to avoid expensive expansions,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:222,Performance,load,loads,222,"// When targeting a floating-point unit with only single-precision; // operations, f64 is legal for the few double-precision instructions which; // are present However, no double-precision operations other than moves,; // loads and stores are provided by the hardware.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:36,Modifiability,extend,extending,36,// ARM does not have floating-point extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:46,Performance,load,loads,46,// ARM does not have floating-point extending loads.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:29,Modifiability,extend,extending,29,// ARM does not have i1 sign extending load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:39,Performance,load,load,39,// ARM does not have i1 sign extending load.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:49,Performance,load,load,49,// ARM supports all 4 flavors of integer indexed load / store.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:32,Performance,load,load,32,"// Thumb-1 has limited post-inc load/store support - LDM r0!, {r1}.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:51,Energy Efficiency,Monitor,Monitors,51,// @llvm.readcyclecounter requires the Performance Monitors extension.; // Default to the 0 expansion on unsupported platforms.; // FIXME: Technically there are older ARM CPUs that have; // implementation-specific ways of obtaining this information.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:39,Performance,Perform,Performance,39,// @llvm.readcyclecounter requires the Performance Monitors extension.; // Default to the 0 expansion on unsupported platforms.; // FIXME: Technically there are older ARM CPUs that have; // implementation-specific ways of obtaining this information.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:31,Energy Efficiency,efficient,efficient,31,"// On v8, we have particularly efficient implementations of atomic fences; // if they can be combined with nearby atomic loads and stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:121,Performance,load,loads,121,"// On v8, we have particularly efficient implementations of atomic fences; // if they can be combined with nearby atomic loads and stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:43,Availability,reliab,reliably,43,"// For targets where __sync_* routines are reliably available, we use them; // if necessary.; //; // ARM Linux always supports 64-bit atomics through kernel-assisted atomic; // routines (kernel 3.1 or later). FIXME: Not with compiler-rt?; //; // ARMv6 targets have native instructions in ARM mode. For Thumb mode,; // such targets should provide __sync_* routines, which use the ARM mode; // instructions. (ARMv6 doesn't have dmb, but it has an equivalent; // encoding; see ARMISD::MEMBARRIER_MCR.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:52,Availability,avail,available,52,"// For targets where __sync_* routines are reliably available, we use them; // if necessary.; //; // ARM Linux always supports 64-bit atomics through kernel-assisted atomic; // routines (kernel 3.1 or later). FIXME: Not with compiler-rt?; //; // ARMv6 targets have native instructions in ARM mode. For Thumb mode,; // such targets should provide __sync_* routines, which use the ARM mode; // instructions. (ARMv6 doesn't have dmb, but it has an equivalent; // encoding; see ARMISD::MEMBARRIER_MCR.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:30,Integrability,rout,routines,30,"// For targets where __sync_* routines are reliably available, we use them; // if necessary.; //; // ARM Linux always supports 64-bit atomics through kernel-assisted atomic; // routines (kernel 3.1 or later). FIXME: Not with compiler-rt?; //; // ARMv6 targets have native instructions in ARM mode. For Thumb mode,; // such targets should provide __sync_* routines, which use the ARM mode; // instructions. (ARMv6 doesn't have dmb, but it has an equivalent; // encoding; see ARMISD::MEMBARRIER_MCR.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:177,Integrability,rout,routines,177,"// For targets where __sync_* routines are reliably available, we use them; // if necessary.; //; // ARM Linux always supports 64-bit atomics through kernel-assisted atomic; // routines (kernel 3.1 or later). FIXME: Not with compiler-rt?; //; // ARMv6 targets have native instructions in ARM mode. For Thumb mode,; // such targets should provide __sync_* routines, which use the ARM mode; // instructions. (ARMv6 doesn't have dmb, but it has an equivalent; // encoding; see ARMISD::MEMBARRIER_MCR.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:355,Integrability,rout,routines,355,"// For targets where __sync_* routines are reliably available, we use them; // if necessary.; //; // ARM Linux always supports 64-bit atomics through kernel-assisted atomic; // routines (kernel 3.1 or later). FIXME: Not with compiler-rt?; //; // ARMv6 targets have native instructions in ARM mode. For Thumb mode,; // such targets should provide __sync_* routines, which use the ARM mode; // instructions. (ARMv6 doesn't have dmb, but it has an equivalent; // encoding; see ARMISD::MEMBARRIER_MCR.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:72,Integrability,rout,routines,72,// We can't assume anything about other targets; just use libatomic; // routines.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:23,Availability,avail,available,23,"// Requires SXTB/SXTH, available on v6 and up in both ARM and Thumb modes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:25,Availability,avail,available,25,// Use __sincos_stret if available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:24,Availability,avail,available,24,"// vmin and vmax aren't available in a scalar form, so we can use; // a NEON instruction with an undef lane instead. This has a performance; // penalty on some cores, so we don't do this unless we have been; // asked to by the core tuning model.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:128,Performance,perform,performance,128,"// vmin and vmax aren't available in a scalar form, so we can use; // a NEON instruction with an undef lane instead. This has a performance; // penalty on some cores, so we don't do this unless we have been; // asked to by the core tuning model.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:25,Integrability,interface,interface,25,//// temporary - rewrite interface to use type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:17,Modifiability,rewrite,rewrite,17,//// temporary - rewrite interface to use type,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:45,Modifiability,extend,extended,45,"// On ARM arguments smaller than 4 bytes are extended, so all arguments; // are at least 4 bytes aligned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:17,Safety,predict,predicted,17,// Prefer likely predicted branches to selects on out-of-order cores.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:63,Availability,avail,available,63,"// When NEON is used for SP, only half of the register file is available; // because operations that define both SP and DP results will be constrained; // to the VFP2 class (D0-D15). We currently model this constraint prior to; // coalescing by double-counting the SP regs. See the FIXME above.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:158,Performance,load,load,158,"// Map v4i64 to QQ registers but do not make the type legal. Similarly map; // v8i64 to QQQQ registers. v4i64 and v8i64 are only used for REG_SEQUENCE to; // load / store 4 to 8 consecutive NEON D registers, or 2 to 4 consecutive; // MVE Q registers.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:81,Availability,avail,available,81,// Load are scheduled for latency even if there instruction itinerary; // is not available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:12,Energy Efficiency,schedul,scheduled,12,// Load are scheduled for latency even if there instruction itinerary; // is not available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load are scheduled for latency even if there instruction itinerary; // is not available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:26,Performance,latency,latency,26,// Load are scheduled for latency even if there instruction itinerary; // is not available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:136,Modifiability,extend,extending,136,"// Check for a signed 16-bit value. We special case SRA because it makes it; // more simple when also looking for SRAs that aren't sign extending a; // smaller value. Without the check, we'd need to take extra care with; // checking order for some operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:85,Usability,simpl,simple,85,"// Check for a signed 16-bit value. We special case SRA because it makes it; // more simple when also looking for SRAs that aren't sign extending a; // smaller value. Without the check, we'd need to take extra care with; // checking order for some operations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:68,Safety,avoid,avoid,68,"// Pass 'this' value directly from the argument to return value, to avoid; // reg unit interference",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,Modifiability,extend,extended,33,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:155,Modifiability,extend,extended,155,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:35,Security,secur,secure,35,// Determine whether this is a non-secure function call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:20,Security,secur,secure,20,"// For both the non-secure calls and the returns from a CMSE entry function,; // the function needs to do some extra work afte r the call, or before the; // return, respectively, thus it cannot end with atail call",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:12,Performance,optimiz,optimizing,12,"// If we're optimizing for minimum size and the function is called three or; // more times in this block, we can improve codesize by calling indirectly; // as BLXr has a 16-bit encoding.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:74,Safety,detect,detected,74,"// We don't support GuaranteedTailCallOpt for ARM, only automatically; // detected sibcalls.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:49,Availability,avail,available,49,"// If this call requires more stack than we have available from; // LowerFormalArguments, tell FrameLowering to reserve space for it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:47,Availability,avail,available,47,"// For sibling tail calls, memory operands are available in our caller's stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:124,Performance,load,loaded,124,"// During a tail call, stores to the argument area must happen after all of; // the function's incoming arguments have been loaded because they may alias.; // This is done by folding in a TokenFactor from LowerFormalArguments, but; // there's no point in doing so repeatedly so this tracks whether that's; // happened yet.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:58,Performance,load,loads,58,"// Walk the register/memloc assignments, inserting copies/loads. In the case; // of tail call optimization, arguments are handled later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:94,Performance,optimiz,optimization,94,"// Walk the register/memloc assignments, inserting copies/loads. In the case; // of tail call optimization, arguments are handled later.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,Modifiability,extend,extended,33,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:155,Modifiability,extend,extended,155,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:74,Availability,Mask,Mask,74,// f16 arguments could have been extended prior to argument lowering.; // Mask them arguments if this is a CMSE nonsecure call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,Modifiability,extend,extended,33,// f16 arguments could have been extended prior to argument lowering.; // Mask them arguments if this is a CMSE nonsecure call.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:27,Safety,avoid,avoid,27,"// ""mov lr, pc; b _foo"" to avoid confusing the RSP",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:18,Availability,mask,mask,18,// Add a register mask operand representing the call-preserved registers.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:45,Availability,mask,mask,45,"// For 'this' returns, use the R0-preserving mask if applicable",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:165,Usability,undo,undo,165,"// If we're guaranteeing tail-calls will be honoured, the callee must; // pop its own argument stack on return. But this call is *not* a tail call so; // we need to undo that after it returns to restore the status-quo.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:129,Energy Efficiency,allocate,allocate,129,"/// HandleByVal - Every parameter *after* a byval parameter is passed; /// on the stack. Remember the next parameter register to allocate,; /// and then confiscate the rest of the parameter registers to insure; /// this.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:76,Energy Efficiency,allocate,allocated,76,"// First register for byval parameter is the first register that wasn't; // allocated before this method call, so it would be ""reg"".; // If parameter is small enough to be saved in range [reg, r4), then; // the end (first after last) register would be reg + param-size-in-regs,; // else parameter would be splitted between registers and stack,; // end register would be r4 in this case.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:27,Energy Efficiency,allocate,allocated,27,"// Note, first register is allocated in the beginning of function already,; // allocate remained amount of registers we need.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:79,Energy Efficiency,allocate,allocate,79,"// Note, first register is allocated in the beginning of function already,; // allocate remained amount of registers we need.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:87,Availability,avail,available,87,/// MatchingStackOffset - Return true if the given stack call argument is; /// already available in the same position (relatively) of the caller's; /// incoming argument stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:94,Performance,optimiz,optimization,94,/// IsEligibleForTailCallOptimization - Check whether the call is eligible; /// for tail call optimization. Targets which want to do tail call; /// optimization should implement this function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:148,Performance,optimiz,optimization,148,/// IsEligibleForTailCallOptimization - Check whether the call is eligible; /// for tail call optimization. Targets which want to do tail call; /// optimization should implement this function.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:325,Availability,avail,available,325,"// Indirect tail calls cannot be optimized for Thumb1 if the args; // to the call take up r0-r3. The reason is that there are no legal registers; // left to hold the pointer to the function to be called.; // Similarly, if the function uses return address sign and authentication,; // r12 is needed to hold the PAC and is not available to hold the callee; // address.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,Performance,optimiz,optimized,33,"// Indirect tail calls cannot be optimized for Thumb1 if the args; // to the call take up r0-r3. The reason is that there are no legal registers; // left to hold the pointer to the function to be called.; // Similarly, if the function uses return address sign and authentication,; // r12 is needed to hold the PAC and is not available to hold the callee; // address.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:264,Security,authenticat,authentication,264,"// Indirect tail calls cannot be optimized for Thumb1 if the args; // to the call take up r0-r3. The reason is that there are no legal registers; // left to hold the pointer to the function to be called.; // Similarly, if the function uses return address sign and authentication,; // r12 is needed to hold the PAC and is not available to hold the callee; // address.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:34,Performance,perform,perform,34,// Look for obvious safe cases to perform tail call optimization that do not; // require ABI changes. This is what gcc calls sibcall.; // Exception-handling functions need a special set of instructions to indicate; // a return to the hardware. Tail-calling another function would probably; // break this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:52,Performance,optimiz,optimization,52,// Look for obvious safe cases to perform tail call optimization that do not; // require ABI changes. This is what gcc calls sibcall.; // Exception-handling functions need a special set of instructions to indicate; // a return to the hardware. Tail-calling another function would probably; // break this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:20,Safety,safe,safe,20,// Look for obvious safe cases to perform tail call optimization that do not; // require ABI changes. This is what gcc calls sibcall.; // Exception-handling functions need a special set of instructions to indicate; // a return to the hardware. Tail-calling another function would probably; // break this.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:22,Performance,optimiz,optimization,22,// Also avoid sibcall optimization if either caller or callee uses struct; // return semantics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:8,Safety,avoid,avoid,8,// Also avoid sibcall optimization if either caller or callee uses struct; // return semantics.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:95,Performance,perform,perform,95,"// If Caller's vararg or byval argument has been split between registers and; // stack, do not perform tail call, since part of the argument is in caller's; // local frame.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:378,Integrability,depend,depending,378,"// See ARM ARM v7 B1.8.3. On exception entry LR is set to a possibly offset; // version of the ""preferred return address"". These offsets affect the return; // instruction if this is a return from PL1 without hypervisor extensions.; // IRQ/FIQ: +4 ""subs pc, lr, #4""; // SWI: 0 ""subs pc, lr, #0""; // ABORT: +4 ""subs pc, lr, #4""; // UNDEF: +4/+2 ""subs pc, lr, #0""; // UNDEF varies depending on where the exception came from ARM or Thumb; // mode. Alongside GCC, we throw our hands up in disgust and pretend it's 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:298,Safety,ABORT,ABORT,298,"// See ARM ARM v7 B1.8.3. On exception entry LR is set to a possibly offset; // version of the ""preferred return address"". These offsets affect the return; // instruction if this is a return from PL1 without hypervisor extensions.; // IRQ/FIQ: +4 ""subs pc, lr, #4""; // SWI: 0 ""subs pc, lr, #0""; // ABORT: +4 ""subs pc, lr, #4""; // UNDEF: +4/+2 ""subs pc, lr, #0""; // UNDEF varies depending on where the exception came from ARM or Thumb; // mode. Alongside GCC, we throw our hands up in disgust and pretend it's 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:23,Deployability,update,updated,23,// Operand #0 = Chain (updated below),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,Availability,error,error,10,// Report error if cmse entry function returns structure through first ptr arg.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:195,Safety,avoid,avoid,195,"// Half-precision return values can be returned like this:; //; // t11 f16 = fadd ...; // t12: i16 = bitcast t11; // t13: i32 = zero_extend t12; // t14: f32 = bitcast t13 <~~~~~~~ Arg; //; // to avoid code generation for bitcasts, we simply set Arg to the node; // that produces the f16 value, t11 in this case.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:234,Usability,simpl,simply,234,"// Half-precision return values can be returned like this:; //; // t11 f16 = fadd ...; // t12: i16 = bitcast t11; // t13: i32 = zero_extend t12; // t14: f32 = bitcast t13 <~~~~~~~ Arg; //; // to avoid code generation for bitcasts, we simply set Arg to the node; // that produces the f16 value, t11 in this case.; //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Availability,Mask,Mask,3,// Mask f16 arguments if this is a CMSE nonsecure entry.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:71,Availability,avail,available,71,// Legalize ret f64 -> ret 2 x i32. We always have fmrrd if f64 is; // available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:61,Safety,avoid,avoiding,61,"// Guarantee that all emitted copies are; // stuck together, avoiding something bad.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Deployability,Update,Update,3,// Update chain and glue.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:81,Performance,perform,perform,81,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:69,Safety,safe,safe,69,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:118,Performance,perform,perform,118,"// We are at the top of this chain.; // If the copy has a glue operand, we conservatively assume it; // isn't safe to perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:110,Safety,safe,safe,110,"// We are at the top of this chain.; // If the copy has a glue operand, we conservatively assume it; // isn't safe to perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:81,Performance,perform,perform,81,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:69,Safety,safe,safe,69,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:106,Integrability,wrap,wrapped,106,"// ConstantPool, JumpTable, GlobalAddress, and ExternalSymbol are lowered as; // their target counterpart wrapped in the ARMISD::Wrapper node. Suppose N is; // one of the above mentioned nodes. It has to be wrapped because otherwise; // Select(N) returns N. So the raw TargetGlobalAddress nodes, etc. can only; // be used to form addressing mode. These wrapped nodes will be selected; // into MOVi.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:129,Integrability,Wrap,Wrapper,129,"// ConstantPool, JumpTable, GlobalAddress, and ExternalSymbol are lowered as; // their target counterpart wrapped in the ARMISD::Wrapper node. Suppose N is; // one of the above mentioned nodes. It has to be wrapped because otherwise; // Select(N) returns N. So the raw TargetGlobalAddress nodes, etc. can only; // be used to form addressing mode. These wrapped nodes will be selected; // into MOVi.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:207,Integrability,wrap,wrapped,207,"// ConstantPool, JumpTable, GlobalAddress, and ExternalSymbol are lowered as; // their target counterpart wrapped in the ARMISD::Wrapper node. Suppose N is; // one of the above mentioned nodes. It has to be wrapped because otherwise; // Select(N) returns N. So the raw TargetGlobalAddress nodes, etc. can only; // be used to form addressing mode. These wrapped nodes will be selected; // into MOVi.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:353,Integrability,wrap,wrapped,353,"// ConstantPool, JumpTable, GlobalAddress, and ExternalSymbol are lowered as; // their target counterpart wrapped in the ARMISD::Wrapper node. Suppose N is; // one of the above mentioned nodes. It has to be wrapped because otherwise; // Select(N) returns N. So the raw TargetGlobalAddress nodes, etc. can only; // be used to form addressing mode. These wrapped nodes will be selected; // into MOVi.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:101,Modifiability,variab,variable,101,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address for Darwin, and return an; /// SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i32] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first word, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""r0"".; ///; /// Since this descriptor may be in a different unit, in general access must; /// proceed along the usual ARM rules. A common sequence to produce is:; ///; /// movw rT1, :lower16:_var$non_lazy_ptr; /// movt rT1, :upper16:_var$non_lazy_ptr; /// ldr r0, [rT1]; /// ldr rT2, [r0]; /// blx rT2; /// [...address now in r0...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:468,Modifiability,variab,variable,468,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address for Darwin, and return an; /// SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i32] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first word, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""r0"".; ///; /// Since this descriptor may be in a different unit, in general access must; /// proceed along the usual ARM rules. A common sequence to produce is:; ///; /// movw rT1, :lower16:_var$non_lazy_ptr; /// movt rT1, :upper16:_var$non_lazy_ptr; /// ldr r0, [rT1]; /// ldr rT2, [r0]; /// blx rT2; /// [...address now in r0...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:65,Performance,load,loads,65,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address for Darwin, and return an; /// SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i32] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first word, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""r0"".; ///; /// Since this descriptor may be in a different unit, in general access must; /// proceed along the usual ARM rules. A common sequence to produce is:; ///; /// movw rT1, :lower16:_var$non_lazy_ptr; /// movt rT1, :upper16:_var$non_lazy_ptr; /// ldr r0, [rT1]; /// ldr rT2, [r0]; /// blx rT2; /// [...address now in r0...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:847,Security,access,access,847,"/// Convert a TLS address reference into the correct sequence of loads; /// and calls to compute the variable's address for Darwin, and return an; /// SDValue containing the final node.; /// Darwin only has one TLS scheme which must be capable of dealing with the; /// fully general situation, in the worst case. This means:; /// + ""extern __thread"" declaration.; /// + Defined in a possibly unknown dynamic library.; ///; /// The general system is that each __thread variable has a [3 x i32] descriptor; /// which contains information used by the runtime to calculate the address. The; /// only part of this the compiler needs to know about is the first word, which; /// contains a function pointer that must be called with the address of the; /// entire descriptor in ""r0"".; ///; /// Since this descriptor may be in a different unit, in general access must; /// proceed along the usual ARM rules. A common sequence to produce is:; ///; /// movw rT1, :lower16:_var$non_lazy_ptr; /// movt rT1, :upper16:_var$non_lazy_ptr; /// ldr r0, [rT1]; /// ldr rT2, [r0]; /// blx rT2; /// [...address now in r0...]",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:110,Modifiability,variab,variable,110,// The first entry in the descriptor is a function pointer that we must call; // to obtain the address of the variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:178,Modifiability,variab,variable,178,"// Finally, we can make the call. This is just a degenerate version of a; // normal AArch64 call node: r0 takes the address of the descriptor, and; // returns the address of the variable in this thread.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load the current TEB (thread environment block),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load the ThreadLocalStoragePointer from the TEB; // A pointer to the TLS array is located at offset 0x2c from the TEB.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:111,Performance,Load,Load,111,// The pointer to the thread's TLS data area is at the TLS Index scaled by 4; // offset into the TLSArray.; // Load the TLS index from the C runtime,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:37,Availability,avail,available,37,// FIXME: is there useful debug info available here?,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:35,Modifiability,variab,variable,35,// The address of the thread local variable is the add of the thread; // pointer with the offset of the variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:104,Modifiability,variab,variable,104,// The address of the thread local variable is the add of the thread; // pointer with the offset of the variable.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:119,Modifiability,variab,variable,119,"// We rely on this decision to inline being idemopotent and unrelated to the; // use-site. We know that if we inline a variable at one use site, we'll; // inline it elsewhere too (and reuse the constant pool entry). Fast-isel; // doesn't know about this optimization, so bail out if it's enabled else; // we could decide to inline here (and thus never emit the GV) but require; // the GV from fast-isel generated code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:254,Performance,optimiz,optimization,254,"// We rely on this decision to inline being idemopotent and unrelated to the; // use-site. We know that if we inline a variable at one use site, we'll; // inline it elsewhere too (and reuse the constant pool entry). Fast-isel; // doesn't know about this optimization, so bail out if it's enabled else; // we could decide to inline here (and thus never emit the GV) but require; // the GV from fast-isel generated code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:398,Usability,simpl,simplicity,398,// The constant islands pass can only really deal with alignment requests; // <= 4 bytes and cannot pad constants itself. Therefore we cannot promote; // any type wanting greater alignment requirements than 4 bytes. We also; // can only promote constants that are multiples of 4 bytes in size or; // are paddable to a multiple of 4. Currently we only try and pad constants; // that are strings for simplicity.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:177,Availability,avail,available,177,"// If we have T2 ops, we can materialize the address directly via movt/movw; // pair. This is always cheaper. If need to generate Execute Only code, and we; // only have Thumb1 available, we can't use a constant pool and are forced to; // use immediate relocations.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:81,Deployability,Release,Release,81,// Swift happens to implement ISHST barriers in a way that's compatible with; // Release semantics but weaker than ISH so we'd be fools not to use; // it. Beware: other processors probably don't!,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,Performance,load,load,10,// Create load node to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:164,Energy Efficiency,allocate,allocate,164,"// The remaining GPRs hold either the beginning of variable-argument; // data, or the beginning of an aggregate passed by value (usually; // byval). Either way, we allocate stack slots adjacent to the data; // provided by our caller, and store the unallocated registers there.; // If this is a variadic function, the va_list pointer will begin with; // these values; otherwise, this reassembles a (byval) structure that; // was split between registers and memory.; // Return: The frame index registers were stored into.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:51,Modifiability,variab,variable-argument,51,"// The remaining GPRs hold either the beginning of variable-argument; // data, or the beginning of an aggregate passed by value (usually; // byval). Either way, we allocate stack slots adjacent to the data; // provided by our caller, and store the unallocated registers there.; // If this is a variadic function, the va_list pointer will begin with; // these values; otherwise, this reassembles a (byval) structure that; // was split between registers and memory.; // Return: The frame index registers were stored into.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:229,Performance,perform,performed,229,"// Currently, two use-cases possible:; // Case #1. Non-var-args function, and we meet first byval parameter.; // Setup first unallocated register as first byval register;; // eat all remained registers; // (these two actions are performed by HandleByVal method).; // Then, here, we initialize stack frame with; // ""store-reg"" instructions.; // Case #2. Var-args function, that doesn't contain byval parameters.; // The same: eat all remained unallocated registers,; // initialize stack frame.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:104,Performance,load,loaded,104,"// Try to store any remaining integer argument regs; // to their spots on the stack so that they may be loaded by dereferencing; // the result of va_next.; // If there is no regs to be stored, just point address after last; // argument passed via stack.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:55,Energy Efficiency,allocate,allocate,55,"// Calculate the amount of stack space that we need to allocate to store; // byval and variadic arguments that are passed in registers.; // We need to know this before we allocate the first byval or variadic; // argument, as they will be allocated a stack slot below the CFA (Canonical; // Frame Address, the stack pointer at entry to the function).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:171,Energy Efficiency,allocate,allocate,171,"// Calculate the amount of stack space that we need to allocate to store; // byval and variadic arguments that are passed in registers.; // We need to know this before we allocate the first byval or variadic; // argument, as they will be allocated a stack slot below the CFA (Canonical; // Frame Address, the stack pointer at entry to the function).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:238,Energy Efficiency,allocate,allocated,238,"// Calculate the amount of stack space that we need to allocate to store; // byval and variadic arguments that are passed in registers.; // We need to know this before we allocate the first byval or variadic; // argument, as they will be allocated a stack slot below the CFA (Canonical; // Frame Address, the stack pointer at entry to the function).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:91,Testability,assert,assert,91,"// If this is an 8 or 16-bit value, it is really passed promoted; // to 32 bits. Insert an assert[sz]ext to capture this, then; // truncate to the right size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,Modifiability,extend,extended,33,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:155,Modifiability,extend,extended,155,"// f16 arguments have their size extended to 4 bytes and passed as if they; // had been copied to the LSBs of a 32-bit register.; // For that, it's passed extended to i32 (soft ABI) or to f32 (hard ABI)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:136,Performance,optimiz,optimization,136,"// FIXME: For now, all byval parameter objects are marked mutable.; // This can be changed with more analysis.; // In case of tail call optimization mark all arguments mutable.; // Since they could be overwritten by lowering of arguments in case of; // a tail call.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,Performance,load,load,10,// Create load nodes to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:167,Availability,mask,mask,167,"// Thumb1 has very limited immediate modes, so turning an ""and"" into a; // shift can save multiple instructions.; //; // If we have (x & C1), and C1 is an appropriate mask, we can transform it; // into ""((x << n) >> n)"". But that isn't necessarily profitable on its; // own. If it's the operand to an unsigned comparison with an immediate,; // we can eliminate one of the shifts: we transform; // ""((x << n) >> n) == C2"" to ""(x << n) == (C2 << n)"".; //; // We avoid transforming cases which aren't profitable due to encoding; // details:; //; // 1. C2 fits into the immediate field of a cmp, and the transformed version; // would not; in that case, we're essentially trading one immediate load for; // another.; // 2. C1 is 255 or 65535, so we can use uxtb or uxth.; // 3. C2 is zero; we have other code for this special case.; //; // FIXME: Figure out profitability for Thumb2; we usually can't save an; // instruction, since the AND is always one instruction anyway, but we could; // use narrow instructions in some cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:689,Performance,load,load,689,"// Thumb1 has very limited immediate modes, so turning an ""and"" into a; // shift can save multiple instructions.; //; // If we have (x & C1), and C1 is an appropriate mask, we can transform it; // into ""((x << n) >> n)"". But that isn't necessarily profitable on its; // own. If it's the operand to an unsigned comparison with an immediate,; // we can eliminate one of the shifts: we transform; // ""((x << n) >> n) == C2"" to ""(x << n) == (C2 << n)"".; //; // We avoid transforming cases which aren't profitable due to encoding; // details:; //; // 1. C2 fits into the immediate field of a cmp, and the transformed version; // would not; in that case, we're essentially trading one immediate load for; // another.; // 2. C1 is 255 or 65535, so we can use uxtb or uxth.; // 3. C2 is zero; we have other code for this special case.; //; // FIXME: Figure out profitability for Thumb2; we usually can't save an; // instruction, since the AND is always one instruction anyway, but we could; // use narrow instructions in some cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:460,Safety,avoid,avoid,460,"// Thumb1 has very limited immediate modes, so turning an ""and"" into a; // shift can save multiple instructions.; //; // If we have (x & C1), and C1 is an appropriate mask, we can transform it; // into ""((x << n) >> n)"". But that isn't necessarily profitable on its; // own. If it's the operand to an unsigned comparison with an immediate,; // we can eliminate one of the shifts: we transform; // ""((x << n) >> n) == C2"" to ""(x << n) == (C2 << n)"".; //; // We avoid transforming cases which aren't profitable due to encoding; // details:; //; // 1. C2 fits into the immediate field of a cmp, and the transformed version; // would not; in that case, we're essentially trading one immediate load for; // another.; // 2. C1 is 255 or 65535, so we can use uxtb or uxth.; // 3. C2 is zero; we have other code for this special case.; //; // FIXME: Figure out profitability for Thumb2; we usually can't save an; // instruction, since the AND is always one instruction anyway, but we could; // use narrow instructions in some cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:752,Usability,ux,uxtb,752,"// Thumb1 has very limited immediate modes, so turning an ""and"" into a; // shift can save multiple instructions.; //; // If we have (x & C1), and C1 is an appropriate mask, we can transform it; // into ""((x << n) >> n)"". But that isn't necessarily profitable on its; // own. If it's the operand to an unsigned comparison with an immediate,; // we can eliminate one of the shifts: we transform; // ""((x << n) >> n) == C2"" to ""(x << n) == (C2 << n)"".; //; // We avoid transforming cases which aren't profitable due to encoding; // details:; //; // 1. C2 fits into the immediate field of a cmp, and the transformed version; // would not; in that case, we're essentially trading one immediate load for; // another.; // 2. C1 is 255 or 65535, so we can use uxtb or uxth.; // 3. C2 is zero; we have other code for this special case.; //; // FIXME: Figure out profitability for Thumb2; we usually can't save an; // instruction, since the AND is always one instruction anyway, but we could; // use narrow instructions in some cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:760,Usability,ux,uxth,760,"// Thumb1 has very limited immediate modes, so turning an ""and"" into a; // shift can save multiple instructions.; //; // If we have (x & C1), and C1 is an appropriate mask, we can transform it; // into ""((x << n) >> n)"". But that isn't necessarily profitable on its; // own. If it's the operand to an unsigned comparison with an immediate,; // we can eliminate one of the shifts: we transform; // ""((x << n) >> n) == C2"" to ""(x << n) == (C2 << n)"".; //; // We avoid transforming cases which aren't profitable due to encoding; // details:; //; // 1. C2 fits into the immediate field of a cmp, and the transformed version; // would not; in that case, we're essentially trading one immediate load for; // another.; // 2. C1 is 255 or 65535, so we can use uxtb or uxth.; // 3. C2 is zero; we have other code for this special case.; //; // FIXME: Figure out profitability for Thumb2; we usually can't save an; // instruction, since the AND is always one instruction anyway, but we could; // use narrow instructions in some cases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:57,Performance,optimiz,optimized,57,"// The specific comparison ""(x<<c) > 0x80000000U"" can be optimized to a; // single ""lsls x, c+1"". The shift sets the ""C"" and ""Z"" flags the same; // way a cmp would.; // FIXME: Add support for ARM/Thumb2; this would need isel patterns, and; // some tweaks to the heuristics for the previous and->shift transform.; // FIXME: Optimize cases where the LHS isn't a shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:323,Performance,Optimiz,Optimize,323,"// The specific comparison ""(x<<c) > 0x80000000U"" can be optimized to a; // single ""lsls x, c+1"". The shift sets the ""C"" and ""Z"" flags the same; // way a cmp would.; // FIXME: Add support for ARM/Thumb2; this would need isel patterns, and; // some tweaks to the heuristics for the previous and->shift transform.; // FIXME: Optimize cases where the LHS isn't a shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:104,Usability,simpl,simplify,104,"// If the RHS is a constant zero then the V (overflow) flag will never be; // set. This can allow us to simplify GE to PL or LT to MI, which can be; // simpler for other passes (like the peephole optimiser) to deal with.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:152,Usability,simpl,simpler,152,"// If the RHS is a constant zero then the V (overflow) flag will never be; // set. This can allow us to simplify GE to PL or LT to MI, which can be; // simpler for other passes (like the peephole optimiser) to deal with.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:192,Integrability,depend,dependency,192,// FIXME: We are currently always generating CMPs because we don't support; // generating CMN through the backend. This is not as good as the natural; // CMP case because it causes a register dependency and cannot be folded; // later.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:59,Availability,Mask,Mask,59,// ARM's BooleanContents value is UndefinedBooleanContent. Mask out the; // undefined bits before doing a full-word comparison with zero.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:206,Energy Efficiency,power,power,206,"// Check if two chained conditionals could be converted into SSAT or USAT.; //; // SSAT can replace a set of two conditional selectors that bound a number to an; // interval of type [k, ~k] when k + 1 is a power of 2. Here are some examples:; //; // x < -k ? -k : (x > k ? k : x); // x < -k ? -k : (x < k ? x : k); // x > -k ? (x > k ? k : x) : -k; // x < k ? (x < -k ? -k : x) : k; // etc.; //; // LLVM canonicalizes these to either a min(max()) or a max(min()); // pattern. This function tries to match one of these and will return a SSAT; // node if successful.; //; // USAT works similarily to SSAT but bounds on the interval [0, k] where k + 1; // is a power of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:658,Energy Efficiency,power,power,658,"// Check if two chained conditionals could be converted into SSAT or USAT.; //; // SSAT can replace a set of two conditional selectors that bound a number to an; // interval of type [k, ~k] when k + 1 is a power of 2. Here are some examples:; //; // x < -k ? -k : (x > k ? k : x); // x < -k ? -k : (x < k ? x : k); // x > -k ? (x > k ? k : x) : -k; // x < k ? (x < -k ? -k : x) : k; // etc.; //; // LLVM canonicalizes these to either a min(max()) or a max(min()); // pattern. This function tries to match one of these and will return a SSAT; // node if successful.; //; // USAT works similarily to SSAT but bounds on the interval [0, k] where k + 1; // is a power of 2.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:46,Modifiability,variab,variable,46,"// If the constant on left and right side, or variable on left and right,; // does not match, early out",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:90,Energy Efficiency,efficient,efficient,90,"// Try to convert expressions of the form x < k ? k : x (and similar forms); // into more efficient bit operations, which is possible when k is 0 or -1; // On ARM and Thumb-2 which have flexible operand 2 this will result in; // single instructions. On Thumb the shift and the bit operation will be two; // instructions.; // Only allow this transformation on full-width (32-bit) operations",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:186,Modifiability,flexible,flexible,186,"// Try to convert expressions of the form x < k ? k : x (and similar forms); // into more efficient bit operations, which is possible when k is 0 or -1; // On ARM and Thumb-2 which have flexible operand 2 this will result in; // single instructions. On Thumb the shift and the bit operation will be two; // instructions.; // Only allow this transformation on full-width (32-bit) operations",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:4,Performance,Optimiz,OptimizeVFPBrcond,4,"/// OptimizeVFPBrcond - With -enable-unsafe-fp-math, it's legal to optimize some; /// f32 and even f64 comparisons to integer ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:67,Performance,optimiz,optimize,67,"/// OptimizeVFPBrcond - With -enable-unsafe-fp-math, it's legal to optimize some; /// f32 and even f64 comparisons to integer ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:37,Safety,unsafe,unsafe-fp-math,37,"/// OptimizeVFPBrcond - With -enable-unsafe-fp-math, it's legal to optimize some; /// f32 and even f64 comparisons to integer ones.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:21,Performance,optimiz,optimization,21,"// If unsafe fp math optimization is enabled and there are no other uses of; // the CMP operands, and the condition code is EQ or NE, we can optimize it; // to an integer comparison.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:141,Performance,optimiz,optimize,141,"// If unsafe fp math optimization is enabled and there are no other uses of; // the CMP operands, and the condition code is EQ or NE, we can optimize it; // to an integer comparison.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:6,Safety,unsafe,unsafe,6,"// If unsafe fp math optimization is enabled and there are no other uses of; // the CMP operands, and the condition code is EQ or NE, we can optimize it; // to an integer comparison.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize {s|u}{add|sub|mul}.with.overflow feeding into a branch; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Optimiz,Optimize,3,// Optimize {s|u}{add|sub|mul}.with.overflow feeding into a branch; // instruction.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:51,Usability,clear,cleared,51,// To get the default FP mode all control bits are cleared:; // FPSCR = FPSCR & (FPStatusBits | FPReservedBits),MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load constant 0xffff'ffff'ffff'ffff to register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:49,Performance,perform,perform,49,"// There is no t2LSRLr instruction so negate and perform an lsll if the; // shift amount is in a register, emulating a right shift.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Safety,Detect,Detect,3,"// Detect VTST (Vector Test Bits) = icmp ne (and (op0, op1), zero).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:23,Testability,Test,Test,23,"// Detect VTST (Vector Test Bits) = icmp ne (and (op0, op1), zero).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:173,Performance,optimiz,optimization,173,"// Note: there are a few 32-bit splat values (specifically: 00ffff00,; // ff000000, ff0000ff, and ffff00ff) that are valid for VMOV.I64 but not; // VMOV.I32. A (very) minor optimization would be to replicate the value; // and fall through here to test for a valid 64-bit splat. But, then the; // caller would also need to check and handle the change in size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:247,Testability,test,test,247,"// Note: there are a few 32-bit splat values (specifically: 00ffff00,; // ff000000, ff0000ff, and ffff00ff) that are valid for VMOV.I64 but not; // VMOV.I32. A (very) minor optimization would be to replicate the value; // and fall through here to test for a valid 64-bit splat. But, then the; // caller would also need to check and handle the change in size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:55,Performance,load,loads,55,// Prevent floating-point constants from using literal loads; // when execute-only is enabled.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:55,Availability,mask,mask,55,// check if an VEXT instruction can handle the shuffle mask when the; // vector sources of the shuffle are the same.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:39,Integrability,wrap,wraps,39,"// Increment the expected index. If it wraps around, just follow it; // back to index zero and keep going.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:39,Integrability,wrap,wraps,39,"// Increment the expected index. If it wraps around, it may still be; // a VEXT but the source vectors must be swapped.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:63,Availability,mask,mask,63,"// We can handle <8 x i8> vector shuffles. If the index in the mask is out of; // range, then 0 is placed into the resulting vector. So pretty much any mask; // of 8 elements can work here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:152,Availability,mask,mask,152,"// We can handle <8 x i8> vector shuffles. If the index in the mask is out of; // range, then 0 is placed into the resulting vector. So pretty much any mask; // of 8 elements can work here.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:30,Availability,mask,mask,30,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:126,Availability,mask,mask,126,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:276,Availability,mask,mask,276,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:442,Availability,mask,mask,442,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:698,Availability,mask,masks,698,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:942,Availability,mask,mask,942,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1099,Availability,mask,mask,1099,"// Checks whether the shuffle mask represents a vector transpose (VTRN) by; // checking that pairs of elements in the shuffle mask represent the same index; // in each vector, incrementing the expected index by 2 at each step.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 2, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,c,g}; // v2={e,f,g,h}; // WhichResult gives the offset for each element in the mask based on which; // of the two results it belongs to.; //; // The transpose can be represented either as:; // result1 = shufflevector v1, v2, result1_shuffle_mask; // result2 = shufflevector v1, v2, result2_shuffle_mask; // where v1/v2 and the shuffle masks have the same number of elements; // (here WhichResult (see below) indicates which result is being checked); //; // or as:; // results = shufflevector v1, v2, shuffle_mask; // where both results are returned in one vector and the shuffle mask has twice; // as many elements as v1/v2 (here WhichResult will always be 0 if true) here we; // want to check the low half and high half of the shuffle mask as if it were; // the other case",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,Availability,mask,mask,10,"// If the mask is twice as long as the input vector then we need to check the; // upper and lower parts of the mask with a matching value for WhichResult; // FIXME: A mask with only even values will be rejected in case the first; // element is undefined, e.g. [-1, 4, 2, 6] will be rejected, because only; // M[0] is used to determine WhichResult",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:111,Availability,mask,mask,111,"// If the mask is twice as long as the input vector then we need to check the; // upper and lower parts of the mask with a matching value for WhichResult; // FIXME: A mask with only even values will be rejected in case the first; // element is undefined, e.g. [-1, 4, 2, 6] will be rejected, because only; // M[0] is used to determine WhichResult",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:167,Availability,mask,mask,167,"// If the mask is twice as long as the input vector then we need to check the; // upper and lower parts of the mask with a matching value for WhichResult; // FIXME: A mask with only even values will be rejected in case the first; // element is undefined, e.g. [-1, 4, 2, 6] will be rejected, because only; // M[0] is used to determine WhichResult",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:141,Availability,Mask,Mask,141,"/// isVTRN_v_undef_Mask - Special case of isVTRNMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 0, 2, 2> instead of <0, 4, 2, 6>.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:30,Availability,mask,mask,30,"// Checks whether the shuffle mask represents a vector unzip (VUZP) by checking; // that the mask elements are either all even and in steps of size 2 or all odd; // and in steps of size 2.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 2, 4, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,c,e,g}; // v2={e,f,g,h}; // Requires similar checks to that of isVTRNMask with; // respect the how results are returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:93,Availability,mask,mask,93,"// Checks whether the shuffle mask represents a vector unzip (VUZP) by checking; // that the mask elements are either all even and in steps of size 2 or all odd; // and in steps of size 2.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 2, 4, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,c,e,g}; // v2={e,f,g,h}; // Requires similar checks to that of isVTRNMask with; // respect the how results are returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:238,Availability,mask,mask,238,"// Checks whether the shuffle mask represents a vector unzip (VUZP) by checking; // that the mask elements are either all even and in steps of size 2 or all odd; // and in steps of size 2.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 2, 4, 6]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,c,e,g}; // v2={e,f,g,h}; // Requires similar checks to that of isVTRNMask with; // respect the how results are returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:141,Availability,Mask,Mask,141,"/// isVUZP_v_undef_Mask - Special case of isVUZPMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 2, 0, 2> instead of <0, 2, 4, 6>,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:30,Availability,mask,mask,30,"// Checks whether the shuffle mask represents a vector zip (VZIP) by checking; // that pairs of elements of the shufflemask represent the same index in each; // vector incrementing sequentially through the vectors.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 1, 5]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,b,f}; // v2={e,f,g,h}; // Requires similar checks to that of isVTRNMask with respect the how results; // are returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:264,Availability,mask,mask,264,"// Checks whether the shuffle mask represents a vector zip (VZIP) by checking; // that pairs of elements of the shufflemask represent the same index in each; // vector incrementing sequentially through the vectors.; // e.g. For v1,v2 of type v4i32 a valid shuffle mask is: [0, 4, 1, 5]; // v1={a,b,c,d} => x=shufflevector v1, v2 shufflemask => x={a,e,b,f}; // v2={e,f,g,h}; // Requires similar checks to that of isVTRNMask with respect the how results; // are returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:141,Availability,Mask,Mask,141,"/// isVZIP_v_undef_Mask - Special case of isVZIPMask for canonical form of; /// ""vector_shuffle v, v"", i.e., ""vector_shuffle v, undef"".; /// Mask is e.g., <0, 0, 1, 1> instead of <0, 4, 1, 5>.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:17,Availability,mask,mask,17,// Make sure the mask has the right size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:17,Availability,mask,mask,17,// Make sure the mask has the right size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:17,Availability,mask,mask,17,// Make sure the mask has the right size.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Testability,Test,Test,3,// Test if the Trunc can be convertable to a VMOVN with this shuffle. We are; // looking for patterns of:; // !rev: 0 N/2 1 N/2+1 2 N/2+2 ...; // rev: N/2 0 N/2+1 1 N/2+2 2 ...,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:83,Modifiability,extend,extend,83,"// If this is a single value copied into all lanes (a splat), we can just sign; // extend that single value",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Loads,3,// Loads are better lowered with insert_vector_elt/ARMISD::BUILD_VECTOR.; // Keep going if we are hitting this case.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:62,Energy Efficiency,reduce,reduce,62,"// Use VDUP for non-constant splats. For f32 constant splats, reduce to; // i32 and try again.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:130,Performance,load,load,130,"// If all elements are constants and the case above didn't get hit, fall back; // to the default expansion, which will generate a load from the constant; // pool.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:100,Testability,test,tests,100,// Reconstruct the BUILDVECTOR to one of the legal shuffles (such as vext and; // vmovn). Empirical tests suggest this is rarely worth it for vectors of; // length <= 2.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:26,Energy Efficiency,efficient,efficient,26,"// If we haven't found an efficient lowering, try splitting a 128-bit vector; // into two 64-bit vectors; we might discover a better way to lower it.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:392,Performance,load,load,392,"// If all else fails, just use a sequence of INSERT_VECTOR_ELT when we; // know the default expansion would otherwise fall back on something even; // worse. For a vector with one or two non-undef values, that's; // scalar_to_vector for the elements followed by a shuffle (provided the; // shuffle is valid for the target) and materialization element by element; // on the stack followed by a load for everything else.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:44,Availability,mask,mask,44,"// Furthermore, shuffles require a constant mask, whereas extractelts; // accept variable indices.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:81,Modifiability,variab,variable,81,"// Furthermore, shuffles require a constant mask, whereas extractelts; // accept variable indices.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Deployability,Update,Update,3,// Update the minimum and maximum lane number seen.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:56,Availability,mask,mask,56,"// The stars all align, our next step is to produce the mask for the shuffle.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:22,Performance,perform,performs,22,"// EXTRACT_VECTOR_ELT performs an implicit any_ext; BUILD_VECTOR an implicit; // trunc. So only std::min(SrcBits, DestBits) actually get defined in this; // segment.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:140,Availability,mask,masks,140,"/// isShuffleMaskLegal - Targets can use this to indicate that they only; /// support *some* VECTOR_SHUFFLE operations, those with specific masks.; /// By default, if a target supports the VECTOR_SHUFFLE node, all mask values; /// are assumed to be legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:214,Availability,mask,mask,214,"/// isShuffleMaskLegal - Targets can use this to indicate that they only; /// support *some* VECTOR_SHUFFLE operations, those with specific masks.; /// By default, if a target supports the VECTOR_SHUFFLE node, all mask values; /// are assumed to be legal.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:36,Integrability,depend,depending,36,// Select either all ones or zeroes depending upon the real predicate bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Safety,Detect,Detect,3,// Detect which mov lane this would be from the first non-undef element.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:23,Availability,mask,mask,23,"// An One-Off Identity mask is one that is mostly an identity mask from as; // single source but contains a single element out-of-place, either from a; // different vector or from another position in the same vector. As opposed to; // lowering this via a ARMISD::BUILD_VECTOR we can generate an extract/insert; // pair directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:62,Availability,mask,mask,62,"// An One-Off Identity mask is one that is mostly an identity mask from as; // single source but contains a single element out-of-place, either from a; // different vector or from another position in the same vector. As opposed to; // lowering this via a ARMISD::BUILD_VECTOR we can generate an extract/insert; // pair directly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:190,Energy Efficiency,efficient,efficient,190,"// Convert shuffles that are directly supported on NEON to target-specific; // DAG nodes, instead of keeping them as shuffles and matching them again; // during code selection. This is more efficient and avoids the possibility; // of inconsistencies between legalization and selection.; // FIXME: floating-point vectors should be canonicalized to integer vectors; // of the same time so that they get CSEd properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:204,Safety,avoid,avoids,204,"// Convert shuffles that are directly supported on NEON to target-specific; // DAG nodes, instead of keeping them as shuffles and matching them again; // during code selection. This is more efficient and avoids the possibility; // of inconsistencies between legalization and selection.; // FIXME: floating-point vectors should be canonicalized to integer vectors; // of the same time so that they get CSEd properly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Testability,Test,Test,3,// Test if V1 is a SCALAR_TO_VECTOR.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Testability,Test,Test,3,// Test if V1 is a BUILD_VECTOR which is equivalent to a SCALAR_TO_VECTOR; // (and probably will turn into a SCALAR_TO_VECTOR once legalization; // reaches it).,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:173,Availability,mask,masks,173,"// Check for Neon shuffles that modify both input vectors in place.; // If both results are used, i.e., if there are two shuffles with the same; // source operands and with masks corresponding to both results of one of; // these operations, DAG memoization will ensure that a single node is; // used for both shuffles.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:252,Security,access,access,252,"// Also check for these shuffles through CONCAT_VECTORS: we canonicalize; // shuffles that produce a result larger than their operands with:; // shuffle(concat(v1, undef), concat(v2, undef)); // ->; // shuffle(concat(v1, v2), undef); // because we can access quad vectors (see PerformVECTOR_SHUFFLECombine).; //; // This is useful in the general case, but there are special cases where; // native shuffles produce larger results: the two-result ops.; //; // Look through the concat when lowering them:; // shuffle(concat(v1, v2), undef); // ->; // concat(VZIP(v1, v2):0, :1); //",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:454,Modifiability,extend,extend,454,"// MVE does not have a single instruction to perform the truncation of a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:739,Modifiability,extend,extend,739,"// MVE does not have a single instruction to perform the truncation of a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1756,Modifiability,extend,extend,1756,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:45,Performance,perform,perform,45,"// MVE does not have a single instruction to perform the truncation of a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1040,Performance,load,loads,1040,"a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1064,Performance,perform,perform,1064,"a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1412,Performance,perform,performed,1412,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:1740,Performance,load,loads,1740,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:2052,Performance,optimiz,optimized,2052,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:2125,Performance,load,loads,2125,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:2330,Performance,load,loads,2330,"p/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert certain trunc's into MVETRUNC, which; // are either optimized where they can be, or eventually lowered into stack; // stores/loads. This prevents us from splitting a v8i16 trunc into two stores; // two early, where other instructions would be better, and stops us from; // having to reconstruct multiple buildvector shuffles into loads/stores.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:311,Safety,avoid,avoid,311,"// MVE does not have a single instruction to perform the truncation of a v4i32; // into the lower half of a v8i16, in the same way that a NEON vmovn would.; // Most of the instructions in MVE follow the 'Beats' system, where moving; // values from different lanes is usually something that the instructions; // avoid.; //; // Instead it has top/bottom instructions such as VMOVLT/B and VMOVNT/B,; // which take a the top/bottom half of a larger lane and extend it (or do the; // opposite, truncating into the top/bottom lane from a larger lane). Note; // that because of the way we widen lanes, a v4i16 is really a v4i32 using the; // bottom 16bits from each vector lane. This works really well with T/B; // instructions, but that doesn't extend to v8i32->v8i16 where the lanes need; // to move order.; //; // But truncates and sext/zext are always going to be fairly common from llvm.; // We have several options for how to deal with them:; // - Wherever possible combine them into an instruction that makes them; // ""free"". This includes loads/stores, which can perform the trunc as part; // of the memory operation. Or certain shuffles that can be turned into; // VMOVN/VMOVL.; // - Lane Interleaving to transform blocks surrounded by ext/trunc. So; // trunc(mul(sext(a), sext(b))) may become; // VMOVNT(VMUL(VMOVLB(a), VMOVLB(b)), VMUL(VMOVLT(a), VMOVLT(b))). (Which in; // this case can use VMULL). This is performed in the; // MVELaneInterleavingPass.; // - Otherwise we have an option. By default we would expand the; // zext/sext/trunc into a series of lane extract/inserts going via GPR; // registers. One for each vector lane in the vector. This can obviously be; // very expensive.; // - The other option is to use the fact that loads/store can extend/truncate; // to turn a trunc into two truncating stack stores and a stack reload. This; // becomes 3 back-to-back memory operations, but at least that is less than; // all the insert/extracts.; //; // In order to do the last, we convert c",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:120,Integrability,depend,depending,120,"/// isExtendedBUILD_VECTOR - Check if N is a constant BUILD_VECTOR where each; /// element has been zero/sign-extended, depending on the isSigned parameter,; /// from an integer type half its size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:110,Modifiability,extend,extended,110,"/// isExtendedBUILD_VECTOR - Check if N is a constant BUILD_VECTOR where each; /// element has been zero/sign-extended, depending on the isSigned parameter,; /// from an integer type half its size.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:68,Modifiability,extend,extended,68,/// isSignExtended - Check if a node is a vector value that is sign-extended; /// or a constant BUILD_VECTOR with sign-extended elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:119,Modifiability,extend,extended,119,/// isSignExtended - Check if a node is a vector value that is sign-extended; /// or a constant BUILD_VECTOR with sign-extended elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:68,Modifiability,extend,extended,68,/// isZeroExtended - Check if a node is a vector value that is zero-extended (or; /// any-extended) or a constant BUILD_VECTOR with zero-extended elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:90,Modifiability,extend,extended,90,/// isZeroExtended - Check if a node is a vector value that is zero-extended (or; /// any-extended) or a constant BUILD_VECTOR with zero-extended elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:137,Modifiability,extend,extended,137,/// isZeroExtended - Check if a node is a vector value that is zero-extended (or; /// any-extended) or a constant BUILD_VECTOR with zero-extended elements.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:64,Modifiability,extend,extend,64,/// AddRequiredExtensionForVMULL - Add a sign/zero extension to extend the total; /// value size to 64 bits. We need a 64-bit D register as an operand to VMULL.; /// We insert the required extension here to get the vector to fill a D register.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:59,Modifiability,extend,extended,59,// The vector originally had a size of OrigTy. It was then extended to ExtTy.; // We expect the ExtTy to be 128-bits total. If the OrigTy is less than; // 64-bits we need to insert a new extension so that it will be 64-bits.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:8,Modifiability,extend,extend,8,// Must extend size to at least 64 bits to be used as an operand for VMULL.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:347,Modifiability,extend,extending,347,"/// SkipLoadExtensionForVMULL - return a load of the original vector size that; /// does not do any sign/zero extension. If the original vector is less; /// than 64 bits, an appropriate extension will be added after the load to; /// reach a total size of 64 bits. We have to add the extension separately; /// because ARM does not have a sign/zero extending load for vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:41,Performance,load,load,41,"/// SkipLoadExtensionForVMULL - return a load of the original vector size that; /// does not do any sign/zero extension. If the original vector is less; /// than 64 bits, an appropriate extension will be added after the load to; /// reach a total size of 64 bits. We have to add the extension separately; /// because ARM does not have a sign/zero extending load for vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:220,Performance,load,load,220,"/// SkipLoadExtensionForVMULL - return a load of the original vector size that; /// does not do any sign/zero extension. If the original vector is less; /// than 64 bits, an appropriate extension will be added after the load to; /// reach a total size of 64 bits. We have to add the extension separately; /// because ARM does not have a sign/zero extending load for vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:357,Performance,load,load,357,"/// SkipLoadExtensionForVMULL - return a load of the original vector size that; /// does not do any sign/zero extension. If the original vector is less; /// than 64 bits, an appropriate extension will be added after the load to; /// reach a total size of 64 bits. We have to add the extension separately; /// because ARM does not have a sign/zero extending load for vectors.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:7,Performance,load,load,7,// The load already has the right type.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:66,Performance,load,load,66,// We need to create a zextload/sextload. We cannot just create a load; // followed by a zext/zext node because LowerMUL is also run during normal; // operation legalization where we can't create illegal types.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:92,Modifiability,extend,extending,92,"/// SkipExtensionForVMULL - For a node that is a SIGN_EXTEND, ZERO_EXTEND,; /// ANY_EXTEND, extending load, or BUILD_VECTOR with extended elements, return; /// the unextended value. The unextended vector should be 64 bits so that it can; /// be used as an operand to a VMULL instruction. If the original vector size; /// before extension is less than 64 bits we add a an extension to resize; /// the vector to 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:129,Modifiability,extend,extended,129,"/// SkipExtensionForVMULL - For a node that is a SIGN_EXTEND, ZERO_EXTEND,; /// ANY_EXTEND, extending load, or BUILD_VECTOR with extended elements, return; /// the unextended value. The unextended vector should be 64 bits so that it can; /// be used as an operand to a VMULL instruction. If the original vector size; /// before extension is less than 64 bits we add a an extension to resize; /// the vector to 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:102,Performance,load,load,102,"/// SkipExtensionForVMULL - For a node that is a SIGN_EXTEND, ZERO_EXTEND,; /// ANY_EXTEND, extending load, or BUILD_VECTOR with extended elements, return; /// the unextended value. The unextended vector should be 64 bits so that it can; /// be used as an operand to a VMULL instruction. If the original vector size; /// before extension is less than 64 bits we add a an extension to resize; /// the vector to 64 bits.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:88,Safety,detect,detected,88,// Multiplications are only custom-lowered for 128-bit vectors so that; // VMULL can be detected. Otherwise v2i64 multiplications are not legal.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Optimiz,Optimizing,3,"// Optimizing (zext A + zext B) * C, to (VMULL A, C) + (VMULL B, C) during; // isel lowering to take advantage of no-stall back to back vmul + vmla.; // vmull q0, d4, d6; // vmlal q0, d5, d6; // is faster than; // vaddl q0, d4, d5; // vmovl q1, d6; // vmul q0, q0, q1",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:200,Testability,test,tested,200,"// Because char has a smaller range than uchar, we can actually get away; // without any newton steps. This requires that we use a weird bias; // of 0xb000, however (again, this has been exhaustively tested).; // float4 result = as_float4(as_int4(xf*recip) + 0xb000);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:204,Testability,test,tested,204,"// Because short has a smaller range than ushort, we can actually get away; // with only a single newton step. This requires that we use a weird bias; // of 89, however (again, this has been exhaustively tested).; // float4 result = as_float4(as_int4(xf*recip) + 0x89);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:115,Testability,test,testing,115,"// Simply multiplying by the reciprocal estimate can leave us a few ulps; // too low, so we add 2 ulps (exhaustive testing shows that this is enough,; // and that it will never cause us to return an answer too large).; // float4 result = as_float4(as_int4(xf*recip) + 2);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Usability,Simpl,Simply,3,"// Simply multiplying by the reciprocal estimate can leave us a few ulps; // too low, so we add 2 ulps (exhaustive testing shows that this is enough,; // and that it will never cause us to return an answer too large).; // float4 result = as_float4(as_int4(xf*recip) + 2);",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:61,Energy Efficiency,power,power,61,"// ARM mode is a bit simpler than Thumb: we can handle large power; // of 2 immediates with 1 mov instruction; no further checks required,; // just return the sdiv node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:21,Usability,simpl,simpler,21,"// ARM mode is a bit simpler than Thumb: we can handle large power; // of 2 immediates with 1 mov instruction; no further checks required,; // just return the sdiv node.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:51,Performance,load,loads,51,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:138,Performance,load,loaded,138,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:200,Performance,load,loaded,200,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:247,Performance,load,loading,247,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:383,Performance,load,load,383,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:528,Performance,load,load,528,"// The basic MVE VLDR on a v2i1/v4i1/v8i1 actually loads the entire 16bit; // predicate, with the ""v4i1"" bits spread out over the 16 bits loaded. We; // need to make sure that 8/4/2 bits are actually loaded into the correct; // place, which means loading the value and then shuffling the values into; // the bottom bits of the predicate.; // Equally, VLDR for an v16i1 will actually load 32bits (so will be incorrect; // for BE).; // Speaking of BE, apparently the rest of llvm will assume a reverse order to; // a natural VMSR(load), so needs to be reversed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:7,Availability,Mask,Masked,7,"// MVE Masked loads use zero as the passthru value. Here we convert undef to; // zero too, and other values are lowered to a select.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:14,Performance,load,loads,14,"// MVE Masked loads use zero as the passthru value. Here we convert undef to; // zero too, and other values are lowered to a select.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:50,Availability,down,down,50,"// Use Mul(X, Rev(X)) until 4 items remain. Going down to 4 vector elements; // allows us to easily extract vector elements from the lanes.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:87,Availability,avail,available,87,// Acquire/Release load/store is not legal for targets without a dmb or; // equivalent available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:11,Deployability,Release,Release,11,// Acquire/Release load/store is not legal for targets without a dmb or; // equivalent available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:19,Performance,load,load,19,// Acquire/Release load/store is not legal for targets without a dmb or; // equivalent available.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:13,Performance,load,load,13,// Monotonic load/store is legal for all targets.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:9,Energy Efficiency,Power,Power,9,"// Under Power Management extensions, the cycle-count is:; // mrc p15, #0, <Rt>, c9, c13, #0",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:89,Energy Efficiency,Schedul,Scheduler,89,//===----------------------------------------------------------------------===//; // ARM Scheduler Hooks; //===----------------------------------------------------------------------===//; /// SetupEntryBlockForSjLj - Insert code into the entry block that creates and; /// registers the function context.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load the address of the dispatch MBB into the jump buffer.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:20,Testability,assert,assert,20,// FIXME: We should assert that the EH_LABEL is the first MI in the landing; // pad.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:18,Availability,mask,mask,18,"// Add a register mask with no preserved registers. This results in all; // registers being marked as clobbered. This can't work if the dispatch block; // is in a Thumb1 function and is linked with ARM code which uses the FP; // registers, as there is no way to preserve the FP registers in Thumb1 mode.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:15,Performance,load,load,15,"/// Return the load opcode for a given load size. If load size >= 8,; /// neon opcode will be returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:39,Performance,load,load,39,"/// Return the load opcode for a given load size. If load size >= 8,; /// neon opcode will be returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:53,Performance,load,load,53,"/// Return the load opcode for a given load size. If load size >= 8,; /// neon opcode will be returned.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:26,Performance,load,load,26,/// Emit a post-increment load operation with given size. The instructions; /// will be added to BB at Pos.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,Deployability,update,update,10,// load + update AddrIn,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,load,load,3,// load + update AddrIn,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:11,Deployability,update,update,11,// store + update AddrIn,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:62,Performance,load,load,62,// Select the correct opcode and register class for unit size load/store,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:3,Performance,Load,Load,3,// Load an immediate to varEnd.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:18,Modifiability,variab,variable,18,// Decrement loop variable by UnitSize.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:41,Energy Efficiency,allocate,allocate,41,"// __chkstk takes the number of words to allocate on the stack in R4, and; // returns the stack adjustment in number of bytes in R4. This will not; // clober any other registers (other than the obvious lr).; //; // Although, technically, IP should be considered a register which may be; // clobbered, the call itself will not touch it. Windows on ARM is a pure; // thumb-2 environment, so there is no interworking required. As a result, we; // do not expect a veneer to be emitted by the linker, clobbering IP.; //; // Each module receives its own copy of __chkstk, so no import thunk is; // required, again, ensuring that IP is not clobbered.; //; // Finally, although some linkers may theoretically provide a trampoline for; // out of range calls (which is quite common due to a 32M range limitation of; // branches for Thumb), we can generate the long-call version via; // -mcmodel=large, alleviating the need for the trampoline which may clobber; // IP.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:27,Deployability,update,update,27,// Should have kill-flag - update below.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:9,Testability,log,logic,9,/// Adds logic in loop entry MBB to calculate loop iteration count and adds; /// t2WhileLoopSetup and t2WhileLoopStart to generate WLS loop,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp
