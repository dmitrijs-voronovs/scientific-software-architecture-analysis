id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:33816,Testability,test,test,33816,"ss than or equal to `x`. Examples; --------. >>> hl.eval(hl.floor(3.1)); 3.0. Parameters; ----------; x : :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""floor"", x.dtype, x). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34182,Testability,test,test,34182,"oadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference g",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34910,Testability,test,test,34910,"0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[st",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:35058,Testability,test,test,35058,"0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[st",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:35487,Testability,test,test,35487,"brium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[str, ReferenceGenome] = 'default') -> LocusExpression:; """"""Construct a locus expression from a chromosome and position. Examples; --------. >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37). Parameters; ----------; contig : str or :class:`.StringExpression`; Chromosome.; pos : int or :class:`.Expression` of type :py:data:`.tint32`; Base position along the chromosome.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:51512,Testability,test,test,51512,"----+-----------------+; | ploidy | Phased | Unphased |; +========+=================+=================+; | 0 | ``|-`` | ``-`` |; +--------+-----------------+-----------------+; | 1 | ``|i`` | ``i`` |; +--------+-----------------+-----------------+; | 2 | ``i|j`` | ``i/j`` |; +--------+-----------------+-----------------+; | 3 | ``i|j|k`` | ``i/j/k`` |; +--------+-----------------+-----------------+; | N | ``i|j|k|...|N`` | ``i/j/k/.../N`` |; +--------+-----------------+-----------------+. Parameters; ----------; s : str or :class:`.StringExpression`; String to parse. Returns; -------; :class:`.CallExpression`; """"""; return _func('Call', tcall, s). [docs]@typecheck(expression=expr_any); def is_defined(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is not missing. Examples; --------. >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is not missing, ``False`` otherwise.; """"""; return ~apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(expression=expr_any); def is_missing(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is missing. Examples; --------. >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is missing, ``False`` otherwise.; """"""; return apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def is_nan(x) -> BooleanExpression:; """"""Returns ``True`` if the argument is ``nan`` (not a number). Examples; --------. >>> hl.eval(hl.is_nan(0)",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:52057,Testability,test,test,52057,"xpression`; String to parse. Returns; -------; :class:`.CallExpression`; """"""; return _func('Call', tcall, s). [docs]@typecheck(expression=expr_any); def is_defined(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is not missing. Examples; --------. >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is not missing, ``False`` otherwise.; """"""; return ~apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(expression=expr_any); def is_missing(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is missing. Examples; --------. >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is missing, ``False`` otherwise.; """"""; return apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def is_nan(x) -> BooleanExpression:; """"""Returns ``True`` if the argument is ``nan`` (not a number). Examples; --------. >>> hl.eval(hl.is_nan(0)); False. >>> hl.eval(hl.is_nan(hl.literal(0) / 0)); True. >>> hl.eval(hl.is_nan(hl.literal(0) / hl.missing(hl.tfloat64))); None. Notes; -----; Note that :func:`~.is_missing` will return ``False`` on ``nan`` since ``nan``; is a defined value. Additionally, this method will return missing if `x` is; missing. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; Expression to test or or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.BooleanExpression`; ``True`` if `x` is ``nan``, ``False`` ot",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:52900,Testability,test,test,52900,"val(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is missing, ``False`` otherwise.; """"""; return apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def is_nan(x) -> BooleanExpression:; """"""Returns ``True`` if the argument is ``nan`` (not a number). Examples; --------. >>> hl.eval(hl.is_nan(0)); False. >>> hl.eval(hl.is_nan(hl.literal(0) / 0)); True. >>> hl.eval(hl.is_nan(hl.literal(0) / hl.missing(hl.tfloat64))); None. Notes; -----; Note that :func:`~.is_missing` will return ``False`` on ``nan`` since ``nan``; is a defined value. Additionally, this method will return missing if `x` is; missing. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; Expression to test or or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.BooleanExpression`; ``True`` if `x` is ``nan``, ``False`` otherwise or; :class:`.NDArrayNumericExpression` filled with such values; """"""; return _func(""isnan"", tbool, x). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def is_finite(x) -> BooleanExpression:; """"""Returns ``True`` if the argument is a finite floating-point number. Examples; --------; >>> hl.eval(hl.is_finite(0)); True. >>> hl.eval(hl.is_finite(float('nan'))); False. >>> hl.eval(hl.is_finite(float('inf'))); False. >>> hl.eval(hl.is_finite(hl.missing('float32'))); None. Notes; -----; This method will return missing, not ``True``, if `x` is missing. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.BooleanExpression` or :class:`.NDArrayNumericExpression` filled with such expressions; """""";",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:55872,Testability,log,log,55872,"; --------. >>> hl.eval(hl.json([1,2,3,4,5])); '[1,2,3,4,5]'. >>> hl.eval(hl.json(hl.struct(a='Hello', b=0.12345, c=[1,2], d={'hi', 'bye'}))); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters; ----------; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(10",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:55925,Testability,log,logarithm,55925,"; --------. >>> hl.eval(hl.json([1,2,3,4,5])); '[1,2,3,4,5]'. >>> hl.eval(hl.json(hl.struct(a='Hello', b=0.12345, c=[1,2], d={'hi', 'bye'}))); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters; ----------; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(10",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:55999,Testability,log,log,55999,")); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters; ----------; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56043,Testability,log,log,56043,"bye"",""hi""]}'. Parameters; ----------; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpress",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56077,Testability,log,log,56077,"-; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Ex",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56169,Testability,log,logarithm,56169,"StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArr",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56473,Testability,log,log,56473,"dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56529,Testability,log,log,56529,"dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56788,Testability,log,logarithm,56788,"(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@type",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57301,Testability,log,logit,57301,": float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; ret",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57340,Testability,log,logistic,57340,": float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; ret",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57394,Testability,log,logit,57394,"a:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typechec",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57441,Testability,log,logit,57441," is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Retu",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57706,Testability,log,log,57706,"); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Als",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57859,Testability,log,logistic,57859," `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Also; --------; :func:`.or_else`. Parameters; ----------; args : variable-length args of :cl",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:60747,Testability,test,test,60747,"rns; -------; :class:`.Expression`; """"""; a, b, success = unify_exprs(a, b); if not success:; raise TypeError(; f""'or_else' requires the 'a' and 'b' arguments to have the same type\n""; f"" a: type '{a.dtype}'\n""; f"" b: type '{b.dtype}'""; ); return coalesce(a, b). [docs]@typecheck(predicate=expr_bool, value=expr_any); def or_missing(predicate, value):; """"""Returns `value` if `predicate` is ``True``, otherwise returns missing. Examples; --------. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(h",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:60839,Testability,test,test,60839,"' requires the 'a' and 'b' arguments to have the same type\n""; f"" a: type '{a.dtype}'\n""; f"" b: type '{b.dtype}'""; ); return coalesce(a, b). [docs]@typecheck(predicate=expr_bool, value=expr_any); def or_missing(predicate, value):; """"""Returns `value` if `predicate` is ``True``, otherwise returns missing. Examples; --------. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61060,Testability,test,test,61060,"--. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` o",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61143,Testability,test,test,61143,"--. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` o",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61231,Testability,test,test,61231,"--. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` o",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61437,Testability,Test,Test,61437,"icate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a futu",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61605,Testability,Test,Test,61605,"""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a future '; 'release, please use ""two-sided"" for the `alternative` parameter '; 'to hl.binom_test'; ); alternative = 'two-sided'. alt_enum = {""two-sided"": 0, ""less"": 1, """,MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61759,Testability,Test,Test,61759," p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a future '; 'release, please use ""two-sided"" for the `alternative` parameter '; 'to hl.binom_test'; ); alternative = 'two-sided'. alt_enum = {""two-sided"": 0, ""less"": 1, ""greater"": 2}[alternative]; return _func(""binomTest"", tfloat64, x, n, p, to_expr(alt_enum)). [docs]@typecheck(x=expr_float64, df=expr_float64, ncp=nullable(exp",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:63889,Testability,log,logarithm,63889,"one, lower_tail=False, log_p=False) -> Float64Expression:; """"""Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom. Examples; --------. >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Noncentrality parameter, defaults to 0 if unspecified.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""pchisqtail"", tfloat64, x, df, lower_tail, log_p); else:; return _func(""pnchisqtail"", tfloat64, x, df, ncp, lower_tail, log_p). PGENCHISQ_RETURN_TYPE = tstruct(value=tfloat64, n_iterations=tint32, converged=tbool, fault=tint32). [docs]@typecheck(; x=expr_float64,; w=expr_array(expr_float64),; k=expr_array(expr_int32),; lam=expr_array(expr_float64),; mu=expr_float64,; sigma=expr_float64,; max_iterations=nullable(expr_int32),; min_accuracy=nullable(expr_float64),; ); def pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None) -> Float64Expression:; r""""""The cumulative probability function of a `generalized chi-squared distribution; <https://en.wikipedia.org/wiki/Generalized_chi-squared_distribution>`__. The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this dist",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68660,Testability,test,test,68660,"isq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expressi",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68703,Testability,test,tests,68703,"a=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68724,Testability,test,test,68724,"a=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68763,Testability,test,tests,68763,"a=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:73044,Testability,log,logarithm,73044," `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pnorm"", tfloat64, x, mu, sigma, lower_tail, log_p). [docs]@typecheck(x=expr_float64, n=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pT(x, n, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikipedia.org/wiki/Student%27s_t-distribution>`__ with; `n` degrees of freedom. Examples; --------. >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a t-distributed random variable with `n` degrees of freedom. If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:74363,Testability,log,logarithm,74363,"pression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikipedia.org/wiki/Student%27s_t-distribution>`__ with; `n` degrees of freedom. Examples; --------. >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a t-distributed random variable with `n` degrees of freedom. If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; n : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom of the t-distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`. """"""; return _func(""pT"", tfloat64, x, n, lower_tail, log_p). [docs]@typecheck(x=expr_float64, df1=expr_float64, df2=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pF(x, df1, df2, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `F-distribution; <https://en.wikipedia.org/wiki/F-distribution>`__ with parameters; `df1` and `df2`. Examples; --------. >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a random variable with distribution :math:`F`(df1, df2). If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:75790,Testability,log,logarithm,75790,"ia.org/wiki/F-distribution>`__ with parameters; `df1` and `df2`. Examples; --------. >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a random variable with distribution :math:`F`(df1, df2). If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; df1 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; df2 : float or :class:`.Expression` of type :py:data:`.tfloat64`; Parameter of the F-distribution; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pF"", tfloat64, x, df1, df2, lower_tail, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def ppois(x, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a Poisson distribution. Examples; --------. >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is a; Poisson random variable with rate parameter `lamb`. If `lower_tail` is false,; returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; lamb : float or :class:`.Expression` of type :py:data:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:76868,Testability,log,logarithm,76868,"ype :py:data:`.tfloat64`; """"""; return _func(""pF"", tfloat64, x, df1, df2, lower_tail, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def ppois(x, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a Poisson distribution. Examples; --------. >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is a; Poisson random variable with rate parameter `lamb`. If `lower_tail` is false,; returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; lamb : float or :class:`.Expression` of type :py:data:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""ppois"", tfloat64, x, lamb, lower_tail, log_p). [docs]@typecheck(p=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inverts :func:`~.pchisqtail`. Examples; --------. >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; -----; Returns right-quantile `x` for which `p` = Prob(:math:`Z^2` > x) with; :math:`Z^2` a chi-squared random variable with degrees of freedom specified; by `df`. The probability `p` must satisfy 0 < `p` < 1. Pa",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:77597,Testability,log,log,77597,"a:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""ppois"", tfloat64, x, lamb, lower_tail, log_p). [docs]@typecheck(p=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inverts :func:`~.pchisqtail`. Examples; --------. >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; -----; Returns right-quantile `x` for which `p` = Prob(:math:`Z^2` > x) with; :math:`Z^2` a chi-squared random variable with degrees of freedom specified; by `df`. The probability `p` must satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Corresponds to `ncp` parameter in :func:`.pchisqtail`.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pchisqtail`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""qchisqtail"", tfloat64, p, df, lower_tail, log_p); else:; return _func(""qnchisqtail"", tfloat6",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:79236,Testability,log,log,79236,"er_tail` parameter in :func:`.pchisqtail`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""qchisqtail"", tfloat64, p, df, lower_tail, log_p); else:; return _func(""qnchisqtail"", tfloat64, p, df, ncp, lower_tail, log_p). [docs]@typecheck(p=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qnorm"", tfloat64, p, mu, sigma, lower_tail, log_p). [docs]@typecheck(p=expr_float64, lamb=expr_f",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:81080,Testability,test,testing,81080,"; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qnorm"", tfloat64, p, mu, sigma, lower_tail, log_p). [docs]@typecheck(p=expr_float64, lamb=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def qpois(p, lamb, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The quantile function of a Poisson distribution with rate parameter; `lamb`, inverts :func:`~.ppois`. Examples; --------. >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; -----; Returns the smallest integer :math:`x` such that Prob(:math:`X \leq x`) :math:`\geq` `p` where :math:`X`; is a Poisson random variable with rate parameter `lambda`. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; lamb : float or :class:`.Expression` of type :py:data:`.tfloat64`; Rate parameter of Poisson distribution.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in inverse :func:`.ppois`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p` before testing. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""qpois"", tint32, p, lamb, lower_tail, log_p). [docs]@typecheck(start=expr_int32, stop=nullable(expr_int32), step=expr_int32); def range(start, stop=None, step=1) -> ArrayNumericExpression:; """"""Returns an array of integers from `start` to `stop` by `step`. Examples; --------. >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; -----; The range includes `start`, but excludes `stop`. If provided exactly one argument, the argument is interpreted as `stop` and; `start` is set to zero. This matches the behavior of Python's ``range``. Parameters; ----------; start : int or :class:`.Expression` of type :py:data:`.tint32`; Start of range.; stop : int or :class:`.Expression` of type :py:data:`.tint32`; End of range.; step : int or :class:`.Expression` of type :py:dat",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:138771,Testability,assert,assert,138771,"` or :class:`.HailType`; Type of the set elements. Returns; -------; :class:`.SetExpression`; """"""; return hl.set(empty_array(t)). [docs]@typecheck(collection=expr_oneof(expr_set(), expr_array(), expr_dict(), expr_ndarray())); def array(collection) -> ArrayExpression:; """"""Construct an array expression. Examples; --------. >>> s = {'Bob', 'Charlie', 'Alice'}. >>> hl.eval(hl.array(s)); ['Alice', 'Bob', 'Charlie']. Parameters; ----------; collection : :class:`.ArrayExpression` or :class:`.SetExpression` or :class:`.DictExpression`. Returns; -------; :class:`.ArrayExpression`; """"""; if isinstance(collection.dtype, tarray):; return collection; elif isinstance(collection.dtype, tset):; return apply_expr(lambda c: ir.CastToArray(c), tarray(collection.dtype.element_type), collection); elif isinstance(collection.dtype, tndarray):; if collection.dtype.ndim != 1:; raise ValueError(f'array: only one dimensional ndarrays are supported: {collection.dtype}'); return collection._data_array(); else:; assert isinstance(collection.dtype, tdict); return _func('dictToArray', tarray(ttuple(collection.dtype.key_type, collection.dtype.value_type)), collection). [docs]@typecheck(t=hail_type); def empty_array(t: Union[HailType, builtins.str]) -> ArrayExpression:; """"""Returns an empty array of elements of a type `t`. Examples; --------. >>> hl.eval(hl.empty_array(hl.tint32)); []. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the array elements. Returns; -------; :class:`.ArrayExpression`; """"""; array_t = hl.tarray(t); a = ir.MakeArray([], array_t); return construct_expr(a, array_t). def _ndarray(collection, row_major=None, dtype=None):; """"""Construct a Hail ndarray from either a flat Hail array, a `NumPy` ndarray or python value/nested lists. Parameters; ----------; collection : :class:`numpy.ndarray` or :obj:`numeric` or :obj: `list` of `numeric`; Type of the array elements.; row_major : :obj: `bool` or None. Returns; -------; :class:`.NDArrayExpression`; """""". def list_sh",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:176880,Testability,Test,Tests,176880,"%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; -----; See the `Java documentation <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__; for valid format specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177671,Testability,assert,assert,177671,"uple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_error('cannot shift by a negative value: ' + hl.str(x) + f"" {op} "" + hl.str(y)); ),; x,; y,; ). def _bit_op(x, y, op):; if x.dtype == hl.tint32 and y.dtype == hl.tint32:; t = hl.tint32; else:; t = hl.tint64; coercer = coercer_from_dtype(t); x = coercer.coerce(x); y = coercer.coerce(y). indices, aggregations = unify_all(x, y); return construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_oneof(expr_int32, expr_int64)); def bit_and(x, y):; """"""Bitwise and `x` and `y`. Examples; --------; >>> hl.eval(hl.bit_and(5, 3)); 1. Not",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181181,Testability,log,logical,181181,"ass:`.Int64Expression`; """"""; return _bit_op(x, y, '^'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32); def bit_lshift(x, y):; """"""Bitwise left-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); els",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181226,Testability,log,logical,181226,"of(expr_int32, expr_int64), y=expr_int32); def bit_lshift(x, y):; """"""Bitwise left-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_in",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181345,Testability,log,logical,181345,"al(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181442,Testability,log,logical,181442,"e fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181530,Testability,log,logical,181530,"ed:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181576,Testability,log,logical,181576,"bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :clas",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181653,Testability,log,logical,181653,".bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('~', x._ir), x.dtype, x._i",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181693,Testability,log,logical,181693,".bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('~', x._ir), x.dtype, x._i",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:182006,Testability,log,logical,182006,"ns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('~', x._ir), x.dtype, x._indices, x._aggregations). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_count(x):; """"""Count the number of 1s in the in the `two's complement <https://en.wikipedia.org/wiki/Two%27s_complement>`__ binary representation of `x`. Examples; --------; The binary representation of `7` is `111`, so:. ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:182111,Testability,log,logical,182111,"oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('~', x._ir), x.dtype, x._indices, x._aggregations). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_count(x):; """"""Count the number of 1s in the in the `two's complement <https://en.wikipedia.org/wiki/Two%27s_complement>`__ binary representation of `x`. Examples; --------; The binary representation of `7` is `111`, so:. >>> hl.eval(hl.bit_count(7)); 3. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:184956,Testability,assert,assert,184956,"lement in `array` not smaller; than `elem`. This is a value between 0 and the length of `array`, inclusive; (if all elements in `array` are smaller than `elem`, the returned value is; the length of `array` or the index of the first missing value, if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""; c = coercer_from_dtype(array.dtype.element_type); if not c.can_coerce(elem.dtype):; raise TypeError(; f""'binary_search': cannot search an array of type {array.dtype} for a value of type {elem.dtype}""; ); elem = c.coerce(elem); return hl.switch(elem).when_missing(hl.missing(hl.tint32)).default(_lower_bound(array, elem)). @typecheck(s=expr_str); def _escape_string(s):; return _func(""escapeString"", hl.tstr, s). @typecheck(left=expr_any, right=expr_any, tolerance=expr_float64, absolute=expr_bool); def _values_similar(left, right, tolerance=1e-6, absolute=False):; assert left.dtype == right.dtype; return (is_missing(left) & is_missing(right)) | (; (is_defined(left) & is_defined(right)) & _func(""valuesSimilar"", hl.tbool, left, right, tolerance, absolute); ). @typecheck(coords=expr_array(expr_array(expr_float64)), radius=expr_float64); def _locus_windows_per_contig(coords, radius):; rt = hl.ttuple(hl.tarray(hl.tint32), hl.tarray(hl.tint32)); return _func(""locus_windows_per_contig"", rt, coords, radius). [docs]@typecheck(a=expr_array(), seed=nullable(builtins.int)); def shuffle(a, seed: Optional[builtins.int] = None) -> ArrayExpression:; """"""Randomly permute an array. Example; -------. >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters; ----------; a : :class:`.ArrayExpression`; Array to permute.; seed : :obj:`int`, optional; Random seed. Returns; -------; :class:`.ArrayExpression`; """"""; return sorted(a, key=lambda _: hl.rand_unif(0",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57,Usability,Feedback,Feedback,57,"﻿. Hail | ; hail.expr.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.functions. Source code for hail.expr.functions; import builtins; import functools; import itertools; import operator; from typing import Any, Callable, Iterable, Optional, TypeVar, Union. import numpy as np; import pandas as pd; from deprecated import deprecated. import hail; import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; ArrayNumericExpression,; BooleanExpression,; CallExpression,; DictExpression,; Expression,; ExpressionException,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; IntervalExpression,; LocusExpression,; NumericExpression,; SetExpression,; StreamExpression,; StringExpression,; StructExpression,; TupleExpression,; apply_expr,; cast_expr,; coercer_from_dtype,; construct_expr,; construct_variable,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_dict,; expr_float32,; expr_float64,; expr_int32,; expr_int64,; expr_interval,; expr_locus,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_stream,; expr_struct,; expr_tuple,; impute_type,; to_expr,; unify_all,; unify_exprs,; unify_types_limited,; ); from hail.expr.types import (; HailType,; hail_type,; is_float32,; is_float64,; is_int32,; is_int64,; is_numeric,; is_primitive,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; trngstate,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.genetics.allele_type import AlleleType; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typec",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/functions.html:267,Usability,Guid,Guides,267,"﻿. Hail | ; hail.expr.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.functions. Source code for hail.expr.functions; import builtins; import functools; import itertools; import operator; from typing import Any, Callable, Iterable, Optional, TypeVar, Union. import numpy as np; import pandas as pd; from deprecated import deprecated. import hail; import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; ArrayNumericExpression,; BooleanExpression,; CallExpression,; DictExpression,; Expression,; ExpressionException,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; IntervalExpression,; LocusExpression,; NumericExpression,; SetExpression,; StreamExpression,; StringExpression,; StructExpression,; TupleExpression,; apply_expr,; cast_expr,; coercer_from_dtype,; construct_expr,; construct_variable,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_dict,; expr_float32,; expr_float64,; expr_int32,; expr_int64,; expr_interval,; expr_locus,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_stream,; expr_struct,; expr_tuple,; impute_type,; to_expr,; unify_all,; unify_exprs,; unify_types_limited,; ); from hail.expr.types import (; HailType,; hail_type,; is_float32,; is_float64,; is_int32,; is_int64,; is_numeric,; is_primitive,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; trngstate,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.genetics.allele_type import AlleleType; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typec",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:153,Deployability,Install,Installation,153,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:221,Deployability,Configurat,Configuration,221,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:34727,Deployability,update,update,34727,"ich_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two f",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:34770,Deployability,update,update,34770,"it == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:35212,Deployability,update,update,35212,"th - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}).",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:35255,Deployability,update,update,35255,"te |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _ge",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:56984,Deployability,patch,patch,56984,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:57038,Deployability,update,updated,57038,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:28604,Integrability,wrap,wrapper,28604,"eturn isinstance(other, tdict) and self.key_type == other.key_type and self.value_type == other.value_type. def _pretty(self, b, indent, increment):; b.append('dict<'); self.key_type._pretty(b, indent, increment); b.append(', '); self.value_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Dict[{},{}]"".format(self.key_type._parsable_string(), self.value_type._parsable_string()). def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[dict, frozendict]:; d = {; self.key_type._convert_from_json_na(elt['key'], _should_freeze=True): self.value_type._convert_from_json_na(; elt['value'], _should_freeze=_should_freeze; ); for elt in x; }; if _should_freeze:; return frozendict(d); return d. def _convert_to_json(self, x):; return [; {'key': self.key_type._convert_to_json(k), 'value': self.value_type._convert_to_json(v)}; for k, v in x.items(); ]. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:221,Modifiability,Config,Configuration,221,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:14412,Modifiability,parameteriz,parameterized,14412,"_tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return bool. def _byte_size(self):; return 1. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> bool:; return byte_reader.read_bool(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_bool(value). class _trngstate(HailType):; def __init__(self):; super(_trngstate, self).__init__(). def __str__(self):; return ""rng_state"". def _eq(self, other):; return isinstance(other, _trngstate). def _parsable_string(self):; return ""RNGState"". def unify(self, t):; return t == trngstate. def subst(self):; return self. def clear(self):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert isinstance(self._ndim, NatLiteral), ""tndarray must be realized with a concrete number of dimensions""; return self._ndim.n. def _traverse(self, obj, f):; if f(self, obj):; for elt in np.nditer(obj, ['zerosize_ok']):; self.element_type._traverse(elt.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:18290,Modifiability,variab,variable-length,18290,"ndim)]; total_num_elements = np.product(shape, dtype=np.int64). if self.element_type in _numeric_types:; element_byte_size = self.element_type._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shape=shape, buffer=np.array(elements, dtype=np_type), dtype=np_type, order=""F""). def _convert_to_encoding(self, byte_writer, value: np.ndarray):; for dim in value.shape:; byte_writer.write_int64(dim). if value.size > 0:; if self.element_type in _numeric_types:; byte_writer.write_bytes(value.data); else:; for elem in np.nditer(value, order='F'):; self.element_type._convert_to_encoding(byte_writer, elem). [docs]class tarray(HailType):; """"""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; super(tarray, self).__init__(). @property; def element_type(self):; """"""Array element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not isinstance(annotation, Sequence):; raise TypeError(""type 'array' expected Python 'list', but found typ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:18440,Modifiability,parameteriz,parameterized,18440,"pe._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shape=shape, buffer=np.array(elements, dtype=np_type), dtype=np_type, order=""F""). def _convert_to_encoding(self, byte_writer, value: np.ndarray):; for dim in value.shape:; byte_writer.write_int64(dim). if value.size > 0:; if self.element_type in _numeric_types:; byte_writer.write_bytes(value.data); else:; for elem in np.nditer(value, order='F'):; self.element_type._convert_to_encoding(byte_writer, elem). [docs]class tarray(HailType):; """"""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; super(tarray, self).__init__(). @property; def element_type(self):; """"""Array element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not isinstance(annotation, Sequence):; raise TypeError(""type 'array' expected Python 'list', but found type '%s'"" % type(annotation)). def __str__(self):; return ""array<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:23288,Modifiability,parameteriz,parameterized,23288,"urn ""Stream["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tstream) and self.element_type.unify(t.element_type). def subst(self):; return tstream(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def is_setlike(maybe_setlike):; return isinstance(maybe_setlike, (set, frozenset)). [docs]class tset(HailType):; """"""Hail type for collections of distinct elements. In Python, these are represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; self._array_repr = tarray(element_type); super(tset, self).__init__(). @property; def element_type(self):; """"""Set element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_setlike(annotation):; raise TypeError(""type 'set' expected Python 'set', but found type '%s'"" % type(annotation)). def __str__(self):; return ""set<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tse",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:26159,Modifiability,parameteriz,parameterize,26159,"ncoding(byte_writer, list(value)). def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tset) and self.element_type.unify(t.element_type). def subst(self):; return tset(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). class _freeze_this_type(HailType):; def __init__(self, t):; self.t = t. def _convert_from_json_na(self, x, _should_freeze: bool = False):; return self.t._convert_from_json_na(x, _should_freeze=True). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; return self.t._convert_from_encoding(byte_reader, _should_freeze=True). def _convert_to_encoding(self, byte_writer, x):; return self.t._convert_to_encoding(byte_writer, x). [docs]class tdict(HailType):; """"""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """""". @typecheck_method(key_type=hail_type, value_type=hail_type); def __init__(self, key_type, value_type):; self._key_type = key_type; self._value_type = value_type; self._array_repr = tarray(tstruct(key=_freeze_this_type(key_type), value=value_type)); super(tdict, self).__init__(). @property; def key_type(self):; """"""Dict key type. Returns; -------; :class:`.HailType`; Key type.; """"""; return self._key_type. @property; def value_type(self):; """"""Dict value type. Returns; -------; :class:`.HailType`; Value type.; """"""; return self._value_type. @property; def element_type(self):; return tstruct(key=self._key_type, value=self._value_type). def _traverse(self, obj, f):; if f(self, obj):; for k, v in obj.items():; self.key_type._traverse(k,",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:5224,Performance,load,loads,5224,"formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._convert_to_json(x). def _convert_to_json(self, x):; return x. def _from_json(self, s):; x = json.loads(s); return self._convert_from_json_na(x). def _convert_from_json_na(self, x, _should_freeze: bool = False):; if x is None:; return x; else:; return self._convert_from_json(x, _should_freeze). def _convert_from_json(self, x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the f",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:4154,Security,hash,hash,4154,"ferences) == 0. def _to_json_context(self):; if self._json is None:; self._json = {'reference_genomes': {r: hl.get_reference(r)._config for r in self.references}}; return self._json. @classmethod; def union(cls, *types):; ctxs = [t.get_context() for t in types if not t.get_context().is_empty]; if len(ctxs) == 0:; return _empty_context; if len(ctxs) == 1:; return ctxs[0]; refs = ctxs[0].references.union(*[ctx.references for ctx in ctxs[1:]]); return HailTypeContext(refs). _empty_context = HailTypeContext(). [docs]class HailType(object):; """"""; Hail type superclass.; """""". def __init__(self):; super(HailType, self).__init__(); self._context = None. def __repr__(self):; s = str(self).replace(""'"", ""\\'""); return ""dtype('{}')"".format(s). @abc.abstractmethod; def _eq(self, other):; raise NotImplementedError. def __eq__(self, other):; return isinstance(other, HailType) and self._eq(other). @abc.abstractmethod; def __str__(self):; raise NotImplementedError. def __hash__(self):; # FIXME this is a bit weird; return 43 + hash(str(self)). def pretty(self, indent=0, increment=4):; """"""Returns a prettily formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._conv",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:375,Testability,Log,Log,375,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:15147,Testability,assert,assert,15147,"lf):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert isinstance(self._ndim, NatLiteral), ""tndarray must be realized with a concrete number of dimensions""; return self._ndim.n. def _traverse(self, obj, f):; if f(self, obj):; for elt in np.nditer(obj, ['zerosize_ok']):; self.element_type._traverse(elt.item(), f). def _typecheck_one_level(self, annotation):; if annotation is not None and not isinstance(annotation, np.ndarray):; raise TypeError(""type 'ndarray' expected Python 'numpy.ndarray', but found type '%s'"" % type(annotation)). def __str__(self):; return ""ndarray<{}, {}>"".format(self.element_type, self.ndim). def _eq(self, other):; return isinstance(other, tndarray) and self.element_type == other.element_type and self.ndim == other.ndim. def _pretty(self, b, indent, increment):; b.append('ndarray<'); self._element_type._pretty(b, indent, increment); b.append(', '); b.append(str(self.ndim)); b.append('>'). def _parsable_string(self):; return f'NDArray[{self._element_type._parsable_string()},{self.ndim}]'. def _convert_from_json(self, x, ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:41865,Testability,assert,assert,41865,"ng_byte, which_missing_bit):; answer.append(None); else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); answer.append(field_decoded). return tuple(answer). def _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_pair(2, 7),; allele_pair(3, 7),; allele_pair(4, 7),; allele_pair(5, 7),; allele_pair(6, 7),; allele_pair(7, 7),; ]. class _tcall(Ha",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:41896,Testability,assert,assert,41896,"ng_byte, which_missing_bit):; answer.append(None); else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); answer.append(field_decoded). return tuple(answer). def _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_pair(2, 7),; allele_pair(3, 7),; allele_pair(4, 7),; allele_pair(5, 7),; allele_pair(6, 7),; allele_pair(7, 7),; ]. class _tcall(Ha",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:42023,Testability,assert,assert,42023," _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_pair(2, 7),; allele_pair(3, 7),; allele_pair(4, 7),; allele_pair(5, 7),; allele_pair(6, 7),; allele_pair(7, 7),; ]. class _tcall(HailType):; """"""Hail type for a diploid genotype. In Python, these are represented by :class:`.Call`.; """""". def __init__(self):; super(_tcall, self).__init__(). def _typecheck_one_level(s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:42094,Testability,assert,assert,42094," _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_pair(2, 7),; allele_pair(3, 7),; allele_pair(4, 7),; allele_pair(5, 7),; allele_pair(6, 7),; allele_pair(7, 7),; ]. class _tcall(HailType):; """"""Hail type for a diploid genotype. In Python, these are represented by :class:`.Call`.; """""". def __init__(self):; super(_tcall, self).__init__(). def _typecheck_one_level(s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:44262,Testability,assert,assert,44262," return isinstance(other, _tcall). def _parsable_string(self):; return ""Call"". def _convert_from_json(self, x, _should_freeze: bool = False) -> genetics.Call:; if x == '-':; return genetics.Call([]); if x == '|-':; return genetics.Call([], phased=True); if x[0] == '|':; return genetics.Call([int(x[1:])], phased=True). n = len(x); i = 0; while i < n:; c = x[i]; if c in '|/':; break; i += 1. if i == n:; return genetics.Call([int(x)]). return genetics.Call([int(x[0:i]), int(x[i + 1 :])], phased=(c == '|')). def _convert_to_json(self, x):; return str(x). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> genetics.Call:; int_rep = byte_reader.read_int32(); int_rep = int_rep if int_rep >= 0 else int_rep + 2**32. ploidy = (int_rep >> 1) & 0x3; phased = (int_rep & 1) == 1. def allele_repr(c):; return c >> 3. def ap_j(p):; return p & 0xFFFF. def ap_k(p):; return (p >> 16) & 0xFFFF. def gt_allele_pair(i):; assert i >= 0, ""allele pair value should never be negative""; if i < len(small_allele_pair):; return small_allele_pair[i]; return allele_pair_sqrt(i). def call_allele_pair(i):; if phased:; rep = allele_repr(i); p = gt_allele_pair(rep); j = ap_j(p); k = ap_k(p); return allele_pair(j, k - j); else:; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploi",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:45042,Testability,assert,assert,45042," 0 else int_rep + 2**32. ploidy = (int_rep >> 1) & 0x3; phased = (int_rep & 1) == 1. def allele_repr(c):; return c >> 3. def ap_j(p):; return p & 0xFFFF. def ap_k(p):; return (p >> 16) & 0xFFFF. def gt_allele_pair(i):; assert i >= 0, ""allele pair value should never be negative""; if i < len(small_allele_pair):; return small_allele_pair[i]; return allele_pair_sqrt(i). def call_allele_pair(i):; if phased:; rep = allele_repr(i); p = gt_allele_pair(rep); j = ap_j(p); k = ap_k(p); return allele_pair(j, k - j); else:; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploidy == 2:; int_rep |= allele_pair_rep(value) << 3; int_rep = int_rep if 0 <= int_rep < 2**31 - 1 else int_rep - 2**32. byte_writer.write_int32(int_rep). def unify(self, t):; return t == tcall. def subst(self):; return self. def clear(self):; pass. [docs]class tlocus(HailType):; """"""Hail type for a genomic coordinate with a contig and a position. In Python, these are represented by :class:`.Locus`. Parameters; ----------; reference_genome: :class:`.ReferenceGenome` or :class:`str`; Reference genome to use. See Also; --------; :class:`.LocusExpression`, :func:`.locus`, :func:`.parse_locus`,; :class:`.Locus`; """""". struct_repr = tstruct(contig=_tstr(), pos=_tint32()). @classmethod; @typecheck_method(reference_genome=",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:45226,Testability,assert,assert,45226,"FFFF. def gt_allele_pair(i):; assert i >= 0, ""allele pair value should never be negative""; if i < len(small_allele_pair):; return small_allele_pair[i]; return allele_pair_sqrt(i). def call_allele_pair(i):; if phased:; rep = allele_repr(i); p = gt_allele_pair(rep); j = ap_j(p); k = ap_k(p); return allele_pair(j, k - j); else:; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploidy == 2:; int_rep |= allele_pair_rep(value) << 3; int_rep = int_rep if 0 <= int_rep < 2**31 - 1 else int_rep - 2**32. byte_writer.write_int32(int_rep). def unify(self, t):; return t == tcall. def subst(self):; return self. def clear(self):; pass. [docs]class tlocus(HailType):; """"""Hail type for a genomic coordinate with a contig and a position. In Python, these are represented by :class:`.Locus`. Parameters; ----------; reference_genome: :class:`.ReferenceGenome` or :class:`str`; Reference genome to use. See Also; --------; :class:`.LocusExpression`, :func:`.locus`, :func:`.parse_locus`,; :class:`.Locus`; """""". struct_repr = tstruct(contig=_tstr(), pos=_tint32()). @classmethod; @typecheck_method(reference_genome=nullable(reference_genome_type)); def _schema_from_rg(cls, reference_genome='default'):; # must match TLocus.schemaFromRG; if reference_genome is None:; return hl.tstruct(contig=hl.tstr, p",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:51911,Testability,assert,assert,51911,"terval_as_struct.includes_end,; point_type=self.point_type,; ). def _convert_to_encoding(self, byte_writer, value):; interval_dict = {; 'start': value.start,; 'end': value.end,; 'includes_start': value.includes_start,; 'includes_end': value.includes_end,; }; self._struct_repr._convert_to_encoding(byte_writer, interval_dict). def unify(self, t):; return isinstance(t, tinterval) and self.point_type.unify(t.point_type). def subst(self):; return tinterval(self.point_type.subst()). def clear(self):; self.point_type.clear(). def _get_context(self):; return self.point_type.get_context(). class Box(object):; named_boxes: ClassVar = {}. @staticmethod; def from_name(name):; if name in Box.named_boxes:; return Box.named_boxes[name]; b = Box(); Box.named_boxes[name] = b; return b. def __init__(self):; pass. def unify(self, v):; if hasattr(self, 'value'):; return self.value == v; self.value = v; return True. def clear(self):; if hasattr(self, 'value'):; del self.value. def get(self):; assert hasattr(self, 'value'); return self.value. tvoid = _tvoid(). tint32 = _tint32(); """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int32Expression`, :func:`.int`, :func:`.int32`; """""". tint64 = _tint64(); """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int64Expression`, :func:`.int64`; """""". tint = tint32; """"""Alias for :py:data:`.tint32`."""""". tfloat32 = _tfloat32(); """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`. See Also; --------; :class:`.Float32Expression`, :func:`.float64`; """""". tfloat64 = _tfloat64(); """"""Hail type for 64-bit floating point numbers. In Python, these are represented as :obj:`float`. See Also; --------; :class:`.Float64Expressio",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:53,Usability,Feedback,Feedback,53,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:263,Usability,Guid,Guides,263,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:6441,Usability,clear,clear,6441,"x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the function returns ``True``.; """"""; f(self, obj). @abc.abstractmethod; def unify(self, t):; raise NotImplementedError. @abc.abstractmethod; def subst(self):; raise NotImplementedError. @abc.abstractmethod; def clear(self):; raise NotImplementedError. def _get_context(self):; return _empty_context. def get_context(self):; if self._context is None:; self._context = self._get_context(); return self._context. def to_numpy(self):; return object. hail_type = oneof(HailType, transformed((str, dtype)), type(None)). class _tvoid(HailType):; def __init__(self):; super(_tvoid, self).__init__(). def __str__(self):; return ""void"". def _eq(self, other):; return isinstance(other, _tvoid). def _parsable_string(self):; return ""Void"". def unify(self, t):; return t == tvoid. def subst(self):; return self. def clear(self):; pass. def _convert_from_encoding(self, *_):; raise ValueError(""Cannot decode void type""). def _convert_to_encoding(self, *_):; raise ValueError(""Cannot encode void type""). class _tint32(HailType):; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`.; """""". def __init__(s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:7033,Usability,clear,clear,7033,"f, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the function returns ``True``.; """"""; f(self, obj). @abc.abstractmethod; def unify(self, t):; raise NotImplementedError. @abc.abstractmethod; def subst(self):; raise NotImplementedError. @abc.abstractmethod; def clear(self):; raise NotImplementedError. def _get_context(self):; return _empty_context. def get_context(self):; if self._context is None:; self._context = self._get_context(); return self._context. def to_numpy(self):; return object. hail_type = oneof(HailType, transformed((str, dtype)), type(None)). class _tvoid(HailType):; def __init__(self):; super(_tvoid, self).__init__(). def __str__(self):; return ""void"". def _eq(self, other):; return isinstance(other, _tvoid). def _parsable_string(self):; return ""Void"". def unify(self, t):; return t == tvoid. def subst(self):; return self. def clear(self):; pass. def _convert_from_encoding(self, *_):; raise ValueError(""Cannot decode void type""). def _convert_to_encoding(self, *_):; raise ValueError(""Cannot encode void type""). class _tint32(HailType):; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`.; """""". def __init__(self):; super(_tint32, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_int32(annotation):; raise TypeError(""type 'tint32' expected Python 'int', but found type '%s'"" % type(annotation)); elif not self.min_value <= annotation <= self.max_value:; raise TypeError(; f""Value out of range for 32-bit integer: ""; f""expected [{self.min_value}, {self.max_value}], found {annotation}""; ). def __str__(self):; return ""int32"". def _eq(self, other):; return isinstance(other, _tint32). def _parsable_string(self):; return ""Int32"". @property; def min",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:8205,Usability,clear,clear,8205,"ype""). class _tint32(HailType):; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`.; """""". def __init__(self):; super(_tint32, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_int32(annotation):; raise TypeError(""type 'tint32' expected Python 'int', but found type '%s'"" % type(annotation)); elif not self.min_value <= annotation <= self.max_value:; raise TypeError(; f""Value out of range for 32-bit integer: ""; f""expected [{self.min_value}, {self.max_value}], found {annotation}""; ). def __str__(self):; return ""int32"". def _eq(self, other):; return isinstance(other, _tint32). def _parsable_string(self):; return ""Int32"". @property; def min_value(self):; return -(1 << 31). @property; def max_value(self):; return (1 << 31) - 1. def unify(self, t):; return t == tint32. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.int32. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> int:; return byte_reader.read_int32(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_int32(value). def _byte_size(self):; return 4. class _tint64(HailType):; """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`.; """""". def __init__(self):; super(_tint64, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_int64(annotation):; raise TypeError(""type 'int64' expected Python 'int', but found type '%s'"" % type(annotation)); if not self.min_value <= annotation <= self.max_value:; raise TypeError(; f""Value out of range for 64-bit integer: ""; f""expected [{self.min_value}, {self.max_value}], found {annotation}""; ). def __str__(self):; return ""int64"". def _eq(self, other):; return isin",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:9464,Usability,clear,clear,9464,"alue). def _byte_size(self):; return 4. class _tint64(HailType):; """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`.; """""". def __init__(self):; super(_tint64, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_int64(annotation):; raise TypeError(""type 'int64' expected Python 'int', but found type '%s'"" % type(annotation)); if not self.min_value <= annotation <= self.max_value:; raise TypeError(; f""Value out of range for 64-bit integer: ""; f""expected [{self.min_value}, {self.max_value}], found {annotation}""; ). def __str__(self):; return ""int64"". def _eq(self, other):; return isinstance(other, _tint64). def _parsable_string(self):; return ""Int64"". @property; def min_value(self):; return -(1 << 63). @property; def max_value(self):; return (1 << 63) - 1. def unify(self, t):; return t == tint64. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.int64. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> int:; return byte_reader.read_int64(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_int64(value). def _byte_size(self):; return 8. class _tfloat32(HailType):; """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`.; """""". def __init__(self):; super(_tfloat32, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not is_float32(annotation):; raise TypeError(""type 'float32' expected Python 'float', but found type '%s'"" % type(annotation)). def __str__(self):; return ""float32"". def _eq(self, other):; return isinstance(other, _tfloat32). def _parsable_string(self):; return ""Float32"". def _convert_from_json(self, x, _should_freeze: bool = False):; return float(x). def _convert_to_json(self, x):; if math.isfinite(x):; return x; else:; return str(x)",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:10772,Usability,clear,clear,10772,"loat32(HailType):; """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`.; """""". def __init__(self):; super(_tfloat32, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not is_float32(annotation):; raise TypeError(""type 'float32' expected Python 'float', but found type '%s'"" % type(annotation)). def __str__(self):; return ""float32"". def _eq(self, other):; return isinstance(other, _tfloat32). def _parsable_string(self):; return ""Float32"". def _convert_from_json(self, x, _should_freeze: bool = False):; return float(x). def _convert_to_json(self, x):; if math.isfinite(x):; return x; else:; return str(x). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> float:; return byte_reader.read_float32(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_float32(value). def unify(self, t):; return t == tfloat32. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.float32. def _byte_size(self):; return 4. class _tfloat64(HailType):; """"""Hail type for 64-bit floating point numbers. In Python, these are represented as :obj:`float`.; """""". def __init__(self):; super(_tfloat64, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not is_float64(annotation):; raise TypeError(""type 'float64' expected Python 'float', but found type '%s'"" % type(annotation)). def __str__(self):; return ""float64"". def _eq(self, other):; return isinstance(other, _tfloat64). def _parsable_string(self):; return ""Float64"". def _convert_from_json(self, x, _should_freeze: bool = False):; return float(x). def _convert_to_json(self, x):; if math.isfinite(x):; return x; else:; return str(x). def unify(self, t):; return t == tfloat64. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.float64. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = Fals",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:11646,Usability,clear,clear,11646,"lue):; byte_writer.write_float32(value). def unify(self, t):; return t == tfloat32. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.float32. def _byte_size(self):; return 4. class _tfloat64(HailType):; """"""Hail type for 64-bit floating point numbers. In Python, these are represented as :obj:`float`.; """""". def __init__(self):; super(_tfloat64, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not is_float64(annotation):; raise TypeError(""type 'float64' expected Python 'float', but found type '%s'"" % type(annotation)). def __str__(self):; return ""float64"". def _eq(self, other):; return isinstance(other, _tfloat64). def _parsable_string(self):; return ""Float64"". def _convert_from_json(self, x, _should_freeze: bool = False):; return float(x). def _convert_to_json(self, x):; if math.isfinite(x):; return x; else:; return str(x). def unify(self, t):; return t == tfloat64. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.float64. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> float:; return byte_reader.read_float64(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_float64(value). def _byte_size(self):; return 8. class _tstr(HailType):; """"""Hail type for text strings. In Python, these are represented as strings.; """""". def __init__(self):; super(_tstr, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation and not isinstance(annotation, str):; raise TypeError(""type 'str' expected Python 'str', but found type '%s'"" % type(annotation)). def __str__(self):; return ""str"". def _eq(self, other):; return isinstance(other, _tstr). def _parsable_string(self):; return ""String"". def unify(self, t):; return t == tstr. def subst(self):; return self. def clear(self):; pass. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> str:; length = byte_reader.read_int3",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:12518,Usability,clear,clear,12518,"inite(x):; return x; else:; return str(x). def unify(self, t):; return t == tfloat64. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return np.float64. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> float:; return byte_reader.read_float64(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_float64(value). def _byte_size(self):; return 8. class _tstr(HailType):; """"""Hail type for text strings. In Python, these are represented as strings.; """""". def __init__(self):; super(_tstr, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation and not isinstance(annotation, str):; raise TypeError(""type 'str' expected Python 'str', but found type '%s'"" % type(annotation)). def __str__(self):; return ""str"". def _eq(self, other):; return isinstance(other, _tstr). def _parsable_string(self):; return ""String"". def unify(self, t):; return t == tstr. def subst(self):; return self. def clear(self):; pass. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> str:; length = byte_reader.read_int32(); str_literal = byte_reader.read_bytes(length).decode('utf-8'). return str_literal. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; value_bytes = value.encode('utf-8'); byte_writer.write_int32(len(value_bytes)); byte_writer.write_bytes(value_bytes). class _tbool(HailType):; """"""Hail type for Boolean (``True`` or ``False``) values. In Python, these are represented as :obj:`bool`.; """""". def __init__(self):; super(_tbool, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not isinstance(annotation, bool):; raise TypeError(""type 'bool' expected Python 'bool', but found type '%s'"" % type(annotation)). def __str__(self):; return ""bool"". def _eq(self, other):; return isinstance(other, _tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return sel",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:13532,Usability,clear,clear,13532,"ef _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> str:; length = byte_reader.read_int32(); str_literal = byte_reader.read_bytes(length).decode('utf-8'). return str_literal. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; value_bytes = value.encode('utf-8'); byte_writer.write_int32(len(value_bytes)); byte_writer.write_bytes(value_bytes). class _tbool(HailType):; """"""Hail type for Boolean (``True`` or ``False``) values. In Python, these are represented as :obj:`bool`.; """""". def __init__(self):; super(_tbool, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None and not isinstance(annotation, bool):; raise TypeError(""type 'bool' expected Python 'bool', but found type '%s'"" % type(annotation)). def __str__(self):; return ""bool"". def _eq(self, other):; return isinstance(other, _tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return bool. def _byte_size(self):; return 1. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> bool:; return byte_reader.read_bool(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_bool(value). class _trngstate(HailType):; def __init__(self):; super(_trngstate, self).__init__(). def __str__(self):; return ""rng_state"". def _eq(self, other):; return isinstance(other, _trngstate). def _parsable_string(self):; return ""RNGState"". def unify(self, t):; return t == trngstate. def subst(self):; return self. def clear(self):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:14147,Usability,clear,clear,14147,"):; if annotation is not None and not isinstance(annotation, bool):; raise TypeError(""type 'bool' expected Python 'bool', but found type '%s'"" % type(annotation)). def __str__(self):; return ""bool"". def _eq(self, other):; return isinstance(other, _tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return bool. def _byte_size(self):; return 1. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> bool:; return byte_reader.read_bool(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_bool(value). class _trngstate(HailType):; def __init__(self):; super(_trngstate, self).__init__(). def __str__(self):; return ""rng_state"". def _eq(self, other):; return isinstance(other, _trngstate). def _parsable_string(self):; return ""RNGState"". def unify(self, t):; return t == trngstate. def subst(self):; return self. def clear(self):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:16798,Usability,clear,clear,16798,"her.element_type and self.ndim == other.ndim. def _pretty(self, b, indent, increment):; b.append('ndarray<'); self._element_type._pretty(b, indent, increment); b.append(', '); b.append(str(self.ndim)); b.append('>'). def _parsable_string(self):; return f'NDArray[{self._element_type._parsable_string()},{self.ndim}]'. def _convert_from_json(self, x, _should_freeze: bool = False) -> np.ndarray:; if is_numeric(self._element_type):; np_type = self.element_type.to_numpy(); return np.ndarray(shape=x['shape'], buffer=np.array(x['data'], dtype=np_type), dtype=np_type); else:; raise TypeError(""Hail cannot currently return ndarrays of non-numeric or boolean type.""). def _convert_to_json(self, x):; data = x.flatten(""C"").tolist(). strides = []; axis_one_step_byte_size = x.itemsize; for dimension_size in x.shape:; strides.append(axis_one_step_byte_size); axis_one_step_byte_size *= dimension_size if dimension_size > 0 else 1. json_dict = {""shape"": x.shape, ""data"": data}; return json_dict. def clear(self):; self._element_type.clear(); self._ndim.clear(). def unify(self, t):; return isinstance(t, tndarray) and self._element_type.unify(t._element_type) and self._ndim.unify(t._ndim). def subst(self):; return tndarray(self._element_type.subst(), self._ndim.subst()). def _get_context(self):; return self.element_type.get_context(). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> np.ndarray:; shape = [byte_reader.read_int64() for i in range(self.ndim)]; total_num_elements = np.product(shape, dtype=np.int64). if self.element_type in _numeric_types:; element_byte_size = self.element_type._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.t",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:16831,Usability,clear,clear,16831," other.ndim. def _pretty(self, b, indent, increment):; b.append('ndarray<'); self._element_type._pretty(b, indent, increment); b.append(', '); b.append(str(self.ndim)); b.append('>'). def _parsable_string(self):; return f'NDArray[{self._element_type._parsable_string()},{self.ndim}]'. def _convert_from_json(self, x, _should_freeze: bool = False) -> np.ndarray:; if is_numeric(self._element_type):; np_type = self.element_type.to_numpy(); return np.ndarray(shape=x['shape'], buffer=np.array(x['data'], dtype=np_type), dtype=np_type); else:; raise TypeError(""Hail cannot currently return ndarrays of non-numeric or boolean type.""). def _convert_to_json(self, x):; data = x.flatten(""C"").tolist(). strides = []; axis_one_step_byte_size = x.itemsize; for dimension_size in x.shape:; strides.append(axis_one_step_byte_size); axis_one_step_byte_size *= dimension_size if dimension_size > 0 else 1. json_dict = {""shape"": x.shape, ""data"": data}; return json_dict. def clear(self):; self._element_type.clear(); self._ndim.clear(). def unify(self, t):; return isinstance(t, tndarray) and self._element_type.unify(t._element_type) and self._ndim.unify(t._ndim). def subst(self):; return tndarray(self._element_type.subst(), self._ndim.subst()). def _get_context(self):; return self.element_type.get_context(). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> np.ndarray:; shape = [byte_reader.read_int64() for i in range(self.ndim)]; total_num_elements = np.product(shape, dtype=np.int64). if self.element_type in _numeric_types:; element_byte_size = self.element_type._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shap",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:16851,Usability,clear,clear,16851,"_pretty(self, b, indent, increment):; b.append('ndarray<'); self._element_type._pretty(b, indent, increment); b.append(', '); b.append(str(self.ndim)); b.append('>'). def _parsable_string(self):; return f'NDArray[{self._element_type._parsable_string()},{self.ndim}]'. def _convert_from_json(self, x, _should_freeze: bool = False) -> np.ndarray:; if is_numeric(self._element_type):; np_type = self.element_type.to_numpy(); return np.ndarray(shape=x['shape'], buffer=np.array(x['data'], dtype=np_type), dtype=np_type); else:; raise TypeError(""Hail cannot currently return ndarrays of non-numeric or boolean type.""). def _convert_to_json(self, x):; data = x.flatten(""C"").tolist(). strides = []; axis_one_step_byte_size = x.itemsize; for dimension_size in x.shape:; strides.append(axis_one_step_byte_size); axis_one_step_byte_size *= dimension_size if dimension_size > 0 else 1. json_dict = {""shape"": x.shape, ""data"": data}; return json_dict. def clear(self):; self._element_type.clear(); self._ndim.clear(). def unify(self, t):; return isinstance(t, tndarray) and self._element_type.unify(t._element_type) and self._ndim.unify(t._ndim). def subst(self):; return tndarray(self._element_type.subst(), self._ndim.subst()). def _get_context(self):; return self.element_type.get_context(). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> np.ndarray:; shape = [byte_reader.read_int64() for i in range(self.ndim)]; total_num_elements = np.product(shape, dtype=np.int64). if self.element_type in _numeric_types:; element_byte_size = self.element_type._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shape=shape, buffer=n",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:20269,Usability,clear,clear,20269,"und type '%s'"" % type(annotation)). def __str__(self):; return ""array<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tarray) and self.element_type == other.element_type. def _pretty(self, b, indent, increment):; b.append('array<'); self.element_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Array["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tarray) and self.element_type.unify(t.element_type). def subst(self):; return tarray(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[list, frozenlist]:; length = byte_reader.read_int32(). num_missing_bytes = math.ceil(length / 8); missing_bytes = byte_reader.read_bytes_view(num_missing_bytes). decoded = []; i = 0; current_missing_byte = None; while i < length:; which_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; decoded.append(None); else:; element_decoded = self.element_type._convert_from_encoding(byte_reader, _should_freeze); decoded.append(element_decoded); i += 1; if _should_freeze:; return frozenlist(decoded); return decoded. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if H",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:20301,Usability,clear,clear,20301,"ion)). def __str__(self):; return ""array<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tarray) and self.element_type == other.element_type. def _pretty(self, b, indent, increment):; b.append('array<'); self.element_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Array["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tarray) and self.element_type.unify(t.element_type). def subst(self):; return tarray(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[list, frozenlist]:; length = byte_reader.read_int32(). num_missing_bytes = math.ceil(length / 8); missing_bytes = byte_reader.read_bytes_view(num_missing_bytes). decoded = []; i = 0; current_missing_byte = None; while i < length:; which_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; decoded.append(None); else:; element_decoded = self.element_type._convert_from_encoding(byte_reader, _should_freeze); decoded.append(element_decoded); i += 1; if _should_freeze:; return frozenlist(decoded); return decoded. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:22907,Usability,clear,clear,22907,"s not realizable in Python""). def __str__(self):; return ""stream<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tstream) and self.element_type == other.element_type. def _pretty(self, b, indent, increment):; b.append('stream<'); self.element_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Stream["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tstream) and self.element_type.unify(t.element_type). def subst(self):; return tstream(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def is_setlike(maybe_setlike):; return isinstance(maybe_setlike, (set, frozenset)). [docs]class tset(HailType):; """"""Hail type for collections of distinct elements. In Python, these are represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; self._array_repr = tarray(element_type); super(tset, self).__init__(). @property; def element_type(self):; """"""Set element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:22939,Usability,clear,clear,22939," def __str__(self):; return ""stream<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tstream) and self.element_type == other.element_type. def _pretty(self, b, indent, increment):; b.append('stream<'); self.element_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Stream["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tstream) and self.element_type.unify(t.element_type). def subst(self):; return tstream(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def is_setlike(maybe_setlike):; return isinstance(maybe_setlike, (set, frozenset)). [docs]class tset(HailType):; """"""Hail type for collections of distinct elements. In Python, these are represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; self._array_repr = tarray(element_type); super(tset, self).__init__(). @property; def element_type(self):; """"""Set element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._t",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:25470,Usability,clear,clear,25470,"urn ""Set["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[set, frozenset]:; s = {self.element_type._convert_from_json_na(elt, _should_freeze=True) for elt in x}; if _should_freeze:; return frozenset(s); return s. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[set, frozenset]:; s = self._array_repr._convert_from_encoding(byte_reader, _should_freeze=True); if _should_freeze:; return frozenset(s); return set(s). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; self._array_repr._convert_to_encoding(byte_writer, list(value)). def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tset) and self.element_type.unify(t.element_type). def subst(self):; return tset(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). class _freeze_this_type(HailType):; def __init__(self, t):; self.t = t. def _convert_from_json_na(self, x, _should_freeze: bool = False):; return self.t._convert_from_json_na(x, _should_freeze=True). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; return self.t._convert_from_encoding(byte_reader, _should_freeze=True). def _convert_to_encoding(self, byte_writer, x):; return self.t._convert_to_encoding(byte_writer, x). [docs]class tdict(HailType):; """"""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """""". @typecheck_method(key_t",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:25502,Usability,clear,clear,25502,"e._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[set, frozenset]:; s = {self.element_type._convert_from_json_na(elt, _should_freeze=True) for elt in x}; if _should_freeze:; return frozenset(s); return s. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[set, frozenset]:; s = self._array_repr._convert_from_encoding(byte_reader, _should_freeze=True); if _should_freeze:; return frozenset(s); return set(s). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; self._array_repr._convert_to_encoding(byte_writer, list(value)). def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tset) and self.element_type.unify(t.element_type). def subst(self):; return tset(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). class _freeze_this_type(HailType):; def __init__(self, t):; self.t = t. def _convert_from_json_na(self, x, _should_freeze: bool = False):; return self.t._convert_from_json_na(x, _should_freeze=True). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; return self.t._convert_from_encoding(byte_reader, _should_freeze=True). def _convert_to_encoding(self, byte_writer, x):; return self.t._convert_to_encoding(byte_writer, x). [docs]class tdict(HailType):; """"""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """""". @typecheck_method(key_type=hail_type, value_type=hai",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:29471,Usability,clear,clear,29471,"self, byte_reader, _should_freeze: bool = False) -> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context(self):; return HailTypeContext.union(self.key_type, self.value_type). [docs]class tstruct(HailType, Mapping):; """"""Hail type for structured groups of heterogeneous fields. In Python, these are represented as :class:`.Struct`. Hail's :class:`.tstruct` type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique. Structs are very common in Hail. Each component of a :class:`.Table` and :class:`.MatrixTable`; is a struct:. - :meth:`.Table.row`; - :meth:`.Table.globals`; - :meth:`.MatrixTable.row`; - :meth:`.MatrixTable.col`; - :meth:`.MatrixTable.entry`; - :meth:`.MatrixTable.globals`. Structs appear below the top-level component types as well. Consider the following join:. >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:29499,Usability,clear,clear,29499,"reeze: bool = False) -> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context(self):; return HailTypeContext.union(self.key_type, self.value_type). [docs]class tstruct(HailType, Mapping):; """"""Hail type for structured groups of heterogeneous fields. In Python, these are represented as :class:`.Struct`. Hail's :class:`.tstruct` type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique. Structs are very common in Hail. Each component of a :class:`.Table` and :class:`.MatrixTable`; is a struct:. - :meth:`.Table.row`; - :meth:`.Table.globals`; - :meth:`.MatrixTable.row`; - :meth:`.MatrixTable.col`; - :meth:`.MatrixTable.entry`; - :meth:`.MatrixTable.globals`. Structs appear below the top-level component types as well. Consider the following join:. >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to ``table1`` called ``table",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:29524,Usability,clear,clear,29524,"> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context(self):; return HailTypeContext.union(self.key_type, self.value_type). [docs]class tstruct(HailType, Mapping):; """"""Hail type for structured groups of heterogeneous fields. In Python, these are represented as :class:`.Struct`. Hail's :class:`.tstruct` type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique. Structs are very common in Hail. Each component of a :class:`.Table` and :class:`.MatrixTable`; is a struct:. - :meth:`.Table.row`; - :meth:`.Table.globals`; - :meth:`.MatrixTable.row`; - :meth:`.MatrixTable.col`; - :meth:`.MatrixTable.entry`; - :meth:`.MatrixTable.globals`. Structs appear below the top-level component types as well. Consider the following join:. >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to ``table1`` called ``table2_fields``. In the new",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:36223,Usability,clear,clear,36223,"); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.values()). class tunion(HailType, Mapping):; @typecheck_method(case_types=hail_type); def __init__(self, **case_types):; """"""Tagged union type. Values of type union represent one of several; heterogenous, named cases. Parameters; ----------; cases : keyword args of :class:`.HailType`; The union cases. """""". super(tunion, self).__init__(); self._case_types = case_types; self._cases = tuple(case_types). @property; def cases(self):; """"""Return union case names. Returns; -------; :obj:`tuple` of :class:`str`; Tuple of union case names; """"""; return self._cases. @typecheck_method(item=oneof(int, str)); def __getitem__(self, item):; if isinstance(item, int):; item = self._cases[item]; return self._case_types[item]. def __iter__(self):; return iter(self._case_types). def __len__(self):; return len(self._cases). def __str__(self):; return ""union{{{}}}"".format(', '.join('{}: {}'.format(escape_parsable",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:36266,Usability,clear,clear,36266,"ds); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.values()). class tunion(HailType, Mapping):; @typecheck_method(case_types=hail_type); def __init__(self, **case_types):; """"""Tagged union type. Values of type union represent one of several; heterogenous, named cases. Parameters; ----------; cases : keyword args of :class:`.HailType`; The union cases. """""". super(tunion, self).__init__(); self._case_types = case_types; self._cases = tuple(case_types). @property; def cases(self):; """"""Return union case names. Returns; -------; :obj:`tuple` of :class:`str`; Tuple of union case names; """"""; return self._cases. @typecheck_method(item=oneof(int, str)); def __getitem__(self, item):; if isinstance(item, int):; item = self._cases[item]; return self._case_types[item]. def __iter__(self):; return iter(self._case_types). def __len__(self):; return len(self._cases). def __str__(self):; return ""union{{{}}}"".format(', '.join('{}: {}'.format(escape_parsable(f), str(t)) for f, t in self.item",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:38273,Usability,clear,clear,38273,"elf, other):; return (; isinstance(other, tunion) and self._cases == other._cases and all(self[c] == other[c] for c in self._cases); ). def _pretty(self, b, indent, increment):; if not self._cases:; b.append('union {}'); return. pre_indent = indent; indent += increment; b.append('union {'); for i, (f, t) in enumerate(self.items()):; if i > 0:; b.append(', '); b.append('\n'); b.append(' ' * indent); b.append('{}: '.format(escape_parsable(f))); t._pretty(b, indent, increment); b.append('\n'); b.append(' ' * pre_indent); b.append('}'). def _parsable_string(self):; return ""Union{{{}}}"".format(; ','.join('{}:{}'.format(escape_parsable(f), t._parsable_string()) for f, t in self.items()); ). def unify(self, t):; if not (isinstance(t, tunion) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tunion(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.values()). [docs]class ttuple(HailType, Sequence):; """"""Hail type for tuples. In Python, these are represented as :obj:`tuple`. Parameters; ----------; types: varargs of :class:`.HailType`; Element types. See Also; --------; :class:`.TupleExpression`; """""". @typecheck_method(types=hail_type); def __init__(self, *types):; self._types = types; super(ttuple, self).__init__(). @property; def types(self):; """"""Tuple element types. Returns; -------; :obj:`tuple` of :class:`.HailType`; """"""; return self._types. def _traverse(self, obj, f):; if f(self, obj):; for t, elt in zip(self.types, obj):; t._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation:; if not isinstance(annotation, tuple):; raise TypeError(""type 'tuple' expected Python tuple, but found '%s'"" % type(annotation)); if len(annotation) != len(self.types):; raise TypeError(""%s expected tuple of size '%i', but found ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:38316,Usability,clear,clear,38316,"(other, tunion) and self._cases == other._cases and all(self[c] == other[c] for c in self._cases); ). def _pretty(self, b, indent, increment):; if not self._cases:; b.append('union {}'); return. pre_indent = indent; indent += increment; b.append('union {'); for i, (f, t) in enumerate(self.items()):; if i > 0:; b.append(', '); b.append('\n'); b.append(' ' * indent); b.append('{}: '.format(escape_parsable(f))); t._pretty(b, indent, increment); b.append('\n'); b.append(' ' * pre_indent); b.append('}'). def _parsable_string(self):; return ""Union{{{}}}"".format(; ','.join('{}:{}'.format(escape_parsable(f), t._parsable_string()) for f, t in self.items()); ). def unify(self, t):; if not (isinstance(t, tunion) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tunion(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.values()). [docs]class ttuple(HailType, Sequence):; """"""Hail type for tuples. In Python, these are represented as :obj:`tuple`. Parameters; ----------; types: varargs of :class:`.HailType`; Element types. See Also; --------; :class:`.TupleExpression`; """""". @typecheck_method(types=hail_type); def __init__(self, *types):; self._types = types; super(ttuple, self).__init__(). @property; def types(self):; """"""Tuple element types. Returns; -------; :obj:`tuple` of :class:`.HailType`; """"""; return self._types. def _traverse(self, obj, f):; if f(self, obj):; for t, elt in zip(self.types, obj):; t._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation:; if not isinstance(annotation, tuple):; raise TypeError(""type 'tuple' expected Python tuple, but found '%s'"" % type(annotation)); if len(annotation) != len(self.types):; raise TypeError(""%s expected tuple of size '%i', but found '%s'"" % (self, len(self.types), an",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:41716,Usability,clear,clear,41716,"r i, t in enumerate(self.types):; which_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; answer.append(None); else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); answer.append(field_decoded). return tuple(answer). def _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7)",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:41754,Usability,clear,clear,41754,":; which_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; answer.append(None); else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); answer.append(field_decoded). return tuple(answer). def _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:45551,Usability,clear,clear,45551,"; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploidy == 2:; int_rep |= allele_pair_rep(value) << 3; int_rep = int_rep if 0 <= int_rep < 2**31 - 1 else int_rep - 2**32. byte_writer.write_int32(int_rep). def unify(self, t):; return t == tcall. def subst(self):; return self. def clear(self):; pass. [docs]class tlocus(HailType):; """"""Hail type for a genomic coordinate with a contig and a position. In Python, these are represented by :class:`.Locus`. Parameters; ----------; reference_genome: :class:`.ReferenceGenome` or :class:`str`; Reference genome to use. See Also; --------; :class:`.LocusExpression`, :func:`.locus`, :func:`.parse_locus`,; :class:`.Locus`; """""". struct_repr = tstruct(contig=_tstr(), pos=_tint32()). @classmethod; @typecheck_method(reference_genome=nullable(reference_genome_type)); def _schema_from_rg(cls, reference_genome='default'):; # must match TLocus.schemaFromRG; if reference_genome is None:; return hl.tstruct(contig=hl.tstr, position=hl.tint32); return cls(reference_genome). @typecheck_method(reference_genome=reference_genome_type); def __init__(self, reference_genome='default'):; self._rg = reference_genome; super(tlocus, self).__init__(). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not isinstance(annotation, gene",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:48310,Usability,clear,clear,48310,"lf._rg is None:; self._rg = hl.default_reference(); return self._rg. def _pretty(self, b, indent, increment):; b.append('locus<{}>'.format(escape_parsable(self.reference_genome.name))). def _convert_from_json(self, x, _should_freeze: bool = False) -> genetics.Locus:; return genetics.Locus(x['contig'], x['position'], reference_genome=self.reference_genome). def _convert_to_json(self, x):; return {'contig': x.contig, 'position': x.position}. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> genetics.Locus:; as_struct = tlocus.struct_repr._convert_from_encoding(byte_reader); return genetics.Locus(as_struct.contig, as_struct.pos, self.reference_genome). def _convert_to_encoding(self, byte_writer, value: genetics.Locus):; tlocus.struct_repr._convert_to_encoding(byte_writer, {'contig': value.contig, 'pos': value.position}). def unify(self, t):; return isinstance(t, tlocus) and self.reference_genome == t.reference_genome. def subst(self):; return self. def clear(self):; pass. def _get_context(self):; return HailTypeContext(references={self.reference_genome.name}). [docs]class tinterval(HailType):; """"""Hail type for intervals of ordered values. In Python, these are represented by :class:`.Interval`. Parameters; ----------; point_type: :class:`.HailType`; Interval point type. See Also; --------; :class:`.IntervalExpression`, :class:`.Interval`, :func:`.interval`,; :func:`.parse_locus_interval`; """""". @typecheck_method(point_type=hail_type); def __init__(self, point_type):; self._point_type = point_type; self._struct_repr = tstruct(start=point_type, end=point_type, includes_start=hl.tbool, includes_end=hl.tbool); super(tinterval, self).__init__(). @property; def point_type(self):; """"""Interval point type. Returns; -------; :class:`.HailType`; Interval point type.; """"""; return self._point_type. def _traverse(self, obj, f):; if f(self, obj):; self.point_type._traverse(obj.start, f); self.point_type._traverse(obj.end, f). def _typecheck_one_level(self, ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:51410,Usability,clear,clear,51410,"e,; ). def _convert_to_json(self, x):; return {; 'start': self.point_type._convert_to_json_na(x.start),; 'end': self.point_type._convert_to_json_na(x.end),; 'includeStart': x.includes_start,; 'includeEnd': x.includes_end,; }. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; interval_as_struct = self._struct_repr._convert_from_encoding(byte_reader, _should_freeze); return hl.Interval(; interval_as_struct.start,; interval_as_struct.end,; interval_as_struct.includes_start,; interval_as_struct.includes_end,; point_type=self.point_type,; ). def _convert_to_encoding(self, byte_writer, value):; interval_dict = {; 'start': value.start,; 'end': value.end,; 'includes_start': value.includes_start,; 'includes_end': value.includes_end,; }; self._struct_repr._convert_to_encoding(byte_writer, interval_dict). def unify(self, t):; return isinstance(t, tinterval) and self.point_type.unify(t.point_type). def subst(self):; return tinterval(self.point_type.subst()). def clear(self):; self.point_type.clear(). def _get_context(self):; return self.point_type.get_context(). class Box(object):; named_boxes: ClassVar = {}. @staticmethod; def from_name(name):; if name in Box.named_boxes:; return Box.named_boxes[name]; b = Box(); Box.named_boxes[name] = b; return b. def __init__(self):; pass. def unify(self, v):; if hasattr(self, 'value'):; return self.value == v; self.value = v; return True. def clear(self):; if hasattr(self, 'value'):; del self.value. def get(self):; assert hasattr(self, 'value'); return self.value. tvoid = _tvoid(). tint32 = _tint32(); """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int32Expression`, :func:`.int`, :func:`.int32`; """""". tint64 = _tint64(); """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are r",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:51440,Usability,clear,clear,51440,"(self, x):; return {; 'start': self.point_type._convert_to_json_na(x.start),; 'end': self.point_type._convert_to_json_na(x.end),; 'includeStart': x.includes_start,; 'includeEnd': x.includes_end,; }. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; interval_as_struct = self._struct_repr._convert_from_encoding(byte_reader, _should_freeze); return hl.Interval(; interval_as_struct.start,; interval_as_struct.end,; interval_as_struct.includes_start,; interval_as_struct.includes_end,; point_type=self.point_type,; ). def _convert_to_encoding(self, byte_writer, value):; interval_dict = {; 'start': value.start,; 'end': value.end,; 'includes_start': value.includes_start,; 'includes_end': value.includes_end,; }; self._struct_repr._convert_to_encoding(byte_writer, interval_dict). def unify(self, t):; return isinstance(t, tinterval) and self.point_type.unify(t.point_type). def subst(self):; return tinterval(self.point_type.subst()). def clear(self):; self.point_type.clear(). def _get_context(self):; return self.point_type.get_context(). class Box(object):; named_boxes: ClassVar = {}. @staticmethod; def from_name(name):; if name in Box.named_boxes:; return Box.named_boxes[name]; b = Box(); Box.named_boxes[name] = b; return b. def __init__(self):; pass. def unify(self, v):; if hasattr(self, 'value'):; return self.value == v; self.value = v; return True. def clear(self):; if hasattr(self, 'value'):; del self.value. def get(self):; assert hasattr(self, 'value'); return self.value. tvoid = _tvoid(). tint32 = _tint32(); """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int32Expression`, :func:`.int`, :func:`.int32`; """""". tint64 = _tint64(); """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`. S",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:51837,Usability,clear,clear,51837," interval_as_struct.end,; interval_as_struct.includes_start,; interval_as_struct.includes_end,; point_type=self.point_type,; ). def _convert_to_encoding(self, byte_writer, value):; interval_dict = {; 'start': value.start,; 'end': value.end,; 'includes_start': value.includes_start,; 'includes_end': value.includes_end,; }; self._struct_repr._convert_to_encoding(byte_writer, interval_dict). def unify(self, t):; return isinstance(t, tinterval) and self.point_type.unify(t.point_type). def subst(self):; return tinterval(self.point_type.subst()). def clear(self):; self.point_type.clear(). def _get_context(self):; return self.point_type.get_context(). class Box(object):; named_boxes: ClassVar = {}. @staticmethod; def from_name(name):; if name in Box.named_boxes:; return Box.named_boxes[name]; b = Box(); Box.named_boxes[name] = b; return b. def __init__(self):; pass. def unify(self, v):; if hasattr(self, 'value'):; return self.value == v; self.value = v; return True. def clear(self):; if hasattr(self, 'value'):; del self.value. def get(self):; assert hasattr(self, 'value'); return self.value. tvoid = _tvoid(). tint32 = _tint32(); """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int32Expression`, :func:`.int`, :func:`.int32`; """""". tint64 = _tint64(); """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int64Expression`, :func:`.int64`; """""". tint = tint32; """"""Alias for :py:data:`.tint32`."""""". tfloat32 = _tfloat32(); """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`. See Also; --------; :class:`.Float32Expression`, :func:`.float64`; """""". tfloat64 = _tfloat64(); """"""Hail type for 64-bit floating point numbers. In Python, these are represente",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:56434,Usability,clear,clear,56434,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/expr/types.html:56457,Usability,clear,clear,56457,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:163,Deployability,Install,Installation,163,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:231,Deployability,Configurat,Configuration,231,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:2400,Deployability,update,updated,2400,"code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_STRS[self]. @classmethod; def _missing_(cls, value):; if not isinstance(value, str):; return None; return cls.__members__.get(value.upper()). [docs] @staticmethod; def strings():; """"""Returns the names of the allele types, for use with; :func:`~hail.expr.functions.literal`; """"""; return list(_ALLELE_STRS). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:231,Modifiability,Config,Configuration,231,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:952,Modifiability,Polymorphi,Polymorphism,952,"mmunity Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_S",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:1009,Modifiability,Polymorphi,Polymorphism,1009,"mmunity Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_S",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:1133,Modifiability,Polymorphi,Polymorphism,1133,"mmunity Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_S",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:385,Testability,Log,Log,385,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:63,Usability,Feedback,Feedback,63,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:273,Usability,Guid,Guides,273,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:156,Deployability,Install,Installation,156,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:224,Deployability,Configurat,Configuration,224,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:5872,Deployability,update,updated,5872," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:224,Modifiability,Config,Configuration,224,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:2313,Security,hash,hash,2313," check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the ca",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:2334,Security,hash,hash,2334,"slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:378,Testability,Log,Log,378,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1343,Testability,assert,assert,1343," Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1381,Testability,assert,assert,1381," Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1623,Testability,assert,assert,1623,"mic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1916,Testability,assert,assert,1916," taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:3663,Testability,assert,assert,3663," of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """"""; if self.ploidy < 2:; return False; return self._alleles[0] != self._alleles[1]. [docs] def is_hom_var(self):; """"""True if the call contains identical alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n == 0:; return False. a0 = self._alleles[0]; if a0 == 0:; return False. if n == 1:; return True. assert n == 2; return self._alleles[1] == a0. [docs] def is_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """"""; return any(a > 0 for a in self._alleles). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded r",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:4013,Testability,assert,assert,4013," the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """"""; if self.ploidy < 2:; return False; return self._alleles[0] != self._alleles[1]. [docs] def is_hom_var(self):; """"""True if the call contains identical alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n == 0:; return False. a0 = self._alleles[0]; if a0 == 0:; return False. if n == 1:; return True. assert n == 2; return self._alleles[1] == a0. [docs] def is_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """"""; return any(a > 0 for a in self._alleles). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding fo",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:4273,Testability,assert,assert,4273,"; """"""True if the call contains two different alleles. :rtype: bool; """"""; if self.ploidy < 2:; return False; return self._alleles[0] != self._alleles[1]. [docs] def is_hom_var(self):; """"""True if the call contains identical alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n == 0:; return False. a0 = self._alleles[0]; if a0 == 0:; return False. if n == 1:; return True. assert n == 2; return self._alleles[1] == a0. [docs] def is_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """"""; return any(a > 0 for a in self._alleles). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference.",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:5784,Testability,assert,assert,5784," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:56,Usability,Feedback,Feedback,56,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/call.html:266,Usability,Guid,Guides,266,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:157,Deployability,Install,Installation,157,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:225,Deployability,Configurat,Configuration,225,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:3272,Deployability,update,updated,3272,"l_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGenome`; """"""; return self._rg. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:225,Modifiability,Config,Configuration,225,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2160,Security,hash,hash,2160,":`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2181,Security,hash,hash,2181,"ich is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2204,Security,hash,hash,2204,"ing the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGe",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:379,Testability,Log,Log,379,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:1561,Testability,assert,assert,1561,"ail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:1593,Testability,assert,assert,1593,"ail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:1627,Testability,assert,assert,1627,"ail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:57,Usability,Feedback,Feedback,57,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:267,Usability,Guid,Guides,267,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:160,Deployability,Install,Installation,160,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:228,Deployability,Configurat,Configuration,228,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:8020,Deployability,update,updated,8020,"}]"".format(; missing_sex_count, missing_sex_values; ); ). return Pedigree(trios). @property; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; return list(filter(lambda t: t.is_complete(), self.trios)). [docs] @typecheck_method(samples=sequenceof(nullable(str))); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; sample_set = set(samples). filtered_trios = []; for trio in self._trios:; restricted_trio = trio._restrict_to(sample_set); if restricted_trio is not None:; filtered_trios.append(restricted_trio). return Pedigree(filtered_trios). [docs] @typecheck_method(path=str); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """""". lines = [t._to_fam_file_line() for t in self._trios]. with Env.fs().open(path, mode=""w"") as file:; for line in lines:; file.write(line + ""\n""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:228,Modifiability,Config,Configuration,228,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:2003,Security,hash,hash,2003,"e: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; return hash((self._s, self._pat_id, self._mat_id, self._fam_id, self._is_female)). @property; def s(self):; """"""ID of proband in trio, never missing. :rtype: str; """""". return self._s. @property; def pat_id(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". return self._pat_id. @property; def mat_id(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". return self._mat_id. @property; def fam_id(self):; """"""Family ID. :rtype: str or None; """""". return self._fam_id. @property; def is_male(self):; """"""Returns ``True`` if the proband is a reported male,; ``False`` if reported female, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self._is_female is False. @property; def is_female(self):; """"""Returns ``True`` if the proband is a reported female,; ``False`` if reported male, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:4463,Security,hash,hash,4463,"; self._s,; self._fam_id,; self._pat_id if self._pat_id in ids else None,; self._mat_id if self._mat_id in ids else None,; self._is_female,; ). def _sex_as_numeric_string(self):; if self._is_female is None:; return ""0""; return ""2"" if self.is_female else ""1"". def _to_fam_file_line(self):; def sample_id_or_else_zero(sample_id):; if sample_id is None:; return ""0""; return sample_id. line_list = [; sample_id_or_else_zero(self._fam_id),; self._s,; sample_id_or_else_zero(self._pat_id),; sample_id_or_else_zero(self._mat_id),; self._sex_as_numeric_string(),; ""0"",; ]; return ""\t"".join(line_list). [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @typecheck_method(trios=sequenceof(Trio)); def __init__(self, trios):; self._trios = tuple(trios). def __eq__(self, other):; return isinstance(other, Pedigree) and self._trios == other._trios. def __hash__(self):; return hash(self._trios). def __iter__(self):; return self._trios.__iter__(). [docs] @classmethod; @typecheck_method(fam_path=str, delimiter=str); def read(cls, fam_path, delimiter='\\s+') -> 'Pedigree':; """"""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". trios = []; missing_sex_count = 0; missing_sex_values = set(); with Env.fs().open(fam_path) as file:; for line in file:; split_line = re.split(delimiter, line.strip()); num_fields = len(split_line); if num_fields != 6:; raise FatalError(; ""Require 6 fields per line in .fam, but this line has {}: {}"".format(num_fields, line); ); (fam, kid, dad, mom, sex, _) = tuple(split_line); # 1 is male, 2 is female, 0 is unknown.; is_female = sex == ""2"" if sex ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:382,Testability,Log,Log,382,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:4764,Testability,test,test,4764,"se_zero(sample_id):; if sample_id is None:; return ""0""; return sample_id. line_list = [; sample_id_or_else_zero(self._fam_id),; self._s,; sample_id_or_else_zero(self._pat_id),; sample_id_or_else_zero(self._mat_id),; self._sex_as_numeric_string(),; ""0"",; ]; return ""\t"".join(line_list). [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @typecheck_method(trios=sequenceof(Trio)); def __init__(self, trios):; self._trios = tuple(trios). def __eq__(self, other):; return isinstance(other, Pedigree) and self._trios == other._trios. def __hash__(self):; return hash(self._trios). def __iter__(self):; return self._trios.__iter__(). [docs] @classmethod; @typecheck_method(fam_path=str, delimiter=str); def read(cls, fam_path, delimiter='\\s+') -> 'Pedigree':; """"""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". trios = []; missing_sex_count = 0; missing_sex_values = set(); with Env.fs().open(fam_path) as file:; for line in file:; split_line = re.split(delimiter, line.strip()); num_fields = len(split_line); if num_fields != 6:; raise FatalError(; ""Require 6 fields per line in .fam, but this line has {}: {}"".format(num_fields, line); ); (fam, kid, dad, mom, sex, _) = tuple(split_line); # 1 is male, 2 is female, 0 is unknown.; is_female = sex == ""2"" if sex in {'1', '2'} else None. if is_female is None:; missing_sex_count += 1; missing_sex_values.add(kid). trio = Trio(; kid,; fam if fam != ""0"" else None,; dad if dad != ""0"" else None,; mom if mom != ""0"" else None,; is_female,; ); trios.append(trio). only_ids = [trio.s for trio in trios]; duplicate_ids = [id fo",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:7409,Testability,test,test,7409,"}]"".format(; missing_sex_count, missing_sex_values; ); ). return Pedigree(trios). @property; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; return list(filter(lambda t: t.is_complete(), self.trios)). [docs] @typecheck_method(samples=sequenceof(nullable(str))); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; sample_set = set(samples). filtered_trios = []; for trio in self._trios:; restricted_trio = trio._restrict_to(sample_set); if restricted_trio is not None:; filtered_trios.append(restricted_trio). return Pedigree(filtered_trios). [docs] @typecheck_method(path=str); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """""". lines = [t._to_fam_file_line() for t in self._trios]. with Env.fs().open(path, mode=""w"") as file:; for line in lines:; file.write(line + ""\n""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:60,Usability,Feedback,Feedback,60,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:270,Usability,Guid,Guides,270,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10326,Availability,down,download,10326,"ce genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10345,Availability,avail,available,10345,"ce genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12758,Availability,avail,available,12758,"`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>>",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:14459,Availability,down,download,14459,", dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:14478,Availability,avail,available,14478,", dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:168,Deployability,Install,Installation,168,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:236,Deployability,Configurat,Configuration,236,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:16681,Deployability,update,updated,16681," : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters; ----------; global_pos : int; Zero-based global base position along the reference genome. Returns; -------; :class:`.Locus`; """"""; if global_pos < 0:; raise ValueError(f""global_pos must be non-negative, got {global_pos}""). if self._global_positions_list is None:; # dicts are in insertion order as of 3.7; self._global_positions_list = list(self.global_positions_dict.values()). global_positions = self._global_positions_list; contig = self.contigs[bisect_right(global_positions, global_pos) - 1]; contig_pos = self.global_positions_dict[contig]. if global_pos >= contig_pos + self.lengths[contig]:; raise ValueError(f""global_pos {global_pos} exceeds length of reference genome {self}.""). return hl.Locus(contig, global_pos - contig_pos + 1, self). rg_type.set(ReferenceGenome). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:236,Modifiability,Config,Configuration,236,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3419,Modifiability,config,config,3419,"`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; '",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3602,Modifiability,config,config,3602,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3646,Modifiability,config,config,3646,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3738,Modifiability,config,config,3738,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3759,Modifiability,config,config,3759,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3780,Modifiability,config,config,3780,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3825,Modifiability,config,config,3825,"OT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; s",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12383,Modifiability,config,config,12383,"of(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; ); def from_fasta_file(cls, name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_g",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12538,Modifiability,config,config,12538,"gs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_l",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:7347,Performance,Load,Load,7347," Returns; -------; :obj:`list` of :class:`.Interval`; """""". return self._par. [docs] @typecheck_method(contig=str); def contig_length(self, contig):; """"""Contig length. Parameters; ----------; contig : :class:`str`; Contig name. Returns; -------; :obj:`int`; Length of contig.; """"""; if contig in self.lengths:; return self.lengths[contig]; else:; raise KeyError(""Contig `{}' is not in reference genome."".format(contig)). @property; def global_positions_dict(self):; """"""Get a dictionary mapping contig names to their global genomic positions. Returns; -------; :class:`dict`; A dictionary of contig names to global genomic positions.; """"""; if self._global_positions is None:; gp = {}; lengths = self._lengths; x = 0; for c in self.contigs:; gp[c] = x; x += lengths[c]; self._global_positions = gp; return self._global_positions. @typecheck_method(contig=str); def _contig_global_position(self, contig):; return self.global_positions_dict[contig]. [docs] @classmethod; @typecheck_method(path=str); def read(cls, path):; """"""Load reference genome from a JSON file. Notes; -----. The JSON file must have the following format:. .. code-block:: text. {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. `name` must be unique and not overlap with Hail's pre-instantiated; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``.; The contig names in `xContigs`, `yContigs`, and `mtContigs` must be; present in `contigs`. The intervals listed in `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig l",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:8543,Performance,load,load,8543,"},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. `name` must be unique and not overlap with Hail's pre-instantiated; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``.; The contig names in `xContigs`, `yContigs`, and `mtContigs` must be; present in `contigs`. The intervals listed in `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig length given in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ...",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9256,Performance,Load,Load,9256,"n `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig length given in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_a",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9894,Performance,load,loaded,9894,"ail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_fil",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10975,Performance,load,loaded,10975,"ogle cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; ); def from_fasta_file(cls, name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X ch",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:1567,Security,access,access,1567,"hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~ha",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:1977,Security,access,access,1977,"ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :o",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:5442,Security,hash,hash,5442,"name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; self._par_tuple = par; self._par = [hl.Interval(hl.Locus(c, s, self), hl.Locus(c, e, self)) for (c, s, e) in par]; self._global_positions = None; self._global_positions_list = None. if not _builtin:; Env.backend().add_reference(self). self._sequence_files = None; self._liftovers = dict(). def __str__(self):; return self._config['name']. def __repr__(self):; return 'ReferenceGenome(name=%s, contigs=%s, lengths=%s, x_contigs=%s, y_contigs=%s, mt_contigs=%s, par=%s)' % (; self.name,; self.contigs,; self.lengths,; self.x_contigs,; self.y_contigs,; self.mt_contigs,; self._par_tuple,; ). def __eq__(self, other):; return isinstance(other, ReferenceGenome) and self._config == other._config. def __hash__(self):; return hash(self.name). @property; def name(self):; """"""Name of reference genome. Returns; -------; :class:`str`; """"""; return self._config['name']. @property; def contigs(self):; """"""Contig names. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._contigs. @property; def lengths(self):; """"""Dict of contig name to contig length. Returns; -------; :obj:`dict` of :class:`str` to :obj:`int`; """"""; return self._lengths. @property; def x_contigs(self):; """"""X contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['xContigs']. @property; def y_contigs(self):; """"""Y contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['yContigs']. @property; def mt_contigs(self):; """"""Mitochondrial contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['mtContigs']. @property; def par(self):; """"""Pseudoautosomal regions. Returns; -------; :obj:`list` of :class:`.Interval`; """""". return self._par. [docs] @typecheck_method(con",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9323,Security,Access,Access,9323,"n in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:13648,Security,Access,Access,13648,"ome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compress",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:390,Testability,Log,Log,390,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:3464,Testability,assert,assert,3464,"`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; '",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9867,Testability,test,test,9867,"ail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_fil",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:14061,Testability,test,test,14061,"me_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise V",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:68,Usability,Feedback,Feedback,68,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:278,Usability,Guid,Guides,278,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:153,Deployability,Install,Installation,153,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:221,Deployability,Configurat,Configuration,221,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:1477,Deployability,update,updated,1477,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:221,Modifiability,Config,Configuration,221,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:375,Testability,Log,Log,375,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:53,Usability,Feedback,Feedback,53,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:263,Usability,Guid,Guides,263,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:165,Deployability,Install,Installation,165,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:233,Deployability,Configurat,Configuration,233,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:1291,Deployability,update,updated,1291,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:233,Modifiability,Config,Configuration,233,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:387,Testability,Log,Log,387,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:65,Usability,Feedback,Feedback,65,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:275,Usability,Guid,Guides,275,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:156,Deployability,Install,Installation,156,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:224,Deployability,Configurat,Configuration,224,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:3484,Deployability,update,updated,3484,"he facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; _base_scale_mappings: ClassVar = {; ""shared_xaxes"": ""all"",; ""shared_yaxes"": ""all"",; }. _scale_mappings: ClassVar = {; ""fixed"": _base_scale_mappings,; ""free_x"": {; **_base_scale_mappings,; ""shared_xaxes"": False,; },; ""free_y"": {; **_base_scale_mappings,; ""shared_yaxes"": False,; },; ""free"": {; ""shared_xaxes"": False,; ""shared_yaxes"": False,; },; }. def __init__(; self, facets: StructExpression, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ):; if nrow is not None and ncol is not None:; raise ValueError(""Both `nrow` and `ncol` were specified. "" ""Please specify only one of these values.""); if scales not in self._scale_mappings:; raise ValueError(; f""An unsupported value ({scales}) was provided for `scales`. ""; f""Supported values are: {[k for k in self._scale_mappings.keys()]}.""; ); self.nrow = nrow; self.ncol = ncol; self.facets = facets; self.scales = scales. def get_expr_to_group_by(self) -> StructExpression:; return self.facets. def get_facet_nrows_and_ncols(self, num_facet_values: int) -> Tuple[int, int]:; if self.ncol is not None:; return (n_partitions(num_facet_values, self.ncol), self.ncol); elif self.nrow is not None:; return (self.nrow, n_partitions(num_facet_values, self.nrow)); else:; ncol = int(math.ceil(math.sqrt(num_facet_values))); return (n_partitions(num_facet_values, ncol), ncol). def get_shared_axis_kwargs(self) -> Dict[str, str]:; return self._scale_mappings[self.scales]. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:224,Modifiability,Config,Configuration,224,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:378,Testability,Log,Log,378,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:56,Usability,Feedback,Feedback,56,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:266,Usability,Guid,Guides,266,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14237,Availability,down,down,14237,"n 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def poin",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14871,Availability,down,down,14871,"x_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:16513,Availability,down,down,16513,"p.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixed(j, upper=False); upper_slope = slope_from_fixed(j, upper=True); if upper_slope < min_slope:; # Line must bend down at x[li]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; fix_point_on_result(li, upper=False); j = li + 1; if j >= len(x):; break; li, ui = j, j; min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True); j += 1; continue; elif lower_slope > max_slope:; # Line must bend up at x[ui]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; fix_point_on_result(ui, upper=True); j = ui + 1; if j >= len(x):; break; li, ui = j, j; min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True); j += 1; continue; if j >= len(x):; break; if upper_slope < max_slope:; ui = j; max_slope = upper_slope; if lower_slope > min_slope:; li",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:155,Deployability,Install,Installation,155,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:223,Deployability,Configurat,Configuration,223,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:28060,Deployability,update,updated,28060,"is facing side, none by default. Overrides ``color`` aesthetic. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomArea(mapping, fill=fill, color=color). class GeomRibbon(Geom):; aes_to_arg: ClassVar = {; ""fill"": (""fillcolor"", ""black""),; ""color"": (""line_color"", ""rgba(0, 0, 0, 0)""),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; }. def __init__(self, aes, fill, color):; super().__init__(aes); self.fill = fill; self.color = color. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; def plot_group(df):; trace_args_bottom = {; ""x"": df.x,; ""y"": df.ymin,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""showlegend"": False,; }; self._add_aesthetics_to_trace_args(trace_args_bottom, df); self._update_legend_trace_args(trace_args_bottom, legend_cache). trace_args_top = {; ""x"": df.x,; ""y"": df.ymax,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""fill"": 'tonexty',; }; self._add_aesthetics_to_trace_args(trace_args_top, df); self._update_legend_trace_args(trace_args_top, legend_cache). fig_so_far.add_scatter(**trace_args_bottom); fig_so_far.add_scatter(**trace_args_top). for group_df in grouped_data:; plot_group(group_df). def get_stat(self):; return StatIdentity(). [docs]def geom_ribbon(mapping=aes(), fill=None, color=None):; """"""Creates filled in area between two lines specified by x, ymin, and ymax. Supported aesthetics: ``x``, ``ymin``, ``ymax``, ``color``, ``fill``, ``tooltip``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; fill:; Color of fill to draw, black by default. Overrides ``fill`` aesthetic.; color:; Color of line to draw outlining both side, none by default. Overrides ``color`` aesthetic. :return:; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomRibbon(mapping, fill=fill, color=color). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:18612,Energy Efficiency,power,power,18612,"one),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; ""alpha"": (""marker_opacity"", None),; }. def __init__(self, aes, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; super().__init__(aes); self.k = k; self.smoothing = smoothing; self.fill = fill; self.color = color; self.alpha = alpha; self.smoothed = smoothed. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; from hail.expr.functions import _error_from_cdf_python. def plot_group(df, idx):; data = df.attrs['data']. if self.smoothed:; n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev):; inv_scale = (np.sqrt(n * slope) / self.smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (; (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); ); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). trace_args = {; ""x"": x_d,; ""y"": final,; ""mode"": ""lines"",; ""fill"": ""tozeroy"",; ""row"": facet_row,; ""col"": facet_col,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_scatter(**trace_args); else:; confidence = 5. y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:18632,Energy Efficiency,power,power,18632,"xt"", None),; ""fill_legend"": (""name"", None),; ""alpha"": (""marker_opacity"", None),; }. def __init__(self, aes, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; super().__init__(aes); self.k = k; self.smoothing = smoothing; self.fill = fill; self.color = color; self.alpha = alpha; self.smoothed = smoothed. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; from hail.expr.functions import _error_from_cdf_python. def plot_group(df, idx):; data = df.attrs['data']. if self.smoothed:; n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev):; inv_scale = (np.sqrt(n * slope) / self.smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (; (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); ); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). trace_args = {; ""x"": x_d,; ""y"": final,; ""mode"": ""lines"",; ""fill"": ""tozeroy"",; ""row"": facet_row,; ""col"": facet_col,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_scatter(**trace_args); else:; confidence = 5. y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,; ""col"": facet_col,; ""width""",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:12686,Integrability,interface,interface,12686,"width / 2; bar_width = bin_width; else:; raise ValueError(f""Histogram does not support position = {self.position}""). right_xs = left_xs + bin_width. trace_args = {; ""x"": x,; ""y"": df.y,; ""row"": facet_row,; ""col"": facet_col,; ""customdata"": list(zip(left_xs, right_xs)),; ""width"": bar_width,; ""hovertemplate"": ""Range: [%{customdata[0]:.3f}-%{customdata[1]:.3f})<br>""; ""Count: %{y}<br>""; ""<extra></extra>"",; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_bar(**trace_args). for idx, group_df in enumerate(grouped_data):; plot_group(group_df, idx). fig_so_far.update_layout(barmode=bar_position_plotly_to_gg(self.position)). def get_stat(self):; return StatBin(self.min_val, self.max_val, self.bins). [docs]def geom_histogram(; mapping=aes(),; *,; min_val=None,; max_val=None,; bins=None,; fill=None,; color=None,; alpha=None,; position='stack',; size=None,; ):; """"""Creates a histogram. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; min_val: `int` or `float`; Minimum value to include in histogram; max_val: `int` or `float`; Maximum value to include in histogram; bins: `int`; Number of bins to plot. 30 by default.; fill:; A single fill color for all bars of histogram, overrides ``fill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14602,Integrability,contract,contract,14602,"GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:20345,Integrability,interface,interface,20345,"opy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,; ""col"": facet_col,; ""width"": widths,; ""offset"": 0,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_bar(**trace_args). for idx, group_df in enumerate(grouped_data):; plot_group(group_df, idx). def get_stat(self):; return StatCDF(self.k). [docs]def geom_density(mapping=aes(), *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; """"""Creates a smoothed density plot. This method uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomDensity(mapping, k, smoothing, fill, color, alpha, smoothed). class GeomHLine(Geom):; d",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:223,Modifiability,Config,Configuration,223,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:15394,Modifiability,variab,variables,15394," the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixe",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:15509,Modifiability,variab,variables,15509,"e between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixed(j, upper=False); upper_slope = slope_from_fixed(j, upper=True); if upper_slope < min_slope:; # Line must bend down at x[li]. We know the max-entropy cdf pas",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:21095,Modifiability,variab,variable,21095,"od uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomDensity(mapping, k, smoothing, fill, color, alpha, smoothed). class GeomHLine(Geom):; def __init__(self, yintercept, linetype=""solid"", color=None):; self.yintercept = yintercept; self.aes = aes(); self.linetype = linetype; self.color = color. def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; line_attributes = {""y"": self.yintercept, ""line_dash"": linetype_plotly_to_gg(self.linetype)}; if self.color is not None:; line_attributes[""line_color""] = self.color. fig_so_far.add_hline(**line_attributes). def get_stat(self):; return StatNone(). [docs]def geom_hline(yintercept, *, linetype=""solid"", color=None):; """"""Plots a horizontal line at ``yintercept``. Parameters; ----------; yintercept : :class:`float`; Location to draw line.; linetype : :class:`str`; Type of line to draw. C",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:377,Testability,Log,Log,377,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:55,Usability,Feedback,Feedback,55,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:265,Usability,Guid,Guides,265,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:156,Deployability,Install,Installation,156,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:224,Deployability,Configurat,Configuration,224,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:3599,Deployability,continuous,continuous,3599,"les.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_genome=dtype.reference_genome); else:; self.scales[""x""] = scale_x_discrete(); elif aesthetic_str == ""y"":; if is_continuous:; self.scales[""y""] = scale_y_continuous(); elif is_genomic_type(dtype):; raise ValueError(""Don't yet support y axis genomic""); else:; self.scales[""y""] = scale_y_discrete(); elif aesthetic_str == ""color"" and not is_continuous:; self.scales[""color""] = scale_color_discrete(); elif aesthetic_str == ""color"" and is_continuous:; self.scales[""color""] = scale_color_continuous(); elif aesthetic_str == ""fill"" and not is_continuous:; self.scales[""fill""] = scale_fill_discrete(); elif aesthetic_str == ""fill"" and is_continuous:; self.scales[""fill""] = scale_fill_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:4387,Deployability,update,updated,4387,"l_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping"": hl.struct(**self.aes)}; if self.facet is not None:; fields_to_select[""facet""] = self.facet.get_expr_to_group_by(). for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx); fields_to_select[geom_label] = hl.struct(**geom.aes.properties). name, ht = hl.struct(**fields_to_select)._to_table('__fallback'); return ht.select(**{field: ht[name][field] for field in fields_to_select}). def collect_mappings_and_precomputed(selected):; mapping_per_geom = []; precomputes = {}; for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx). combined_mapping = selected[""figure_mapping""].annotate(**selected[geom_label]). for key in combined_mapping:; if key in self.scales:; combined_mapping = combined_mapping.annotate(**{; key: self.scales[key].transform_data(combined_mapping[key]); }); mappin",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:9337,Deployability,update,update,9337,"red_axis_kwargs(),; }; else:; n_facet_rows = 1; n_facet_cols = 1; subplot_args = {; ""rows"": 1,; ""cols"": 1,; }; fig = make_subplots(**subplot_args). # Need to know what I've added to legend already so we don't do it more than once.; legend_cache = {}. for geom, geom_label, facet_to_grouped_dfs in geoms_and_grouped_dfs_by_facet_idx:; for facet_idx, grouped_dfs in facet_to_grouped_dfs.items():; scaled_grouped_dfs = []; for df in grouped_dfs:; scales_to_consider = list(df.columns) + list(df.attrs); relevant_aesthetics = [scale_name for scale_name in scales_to_consider if scale_name in self.scales]; scaled_df = df; for relevant_aesthetic in relevant_aesthetics:; scaled_df = transformers[relevant_aesthetic](scaled_df); scaled_grouped_dfs.append(scaled_df). facet_row = facet_idx // n_facet_cols + 1; facet_col = facet_idx % n_facet_cols + 1; geom.apply_to_fig(; scaled_grouped_dfs, fig, precomputed[geom_label], facet_row, facet_col, legend_cache, is_faceted; ). # Important to update axes after labels, axes names take precedence.; self.labels.apply_to_fig(fig); if self.scales.get(""x"") is not None:; self.scales[""x""].apply_to_fig(self, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:10483,Deployability,install,installed,10483,"lf, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11676,Deployability,update,updated,11676,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11023,Integrability,interface,interface,11023,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:224,Modifiability,Config,Configuration,224,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:378,Testability,Log,Log,378,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:1691,Testability,assert,assert,1691,"eoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartesian = other; elif isinstance(other, Scale):; copied.scales[other.aesthetic_name] = other; elif isinstance(other, Aesthetic):; copied.aes = copied.aes.merge(other); elif isinstance(other, Faceter):; copied.facet = other; else:; raise ValueError(""Not implemented""). return copied. def add_default_scales(self, aesthetic):; for aesthetic_str, mapped_expr in aesthetic.items():; dtype = mapped_expr.dtype; if aesthetic_str not in self.scales:; is_continuous = is_continuous_type(dtype); # We only know how to come up with a few default scales.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_ge",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11566,Testability,assert,assert,11566,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:56,Usability,Feedback,Feedback,56,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:266,Usability,Guid,Guides,266,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:156,Deployability,Install,Installation,156,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:224,Deployability,Configurat,Configuration,224,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:3286,Deployability,update,updated,3286,"l if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the x-axis label.; """"""; return Labels(xlabel=label). [docs]def ylab(label):; """"""Sets the y-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired y-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the y-axis label.; """"""; return Labels(ylabel=label). def labs(**group_labels):; """"""Sets the labels for the legend groups of a plot. Examples; --------. Create a scatterplot and label the legend groups according to their field names:. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared=ht.idx ** 2); >>> ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); >>> ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); >>> fig = (; ... hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)); ... + hl.ggplot.geom_point(hl.ggplot.aes(color=ht.even, shape=ht.threeven)); ... + hl.ggplot.labs(color=""Even"", shape=""Threeven""); ... ). Parameters; ----------; group_labels:; Map names of plotly ``legendgroup``s to the desired replacement labels. Returns; -------; :class:`.FigureAttribute`; Label object to change the legend group labels.; """"""; return Labels(group_labels=group_labels). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:224,Modifiability,Config,Configuration,224,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:378,Testability,Log,Log,378,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:56,Usability,Feedback,Feedback,56,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:266,Usability,Guid,Guides,266,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:6813,Availability,down,down,6813,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7030,Availability,down,down,7030,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7282,Availability,down,down,7282,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7381,Availability,down,down,7381,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7448,Availability,down,down,7448,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:155,Deployability,Install,Installation,155,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:223,Deployability,Configurat,Configuration,223,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:1509,Deployability,continuous,continuous,1509,"otly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesthetic_name, name, None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; r",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:8936,Deployability,continuous,continuous,8936,"ation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""log10""). [docs]def scale_x_reverse(name=None):; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""reverse""). [docs]def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Sup",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:9573,Deployability,continuous,continuous,9573,"def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_x_discrete(name=None, breaks=None, labels=None):; """"""The default discrete x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`str`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionS",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:12020,Deployability,continuous,continuous,12020," PositionScaleDiscrete(""y"", name=name, breaks=breaks, labels=labels). [docs]def scale_x_genomic(reference_genome, name=None):; """"""The default genomic x scale. This is used when the ``x`` aesthetic corresponds to a :class:`.LocusExpression`. Parameters; ----------; reference_genome:; The reference genome being used.; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleGenomic(""x"", reference_genome, name=name). [docs]def scale_color_discrete():; """"""The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_color_hue(). [docs]def scale_color_hue():; """"""Map discrete colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""color""). [docs]def scale_color_continuous():; """"""The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:13148,Deployability,continuous,continuous,13148,"--; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttr",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:14974,Deployability,update,updated,14974,"s; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""fill"", values=values). def scale_shape_manual(*, values):; """"""A scale that assigns shapes to discrete aesthetics. See `the plotly documentation <https://plotly.com/python-api-reference/generated/plotly.graph_objects.scatter.html#plotly.graph_objects.scatter.Marker.symbol>`__ for a list of supported shapes. Parameters; ----------; values: :class:`list` of :class:`str`; The shapes from which to choose. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""shape"", values=values). def scale_shape_auto():; """"""A scale that automatically assigns shapes to discrete aesthetics. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleShapeAuto(""shape""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:223,Modifiability,Config,Configuration,223,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7262,Security,hash,hash,7262,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:377,Testability,Log,Log,377,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:3024,Testability,log,log,3024," None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; return False. def is_continuous(self):; return False. class PositionScaleContinuous(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None, transformation=""identity""):; super().__init__(axis, name, breaks, labels); self.transformation = transformation. def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far); if self.transformation == ""identity"":; pass; elif self.transformation == ""log10"":; self.update_axis(fig_so_far)(type=""log""); elif self.transformation == ""reverse"":; self.update_axis(fig_so_far)(autorange=""reversed""); else:; raise ValueError(f""Unrecognized transformation {self.transformation}""). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. class PositionScaleDiscrete(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None):; super().__init__(axis, name, breaks, labels). def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return True. def is_continuous(self):; return False. class ScaleContinuous(Scale):; def __init__(self, aesthetic_name):; super().__init__(aesthetic_name). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. def valid_dtype(self, dtype):; return is_continuous_type(dtype",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7669,Testability,log,log,7669,"alues). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""log10""). [docs]def scale_x_reverse(name=None):; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""reverse""). [docs]def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :cla",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7974,Testability,log,log,7974,"agon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""log10""). [docs]def scale_x_reverse(name=None):; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""reverse""). [docs]def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x sc",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:55,Usability,Feedback,Feedback,55,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:265,Usability,Guid,Guides,265,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5169,Availability,resilien,resilience,5169,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:7495,Availability,down,downstream,7495,"2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3, -1]`` is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of ``bm``. - ``bm[::2, ::2]`` is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of ``bm``. Use :meth:`filter`, :meth:`filter_rows`, and :meth:`filter_cols` to; subset to non-slice subsets of rows and columns, e.g. to rows ``[0, 2, 5]``. **Block-sparse representation**. By default, block matrices compute and store all blocks explicitly.; However, some applications involve block matrices in which:. - some blocks consist entirely of zeroes. - some blocks are not of interest. For example, statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`spa",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14365,Availability,error,error,14365,"; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`i",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:15084,Availability,error,error,15084," the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; path = new_temp_file(); cls.write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=mean_impute,; center=center,; normalize=normalize,; axis=axis,; block_size=block_size,; ); return cls.read(path). [docs] @classmethod; @typecheck_method(n_rows=int, n_cols=int, block_size=nullable(int), seed=nullable(int), gaussian=bool); def random(cls, n_rows, n_cols, block_size=None, seed=None, gaussian=True) -> 'BlockMatrix':; """"""Creates a block matrix with standard normal or uniform random entries. Examples; --------; Create a block matrix with",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:20913,Availability,checkpoint,checkpoint,20913,"check_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def write(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Writes the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:21002,Availability,Checkpoint,Checkpoint,21002,"check_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def write(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Writes the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:21341,Availability,checkpoint,checkpointing,21341,"ile at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/m",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:21371,Availability,checkpoint,checkpoint,21371,"n-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Bl",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22455,Availability,down,downsamples,22455,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:24076,Availability,error,error,24076,"ch row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; hl.current_backend().validate_file(path). if not block_size:; block_size = BlockMatrix.default_block_size(). raise_unless_entry_indexed('BlockMatrix.write_from_entry_expr', entry_expr); mt = matrix_table_source('BlockMatrix.write_from_entry_expr', entry_expr). if not (mean_impute or center or normalize):; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; mt.select_entries(field)._write_block_matrix(path, overwrite, field, block_size); else:; field = Env.get_uid(); mt.select_entries(**{field: entry_expr})._write_block_matr",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42406,Availability,redundant,redundant,42406,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:49079,Availability,checkpoint,checkpoints,49079," self.block_size + (; self._last_col_block_width if stop_bcol == self._n_block_cols else self.block_size; ). return self[start_row:stop_row, start_col:stop_col]. @typecheck_method(b=oneof(np.ndarray, block_matrix_type)); def __matmul__(self, b):; """"""Matrix multiplication: a @ b. Parameters; ----------; b: :class:`numpy.ndarray` or :class:`BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; """"""; if isinstance(b, np.ndarray):; b = BlockMatrix(_to_bmir(b, self.block_size)). if self.n_cols != b.n_rows:; raise ValueError(f'incompatible shapes for matrix multiplication: {self.shape} and {b.shape}'). return BlockMatrix(BlockMatrixDot(self._bmir, b._bmir)). [docs] @typecheck_method(b=oneof(np.ndarray, block_matrix_type), splits=int, path_prefix=nullable(str)); def tree_matmul(self, b, *, splits, path_prefix=None):; """"""Matrix multiplication in situations with large inner dimension. This function splits a single matrix multiplication into `split_on_inner` smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by `file_name_prefix`, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters; ----------; b: :class:`numpy.ndarray` or :class:`BlockMatrix`; splits: :obj:`int` (keyword only argument); The number of smaller multiplications to do.; path_prefix: :class:`str` (keyword only argument); The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns; -------; :class:`.BlockMatrix`; """"""; if isinstance(b, np.ndarray):; b = BlockMatrix(_to_bmir(b, self.block_size)). if self.n_cols != b.n_rows:; raise ValueError(f'incompatible shapes for matrix multiplication: {self.shape} and {b.shape}'). if path_prefix is None:; path_prefix = new_temp_file(""tree_matmul_tmp""). if splits != 1:; inner_brange_size = int(math.ceil(self._n_block_cols / splits)); split_points = [*list(range(0, self._n_block_cols, inner_brange_size)), ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:161,Deployability,Install,Installation,161,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:229,Deployability,Configurat,Configuration,229,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22416,Deployability,pipeline,pipelined,22416,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42465,Deployability,pipeline,pipelines,42465,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76454,Deployability,configurat,configuration,76454,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76539,Deployability,install,installing,76539,"own ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound*",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76686,Deployability,configurat,configuration-dependent,76686,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:83141,Deployability,update,updated,83141,"s.richUtils.RichDenseMatrixDouble.importFromDoubles(; Env.spark_backend('_breeze_fromfile').fs._jfs, uri, n_rows, n_cols, True; ). def _check_entries_size(n_rows, n_cols):; n_entries = n_rows * n_cols; if n_entries >= 1 << 31:; raise ValueError(f'number of entries must be less than 2^31, found {n_entries}'). def _breeze_from_ndarray(nd):; if any(i == 0 for i in nd.shape):; raise ValueError(f'from_numpy: ndarray dimensions must be non-zero, found shape {nd.shape}'). nd = _ndarray_as_2d(nd); nd = _ndarray_as_float64(nd); n_rows, n_cols = nd.shape. with with_local_temp_file() as path:; uri = local_path_uri(path); nd.tofile(path); return _breeze_fromfile(uri, n_rows, n_cols). def _svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True):; """"""; SciPy supports two Lapack algorithms:; DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesdd; GR: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesvd; DC (gesdd) is faster but uses O(elements) memory; lwork may overflow int32; """"""; try:; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesdd',; ); except ValueError as e:; if 'Too large work array required' in str(e):; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesvd',; ); else:; raise. def _eigh(a):; """"""; Only the lower triangle is used. Returns eigenvalues, eigenvectors.; NumPy and SciPy apply different Lapack algorithms:; NumPy uses DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevd; SciPy uses RRR: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevr; DC (syevd) is faster but uses O(elements) memory; lwork overflows int32 for dim_a > 32766; """"""; return np.linalg.eigh(a) if a.shape[0] <= 32766 else spla.eigh(a). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:3127,Energy Efficiency,power,power,3127,"e_level, with_local_temp_file; from hail.utils.java import Env. block_matrix_type = lazy(). [docs]class BlockMatrix(object):; """"""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exceptio",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:27991,Energy Efficiency,efficient,efficient,27991,"of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(rows_to_keep, self.n_rows); return BlockMatrix(BlockMatrixFilter(self._bmir, [rows_to_keep, []])). [docs] @typecheck_method(cols_to_keep=sequenceof(int)); def filter_cols(self, cols_to_keep):; """"""Filters matrix columns. Parameters; ----------; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(cols_to_keep, self.n_cols); return BlockMatrix(BlockMatrixFilter(self._bmir, [[], cols_to_keep])). [docs] @typecheck_method(rows_to_keep=sequenceof(int), cols_to_keep=sequenceof(int)); def filter(self, rows_to_keep, cols_to_keep):; """"""Filters matrix rows and columns. Notes; -----; This method has the same effect as :meth:`BlockMatrix.filter_cols`; followed by :meth:`BlockMatrix.filter_rows` (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters; ----------; rows_to_keep: :obj:`list` of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(rows_to_keep, self.n_rows); BlockMatrix._check_indices(cols_to_keep, self.n_cols); return BlockMatrix(BlockMatrixFilter(self._bmir, [rows_to_keep, cols_to_keep])). @staticmethod; def _pos_index(i, size, name, allow_size=False):; if 0 <= i < size or (i == size and allow_size):; return i; elif 0 <= i + size < size:; return i + size; else:; raise ValueError(f'invalid {name} {i} for axis of size {size}'). @staticmethod; def _range_to_keep(idx, size):; if isinstance(idx, int):; pos_idx = BlockMatrix._pos_index(idx, size, 'index'); return slice(pos_idx, pos_idx + 1, 1). assert isinstance(idx, slice); if idx.step and idx.step <= 0:; raise ValueE",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:32202,Energy Efficiency,efficient,efficient,32202," by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well. The band is defined in terms of inclusive `lower` and `upper` indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; :math:`(i, j)` such that. .. math::. \mathrm{lower} \leq j - i \leq \mathrm{upper}. `lower` must be less than or equal to `upper`, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters; ----------; lower: :obj:`int`; Index of lowest band relative to the diagonal.; upper: :obj:`int`; Index of highest band relative to the diagonal.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the band to zero.; If ``True``, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower > upper:; raise ValueError(f'sparsify_band: lower={lower} is greater than upper={upper}'). bounds = hl.literal((lower, upper), hl.ttuple(hl.tint64, hl.tint64)); return BlockMatrix(BlockMatrixSparsify(self._bmir, bounds._ir, BandSparsifier(blocks_only))). [docs] @typecheck_method(lower=bool, blocks_only=bool); def sparsify_triangle(self, lower=False, blocks_only=False):; """"""Filter to the upper or lower triangle. Examples; --------; Consider the following block matrix:. >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:34006,Energy Efficiency,efficient,efficient,34006,"e=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:. >>> bm.sparsify_triangle(blocks_only=True).to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters; ----------; lower: :obj:`bool`; If ``False``, keep the upper triangle.; If ``True``, keep the lower triangle.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the triangle to zero.; If ``True``, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if lower:; lower_band = 1 - self.n_rows; upper_band = 0; else:; lower_band = 0; upper_band = self.n_cols - 1. return self.sparsify_band(lower_band, upper_band, blocks_only). @typecheck_method(intervals=expr_tuple([expr_array(expr_int64), expr_array(expr_int64)]), blocks_only=bool); def _sparsify_row_intervals_expr(self, intervals, blocks_only=False):; return BlockMatrix(BlockMatrixSparsify(self._bmir, intervals._ir, RowIntervalSparsifier(blocks_only))). @typecheck_method(indices=expr_array(expr_int32)); def _sparsify_blocks(self, indices):; return BlockMatrix(BlockMatrixSparsify(self._bmir, indices._ir, PerBlockSparsifier())). [docs] @typecheck_method(; starts=oneof(sequenceof(int), np.ndarray), stops=oneof(sequenceof(int), np.ndarray), blocks_only=bool; ); def sparsify_row_intervals(self, starts, stops, blocks_only=False):; """"""Creates a block-sparse matrix by filterin",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:36875,Energy Efficiency,efficient,efficient,36875,"; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well. `starts` and `stops` must both have length equal to the number of; rows. The interval for row ``i`` is ``[starts[i], stops[i])``. In; particular, ``0 <= starts[i] <= stops[i] <= n_cols`` is required; for all ``i``. This method requires the number of rows to be less than :math:`2^{31}`. Parameters; ----------; starts: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Start indices for each row (inclusive).; stops: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Stop indices for each row (exclusive).; blocks_only: :obj:`bool`; If ``False``, set all elements outside row intervals to zero.; If ``True``, only set all blocks outside row intervals to blocks; of zeros; this is more efficient.; Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""; if isinstance(starts, np.ndarray):; if starts.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: starts ndarray must have dtype 'int32' or 'int64'""); starts = [int(s) for s in starts]; if isinstance(stops, np.ndarray):; if stops.dtype not in (np.int32, np.int64):; raise ValueError(""sparsify_row_intervals: stops ndarray must have dtype 'int32' or 'int64'""); stops = [int(s) for s in stops]. n_rows = self.n_rows; n_cols = self.n_cols; if n_rows >= (1 << 31):; raise ValueError(f'n_rows must be less than 2^31, found {n_rows}'); if len(starts) != n_rows or len(stops) != n_rows:; raise ValueError(f'starts and stops must both have length {n_rows} (the number of rows)'); if any([start < 0 for start in starts]):; raise ValueError('all start values must be non-negative'); if any([stop > self.n_cols for stop in stops]):; raise ValueError(f'all stop valu",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55708,Energy Efficiency,reduce,reduce,55708,"index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:57213,Energy Efficiency,reduce,reduce,57213,": {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; delimiter=str,; header=nullable(str),; add_index=bool,; parallel=nullable(ExportType.checker),; partition_size=nullable(int),; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'strict_upper'),; ); def export(; path_in,; path_out,; delimiter='\t',; header=None,; add_index=False,; paral",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:60407,Energy Efficiency,efficient,efficient,60407,"k:: text. idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. .. code-block:: text. idx A B C; 2 0.7 0.3 1.0. Warning; -------; The block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:60535,Energy Efficiency,reduce,reduces,60535,"he block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class:`str`, optional; If ``'header_per_shard'``, create a folder with one file per; partition, each with a header if provid",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72554,Energy Efficiency,reduce,reduced,72554,"-------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \time",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72984,Energy Efficiency,reduce,reduced,72984,"ambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computationa",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:73395,Energy Efficiency,reduce,reduced,73395,"nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :ma",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:74351,Energy Efficiency,reduce,reduced,74351,"most; :math:`r` non-zero singular values. The reduced SVD of :math:`X`; has the form. .. math::. X = U \Sigma V^T. where. - :math:`U` is an :math:`n \times r` matrix whose columns are; (orthonormal) left singular vectors,. - :math:`\Sigma` is an :math:`r \times r` diagonal matrix of non-negative; singular values in descending order,. - :math:`V^T` is an :math:`r \times m` matrix whose rows are; (orthonormal) right singular vectors. If the singular values in :math:`\Sigma` are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; com",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:74916,Energy Efficiency,efficient,efficient,74916,"ding left and; right singular vectors by -1. The computational complexity of SVD is; roughly :math:`nmr`. We now describe the implementation in more detail.; If :math:`\sqrt[3]{nmr}` is less than or equal to `complexity_bound`,; then :math:`X` is localized to an ndarray on which; :func:`scipy.linalg.svd` is called. In this case, all components are; returned as ndarrays. If :math:`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:75214,Energy Efficiency,reduce,reduced,75214,":`\sqrt[3]{nmr}` is greater than `complexity_bound`, then the; reduced SVD is computed via the smaller gramian matrix of :math:`X`. For; :math:`n > m`, the three stages are:. 1. Compute (and localize) the gramian matrix :math:`X^T X`,. 2. Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition :math:`X^T X = V S V^T` with; :func:`numpy.linalg.eigh` or :func:`scipy.linalg.eigh`,. 3. Compute the singular values as :math:`\Sigma = S^\frac{1}{2}` and the; the left singular vectors as the block matrix; :math:`U = X V \Sigma^{-1}`. In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice :math:`U` (e.g. based on the singular values), or; discard :math:`U` entirely. If :math:`n \leq m`, the three stages instead use the gramian; :math:`X X^T = U S U^T` and return :math:`V^T` as the; block matrix :math:`\Sigma^{-1} U^T X`. Warning; -------; Computing reduced SVD via the gramian presents an added wrinkle when; :math:`X` is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it ma",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:4650,Integrability,depend,dependency,4650,". One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the same; effect can be achieved for ``*`` by using ``@``. Warning; -------. For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read(",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:8146,Integrability,depend,depend,8146,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76373,Integrability,depend,depends,76373,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76700,Integrability,depend,dependent,76700,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:229,Modifiability,Config,Configuration,229,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76454,Modifiability,config,configuration,76454,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76686,Modifiability,config,configuration-dependent,76686,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5153,Performance,perform,performance,5153,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5460,Performance,cache,cache,5460,"darray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 rows and 10 columns:. - ``bm[0, 0]`` is the element in row 0 and column 0 of ``bm``. - ``bm[0:1, 0]`` is a block matrix with 1 row, 1 column,; and element ``bm[0, 0]``. - ``bm[2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3,",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:11121,Performance,load,load,11121,"i, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__,; reads a binary file of float64 values in row-major order, such as that; produced by `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__; or :meth:`BlockMatrix.tofile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. A NumPy ndarray must have type float64 for the output of; func:`numpy.tofile` to be a valid binary input to :meth:`.fromfile`.; This is not checked. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary input file.; n_rows: :obj:`int`; Number of rows.; n_cols: :obj:`int`; Number of columns.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`default_block_size`. See Also; --------; :meth:`.from_numpy`; """""". if not block_size:; block_size = BlockMatrix.default_block_size(). return cls(; BlockMatrixRead(BlockMatrixBinaryReader(uri, [n_rows, n_cols], block_size), _assert_type=_assert_type); ). [docs] @classmethod; @typecheck_method(ndarray=np.ndarray, block_size=nullable(int)); def from_numpy(cls, ndarray, block_size=None):; """"""Distributes a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__; as a b",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14308,Performance,perform,performance,14308,", n_rows, n_cols, block_size). [docs] @classmethod; @typecheck_method(; entry_expr=expr_float64,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the;",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14571,Performance,concurren,concurrently,14571,"n_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; path = new_temp_file(); cls.write_from_entry_e",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22343,Performance,load,loaded,22343,"re checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repa",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22593,Performance,perform,performance,22593,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23378,Performance,perform,performance,23378,"BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23439,Performance,concurren,concurrently,23439," transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:39142,Performance,load,load,39142,"ri):; """"""Collects and writes data to a binary file. Examples; --------; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') # doctest: +SKIP. To create a :class:`numpy.ndarray` of the same dimensions:. >>> a = np.fromfile('/local/file').reshape((10, 20)) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__,; produces a binary file of float64 values in row-major order, which can; be read by functions such as `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__; (if a local file) and :meth:`BlockMatrix.fromfile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary output file. See Also; --------; :meth:`.to_numpy`; """"""; hl.current_backend().validate_file(uri). _check_entries_size(self.n_rows, self.n_cols). writer = BlockMatrixBinaryWriter(uri); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(_force_blocking=bool); def to_numpy(self, _force_blocking=False):; """"""Collects the block matrix into a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__. Examples; --------; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; -----; The resulting ndarray will have the same shape as the block matrix. Returns; -------; :class:`numpy.ndarray`; """"""; from hail.backend.service_backend import ServiceBackend. if self.n_rows * self.n_cols > 1 << 31 or _force_blocking:; path = new_temp_file(",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:41819,Performance,cache,cache,41819,"DArrayExpression`; """"""; ir = BlockMatrixCollect(self._bmir); return construct_expr(ir, hl.tndarray(hl.tfloat64, 2)). @property; def is_sparse(self):; """"""Returns ``True`` if block-sparse. Notes; -----; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns; -------; :obj:`bool`; """"""; return Env.backend()._to_java_blockmatrix_ir(self._bmir).typ().isSparse(). @property; def T(self):; """"""Matrix transpose. Returns; -------; :class:`.BlockMatrix`; """"""; if self.n_rows == 1 and self.n_cols == 1:; return self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str;",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42022,Performance,Cache,Cached,42022," Notes; -----; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns; -------; :obj:`bool`; """"""; return Env.backend()._to_java_blockmatrix_ir(self._bmir).typ().isSparse(). @property; def T(self):; """"""Matrix transpose. Returns; -------; :class:`.BlockMatrix`; """"""; if self.n_rows == 1 and self.n_cols == 1:; return self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42316,Performance,cache,cache,42316,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42444,Performance,perform,performance,42444,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55455,Performance,cache,cache,55455,"cache_memory_in_bytes=nullable(int)); def to_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; - **row_idx** (:py:data.`tint64`, key field) -- Row index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Tabl",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:56960,Performance,cache,cache,56960," Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; de",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72330,Performance,load,loadtxt,72330,"ory where rectangles were written.; binary: :obj:`bool`; If true, reads the files as binary, otherwise as text delimited. Returns; -------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76341,Performance,perform,performance,76341,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:8201,Safety,avoid,avoiding,8201,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14054,Safety,avoid,avoid,14054," isinstance(hl.current_backend(), ServiceBackend):; path = hl.TemporaryFilename().name; hl.current_backend().fs.open(path, mode='wb').write(nd.tobytes()); uri = path; else:; path = new_local_temp_file(); nd.tofile(path); uri = local_path_uri(path); return cls.fromfile(uri, n_rows, n_cols, block_size). [docs] @classmethod; @typecheck_method(; entry_expr=expr_float64,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before cen",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42400,Safety,avoid,avoid,42400,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42406,Safety,redund,redundant,42406,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:383,Testability,Log,Log,383,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:9580,Testability,log,logarithm,9580," ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """""". def __init__(self, bmir):; self._bmir = bmir. [docs] @classmethod; @typecheck_method(path=str, _assert_type=nullable(tblockmatrix)); def read(cls, path, *, _assert_type=None):; """"""Reads a block matrix. Parameters; ----------; path: :class:`str`; Path to input file. Returns; -------; :class:`.BlockMatrix`; """"""; return cls(BlockMatrixRead(BlockMatrixNativeReader(path), _assert_type=_assert_type)). [docs] @classmethod; @typecheck_method(uri=str, n_rows=int, n_cols=int, block_size=nullable(int), _assert_type=nullable(tblockmatrix)); def fromfile(cls, uri, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https:/",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:9598,Testability,log,log,9598," ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """""". def __init__(self, bmir):; self._bmir = bmir. [docs] @classmethod; @typecheck_method(path=str, _assert_type=nullable(tblockmatrix)); def read(cls, path, *, _assert_type=None):; """"""Reads a block matrix. Parameters; ----------; path: :class:`str`; Path to input file. Returns; -------; :class:`.BlockMatrix`; """"""; return cls(BlockMatrixRead(BlockMatrixNativeReader(path), _assert_type=_assert_type)). [docs] @classmethod; @typecheck_method(uri=str, n_rows=int, n_cols=int, block_size=nullable(int), _assert_type=nullable(tblockmatrix)); def fromfile(cls, uri, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https:/",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:17878,Testability,test,test,17878,"n, [n_rows, n_cols], block_size); return BlockMatrix(rand). [docs] @classmethod; @typecheck_method(n_rows=int, n_cols=int, value=numeric, block_size=nullable(int)); def fill(cls, n_rows, n_cols, value, block_size=None):; """"""Creates a block matrix with all elements the same value. Examples; --------; Create a block matrix with 10 rows, 20 columns, and all elements equal to ``1.0``:. >>> bm = BlockMatrix.fill(10, 20, 1.0). Parameters; ----------; n_rows: :obj:`int`; Number of rows.; n_cols: :obj:`int`; Number of columns.; value: :obj:`float`; Value of all elements.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`default_block_size`. Returns; -------; :class:`.BlockMatrix`; """"""; if not block_size:; block_size = BlockMatrix.default_block_size(). bmir = BlockMatrixBroadcast(_to_bmir(value, block_size), [], [n_rows, n_cols], block_size); return BlockMatrix(bmir). @classmethod; @typecheck_method(n_rows=int, n_cols=int, data=sequenceof(float), block_size=nullable(int)); def _create(cls, n_rows, n_cols, data, block_size=None):; """"""Private method for creating small test matrices."""""". if block_size is None:; block_size = BlockMatrix.default_block_size(). return BlockMatrix(ValueToBlockMatrix(hl.literal(data)._ir, [n_rows, n_cols], block_size)). [docs] @classmethod; @typecheck_method(ndarray_expression=expr_ndarray(), block_size=int); def from_ndarray(cls, ndarray_expression, block_size=4096):; """"""Create a BlockMatrix from an ndarray""""""; if ndarray_expression.dtype.element_type != hl.tfloat64:; raise ValueError(""BlockMatrix.from_ndarray expects an ndarray of type float64""). shape = hl.eval(ndarray_expression.shape). if shape is None:; raise ValueError(""Cannot make a BlockMatrix from a missing NDArray""); return BlockMatrix(ValueToBlockMatrix(ndarray_expression._ir, shape, block_size)). [docs] @staticmethod; def default_block_size():; """"""Default block side length."""""". # This should match BlockMatrix.defaultBlockSize in the Scala backend.; return 4096 # 32 * ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:28874,Testability,assert,assert,28874,"rix.filter_rows` (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters; ----------; rows_to_keep: :obj:`list` of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""; BlockMatrix._check_indices(rows_to_keep, self.n_rows); BlockMatrix._check_indices(cols_to_keep, self.n_cols); return BlockMatrix(BlockMatrixFilter(self._bmir, [rows_to_keep, cols_to_keep])). @staticmethod; def _pos_index(i, size, name, allow_size=False):; if 0 <= i < size or (i == size and allow_size):; return i; elif 0 <= i + size < size:; return i + size; else:; raise ValueError(f'invalid {name} {i} for axis of size {size}'). @staticmethod; def _range_to_keep(idx, size):; if isinstance(idx, int):; pos_idx = BlockMatrix._pos_index(idx, size, 'index'); return slice(pos_idx, pos_idx + 1, 1). assert isinstance(idx, slice); if idx.step and idx.step <= 0:; raise ValueError(f'slice step must be positive, found {idx.step}'). start = 0 if idx.start is None else BlockMatrix._pos_index(idx.start, size, 'start index'); stop = size if idx.stop is None else BlockMatrix._pos_index(idx.stop, size, 'stop index', allow_size=True); step = 1 if idx.step is None else idx.step. if start < stop:; return slice(start, stop, step); else:; raise ValueError(f'slice {start}:{stop}:{step} is empty'). @typecheck_method(indices=tupleof(oneof(int, sliceof(nullable(int), nullable(int), nullable(int))))); def __getitem__(self, indices):; if len(indices) != 2:; raise ValueError(f'tuple of indices or slices must have length two, found {len(indices)}'). row_idx, col_idx = indices. if isinstance(row_idx, int) and isinstance(col_idx, int):; i = BlockMatrix._pos_index(row_idx, self.n_rows, 'row index'); j = BlockMatrix._pos_index(col_idx, self.n_cols, 'col index'). return Env.backend().execute(BlockMatrixToValueApply(self._bmir,",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:51683,Testability,log,log,51683,"numeric); def __pow__(self, x):; """"""Element-wise exponentiation: a ** x. Parameters; ----------; x: :obj:`int` or :obj:`float`; Exponent. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda i: i**x, needs_dense=False). def _map_dense(self, func):; return self._apply_map(func, True). def _map_sparse(self, func):; return self._apply_map(func, False). [docs] def sqrt(self):; """"""Element-wise square root. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.sqrt, needs_dense=False). [docs] def ceil(self):; """"""Element-wise ceiling. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.ceil, needs_dense=False). [docs] def floor(self):; """"""Element-wise floor. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.floor, needs_dense=False). [docs] def abs(self):; """"""Element-wise absolute value. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.abs, needs_dense=False). [docs] def log(self):; """"""Element-wise natural logarithm. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda x: hl.log(x), needs_dense=True). [docs] def diagonal(self):; """"""Extracts diagonal elements as a row vector. Returns; -------; :class:`.BlockMatrix`; """"""; diag_bmir = BlockMatrixBroadcast(self._bmir, [0, 0], [1, min(self.n_rows, self.n_cols)], self.block_size); return BlockMatrix(diag_bmir). [docs] @typecheck_method(axis=nullable(int)); def sum(self, axis=None):; """"""Sums array elements over one or both axes. Examples; --------; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters; ----------; axis: :obj:`int`, optional; Axis over which to sum.; By default, sum all elements.; If ``0``, sum over rows.; If ``1``, sum over columns. Returns; -------; :obj:`float` or :class:`BlockMa",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:51719,Testability,log,logarithm,51719,"numeric); def __pow__(self, x):; """"""Element-wise exponentiation: a ** x. Parameters; ----------; x: :obj:`int` or :obj:`float`; Exponent. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda i: i**x, needs_dense=False). def _map_dense(self, func):; return self._apply_map(func, True). def _map_sparse(self, func):; return self._apply_map(func, False). [docs] def sqrt(self):; """"""Element-wise square root. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.sqrt, needs_dense=False). [docs] def ceil(self):; """"""Element-wise ceiling. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.ceil, needs_dense=False). [docs] def floor(self):; """"""Element-wise floor. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.floor, needs_dense=False). [docs] def abs(self):; """"""Element-wise absolute value. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.abs, needs_dense=False). [docs] def log(self):; """"""Element-wise natural logarithm. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda x: hl.log(x), needs_dense=True). [docs] def diagonal(self):; """"""Extracts diagonal elements as a row vector. Returns; -------; :class:`.BlockMatrix`; """"""; diag_bmir = BlockMatrixBroadcast(self._bmir, [0, 0], [1, min(self.n_rows, self.n_cols)], self.block_size); return BlockMatrix(diag_bmir). [docs] @typecheck_method(axis=nullable(int)); def sum(self, axis=None):; """"""Sums array elements over one or both axes. Examples; --------; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters; ----------; axis: :obj:`int`, optional; Axis over which to sum.; By default, sum all elements.; If ``0``, sum over rows.; If ``1``, sum over columns. Returns; -------; :obj:`float` or :class:`BlockMa",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:51812,Testability,log,log,51812,"`; Exponent. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda i: i**x, needs_dense=False). def _map_dense(self, func):; return self._apply_map(func, True). def _map_sparse(self, func):; return self._apply_map(func, False). [docs] def sqrt(self):; """"""Element-wise square root. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.sqrt, needs_dense=False). [docs] def ceil(self):; """"""Element-wise ceiling. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.ceil, needs_dense=False). [docs] def floor(self):; """"""Element-wise floor. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.floor, needs_dense=False). [docs] def abs(self):; """"""Element-wise absolute value. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(hl.abs, needs_dense=False). [docs] def log(self):; """"""Element-wise natural logarithm. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda x: hl.log(x), needs_dense=True). [docs] def diagonal(self):; """"""Extracts diagonal elements as a row vector. Returns; -------; :class:`.BlockMatrix`; """"""; diag_bmir = BlockMatrixBroadcast(self._bmir, [0, 0], [1, min(self.n_rows, self.n_cols)], self.block_size); return BlockMatrix(diag_bmir). [docs] @typecheck_method(axis=nullable(int)); def sum(self, axis=None):; """"""Sums array elements over one or both axes. Examples; --------; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters; ----------; axis: :obj:`int`, optional; Axis over which to sum.; By default, sum all elements.; If ``0``, sum over rows.; If ``1``, sum over columns. Returns; -------; :obj:`float` or :class:`BlockMatrix`; If None, returns a float.; If ``0``, returns a block matrix with a single row.; If ``1``, returns a block matrix with ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:61,Usability,Feedback,Feedback,61,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:271,Usability,Guid,Guides,271,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22673,Usability,clear,clear,22673,"rix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.bu",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42713,Usability,guid,guide,42713," zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpersisted block matrix.; """"""; return Env.backend().unpersist_blockmatrix(self). def __pos__(self):; return self. def __neg__(self):; """"""Negation: -a. Returns; -------; :class:`.BlockMatrix`; """"""; return self._apply_map(lambda x: construct_expr(ApplyUnaryPrimOp('-', x._ir), hl.tfloat64), needs_dense=False). ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4236,Availability,error,errors,4236,"d:int,is_female:bool,fam_id:str}>'). trios_sym = Env.get_uid(); entries_sym = Env.get_uid(); cols_sym = Env.get_uid(). mt = mt.annotate_globals(**{trios_sym: hl.literal(trios, trios_type)}); mt = mt._localize_entries(entries_sym, cols_sym); mt = mt.annotate_globals(**{; cols_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; id=mt[cols_sym][t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four ta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4566,Availability,error,errors,4566,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4574,Availability,error,errors,4574,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4592,Availability,error,errors,4592,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4615,Availability,error,errors,4615,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4789,Availability,error,errors,4789,"bda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4911,Availability,error,errors,4911,"ym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5037,Availability,error,errors,5037,"trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keye",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5500,Availability,error,errors,5500,"ance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors pe",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5548,Availability,error,error,5548,"four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. E",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5900,Availability,error,error,5900,"of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`)",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5941,Availability,error,errors,5941,"dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this;",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6286,Availability,error,errors,6286,"e `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with;",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6335,Availability,error,errors,6335,"genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel err",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6423,Availability,error,errors,6423,"`, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6480,Availability,error,errors,6480,"ble:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the tab",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6553,Availability,error,error,6553,":class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6782,Availability,error,errors,6782,"nd ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6831,Availability,error,errors,6831,"tr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - Hem",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6923,Availability,error,errors,6923,"nd table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6984,Availability,error,errors,6984,"ins one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; de",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7149,Availability,error,errors,7149," - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+===",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7198,Availability,error,errors,7198,"y ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+=========",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7333,Availability,error,error,7333," - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+===============+; | 1 | HomVar | HomVar | Het | Auto | Dad, Mom, Kid |; +------+---------+---------+--------+------------+---------------+; | 2 | HomRef | HomRef | Het | Auto | Dad",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10691,Availability,error,errors,10691,"meters; ----------; dataset : :class:`.MatrixTable`; pedigree : :class:`.Pedigree`. Returns; -------; (:class:`.Table`, :class:`.Table`, :class:`.Table`, :class:`.Table`); """"""; source = call._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""'mendel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_el",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10933,Availability,error,errors,10933,":; raise ValueError(; ""'mendel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = ta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10947,Availability,error,errors,10947,"endel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.st",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11125,Availability,error,errors,11125,"alar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11150,Availability,error,errors,11150,"e.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11233,Availability,error,errors,11233,"el_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11258,Availability,error,errors,11258,"ataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struc",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12032,Availability,error,errors,12032,"aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12182,Availability,error,errors,12182,"le2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12333,Availability,error,errors,12333,". # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctes",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12540,Availability,error,errors,12540,"],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12565,Availability,error,errors,12565,"],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> |",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12662,Availability,error,errors,12662,"e_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14960,Availability,error,errors,14960,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18631,Availability,error,error,18631,"nnotate_rows(auto_or_x_par=dataset.locus.in_autosome() | dataset.locus.in_x_par()); dataset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24688,Availability,error,error,24688,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:29286,Availability,failure,failure,29286,"R),; ); else:; n_alt_alleles = hl.agg.sum(mt.GT.n_alt_alleles()); total_alleles = 2 * hl.agg.sum(hl.is_defined(mt.GT)); # subtract 1 from __alt_alleles to correct for the observed genotype; mt = mt.annotate_rows(; __prior=pop_frequency_prior,; __alt_alleles=n_alt_alleles,; __site_freq=hl.max((n_alt_alleles - 1) / total_alleles, pop_frequency_prior, MIN_POP_PRIOR),; ). mt = require_biallelic(mt, 'de_novo'). tm = trio_matrix(mt, pedigree, complete_trios=True). autosomal = tm.locus.in_autosome_or_par() | (tm.locus.in_x_nonpar() & tm.is_female); hemi_x = tm.locus.in_x_nonpar() & ~tm.is_female; hemi_y = tm.locus.in_y_nonpar() & ~tm.is_female; hemi_mt = tm.locus.in_mito() & tm.is_female. is_snp = hl.is_snp(tm.alleles[0], tm.alleles[1]); n_alt_alleles = tm.__alt_alleles; prior = tm.__site_freq; het_hom_hom = tm.proband_entry.GT.is_het() & tm.father_entry.GT.is_hom_ref() & tm.mother_entry.GT.is_hom_ref(); kid_ad_fail = tm.proband_entry.AD[1] / hl.sum(tm.proband_entry.AD) < min_child_ab. failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30213,Availability,failure,failure,30213,"il = tm.proband_entry.AD[1] / hl.sum(tm.proband_entry.AD) < min_child_ab. failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30308,Availability,failure,failure,30308,"failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30371,Availability,failure,failure,30371,".proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30482,Availability,failure,failure,30482," = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30519,Availability,failure,failure,30519," kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:31871,Availability,failure,failure,31871,"novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:31958,Availability,failure,failure,31958,".when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32000,Availability,failure,failure,32000,"> 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confiden",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32066,Availability,failure,failure,32066,"t_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32101,Availability,failure,failure,32101," 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case();",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:33130,Availability,failure,failure,33130,"iven_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case(); .when(~het_hom_hom | kid_ad_fail, failure); .when(autosomal, hl.bind(call_auto, kid_pp, dad_pp, mom_pp, kid_ad_ratio)); .when(hemi_x | hemi_mt, hl.bind(call_hemi, kid_pp, mom, mom_pp, kid_ad_ratio)); .when(hemi_y, hl.bind(call_hemi, kid_pp, dad, dad_pp, kid_ad_ratio)); .or_missing(); ). tm = tm.annotate_entries(__call=de_novo_call); tm = tm.filter_entries(hl.is_defined(tm.__call)); entries = tm.entries(); return entries.select(; '__site_freq',; 'proband',; 'father',; 'mother',; 'proband_entry',; 'father_entry',; 'mother_entry',; 'is_female',; **entries.__call,; ).rename({'__site_freq': 'prior'}). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:165,Deployability,Install,Installation,165,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:233,Deployability,Configurat,Configuration,233,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14866,Deployability,configurat,configurations,14866,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14971,Deployability,configurat,configurations,14971,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22386,Deployability,configurat,configuration,22386,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:33740,Deployability,update,updated,33740,"iven_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case(); .when(~het_hom_hom | kid_ad_fail, failure); .when(autosomal, hl.bind(call_auto, kid_pp, dad_pp, mom_pp, kid_ad_ratio)); .when(hemi_x | hemi_mt, hl.bind(call_hemi, kid_pp, mom, mom_pp, kid_ad_ratio)); .when(hemi_y, hl.bind(call_hemi, kid_pp, dad, dad_pp, kid_ad_ratio)); .or_missing(); ). tm = tm.annotate_entries(__call=de_novo_call); tm = tm.filter_entries(hl.is_defined(tm.__call)); entries = tm.entries(); return entries.select(; '__site_freq',; 'proband',; 'father',; 'mother',; 'proband_entry',; 'father_entry',; 'mother_entry',; 'is_female',; **entries.__call,; ).rename({'__site_freq': 'prior'}). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:23274,Integrability,depend,depends,23274,"rents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid d)` and :math:`\mathrm{P}(x \mid m)`; are computed from the PL (genotype likelihood) fields using these; factorizations:. .. math::; \mathrm{P}(x = (AA, AA, AB) \mid d) = \left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:233,Modifiability,Config,Configuration,233,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4482,Modifiability,inherit,inheritance,4482,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7374,Modifiability,extend,extending,7374," - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+===============+; | 1 | HomVar | HomVar | Het | Auto | Dad, Mom, Kid |; +------+---------+---------+--------+------------+---------------+; | 2 | HomRef | HomRef | Het | Auto | Dad",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14866,Modifiability,config,configurations,14866,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14971,Modifiability,config,configurations,14971,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18706,Modifiability,config,config,18706,"aset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: Matrix",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18977,Modifiability,config,config,18977,"f, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:19211,Modifiability,config,config,19211," (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pe",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22386,Modifiability,config,configuration,22386,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:25011,Modifiability,variab,variables,25011,"); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:. .. code-block:: text. (p > 0.5) AND (AB > 0.3) A",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12896,Performance,Perform,Performs,12896,"tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 | 4.00e+00 | 4.55e-02 |; | 2:167262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+---------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:19453,Performance,cache,cache,19453,"nt_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:20555,Performance,throughput,throughput,20555,",; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; -----; This method assumes the GATK high-throughput sequencing fields exist:; `GT`, `AD`, `DP`, `GQ`, `PL`. This method replicates the functionality of `Kaitlin Samocha's de novo; caller <https://github.com/ksamocha/de_novo_scripts>`__. The version; corresponding to git commit ``bde3e40`` is implemented in Hail with her; permission and assistance. This method produces a :class:`.Table` with the following fields:. - `locus` (``locus``) -- Variant locus.; - `alleles` (``array<str>``) -- Variant alleles.; - `id` (``str``) -- Proband sample ID.; - `prior` (``float64``) -- Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; `pop_frequency_prior` parameter, and the global prior; ``1 / 3e7``. If the `ignore_in_sample_allele_frequency` parameter is ``True``,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Fath",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24709,Performance,throughput,throughput,24709,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:26566,Performance,throughput,throughput,26566," 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:. .. code-block:: text. (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:. .. code-block:: text. (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the `min_gq` parameter, if the proband allele balance is; lower than the `min_child_ab` parameter, if the depth ratio between the; proband and parents is smaller than the `min_depth_ratio` parameter, if; the allele balance in a parent is above the `max_parent_ab` parameter, or; if the posterior probability `p` is smaller than the `min_p` parameter. Parameters; ----------; mt : :class:`.MatrixTable`; High-throughput sequencing dataset.; pedigree : :class:`.Pedigree`; Sample pedigree.; pop_frequency_prior : :class:`.Float64Expression`; Expression for population alternate allele frequency prior.; min_gq; Minimum proband GQ to be considered for *de novo* calling.; min_p; Minimum posterior probability to be considered for *de novo* calling.; max_parent_ab; Maximum parent allele balance.; min_child_ab; Minimum proband allele balance/; min_dp_ratio; Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency; Ignore in-sample allele frequency in computing site prior. Experimental.; Returns; -------; :class:`.Table`; """"""; DE_NOVO_PRIOR = 1 / 30000000; MIN_POP_PRIOR = 100 / 30000000. required_entry_fields = {'GT', 'AD', 'DP', 'GQ', 'PL'}; missing_fields = required_entry_fields - set(mt.entry); if missing_fields:; raise ValueError(; f""'de_novo': expected 'MatrixTable' to have at least {required_entry_fields}, "" f""missing {missing_fields}""; ). pop_frequency_prio",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18672,Safety,avoid,avoids,18672,"aset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: Matrix",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22084,Security,Validat,Validation,22084,"4``) -- Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; `pop_frequency_prior` parameter, and the global prior; ``1 / 3e7``. If the `ignore_in_sample_allele_frequency` parameter is ``True``,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24653,Security,validat,validation,24653,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24890,Security,validat,validation,24890,"n{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:387,Testability,Log,Log,387,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12937,Testability,test,test,12937,"tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 | 4.00e+00 | 4.55e-02 |; | 2:167262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+---------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14065,Testability,test,test,14065,"de:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 | 4.00e+00 | 4.55e-02 |; | 2:167262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:. >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus wi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:65,Usability,Feedback,Feedback,65,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:275,Usability,Guid,Guides,275,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22347,Usability,simpl,simplifying,22347,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15392,Availability,error,errors,15392,"call}. fam_exprs = {; 'fam_id': expr_or_else(fam_id, '0'),; 'ind_id': hl.or_else(ind_id, '0'),; 'pat_id': expr_or_else(pat_id, '0'),; 'mat_id': expr_or_else(mat_id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15536,Availability,error,errors,15536,"expr_or_else(pat_id, '0'),; 'mat_id': expr_or_else(mat_id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15628,Availability,error,errors,15628,"id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15663,Availability,error,errors,15663,",; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the output file name; for `blocked GZIP <http://www.htslib.org/doc/tabix.html>`__ compressi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15698,Availability,error,errors,15698,"(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the output file name; for `blocked GZIP <http://www.htslib.org/doc/tabix.html>`__ compression. Note; ----; We stron",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:19366,Availability,down,downstream,19366,"om their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the `metadata` parameter to supply a dictionary; with the relevant information. See; :func:`get_vcf_metadata` for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured. The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset. The output VCF header will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, re",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:33133,Availability,error,error,33133,"alid_intervals; ),; target=t['f3'],; ). else:; raise FatalError(""too few fields for BED file: expected 3 or more, but found {}"".format(len(t.row))). if skip_invalid_intervals and reference_genome:; t = t.filter(hl.is_defined(t.interval)). return t.key_by('interval'). [docs]@typecheck(path=str, quant_pheno=bool, delimiter=str, missing=str); def import_fam(path, quant_pheno=False, delimiter=r'\\s+', missing='NA') -> Table:; """"""Import a PLINK FAM file into a :class:`.Table`. Examples; --------. Import a tab-separated; `FAM file <https://www.cog-genomics.org/plink2/formats#fam>`__; with a case-control phenotype:. >>> fam_kt = hl.import_fam('data/case_control_study.fam'). Import a FAM file with a quantitative phenotype:. >>> fam_kt = hl.import_fam('data/quantitative_study.fam', quant_pheno=True). Notes; -----. In Hail, unlike PLINK, the user must *explicitly* distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype with ``quant_pheno=False`` will return an error; (unless all values happen to be `0`, `1`, `2`, or `-9`):. The resulting :class:`.Table` will have fields, types, and values that are interpreted as missing. - *fam_id* (:py:data:`.tstr`) -- Family ID (missing = ""0""); - *id* (:py:data:`.tstr`) -- Sample ID (key column); - *pat_id* (:py:data:`.tstr`) -- Paternal ID (missing = ""0""); - *mat_id* (:py:data:`.tstr`) -- Maternal ID (missing = ""0""); - *is_female* (:py:data:`.tstr`) -- Sex (missing = ""NA"", ""-9"", ""0""). One of:. - *is_case* (:py:data:`.tbool`) -- Case-control phenotype (missing = ""0"", ""-9"",; non-numeric or the ``missing`` argument, if given.; - *quant_pheno* (:py:data:`.tfloat64`) -- Quantitative phenotype (missing = ""NA"" or; the ``missing`` argument, if given. Warning; -------; Hail will interpret the value ""-9"" as a valid quantitative phenotype, which; differs from default PLINK behavior. Use ``missing='-9'`` to interpret this; value as missing. Parameters; ----------; path : :class:`str`; Path to FAM file.; quant_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:47079,Availability,toler,tolerance,47079,"e TypeError(; f""'import_bgen' requires all elements in 'variants' are a non-empty prefix of the BGEN key type: {expected_vtype!r}""; ). vir = variants._tir; if (; isinstance(vir, ir.TableRead); and isinstance(vir.reader, ir.TableNativeReader); and vir.reader.intervals is None; and variants.count() == variants.distinct().count(); ):; variants_path = vir.reader.path; else:; variants_path = new_temp_file(prefix='bgen_included_vars', extension='ht'); variants.distinct().write(variants_path); else:; variants_path = None. reader = ir.MatrixBGENReader(path, sample_file, index_file_map, n_partitions, block_size, variants_path). mt = MatrixTable(ir.MatrixRead(reader)).drop(; *[fd for fd in ['GT', 'GP', 'dosage'] if fd not in entry_set],; *[fd for fd in ['rsid', 'varid', 'offset', 'file_idx'] if fd not in row_set],; ). return mt. [docs]@typecheck(; path=oneof(str, sequenceof(str)),; sample_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:47324,Availability,toler,tolerance,47324,"e TypeError(; f""'import_bgen' requires all elements in 'variants' are a non-empty prefix of the BGEN key type: {expected_vtype!r}""; ). vir = variants._tir; if (; isinstance(vir, ir.TableRead); and isinstance(vir.reader, ir.TableNativeReader); and vir.reader.intervals is None; and variants.count() == variants.distinct().count(); ):; variants_path = vir.reader.path; else:; variants_path = new_temp_file(prefix='bgen_included_vars', extension='ht'); variants.distinct().write(variants_path); else:; variants_path = None. reader = ir.MatrixBGENReader(path, sample_file, index_file_map, n_partitions, block_size, variants_path). mt = MatrixTable(ir.MatrixRead(reader)).drop(; *[fd for fd in ['GT', 'GP', 'dosage'] if fd not in entry_set],; *[fd for fd in ['rsid', 'varid', 'offset', 'file_idx'] if fd not in row_set],; ). return mt. [docs]@typecheck(; path=oneof(str, sequenceof(str)),; sample_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:49605,Availability,toler,tolerance,49605,"ence_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:49933,Availability,toler,tolerance,49933,"lumn if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""; gen_table = import_lines(path, min_partitions); sample_table = import_lines(sample_file); rg = reference_genome.name if reference_genome else None; if contig_recoding is None:; contig_recoding = hl.empty_dict(hl.tstr, hl.tstr); else:; contig_recoding = hl.dict(contig_recoding). gen_table = gen_table",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:50049,Availability,toler,tolerance,50049,"ata:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""; gen_table = import_lines(path, min_partitions); sample_table = import_lines(sample_file); rg = reference_genome.name if reference_genome else None; if contig_recoding is None:; contig_recoding = hl.empty_dict(hl.tstr, hl.tstr); else:; contig_recoding = hl.dict(contig_recoding). gen_table = gen_table.transmute(data=gen_table.text.split(' ')). if chromosome is None:; last_rowf_i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:52729,Availability,toler,tolerance,52729,"us(contig_holder, position, rg). gen_table = gen_table.annotate(locus=locus, alleles=alleles, rsid=rsid, varid=varid); gen_table = gen_table.annotate(; entries=gen_table.data[last_rowf_idx + 1 :]; .map(lambda x: hl.float64(x)); .grouped(3); .map(lambda x: hl.struct(GP=x)); ); if skip_invalid_loci:; gen_table = gen_table.filter(hl.is_defined(gen_table.locus)). sample_table_count = sample_table.count() - 2 # Skipping first 2 unneeded rows in sample file; gen_table = gen_table.annotate_globals(cols=hl.range(sample_table_count).map(lambda x: hl.struct(col_idx=x))); mt = gen_table._unlocalize_entries('entries', 'cols', ['col_idx']). sample_table = sample_table.tail(sample_table_count).add_index(); sample_table = sample_table.annotate(s=sample_table.text.split(' ')[0]); sample_table = sample_table.key_by(sample_table.idx); mt = mt.annotate_cols(s=sample_table[hl.int64(mt.col_idx)].s). mt = mt.annotate_entries(; GP=hl.rbind(; hl.sum(mt.GP),; lambda gp_sum: hl.if_else(; hl.abs(1.0 - gp_sum) > tolerance, hl.missing(hl.tarray(hl.tfloat64)), hl.abs((1 / gp_sum) * mt.GP); ),; ); ); mt = mt.annotate_entries(; GT=hl.rbind(; hl.argmax(mt.GP),; lambda max_idx: hl.if_else(; hl.len(mt.GP.filter(lambda y: y == mt.GP[max_idx])) == 1,; hl.switch(max_idx); .when(0, hl.call(0, 0)); .when(1, hl.call(0, 1)); .when(2, hl.call(1, 1)); .or_error(""error creating gt field.""),; hl.missing(hl.tcall),; ),; ); ); mt = mt.filter_entries(hl.is_defined(mt.GP)). mt = mt.key_cols_by('s').drop('col_idx', 'file', 'data'); mt = mt.key_rows_by('locus', 'alleles').select_entries('GT', 'GP'); return mt. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; delimiter=str,; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; force_bgz=bool,; filter=nullable(str),; find_replace=nullable(sized_tupleof(str, str)),; force=bool,; sour",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:53070,Availability,error,error,53070,"able.locus)). sample_table_count = sample_table.count() - 2 # Skipping first 2 unneeded rows in sample file; gen_table = gen_table.annotate_globals(cols=hl.range(sample_table_count).map(lambda x: hl.struct(col_idx=x))); mt = gen_table._unlocalize_entries('entries', 'cols', ['col_idx']). sample_table = sample_table.tail(sample_table_count).add_index(); sample_table = sample_table.annotate(s=sample_table.text.split(' ')[0]); sample_table = sample_table.key_by(sample_table.idx); mt = mt.annotate_cols(s=sample_table[hl.int64(mt.col_idx)].s). mt = mt.annotate_entries(; GP=hl.rbind(; hl.sum(mt.GP),; lambda gp_sum: hl.if_else(; hl.abs(1.0 - gp_sum) > tolerance, hl.missing(hl.tarray(hl.tfloat64)), hl.abs((1 / gp_sum) * mt.GP); ),; ); ); mt = mt.annotate_entries(; GT=hl.rbind(; hl.argmax(mt.GP),; lambda max_idx: hl.if_else(; hl.len(mt.GP.filter(lambda y: y == mt.GP[max_idx])) == 1,; hl.switch(max_idx); .when(0, hl.call(0, 0)); .when(1, hl.call(0, 1)); .when(2, hl.call(1, 1)); .or_error(""error creating gt field.""),; hl.missing(hl.tcall),; ),; ); ); mt = mt.filter_entries(hl.is_defined(mt.GP)). mt = mt.key_cols_by('s').drop('col_idx', 'file', 'data'); mt = mt.key_rows_by('locus', 'alleles').select_entries('GT', 'GP'); return mt. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; delimiter=str,; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; force_bgz=bool,; filter=nullable(str),; find_replace=nullable(sized_tupleof(str, str)),; force=bool,; source_file_field=nullable(str),; ); def import_table(; paths,; key=None,; min_partitions=None,; impute=False,; no_header=False,; comment=(),; delimiter=""\t"",; missing=""NA"",; types={},; quote=None,; skip_blank_lines=False,; force_bgz=False,; filter=None,; find_replace=None,; force=False,; source_file_field=None,; ) -> Table:; """"""Import delimited text",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:59579,Availability,error,error,59579,"Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:66441,Availability,Error,Error,66441,"----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir.StringTableReader(paths, min_partitions, force_bgz, force, file_per_partition); table_type = hl.ttable(global_type=hl.tstruct(), row_type=hl.tstruct(file=hl.tstr, text=hl.tstr), row_key=[]); string_table = Table(ir.TableRead(st_reader, _assert_type=table_type)); return string_table. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; row_fields=dictof(str, hail_type),; row_key=oneof(str, sequenceof(str)),; entry_type=enumeration(tint32, tint64, tfloat32, tfloat64, tstr),; missing=str,; min_partitions=nullable(int),; no_header=bool,; force_bgz=bool,; sep=nullable(str),; delimiter=nul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:76807,Availability,error,error,76807,"ile, True)); + hl.str("" on line ""); + hl.str(row.row_id - get_file_start(row) + 1); + hl.str("" at value '""); + hl.str(row.split_array[idx]); + hl.str(""':\n""); + hl.str(msg); ). def parse_type_or_error(hail_type, row, idx, not_entries=True):; value = row.split_array[idx]; if hail_type == hl.tint32:; parsed_type = hl.parse_int32(value); elif hail_type == hl.tint64:; parsed_type = hl.parse_int64(value); elif hail_type == hl.tfloat32:; parsed_type = hl.parse_float32(value); elif hail_type == hl.tfloat64:; parsed_type = hl.parse_float64(value); else:; parsed_type = value. if not_entries:; error_clarify_msg = hl.str("" at row field '"") + hl.str(hl_row_fields[idx]) + hl.str(""'""); else:; error_clarify_msg = (; hl.str("" at column id '""); + hl.str(hl_columns[idx - num_of_row_fields]); + hl.str(""' for entry field 'x' ""); ). return hl.if_else(; hl.is_missing(value),; hl.missing(hail_type),; hl.case(); .when(~hl.is_missing(parsed_type), parsed_type); .or_error(error_msg(row, idx, f""error parsing value into {hail_type!s}"" + error_clarify_msg)),; ). num_of_row_fields = len(row_fields.keys()); add_row_id = False; if len(row_key) == 0:; add_row_id = True; row_key = ['row_id']. if sep is not None:; if delimiter is not None:; raise ValueError(f'expecting either sep or delimiter but received both: ' f'{sep}, {delimiter}'); delimiter = sep; del sep. if delimiter is None:; delimiter = '\t'; if len(delimiter) != 1:; raise FatalError('delimiter or sep must be a single character'). if add_row_id:; if 'row_id' in row_fields:; raise FatalError(; ""import_matrix_table reserves the field name 'row_id' for"" 'its own use, please use a different name'; ). for k, v in row_fields.items():; if v not in {tint32, tint64, tfloat32, tfloat64, tstr}:; raise FatalError(; f'import_matrix_table expects field types to be one of:'; f""'int32', 'int64', 'float32', 'float64', 'str': field {k!r} had type '{v}'""; ). if entry_type not in {tint32, tint64, tfloat32, tfloat64, tstr}:; raise FatalError(; """"""import_matrix_t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101853,Availability,down,downstream,101853,"ant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:117882,Availability,error,error,117882,"aths : :class:`str` or :obj:`list` of :obj:`str`; Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:156,Deployability,Install,Installation,156,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:224,Deployability,Configurat,Configuration,224,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:19950,Deployability,pipeline,pipeline,19950,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112139,Deployability,configurat,configuration,112139,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112846,Deployability,patch,patch,112846,". note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length. original_determine_file_length = DataFileReader.determine_file_length. try:; DataFileReader.determine_file_length = patched_determine_file_length. with DataFileReader(avro_file, DatumReader()) as data_file_reader:; tr = ir.AvroTableReader(avro.schema.parse(data_file_reader.schema), paths, key, intervals). finally:; DataFileReader.determine_file_length = original_determine_file_length. return Table(ir.TableRead(tr)). @typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; f",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:119522,Deployability,update,updated,119522," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:20905,Integrability,interface,interface,20905,"a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""; hl.current_backend().validate_file(output). _, ext = os.path.splitext(output); if ext == '.gz':; warning(; 'VCF export with standard gzip compression requested. This is almost *never* desired and will '; 'cause issues with other tools that consume VCF files. The compression format used for VCF '; 'files is traditionally *block* gzip compression. To use block gzip compression with hail VCF '; 'export, use a path ending in `.bgz`.'; ). if isinstance(dataset, Table):; mt = MatrixTable.from_rows_table(dataset); dataset = mt.key_cols_by(sample=""""). require_col_key_str(dataset, 'export_vcf'); require_row_key_variant(dataset, 'export_vcf'). if 'filters' in dataset.row and dataset.filters.dtype != hl.tset(hl.tstr):; raise ValueError(; f""'export_vcf': expect the 'filters' field to be set<str>, found {dataset.filters.dtype}""; f""\n Either transform this field to set<str> to export as VCF FILTERS field, or drop it from the dataset.""; ). in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:89395,Integrability,depend,dependent,89395,"-------; bed : :class:`str`; PLINK BED file. bim : :class:`str`; PLINK BIM file. fam : :class:`str`; PLINK FAM file. min_partitions : :obj:`int`, optional; Minimum number of partitions. Useful in conjunction with `block_size`. missing : :class:`str`; String used to denote missing values **only** for the phenotype field.; This is in addition to ""-9"", ""0"", and ""N/A"" for case-control; phenotypes. delimiter : :class:`str`; FAM file field delimiter regex. quant_pheno : :obj:`bool`; If ``True``, FAM phenotype is interpreted as quantitative. a2_reference : :obj:`bool`; If ``True``, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele. reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use. contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by ``reference_genome``. If ``None``, the; default is dependent on the ``reference_genome``. For ""GRCh37"", the default; is ``{'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}``. For ""GRCh38"", the; default is ``{'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}``. skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. n_partitions : :obj:`int`, optional; Number of partitions. If both `n_partitions` and `block_size`; are specified, `n_partitions` will be used. block_size : :obj:`int`, optional; Block size, in MB. Default: 128MB blocks. Returns; -------; :class:`.MatrixTable`. """""". if contig_recoding is None:; if reference_genome is None:; contig_recoding = {}; elif reference_genome.name == ""GRCh37"":; contig_recoding = {'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}; elif reference_genome.name == ""GRCh38"":; contig_recoding = {; **{str(i): f'chr{i}' for i in range(1, 23)},; **{'23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'},; }; else:; contig_recoding = {}.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:224,Modifiability,Config,Configuration,224,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:24203,Modifiability,parameteriz,parameterized,24203,"vals(; path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs; ) -> Table:; """"""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:29306,Modifiability,parameteriz,parameterized,29306,"rack name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field `cnv_region` indicating inclusion in; at least one interval of the three-column BED file:. >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field `cnv_id` with the value given by the; fourth column of a BED file:. >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; -----. The table produced by this method has one of two possible structures. If; the .bed file has only three fields (`chrom`, `chromStart`, and; `chromEnd`), then the produced table has only one column:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :py:data:`.tstr` and `position` with type :py:data:`.tint32`. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. - *interval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:35608,Modifiability,sandbox,sandbox,35608,"na(type_and_data['data']), typ, key=['id']). [docs]@typecheck(regex=str, path=oneof(str, sequenceof(str)), max_count=int, show=bool, force=bool, force_bgz=bool); def grep(regex, path, max_count=100, *, show: bool = True, force: bool = False, force_bgz: bool = False):; r""""""Searches given paths for all lines containing regex matches. Examples; --------. Print all lines containing the string ``hello`` in *file.txt*:. >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; -----; :func:`.grep` mimics the basic functionality of Unix ``grep`` in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses `Java regular expression; patterns; <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html>`__.; The `RegExr sandbox <http://regexr.com/>`__ may be helpful. Parameters; ----------; regex : :class:`str`; The regular expression to match.; path : :class:`str` or :obj:`list` of :obj:`str`; The files to search.; max_count : :obj:`int`; The maximum number of matches to return; show : :obj:`bool`; When `True`, show the values on stdout. When `False`, return a; dictionary mapping file names to lines.; force_bgz : :obj:`bool`; If ``True``, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; ---; :obj:`dict` of :class:`str` to :obj:`list` of :obj:`str`; """"""; from hail.backend.spark_backend import S",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:40823,Modifiability,parameteriz,parameterized,40823,"op Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probab",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:48549,Modifiability,parameteriz,parameterized,48549,"port_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the prob",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:86597,Modifiability,parameteriz,parameterized,86597,"miter='\\\\s+',; missing='NA',; quant_pheno=False,; a2_reference=True,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; n_partitions=None,; block_size=None,; ) -> MatrixTable:; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9""",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:100358,Modifiability,parameteriz,parameterized,100358," the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`. Otherwise, the type will be a :class:`.tstruct` with; two fields: `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; - `filters` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:111437,Modifiability,config,config,111437,"ue,; _create_row_uids=False,; ) -> Table:; """"""Read in a :class:`.Table` written with :meth:`.Table.write`. Parameters; ----------; path : :class:`str`; File to read. Returns; -------; :class:`.Table`; """"""; if _load_refs:; for rg_config in Env.backend().load_references_from_dataset(path):; hl.ReferenceGenome._from_config(rg_config). if _intervals is not None and _n_partitions is not None:; raise ValueError(""'read_table' does not support both _intervals and _n_partitions""); tr = ir.TableNativeReader(path, _intervals, _filter_intervals); ht = Table(ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:111562,Modifiability,config,config,111562,"ue,; _create_row_uids=False,; ) -> Table:; """"""Read in a :class:`.Table` written with :meth:`.Table.write`. Parameters; ----------; path : :class:`str`; File to read. Returns; -------; :class:`.Table`; """"""; if _load_refs:; for rg_config in Env.backend().load_references_from_dataset(path):; hl.ReferenceGenome._from_config(rg_config). if _intervals is not None and _n_partitions is not None:; raise ValueError(""'read_table' does not support both _intervals and _n_partitions""); tr = ir.TableNativeReader(path, _intervals, _filter_intervals); ht = Table(ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112044,Modifiability,config,config,112044,"ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112139,Modifiability,config,configuration,112139,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112391,Modifiability,config,config,112391,"t=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length. original_determine_file_length = DataFileReader.determine_file_length. try:; DataFileReader.determine_file_length = patched_determine_file_length. with DataFileReader(avro_file, DatumReader()) as data_file_reader:; tr = ir.AvroTableReader(avro.s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:25602,Performance,load,loaded,25602,"arget** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). t = import_table(; path,; comment=""@"",; impute=False,; no_header=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; **kwargs,; ). if t.row.dtype == tstruct(f0=tstr):; if reference_genome:; t = t.select(interval=hl.parse_locus_interval(t['f0'], reference_genome)); else:; interval_regex = r""([^:]*):(\d+)\-(\d+)"". def checked_match",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:30662,Performance,load,loaded,30662,"nterval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid arguments here except:; `no_header`, `delimiter`, `impute`, `skip_blank_lines`, `types`, and `comment` as these; are used by import_bed. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". # UCSC BED spec defined here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1. t = import_table(; path,; no_header=True,; delimiter=r""\s+"",; impute=False,; skip_blank_lines=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; comment=[""""""^browser.*"""""", """"""^track.*"""""", r""""""^\w+=(""[\w\d ]+""|\d+).*""""""],; **kwargs,; ). if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38391,Performance,Load,Load,38391,"', 'GP', 'dosage')),; n_partitions=nullable(int),; block_size=nullable(int),; index_file_map=nullable(dictof(str, str)),; variants=nullable(; oneof(sequenceof(hl.utils.Struct), sequenceof(hl.genetics.Locus), StructExpression, LocusExpression, Table); ),; _row_fields=sequenceof(enumeration('varid', 'rsid')),; ); def import_bgen(; path,; entry_fields,; sample_file=None,; n_partitions=None,; block_size=None,; index_file_map=None,; variants=None,; _row_fields=['varid', 'rsid'],; ) -> MatrixTable:; """"""Import BGEN file(s) as a :class:`.MatrixTable`. Examples; --------. Import a BGEN file as a matrix table with GT and GP entry fields:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes mu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38624,Performance,Load,Load,38624," ),; _row_fields=sequenceof(enumeration('varid', 'rsid')),; ); def import_bgen(; path,; entry_fields,; sample_file=None,; n_partitions=None,; block_size=None,; index_file_map=None,; variants=None,; _row_fields=['varid', 'rsid'],; ) -> MatrixTable:; """"""Import BGEN file(s) as a :class:`.MatrixTable`. Examples; --------. Import a BGEN file as a matrix table with GT and GP entry fields:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index f",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38995,Performance,Load,Load,38995,"lds:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:dat",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:39784,Performance,load,load,39784,"y('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` para",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:40490,Performance,perform,performance,40490,"ability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:41496,Performance,perform,performance,41496,"ssary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; - `dosage` (:py:data:`.tfloat64`) -- The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See Also; --------; :func:`.index_bgen`. Parameters; ----------; path : :class:`s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:48011,Performance,load,load,48011,"mple_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:59652,Performance,load,load,59652,"s : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:60415,Performance,load,load,60415,"efining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """"""; if len(delimiter) < 1:; raise ValueError('import_table: empty delimiter is not supported'). paths = wrap_to_list(paths); comment = wrap_to_list(comment); missing = wrap_to_list(missing). ht = hl.import_lines(paths, min_partitions, force_bgz, force). should_remove_line_expr = should_remove_line(; ht.text, filter=filter, comment=comment, skip_blank_lines=skip_blank_lines; ); if should_remove_line_expr is not None:; ht = ht.filter(should_remove_line_expr, keep=False). try:; if len(paths) <= 1:; # With zero or one files and no filters, the first row, if it exists must be in the first; # partiti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:63958,Performance,Load,Loading,63958,"d) is None:; fields_to_impute_idx.append(idx); fields_to_guess.append(field). hl.utils.info('Reading table to impute column types'); guessed = ht.aggregate(; hl.agg.array_agg(lambda x: hl.agg._impute_type(x), [ht.split_text[i] for i in fields_to_impute_idx]); ). reasons = {f: 'user-supplied type' for f in types}; imputed_types = dict(); for field, s in zip(fields_to_guess, guessed):; if not s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s['supportsBool']:; imputed_types[field] = hl.tbool; elif s['supportsInt32']:; imputed_types[field] = hl.tint32; elif s['supportsInt64']:; imputed_types[field] = hl.tint64; elif s['supportsFloat64']:; imputed_types[field] = hl.tfloat64; else:; imputed_types[field] = hl.tstr; reasons[field] = 'imputed'. strs.append('Finished type imputation'). all_types = dict(**types, **imputed_types). for f_idx, field in enumerate(fields):; strs.append(f' Loading field {field!r} as type {all_types[field]} ({reasons[field]})'); fields_to_value[field] = parse_type(ht.split_text[f_idx], all_types[field]); else:; strs.append('Reading table without type imputation'); for f_idx, field in enumerate(fields):; reason = 'user-supplied' if field in types else 'not specified'; t = types.get(field, hl.tstr); fields_to_value[field] = parse_type(ht.split_text[f_idx], t); strs.append(f' Loading field {field!r} as type {t} ({reason})'). ht = ht.annotate(**fields_to_value).drop('split_text'); if source_file_field is not None:; source_file = {source_file_field: ht.file}; ht = ht.annotate(**source_file); ht = ht.drop('file'). if len(fields) < 30:; hl.utils.info('\n'.join(strs)); else:; from collections import Counter. strs2 = [f'Loading {ht.row} fields. Counts by type:']; for name, count in Counter(ht[f].dtype for f in fields).most_common():; strs2.append(f' {name}: {count}'); hl.utils.info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:64382,Performance,Load,Loading,64382,"t s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s['supportsBool']:; imputed_types[field] = hl.tbool; elif s['supportsInt32']:; imputed_types[field] = hl.tint32; elif s['supportsInt64']:; imputed_types[field] = hl.tint64; elif s['supportsFloat64']:; imputed_types[field] = hl.tfloat64; else:; imputed_types[field] = hl.tstr; reasons[field] = 'imputed'. strs.append('Finished type imputation'). all_types = dict(**types, **imputed_types). for f_idx, field in enumerate(fields):; strs.append(f' Loading field {field!r} as type {all_types[field]} ({reasons[field]})'); fields_to_value[field] = parse_type(ht.split_text[f_idx], all_types[field]); else:; strs.append('Reading table without type imputation'); for f_idx, field in enumerate(fields):; reason = 'user-supplied' if field in types else 'not specified'; t = types.get(field, hl.tstr); fields_to_value[field] = parse_type(ht.split_text[f_idx], t); strs.append(f' Loading field {field!r} as type {t} ({reason})'). ht = ht.annotate(**fields_to_value).drop('split_text'); if source_file_field is not None:; source_file = {source_file_field: ht.file}; ht = ht.annotate(**source_file); ht = ht.drop('file'). if len(fields) < 30:; hl.utils.info('\n'.join(strs)); else:; from collections import Counter. strs2 = [f'Loading {ht.row} fields. Counts by type:']; for name, count in Counter(ht[f].dtype for f in fields).most_common():; strs2.append(f' {name}: {count}'); hl.utils.info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:64727,Performance,Load,Loading,64727,"s[field] = hl.tfloat64; else:; imputed_types[field] = hl.tstr; reasons[field] = 'imputed'. strs.append('Finished type imputation'). all_types = dict(**types, **imputed_types). for f_idx, field in enumerate(fields):; strs.append(f' Loading field {field!r} as type {all_types[field]} ({reasons[field]})'); fields_to_value[field] = parse_type(ht.split_text[f_idx], all_types[field]); else:; strs.append('Reading table without type imputation'); for f_idx, field in enumerate(fields):; reason = 'user-supplied' if field in types else 'not specified'; t = types.get(field, hl.tstr); fields_to_value[field] = parse_type(ht.split_text[f_idx], t); strs.append(f' Loading field {field!r} as type {t} ({reason})'). ht = ht.annotate(**fields_to_value).drop('split_text'); if source_file_field is not None:; source_file = {source_file_field: ht.file}; ht = ht.annotate(**source_file); ht = ht.drop('file'). if len(fields) < 30:; hl.utils.info('\n'.join(strs)); else:; from collections import Counter. strs2 = [f'Loading {ht.row} fields. Counts by type:']; for name, count in Counter(ht[f].dtype for f in fields).most_common():; strs2.append(f' {name}: {count}'); hl.utils.info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`;",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:65858,Performance,load,load,65858,"info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:66158,Performance,load,load,66158,"e, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir.StringTableReader(paths, min_partitions, force_bgz, force, file_per_partition); table_type = hl.ttable(global_type=hl.tstruct(), row_type=hl.tstruct(file=hl.tstr, text=hl.tstr), row_key=[]); string_table = Table(ir.TableRead(st_reader, _assert_type=table_type)); return s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:72287,Performance,load,load,72287," will never be missing, even if the `missing` string appears; in the column IDs. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; row_fields: :obj:`dict` of :class:`str` to :class:`.HailType`; Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key: :class:`str` or :obj:`list` of :obj:`str`; Key fields(s). If empty, creates an index `row_id` to use as key.; entry_type: :class:`.HailType`; Type of entries in matrix table. Must be one of: :py:data:`.tint32`,; :py:data:`.tint64`, :py:data:`.tfloat32`, :py:data:`.tfloat64`, or; :py:data:`.tstr`. Default: :py:data:`.tint32`.; missing: :class:`str`; Identifier to be treated as missing. Default: NA; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header: :obj:`bool`; If ``True``, assume the file has no header and name the row fields `f0`,; `f1`, ... `fK` (0-indexed) and the column keys 0, 1, ... N.; force_bgz : :obj:`bool`; If ``True``, load **.gz** files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep : :class:`str`; This parameter is a deprecated name for `delimiter`, please use that; instead.; delimiter : :class:`str`; A single character string which separates values in the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns; -------; :class:`.MatrixTable`; MatrixTable constructed from imported data.; """"""; row_key = wrap_to_list(row_key); comment = wrap_to_list(comment); paths = [hl.current_backend().fs.canonicalize_path(p) for p in wrap_to_list(paths)]; missing_list = wrap_to_list(missing). def comment_filter(table):; return (; hl.rbind(; hl.array(comment),; lambda hl_comment: hl_comment.any(; lambda com: hl.if_else(hl.len(com) == 1, tab",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:98365,Performance,load,load,98365,"+------------+------+---------+----------+--------------+; <BLANKLINE>; +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+; <BLANKLINE>; +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; -----. Hail is designed to be maximally compatible with files in the `VCF v4.2; spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :func:`.import_vcf` takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); cont",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:99295,Performance,perform,perform,99295,"e same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101817,Performance,load,load,101817,"s` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101974,Performance,load,load,101974," (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_e",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102323,Performance,load,load,102323,"`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102413,Performance,load,load,102413,"ated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102511,Performance,load,load,102511,"ference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102547,Performance,load,loaded,102547,"d as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102797,Performance,load,loaded,102797,"s; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:data:`.tfloat32` or :py:data:`.tfloat64`. Default:; :py:data:`.tfloat64`.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:117955,Performance,load,load,117955,"list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:118718,Performance,load,load,118718," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:62544,Safety,avoid,avoid,62544,"first_row_ht = ht.head(1). if find_replace is not None:; ht = ht.annotate(text=ht['text'].replace(*find_replace)). first_rows = first_row_ht.annotate(; header=first_row_ht.text._split_line(; delimiter, missing=hl.empty_array(hl.tstr), quote=quote, regex=len(delimiter) > 1; ); ).collect(); except FatalError as err:; if '_filter_partitions: no partition with index 0' in err.args[0]:; first_rows = []; else:; raise. if len(first_rows) == 0:; raise ValueError(f""Invalid file: no lines remaining after filters\n Files provided: {', '.join(paths)}""); first_row = first_rows[0]. if no_header:; fields = [f'f{index}' for index in range(0, len(first_row.header))]; else:; maybe_duplicated_fields = first_row.header; renamings, fields = deduplicate(maybe_duplicated_fields); ht = ht.filter(; ht.text == first_row.text, keep=False; ) # FIXME: seems wrong. Could easily fix with partition index and row_within_partition_index.; if renamings:; hl.utils.warning(; f'import_table: renamed the following {plural(""field"", len(renamings))} to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renamings); ). ht = ht.annotate(; split_text=(; hl.case(); .when(hl.len(ht.text) > 0, split_lines(ht, fields, delimiter=delimiter, missing=missing, quote=quote)); .or_error(hl.str(""Blank line found in file "") + ht.file); ); ); ht = ht.drop('text'). fields_to_value = {}; strs = []; if impute:; fields_to_impute_idx = []; fields_to_guess = []; for idx, field in enumerate(fields):; if types.get(field) is None:; fields_to_impute_idx.append(idx); fields_to_guess.append(field). hl.utils.info('Reading table to impute column types'); guessed = ht.aggregate(; hl.agg.array_agg(lambda x: hl.agg._impute_type(x), [ht.split_text[i] for i in fields_to_impute_idx]); ). reasons = {f: 'user-supplied type' for f in types}; imputed_types = dict(); for field, s in zip(fields_to_guess, guessed):; if not s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s[",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:111993,Safety,avoid,avoid,111993,"ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:378,Testability,Log,Log,378,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:35608,Testability,sandbox,sandbox,35608,"na(type_and_data['data']), typ, key=['id']). [docs]@typecheck(regex=str, path=oneof(str, sequenceof(str)), max_count=int, show=bool, force=bool, force_bgz=bool); def grep(regex, path, max_count=100, *, show: bool = True, force: bool = False, force_bgz: bool = False):; r""""""Searches given paths for all lines containing regex matches. Examples; --------. Print all lines containing the string ``hello`` in *file.txt*:. >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; -----; :func:`.grep` mimics the basic functionality of Unix ``grep`` in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses `Java regular expression; patterns; <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html>`__.; The `RegExr sandbox <http://regexr.com/>`__ may be helpful. Parameters; ----------; regex : :class:`str`; The regular expression to match.; path : :class:`str` or :obj:`list` of :obj:`str`; The files to search.; max_count : :obj:`int`; The maximum number of matches to return; show : :obj:`bool`; When `True`, show the values on stdout. When `False`, return a; dictionary mapping file names to lines.; force_bgz : :obj:`bool`; If ``True``, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; ---; :obj:`dict` of :class:`str` to :obj:`list` of :obj:`str`; """"""; from hail.backend.spark_backend import S",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:45543,Testability,assert,assert,45543,"iants = hl.struct(locus=variants). if len(variants.dtype) == 0 or not variants.dtype._is_prefix_of(expected_vtype):; raise TypeError(; ""'import_bgen' requires the expression type for 'variants' is a non-empty prefix of the BGEN key type: \n""; + f""\tFound: {variants.dtype!r}\n""; + f""\tExpected: {expected_vtype!r}\n""; ). uid = Env.get_uid(); fnames = list(variants.dtype); name, variants = variants._to_table(; uid; ) # This will add back the other key fields of the source, which we don't want; variants = variants.key_by(**{fname: variants[name][fname] for fname in fnames}); variants = variants.select(); elif isinstance(variants, Table):; if len(variants.key) == 0 or not variants.key.dtype._is_prefix_of(expected_vtype):; raise TypeError(; ""'import_bgen' requires the row key type for 'variants' is a non-empty prefix of the BGEN key type: \n""; + f""\tFound: {variants.key.dtype!r}\n""; + f""\tExpected: {expected_vtype!r}\n""; ); variants = variants.select(); else:; assert isinstance(variants, list); try:; if len(variants) == 0:; variants = hl.Table.parallelize(variants, schema=expected_vtype, key=['locus', 'alleles']); else:; first_v = variants[0]; if isinstance(first_v, hl.Locus):; variants = hl.Table.parallelize(; [hl.Struct(locus=v) for v in variants], schema=hl.tstruct(locus=lt), key='locus'; ); else:; assert isinstance(first_v, hl.utils.Struct); if len(first_v) == 1:; variants = hl.Table.parallelize(variants, schema=hl.tstruct(locus=lt), key='locus'); else:; variants = hl.Table.parallelize(variants, schema=expected_vtype, key=['locus', 'alleles']); except Exception:; raise TypeError(; f""'import_bgen' requires all elements in 'variants' are a non-empty prefix of the BGEN key type: {expected_vtype!r}""; ). vir = variants._tir; if (; isinstance(vir, ir.TableRead); and isinstance(vir.reader, ir.TableNativeReader); and vir.reader.intervals is None; and variants.count() == variants.distinct().count(); ):; variants_path = vir.reader.path; else:; variants_path = new_temp_file(pref",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:45891,Testability,assert,assert,45891,"r}\n""; ). uid = Env.get_uid(); fnames = list(variants.dtype); name, variants = variants._to_table(; uid; ) # This will add back the other key fields of the source, which we don't want; variants = variants.key_by(**{fname: variants[name][fname] for fname in fnames}); variants = variants.select(); elif isinstance(variants, Table):; if len(variants.key) == 0 or not variants.key.dtype._is_prefix_of(expected_vtype):; raise TypeError(; ""'import_bgen' requires the row key type for 'variants' is a non-empty prefix of the BGEN key type: \n""; + f""\tFound: {variants.key.dtype!r}\n""; + f""\tExpected: {expected_vtype!r}\n""; ); variants = variants.select(); else:; assert isinstance(variants, list); try:; if len(variants) == 0:; variants = hl.Table.parallelize(variants, schema=expected_vtype, key=['locus', 'alleles']); else:; first_v = variants[0]; if isinstance(first_v, hl.Locus):; variants = hl.Table.parallelize(; [hl.Struct(locus=v) for v in variants], schema=hl.tstruct(locus=lt), key='locus'; ); else:; assert isinstance(first_v, hl.utils.Struct); if len(first_v) == 1:; variants = hl.Table.parallelize(variants, schema=hl.tstruct(locus=lt), key='locus'); else:; variants = hl.Table.parallelize(variants, schema=expected_vtype, key=['locus', 'alleles']); except Exception:; raise TypeError(; f""'import_bgen' requires all elements in 'variants' are a non-empty prefix of the BGEN key type: {expected_vtype!r}""; ). vir = variants._tir; if (; isinstance(vir, ir.TableRead); and isinstance(vir.reader, ir.TableNativeReader); and vir.reader.intervals is None; and variants.count() == variants.distinct().count(); ):; variants_path = vir.reader.path; else:; variants_path = new_temp_file(prefix='bgen_included_vars', extension='ht'); variants.distinct().write(variants_path); else:; variants_path = None. reader = ir.MatrixBGENReader(path, sample_file, index_file_map, n_partitions, block_size, variants_path). mt = MatrixTable(ir.MatrixRead(reader)).drop(; *[fd for fd in ['GT', 'GP', 'dosage'] if fd n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:85943,Testability,test,test,85943,"ht = ht.annotate_globals(; cols=hl.range(0, len(header_dict['column_ids'])).map(lambda col_idx: hl.struct(col_id=hl_columns[col_idx])); ). if not add_row_id:; ht = ht.drop('row_id'). mt = ht._unlocalize_entries('entries', 'cols', ['col_id']); mt = mt.key_rows_by(*row_key); return mt. [docs]@typecheck(; bed=str,; bim=str,; fam=str,; min_partitions=nullable(int),; delimiter=str,; missing=str,; quant_pheno=bool,; a2_reference=bool,; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; n_partitions=nullable(int),; block_size=nullable(int),; ); def import_plink(; bed,; bim,; fam,; min_partitions=None,; delimiter='\\\\s+',; missing='NA',; quant_pheno=False,; a2_reference=True,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; n_partitions=None,; block_size=None,; ) -> MatrixTable:; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_refer",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:85969,Testability,test,test,85969,"hl.range(0, len(header_dict['column_ids'])).map(lambda col_idx: hl.struct(col_id=hl_columns[col_idx])); ). if not add_row_id:; ht = ht.drop('row_id'). mt = ht._unlocalize_entries('entries', 'cols', ['col_id']); mt = mt.key_rows_by(*row_key); return mt. [docs]@typecheck(; bed=str,; bim=str,; fam=str,; min_partitions=nullable(int),; delimiter=str,; missing=str,; quant_pheno=bool,; a2_reference=bool,; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; n_partitions=nullable(int),; block_size=nullable(int),; ); def import_plink(; bed,; bim,; fam,; min_partitions=None,; delimiter='\\\\s+',; missing='NA',; quant_pheno=False,; a2_reference=True,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; n_partitions=None,; block_size=None,; ) -> MatrixTable:; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:85995,Testability,test,test,85995,"t['column_ids'])).map(lambda col_idx: hl.struct(col_id=hl_columns[col_idx])); ). if not add_row_id:; ht = ht.drop('row_id'). mt = ht._unlocalize_entries('entries', 'cols', ['col_id']); mt = mt.key_rows_by(*row_key); return mt. [docs]@typecheck(; bed=str,; bim=str,; fam=str,; min_partitions=nullable(int),; delimiter=str,; missing=str,; quant_pheno=bool,; a2_reference=bool,; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; n_partitions=nullable(int),; block_size=nullable(int),; ); def import_plink(; bed,; bim,; fam,; min_partitions=None,; delimiter='\\\\s+',; missing='NA',; quant_pheno=False,; a2_reference=True,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; n_partitions=None,; block_size=None,; ) -> MatrixTable:; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:56,Usability,Feedback,Feedback,56,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:266,Usability,Guid,Guides,266,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112101,Usability,guid,guide,112101,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:6424,Availability,checkpoint,checkpoint,6424,"rce; if not isinstance(source, Table):; raise ValueError(; ""'maximal_independent_set' expects an expression of 'Table'. Found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). if i._indices.source != j._indices.source:; raise ValueError(; ""'maximal_independent_set' expects arguments `i` and `j` to be expressions of the same Table. ""; ""Found\n{}\n{}"".format(i, j); ). node_t = i.dtype. if tie_breaker:; wrapped_node_t = ttuple(node_t); left_id = Env.get_uid(); right_id = Env.get_uid(); left = construct_variable(left_id, wrapped_node_t); right = construct_variable(right_id, wrapped_node_t); tie_breaker_expr = hl.float64(tie_breaker(left[0], right[0])); tie_breaker_ir = tie_breaker_expr._ir; t, _ = source._process_joins(i, j, tie_breaker_expr); else:; left_id, right_id, tie_breaker_ir = None, None, None; t, _ = source._process_joins(i, j). edges = t.select(__i=i, __j=j).key_by().select('__i', '__j'); edges = edges.checkpoint(new_temp_file()). mis_nodes = hl.set(; construct_expr(; ir.ArrayMaximalIndependentSet(edges.collect(_localize=False)._ir, left_id, right_id, tie_breaker_ir),; hl.tarray(node_t),; ); ). nodes = edges.select(node=[edges.__i, edges.__j]); nodes = nodes.explode(nodes.node); nodes = nodes.annotate_globals(mis_nodes=mis_nodes); nodes = nodes.filter(nodes.mis_nodes.contains(nodes.node), keep); nodes = nodes.select_globals(); if keyed:; return nodes.key_by('node').distinct(); return nodes. def require_col_key_str(dataset: MatrixTable, method: str):; if not len(dataset.col_key) == 1 or dataset[next(iter(dataset.col_key))].dtype != hl.tstr:; raise ValueError(; f""Method '{method}' requires column key to be one field of type 'str', found ""; f""{list(str(x.dtype) for x in dataset.col_key.values())}""; ). def require_table_key_variant(ht, method):; if (; list(ht.key) != ['locus', 'alleles']; or not isinstance(ht['locus'].dtype, tlocus); or not ht['alleles'].dtype == tarray(tstr); ):; raise ValueError(; """,MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:155,Deployability,Install,Installation,155,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:223,Deployability,Configurat,Configuration,223,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:16992,Deployability,update,updated,16992,"pects a table with interval keys""); point_type = ht.key[0].dtype.point_type; if isinstance(points, Table):; if len(points.key) != 1 or points.key[0].dtype != point_type:; raise ValueError(; ""'segment_intervals' expects points to be a table with a single""; "" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {list(points.key.dtype.values())}""; ); points = hl.array(hl.set(points.collect(_localize=False))); if points.dtype.element_type != point_type:; raise ValueError(; f""'segment_intervals' expects points to be a table with a single""; f"" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {points.dtype.element_type}""; ). points = hl._sort_by(points, lambda l, r: hl._compare(l, r) < 0). ht = ht.annotate_globals(__points=points). interval = ht.key[0]; points = ht.__points; lower = hl.expr.functions._lower_bound(points, interval.start); higher = hl.expr.functions._lower_bound(points, interval.end); n_points = hl.len(points); lower = hl.if_else((lower < n_points) & (points[lower] == interval.start), lower + 1, lower); higher = hl.if_else((higher < n_points) & (points[higher] == interval.end), higher - 1, higher); interval_results = hl.rbind(; lower,; higher,; lambda lower, higher: hl.if_else(; lower >= higher,; [interval],; hl.flatten([; [; hl.interval(; interval.start, points[lower], includes_start=interval.includes_start, includes_end=False; ); ],; hl.range(lower, higher - 1).map(; lambda x: hl.interval(points[x], points[x + 1], includes_start=True, includes_end=False); ),; [; hl.interval(; points[higher - 1], interval.end, includes_start=True, includes_end=interval.includes_end; ); ],; ]),; ),; ); ht = ht.annotate(__new_intervals=interval_results, lower=lower, higher=higher).explode('__new_intervals'); return ht.key_by(**{next(iter(ht.key)): ht.__new_intervals}).drop('__new_intervals'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:223,Modifiability,Config,Configuration,223,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:11040,Modifiability,parameteriz,parameterized,11040,"(dataset, method); return dataset._select_rows(; method,; hl.case(); .when(dataset.alleles.length() == 2, dataset._rvrow); .or_error(; f""'{method}' expects biallelic variants ('alleles' field of length 2), found ""; + hl.str(dataset.locus); + "", ""; + hl.str(dataset.alleles); ),; ). [docs]@typecheck(dataset=MatrixTable, name=str); def rename_duplicates(dataset, name='unique_id') -> MatrixTable:; """"""Rename duplicate column keys. .. include:: ../_templates/req_tstring.rst. Examples; --------. >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; -----. This method produces a new column field from the string column key by; appending a unique suffix ``_N`` as necessary. For example, if the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """""". require_col_key_str(dataset, 'rename_duplicates'); ids = dataset.col_key[0].collect(). mapping, new_ids = deduplicate(ids). if mapping:; info(; f'Renamed {len(mapping)} duplicate {plural(""sample ID"", len(mapping))}. Mangled IDs as follows:'; + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:12495,Performance,load,loaded,12495," + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:12580,Performance,latency,latency,12580,"otate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v in partial.items():; if full[k] != v:; return False; return True. if point_type == k_type[0]:; needs_wrapper = True; k_name = k_type.f",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:377,Testability,Log,Log,377,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:7754,Testability,assert,assert,7754,"lter(nodes.mis_nodes.contains(nodes.node), keep); nodes = nodes.select_globals(); if keyed:; return nodes.key_by('node').distinct(); return nodes. def require_col_key_str(dataset: MatrixTable, method: str):; if not len(dataset.col_key) == 1 or dataset[next(iter(dataset.col_key))].dtype != hl.tstr:; raise ValueError(; f""Method '{method}' requires column key to be one field of type 'str', found ""; f""{list(str(x.dtype) for x in dataset.col_key.values())}""; ). def require_table_key_variant(ht, method):; if (; list(ht.key) != ['locus', 'alleles']; or not isinstance(ht['locus'].dtype, tlocus); or not ht['alleles'].dtype == tarray(tstr); ):; raise ValueError(; ""Method '{}' requires key to be two fields 'locus' (type 'locus<any>') and ""; ""'alleles' (type 'array<str>')\n""; "" Found:{}"".format(method, ''.join(""\n '{}': {}"".format(k, str(ht[k].dtype)) for k in ht.key)); ). def require_row_key_variant(dataset, method):; if isinstance(dataset, Table):; key = dataset.key; else:; assert isinstance(dataset, MatrixTable); key = dataset.row_key; if (; list(key) != ['locus', 'alleles']; or not isinstance(dataset['locus'].dtype, tlocus); or not dataset['alleles'].dtype == tarray(tstr); ):; raise ValueError(; ""Method '{}' requires row key to be two fields 'locus' (type 'locus<any>') and ""; ""'alleles' (type 'array<str>')\n""; "" Found:{}"".format(method, ''.join(""\n '{}': {}"".format(k, str(dataset[k].dtype)) for k in key)); ). def require_alleles_field(dataset, method):; if 'alleles' not in dataset.row:; raise ValueError(f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""); if dataset.alleles.dtype != tarray(tstr):; raise ValueError(; f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""; f"" Found:\n""; f"" 'alleles': {dataset.alleles.dtype}""; ). def require_row_key_variant_w_struct_locus(dataset, method):; if (; list(dataset.row_key) != ['locus', 'alleles']; or not dataset['alleles'].dtype == tarray(tstr); or (; not isinstance(dataset['locus'].dtype, tlocus); ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:9275,Testability,assert,assert,9275,"f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""); if dataset.alleles.dtype != tarray(tstr):; raise ValueError(; f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""; f"" Found:\n""; f"" 'alleles': {dataset.alleles.dtype}""; ). def require_row_key_variant_w_struct_locus(dataset, method):; if (; list(dataset.row_key) != ['locus', 'alleles']; or not dataset['alleles'].dtype == tarray(tstr); or (; not isinstance(dataset['locus'].dtype, tlocus); and dataset['locus'].dtype != hl.dtype('struct{contig: str, position: int32}'); ); ):; raise ValueError(; ""Method '{}' requires row key to be two fields 'locus'""; "" (type 'locus<any>' or 'struct{{contig: str, position: int32}}') and ""; ""'alleles' (type 'array<str>')\n""; "" Found:{}"".format(; method, ''.join(""\n '{}': {}"".format(k, str(dataset[k].dtype)) for k in dataset.row_key); ); ). def require_first_key_field_locus(dataset, method):; if isinstance(dataset, Table):; key = dataset.key; else:; assert isinstance(dataset, MatrixTable); key = dataset.row_key; if len(key) == 0 or not isinstance(key[0].dtype, tlocus):; raise ValueError(; ""Method '{}' requires first key field of type 'locus<any>'.\n"" "" Found:{}"".format(; method, ''.join(""\n '{}': {}"".format(k, str(dataset[k].dtype)) for k in key); ); ). @typecheck(table=Table, method=str); def require_key(table, method):; if len(table.key) == 0:; raise ValueError(""Method '{}' requires a non-empty key"".format(method)). @typecheck(dataset=MatrixTable, method=str, tolerate_generic_locus=bool); def require_biallelic(dataset, method, tolerate_generic_locus: bool = False) -> MatrixTable:; if tolerate_generic_locus:; require_row_key_variant_w_struct_locus(dataset, method); else:; require_row_key_variant(dataset, method); return dataset._select_rows(; method,; hl.case(); .when(dataset.alleles.length() == 2, dataset._rvrow); .or_error(; f""'{method}' expects biallelic variants ('alleles' field of length 2), found ""; + hl.str(dataset.locus); + "", ""; + hl.str(data",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:13241,Testability,assert,assert,13241,"tes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v in partial.items():; if full[k] != v:; return False; return True. if point_type == k_type[0]:; needs_wrapper = True; k_name = k_type.fields[0]; point_type = hl.tstruct(**{k_name: k_type[k_name]}); elif isinstance(point_type, tstruct) and is_struct_prefix(point_type, k_type):; needs_wrapper = False; else:; raise TypeError(; ""The point type is incompatible with key type of the dataset ('{}', '{}')"".format(; repr(point_type), repr(k_type); ); ). def wrap_input(interval):; if interval is None:; raise TypeError(""'filter_intervals' does not allow missing values in 'intervals'.""); elif needs_wrapper:; return Interval(; Struct(**{k_name: interval.start}),; Struct(**{k_name: interval.end}),; interval.includes_start,; interval.includes_end,; ); else:; return interval. intervals = hl.eval(interval",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:55,Usability,Feedback,Feedback,55,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/misc.html:265,Usability,Guid,Guides,265,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:5195,Availability,error,error,5195,"om hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`. Hail will return an error if `entry_expr` evaluates to missing, nan, or; infinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:13881,Availability,checkpoint,checkpoint,13881,":`\mathrm{span}(U) = \mathcal{K}_p(AA^T, AV_0)`; * :math:`V[:, :b] = V_0`; * :math:`R\in\mathbb{R}^{b\times b}` is upper triangular; where :math:`\mathcal{K}_p(X, Y)` is the block Krylov subspace; :math:`\mathrm{span}(Y, XY, \dots, X^pY)`. Parameters; ----------; A_expr; V0; p; compute_U. Returns; -------. """"""; t = A.block_table; A_expr = A.block_expr. g_list = [V0]; G_i = V0; k = hl.eval(V0.shape[1]). for j in range(0, p):; info(f""krylov_factorization: Beginning iteration {j+1}/{p}""); G_i = t.aggregate(hl.agg.ndarray_sum(A_expr.T @ (A_expr @ G_i)), _localize=False); G_i = hl.nd.qr(G_i)[0]._persist(); g_list.append(G_i). info(""krylov_factorization: Iterations complete. Computing local QR""); V0 = hl.nd.hstack(g_list). if compute_V:; V = hl.nd.qr(V0)[0]._persist(); t = t.annotate(AV=A_expr @ V); else:; V = hl.nd.qr(V0)[0]; t = t.annotate(AV=A_expr @ V); V = None. if compute_U:; temp_file_name = hl.utils.new_temp_file(""_krylov_factorization_intermediate"", ""ht""); t = t.checkpoint(temp_file_name); AV_local = t.aggregate(hl.nd.vstack(hl.agg.collect(t.AV)), _localize=False); U, R = hl.nd.qr(AV_local)._persist(); else:; Rs = t.aggregate(hl.nd.vstack(hl.agg.collect(hl.nd.qr(t.AV)[1])), _localize=False); R = hl.nd.qr(Rs)[1]._persist(); U = None. return KrylovFactorization(U, R, V, k). def _reduced_svd(A: TallSkinnyMatrix, k=10, compute_U=False, iterations=2, iteration_size=None):; # Set Parameters; q = iterations; if iteration_size is None:; L = k + 2; else:; L = iteration_size; assert (q + 1) * L >= k; n = A.ncols. # Generate random matrix G; G = hl.rand_norm(0, 1, size=(n, L)); G = hl.nd.qr(G)[0]._persist(). fact = _krylov_factorization(A, G, q, compute_U); info(""_reduced_svd: Computing local SVD""); return fact.reduced_svd(k). @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix), num_moments=int, p=nullable(int), moment_samples=int, block_size=int; ); def _spectral_moments(A, num_moments, p=None, moment_samples=500, block_size=128):; if not isinstance(A, TallSkinnyMatrix):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:18894,Availability,error,error,18894,"adings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; compute_scores=bool,; transpose=bool,; ); def _blanczos_pca(; A,; k=10,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; compute_scores=True,; transpose=False,; ):; r""""""Run randomized principal component analysis approximation (PCA); on numeric columns derived from a matrix table. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl._blanczos_pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`. Hail will return an error if `entry_expr` evaluates to missing, nan, or; infinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:154,Deployability,Install,Installation,154,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:222,Deployability,Configurat,Configuration,222,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:24481,Deployability,update,updated,24481,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21176,Energy Efficiency,power,power,21176,"The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:222,Modifiability,Config,Configuration,222,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:2110,Performance,load,loadings,2110,"annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:3331,Performance,load,loadings,3331,"f projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4049,Performance,load,loadings,4049,"f terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4204,Performance,load,loadings,4204,"hile this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6238,Performance,load,loadings,6238,"nfinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr :",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6365,Performance,load,loadings,6365,"ntly on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute ro",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6686,Performance,load,loadings,6686,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBack",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6704,Performance,load,loadings,6704,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBack",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6919,Performance,Load,Loadings,6919,"rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_e",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7018,Performance,load,loadings,7018,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7091,Performance,load,loadings,7091,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7167,Performance,load,loadings,7167,"tive of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixT",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7419,Performance,load,loadings,7419,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7574,Performance,load,loadings,7574,"ingular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t = None; return hl.eval(g.eigenvalues), scores, None if t is None else t.drop('eigenvalues', 'scores'). class TallSkinnyMatrix:; def __init__(self, blo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:17702,Performance,load,loadings,17702,".T @ G2); Q1, R1 = hl.nd.qr(G2)._persist(); fact2 = _krylov_factorization(A, Q1, p, compute_U=False); moments_and_stdevs = fact2.spectral_moments(num_moments, R1); # Add back exact moments; moments = moments_and_stdevs.moments + hl.nd.array([; fact.S.map(lambda x: x ** (2 * i)).sum() for i in range(1, num_moments + 1); ]); moments_and_stdevs = hl.eval(hl.struct(moments=moments, stdevs=moments_and_stdevs.stdevs)); moments = moments_and_stdevs.moments; stdevs = moments_and_stdevs.stdevs. scores = V * S; eigens = hl.eval(S * S); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). hail_array_scores = scores._data_array(); cols_and_scores = hl.zip(A.source_table.index_globals().cols, hail_array_scores).map(; lambda tup: tup[0].annotate(scores=tup[1]); ); st = hl.Table.parallelize(cols_and_scores, key=A.col_key). if compute_loadings:; lt = A.source_table.select(); lt = lt.annotate_globals(U=U); idx_name = '_tmp_pca_loading_index'; lt = lt.add_index(idx_name); lt = lt.annotate(loadings=hl.array(lt.U[lt[idx_name], :])).select_globals(); lt = lt.drop(lt[idx_name]); else:; lt = None. return eigens, st, lt, moments, stdevs. @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix),; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; compute_scores=bool,; transpose=bool,; ); def _blanczos_pca(; A,; k=10,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; compute_scores=True,; transpose=False,; ):; r""""""Run randomized principal component analysis approximation (PCA); on numeric columns derived from a matrix table. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl._blanczos_pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2).",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:19937,Performance,load,loadings,19937,"nfinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr :",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20064,Performance,load,loadings,20064,"ntly on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute ro",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20385,Performance,load,loadings,20385,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20403,Performance,load,loadings,20403,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; ---",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20618,Performance,Load,Loadings,20618,"rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20717,Performance,load,loadings,20717,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20790,Performance,load,loadings,20790,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20866,Performance,load,loadings,20866,"tive of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_it",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21118,Performance,load,loadings,21118,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21526,Performance,load,loadings,21526,"ey of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); retur",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:22613,Performance,load,loadings,22613,"ersampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:22809,Performance,load,loadings,22809,"ersampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23359,Performance,load,loadings,23359,"s().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_itera",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23973,Performance,load,loadings,23973,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:24128,Performance,load,loadings,24128,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:376,Testability,Log,Log,376,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:14395,Testability,assert,assert,14395,"j in range(0, p):; info(f""krylov_factorization: Beginning iteration {j+1}/{p}""); G_i = t.aggregate(hl.agg.ndarray_sum(A_expr.T @ (A_expr @ G_i)), _localize=False); G_i = hl.nd.qr(G_i)[0]._persist(); g_list.append(G_i). info(""krylov_factorization: Iterations complete. Computing local QR""); V0 = hl.nd.hstack(g_list). if compute_V:; V = hl.nd.qr(V0)[0]._persist(); t = t.annotate(AV=A_expr @ V); else:; V = hl.nd.qr(V0)[0]; t = t.annotate(AV=A_expr @ V); V = None. if compute_U:; temp_file_name = hl.utils.new_temp_file(""_krylov_factorization_intermediate"", ""ht""); t = t.checkpoint(temp_file_name); AV_local = t.aggregate(hl.nd.vstack(hl.agg.collect(t.AV)), _localize=False); U, R = hl.nd.qr(AV_local)._persist(); else:; Rs = t.aggregate(hl.nd.vstack(hl.agg.collect(hl.nd.qr(t.AV)[1])), _localize=False); R = hl.nd.qr(Rs)[1]._persist(); U = None. return KrylovFactorization(U, R, V, k). def _reduced_svd(A: TallSkinnyMatrix, k=10, compute_U=False, iterations=2, iteration_size=None):; # Set Parameters; q = iterations; if iteration_size is None:; L = k + 2; else:; L = iteration_size; assert (q + 1) * L >= k; n = A.ncols. # Generate random matrix G; G = hl.rand_norm(0, 1, size=(n, L)); G = hl.nd.qr(G)[0]._persist(). fact = _krylov_factorization(A, G, q, compute_U); info(""_reduced_svd: Computing local SVD""); return fact.reduced_svd(k). @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix), num_moments=int, p=nullable(int), moment_samples=int, block_size=int; ); def _spectral_moments(A, num_moments, p=None, moment_samples=500, block_size=128):; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_spectral_moments/entry_expr', A); A = _make_tsm(A, block_size). n = A.ncols. if p is None:; p = min(num_moments // 2, 10). # TODO: When moment_samples > n, we should just do a TSQR on A, and compute; # the spectrum of R.; assert moment_samples < n, '_spectral_moments: moment_samples must be smaller than num cols of A'; G = hl.rand_unif(-1, 1, size=(n, moment_samples)).map(lambd",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:15153,Testability,assert,assert,15153,"uced_svd(A: TallSkinnyMatrix, k=10, compute_U=False, iterations=2, iteration_size=None):; # Set Parameters; q = iterations; if iteration_size is None:; L = k + 2; else:; L = iteration_size; assert (q + 1) * L >= k; n = A.ncols. # Generate random matrix G; G = hl.rand_norm(0, 1, size=(n, L)); G = hl.nd.qr(G)[0]._persist(). fact = _krylov_factorization(A, G, q, compute_U); info(""_reduced_svd: Computing local SVD""); return fact.reduced_svd(k). @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix), num_moments=int, p=nullable(int), moment_samples=int, block_size=int; ); def _spectral_moments(A, num_moments, p=None, moment_samples=500, block_size=128):; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_spectral_moments/entry_expr', A); A = _make_tsm(A, block_size). n = A.ncols. if p is None:; p = min(num_moments // 2, 10). # TODO: When moment_samples > n, we should just do a TSQR on A, and compute; # the spectrum of R.; assert moment_samples < n, '_spectral_moments: moment_samples must be smaller than num cols of A'; G = hl.rand_unif(-1, 1, size=(n, moment_samples)).map(lambda x: hl.sign(x)); Q1, R1 = hl.nd.qr(G)._persist(); fact = _krylov_factorization(A, Q1, p, compute_U=False); moments_and_stdevs = hl.eval(fact.spectral_moments(num_moments, R1)); moments = moments_and_stdevs.moments; stdevs = moments_and_stdevs.stdevs; return moments, stdevs. @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix),; k=int,; num_moments=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; moment_samples=int,; ); def _pca_and_moments(; A,; k=10,; num_moments=5,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; moment_samples=100,; ):; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_spectral_moments/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. # Set Parameters; q = q_iterations; L = k + oversampling_param; n =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:54,Usability,Feedback,Feedback,54,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:264,Usability,Guid,Guides,264,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/pca.html:2670,Usability,simpl,simply,2670,"ormalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance e",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:3409,Availability,error,error,3409,"-------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; -----. This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; `name` parameter. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `call_rate` (``float64``) -- Fraction of calls not missing or filtered.; Equivalent to `n_called` divided by :meth:`.count_rows`.; - `n_called` (``int64``) -- Number of non-missing calls.; - `n_not_called` (``int64``) -- Number of missing calls.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_hom_ref` (``int64``) -- Number of homozygous reference calls.; - `n_het` (``int64``) -- Number of heterozygous calls.; - `n_hom_var` (``int64``) -- Number of homozygous alternate calls.; - `n_non_ref` (``int64``) -- Sum of `n_het` and `n_hom_var`.; - `n_snp` (``int64``) -- Number of SNP alternate alleles.; - `n_insertion` (``int64``) -- Number of insertion alternate alleles.; - `n_deletion` (``int64``) -- Number of deletion alternate alleles.; - `n_singleton` (``int64``) -- Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; - ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:9097,Availability,error,error,9097,"atrixTable, name=str); def variant_qc(mt, name='variant_qc') -> MatrixTable:; """"""Compute common variant statistics (quality control metrics). .. include:: ../_templates/req_tvariant.rst. Examples; --------. >>> dataset_result = hl.variant_qc(dataset). Notes; -----; This method computes variant statistics from the genotype data, returning; a new struct field `name` with the following metrics based on the fields; present in the entry schema. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `AF` (``array<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference al",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26247,Availability,avail,available,26247,"ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44262,Availability,checkpoint,checkpoint,44262,"tes with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44884,Availability,toler,tolerateParseError,44884,". ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; -----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:58194,Availability,down,down,58194,"leles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters; ----------; mt : :class:`.MatrixTable` or :class:`.Table`; Matrix table with a variant (locus / alleles) row key.; show : :obj:`bool`; If ``True``, print results instead of returning them.; handler. Notes; -----; The result returned if `show` is ``False`` is a :class:`.Struct` with; five fields:. - `n_variants` (:obj:`int`): Number of variants present in the matrix table.; - `allele_types` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of alternate alleles in; each allele allele category.; - `contigs` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of variants on each contig.; - `allele_counts` (:obj:`dict` [:obj:`int`, :obj:`int`]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; - `r_ti_tv` (:obj:`float`): Ratio of transition alternate alleles to; transversion alternate alleles. Returns; -------; :obj:`None` or :class:`.Struct`; Returns ``None`` if `show` is ``True``, or returns results as a struct.; """"""; require_row_key_variant(mt, 'summarize_variants'); if isinstance(mt, MatrixTable):; ht = mt.rows(); else:; ht = mt; allele_pairs = hl.range(1, hl.len(ht.alleles)).map(lambda i: (ht.alleles[0], ht.alleles[i])). def explode_result(alleles):; ref, alt = alleles; return (; hl.agg.counter(hl.allele_type(ref, alt)),; hl.agg.count_where(hl.is_transition(ref, alt)),; hl.agg.count_where(hl.is_transversion(ref, alt)),; ). (allele_types, nti, ntv), contigs, allele_counts, n_variants = ht.aggregate((; hl.agg.explode(explode_result, allele_pairs),; hl.agg.counter(ht.locus.contig),; hl.agg.counter(hl.len(ht.alleles)),; hl.agg.count(),; )); rg = ht.locus.dtype.reference_genome; if show:; summary = _Var",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:153,Deployability,Install,Installation,153,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:221,Deployability,Configurat,Configuration,221,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23077,Deployability,configurat,configuration,23077,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26844,Deployability,configurat,configuration,26844,"1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28802,Deployability,configurat,configuration,28802,"cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_comma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:31890,Deployability,configurat,configuration,31890,"e {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:33515,Deployability,update,update,33515,"ckend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env); b.create_job(; vep_config.image,; vep_config.batch_run_csq_header_command,; attributes={'name': 'csq-header'},; resources={'cpu': '1', 'memory': 'standard'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tol",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:34683,Deployability,update,update,34683,"d'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_INPUT_FILE': local_input_file,; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env). b.create_job(; vep_config.image,; vep_config.batch_run_command,; attributes={'name': f'vep-{part_id}'},; resources={'cpu': '1', 'memory': 'standard'},; input_files=[(path, local_input_file)],; output_files=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, canc",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37453,Deployability,configurat,configuration,37453,"tions.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37508,Deployability,Install,Installation,37508,"annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37572,Deployability,install,installed,37572,"n(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37673,Deployability,install,installing,37673,"tions.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37856,Deployability,Configurat,Configuration,37856,",; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will chang",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37894,Deployability,configurat,configuration,37894,"; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invok",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38244,Deployability,configurat,configuration,38244,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38882,Deployability,configurat,configuration,38882,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38918,Deployability,release,release,38918,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38930,Deployability,install,installed,38930,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:41977,Deployability,configurat,configuration,41977,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42372,Deployability,Configurat,Configuration,42372,"[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42449,Deployability,configurat,configuration,42449,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42498,Deployability,configurat,configuration,42498,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43123,Deployability,configurat,configuration,43123,"ud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45994,Deployability,Configurat,Configuration,45994,"+ '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46039,Deployability,configurat,configuration,46039,"bals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46867,Deployability,configurat,configuration,46867,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51732,Deployability,configurat,configuration,51732,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:63354,Deployability,update,updated,63354," ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', 'GQ']):; ad_field = 'AD'; gt_field = 'GT'; else:; raise ValueError(; f""'compute_charr': require a VDS or MatrixTable with fields LAD/LAD/LGT/GQ/DP or AD/GT/GQ/DP,""; f"" found entry fields {list(mt.entry)}""; ); # Annotate reference allele frequency when it is not defined in the original data, and name it 'ref_AF'.; ref_af_field = '__ref_af'; if ref_AF is None:; n_samples = mt.count_cols(); if n_samples < 10000:; raise ValueError(; ""'compute_charr': with fewer than 10,000 samples, require a reference AF in 'reference_data_source'.""; ). n_alleles = 2 * n_samples; mt = mt.annotate_rows(**{ref_af_field: 1 - hl.agg.sum(mt[gt_field].n_alt_alleles()) / n_alleles}); else:; mt = mt.annotate_rows(**{ref_af_field: ref_AF}). # Filter to autosomal biallelic SNVs with reference allele frequency within the range (min_af, max_af); rg = mt.locus.dtype.reference_genome.name; if rg == 'GRCh37':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('1-22', reference_genome=rg)]); elif rg == 'GRCh38':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('chr1-chr22', reference_genome=rg)]); else:; mt = mt.filter_rows(mt.locus.in_autosome()). mt = mt.filter_rows(; (hl.len(mt.alleles) == 2); & hl.is_snp(mt.alleles[0], mt.alleles[1]); & (mt[ref_af_field] > min_af); & (mt[ref_af_field] < max_af); ). # Filter to variant calls with GQ above min_gq and DP within the range (min_dp, max_dp); ad_dp = mt['DP'] if 'DP' in mt.entry else hl.sum(mt[ad_field]); mt = mt.filter_entries(mt[gt_field].is_hom_var() & (mt.GQ >= min_gq) & (ad_dp >= min_dp) & (ad_dp <= max_dp)). # Compute CHARR; mt = mt.select_cols(charr=hl.agg.mean((mt[ad_field][0] / (mt[ad_field][0] + mt[ad_field][1])) / mt[ref_af_field])). mt = mt.select_globals(; af_min=min_af,; af_max=max_af,; dp_min=min_dp,; dp_max=max_dp,; gq_min=min_gq,; ). return mt.cols(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:35898,Integrability,message,message,35898,"=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; )",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:36024,Integrability,message,message,36024,"vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38475,Integrability,depend,depending,38475,"""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOU",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:221,Modifiability,Config,Configuration,221,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:1428,Modifiability,config,config,1428,"e for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; return hl.bind(; lambda at: hl.if_else(; at == AlleleType.SNP,; hl.if_else(hl.is_transition(ref, alt), AlleleType.TRANSITION, AlleleType.TRANSVERSION),; at,; ),; numeric_allele_type(ref, alt),; ). [docs]@typecheck(mt=MatrixTable, name=str); def sample_qc(mt, name='sample_qc') -> MatrixTable:; """"""Compute per-sample metrics useful for quality control. .. include:: ../_templates/req_tvariant.rst. Examples; --------. Compute sample QC metrics and rem",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23037,Modifiability,config,configuring,23037,",; variant_allele=tstr,; ); ),; seq_region_name=tstr,; start=tint32,; strand=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run V",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23077,Modifiability,config,configuration,23077,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23141,Modifiability,inherit,inherits,23141,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23833,Modifiability,variab,variables,23833,"nd=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25202,Modifiability,plugin,plugin,25202,"an run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPU",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25505,Modifiability,variab,variables,25505,"file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_co",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25772,Modifiability,config,config,25772,"hon3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26231,Modifiability,variab,variable,26231,"ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26844,Modifiability,config,configuration,26844,"1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28446,Modifiability,plugin,plugin,28446,"lf,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28802,Modifiability,config,configuration,28802,"cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_comma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30633,Modifiability,plugin,plugin,30633,"lf.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domai",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30674,Modifiability,Plugin,Plugins,30674,"lf.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domai",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30900,Modifiability,Plugin,Plugins,30900,", 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep config",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:31890,Modifiability,config,configuration,31890,"e {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:32066,Modifiability,config,config,32066,"ta/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:32370,Modifiability,config,config,32370,"38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:32404,Modifiability,config,config,32404,"38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:36804,Modifiability,config,config,36804,"failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:36962,Modifiability,config,config,36962,"failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37453,Modifiability,config,configuration,37453,"tions.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37856,Modifiability,Config,Configuration,37856,",; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will chang",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37894,Modifiability,config,configuration,37894,"; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invok",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37954,Modifiability,config,config,37954,"ig: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38095,Modifiability,config,config,38095,"bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_F",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38244,Modifiability,config,configuration,38244,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38537,Modifiability,variab,variables,38537," if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38882,Modifiability,config,configuration,38882,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38966,Modifiability,plugin,plugin,38966,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:39189,Modifiability,plugin,plugin,39189,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:41977,Modifiability,config,configuration,41977,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42220,Modifiability,config,config,42220,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42293,Modifiability,variab,variable,42293,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42343,Modifiability,config,config,42343,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42372,Modifiability,Config,Configuration,42372,"[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42395,Modifiability,config,config,42395,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42449,Modifiability,config,configuration,42449,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42498,Modifiability,config,configuration,42498,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43055,Modifiability,config,config,43055,"GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43123,Modifiability,config,configuration,43123,"ud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44153,Modifiability,config,config,44153,"fig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return datase",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44301,Modifiability,config,config,44301,"tes with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44451,Modifiability,config,config,44451,"valid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44603,Modifiability,config,config,44603,"e`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44638,Modifiability,config,config,44638,"ntaining VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44708,Modifiability,config,config,44708,"xTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44826,Modifiability,config,config,44826,". ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; -----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44835,Modifiability,config,config,44835,". ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; -----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45396,Modifiability,config,config,45396,"aybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by d",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45483,Modifiability,config,config,45483,"aybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by d",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45994,Modifiability,Config,Configuration,45994,"+ '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46039,Modifiability,config,configuration,46039,"bals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46415,Modifiability,variab,variable,46415,"ck_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46867,Modifiability,config,configuration,46867,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51485,Modifiability,inherit,inheritance,51485,"Hc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51693,Modifiability,config,config,51693,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51732,Modifiability,config,configuration,51732,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:52285,Modifiability,config,config,52285," }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.alleles_per_variant = alleles_per_variant; self.variants_per_contig = variants_per_contig; self.allele_types = allele_types; self.nti = nti; self.ntv = ntv. def __repr__(self):; return self.__str__(). def _repr_html_(self):; return self._html_string(). def __str__(self):; contig_idx = {contig: i for i, contig in enumerate(self.rg.contigs)}; max_contig_len = max(len(contig) for contig in self.variants_per_contig); contig_formatter = f'%{max_contig_len}s'. max_allele_count_len = max(len(str(x)) for x in self.alleles_per_variant); allele_count_formatter = f'%{max_allele_cou",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:52294,Modifiability,config,config,52294," }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.alleles_per_variant = alleles_per_variant; self.variants_per_contig = variants_per_contig; self.allele_types = allele_types; self.nti = nti; self.ntv = ntv. def __repr__(self):; return self.__str__(). def _repr_html_(self):; return self._html_string(). def __str__(self):; contig_idx = {contig: i for i, contig in enumerate(self.rg.contigs)}; max_contig_len = max(len(contig) for contig in self.variants_per_contig); contig_formatter = f'%{max_contig_len}s'. max_allele_count_len = max(len(str(x)) for x in self.alleles_per_variant); allele_count_formatter = f'%{max_allele_cou",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14032,Performance,perform,performs,14032,"0].p_value,; 'p_value_excess_het': hwe[1].p_value,; }),; ),; ). return mt.annotate_rows(**{name: result}). [docs]@typecheck(left=MatrixTable, right=MatrixTable, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14193,Performance,perform,performs,14193,"ble, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or filtered entry); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var. The first index is the state in the left dataset and the secon",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25117,Performance,cache,cache,25117,"Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is Fals",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28361,Performance,cache,cache,28361,"The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is locate",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30485,Performance,cache,cache,30485,":`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(;",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:39125,Performance,cache,cache,39125,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46644,Performance,cache,cache,46644,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46667,Performance,cache,cache,46667,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:47076,Performance,cache,cache,47076,"/en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:47101,Performance,Cache,Cache,47101,"/en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:60443,Performance,load,loaded,60443,"heck(; ds=oneof(MatrixTable, lambda: hl.vds.VariantDataset),; min_af=numeric,; max_af=numeric,; min_dp=int,; max_dp=int,; min_gq=int,; ref_AF=nullable(expr_float64),; ); def compute_charr(; ds: Union[MatrixTable, 'hl.vds.VariantDataset'],; min_af: float = 0.05,; max_af: float = 0.95,; min_dp: int = 10,; max_dp: int = 100,; min_gq: int = 20,; ref_AF: Optional[Float64Expression] = None,; ):; """"""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """""". # Determine whether the input data is in the VDS format; if not, convert matrixtable to VDS and extract only the variant call information; if isinstance(ds, hl.vds.VariantDataset):; mt = ds.variant_data; else:; mt = ds. if all(x in mt.entry for x in ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', '",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:60530,Performance,load,load,60530,"heck(; ds=oneof(MatrixTable, lambda: hl.vds.VariantDataset),; min_af=numeric,; max_af=numeric,; min_dp=int,; max_dp=int,; min_gq=int,; ref_AF=nullable(expr_float64),; ); def compute_charr(; ds: Union[MatrixTable, 'hl.vds.VariantDataset'],; min_af: float = 0.05,; max_af: float = 0.95,; min_dp: int = 10,; max_dp: int = 100,; min_gq: int = 20,; ref_AF: Optional[Float64Expression] = None,; ):; """"""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """""". # Determine whether the input data is in the VDS format; if not, convert matrixtable to VDS and extract only the variant call information; if isinstance(ds, hl.vds.VariantDataset):; mt = ds.variant_data; else:; mt = ds. if all(x in mt.entry for x in ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', '",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37227,Safety,Predict,Predictor,37227," annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:50277,Security,hasH,hasHomoplasmy,50277,"int32,; afrHc: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; amrHc: int32,; easAf: float64,; easAc: int32,; easAn: int32,; easHc: int32,; finAf: float64,; finAc: int32,; finAn: int32,; finHc: int32,; nfeAf: float64,; nfeAc: int32,; nfeAn: int32,; nfeHc: int32,; othAf: float64,; othAc: int32,; othAn: int32,; othHc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:50299,Security,hasH,hasHeteroplasmy,50299,"amrAf: float64,; amrAc: int32,; amrAn: int32,; amrHc: int32,; easAf: float64,; easAc: int32,; easAn: int32,; easHc: int32,; finAf: float64,; finAc: int32,; finAn: int32,; finHc: int32,; nfeAf: float64,; nfeAc: int32,; nfeAn: int32,; nfeHc: int32,; othAf: float64,; othAc: int32,; othAn: int32,; othHc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:375,Testability,Log,Log,375,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:493,Testability,log,logging,493,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:1650,Testability,log,log,1650,"rt hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; return hl.bind(; lambda at: hl.if_else(; at == AlleleType.SNP,; hl.if_else(hl.is_transition(ref, alt), AlleleType.TRANSITION, AlleleType.TRANSVERSION),; at,; ),; numeric_allele_type(ref, alt),; ). [docs]@typecheck(mt=MatrixTable, name=str); def sample_qc(mt, name='sample_qc') -> MatrixTable:; """"""Compute per-sample metrics useful for quality control. .. include:: ../_templates/req_tvariant.rst. Examples; --------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:1656,Testability,log,logging,1656,"rt hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; return hl.bind(; lambda at: hl.if_else(; at == AlleleType.SNP,; hl.if_else(hl.is_transition(ref, alt), AlleleType.TRANSITION, AlleleType.TRANSVERSION),; at,; ),; numeric_allele_type(ref, alt),; ). [docs]@typecheck(mt=MatrixTable, name=str); def sample_qc(mt, name='sample_qc') -> MatrixTable:; """"""Compute per-sample metrics useful for quality control. .. include:: ../_templates/req_tvariant.rst. Examples; --------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:10330,Testability,test,test,10330," - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; require_alleles_field(mt, 'variant_qc'). bound_exprs = {}; gq_dp_exprs = {}. def has_field_of_type(name, dtype):; return name in mt.entry ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:10487,Testability,test,test,10487,"lleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; require_alleles_field(mt, 'variant_qc'). bound_exprs = {}; gq_dp_exprs = {}. def has_field_of_type(name, dtype):; return name in mt.entry and mt[name].dtype == dtype. if has_field_of_type('DP', hl.tint32):; gq_dp_exprs['dp_stats'] = hl.agg.stats(mt.DP).select('mean', 'stdev', 'min', 'max'). if has_field_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:10793,Testability,test,test,10793,"not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; require_alleles_field(mt, 'variant_qc'). bound_exprs = {}; gq_dp_exprs = {}. def has_field_of_type(name, dtype):; return name in mt.entry and mt[name].dtype == dtype. if has_field_of_type('DP', hl.tint32):; gq_dp_exprs['dp_stats'] = hl.agg.stats(mt.DP).select('mean', 'stdev', 'min', 'max'). if has_field_of_type('GQ', hl.tint32):; gq_dp_exprs['gq_stats'] = hl.agg.stats(mt.GQ).select('mean', 'stdev', 'min', 'max'). if not has_field_of_type('GT', hl.tcall):; raise ValueError(""'variant_qc': expect an entry field 'GT' of type 'call'""). bound_exprs['n_called'] = hl.agg.count_where(hl.is_defined(mt['GT'])); bound_exprs['n_not_called'] = hl.ag",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:35970,Testability,log,log,35970,"ame}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Opti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:35988,Testability,log,log,35988,"ep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:53,Usability,Feedback,Feedback,53,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/qc.html:263,Usability,Guid,Guides,263,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:9740,Availability,error,error,9740,"s a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:25796,Availability,checkpoint,checkpoint,25796,"ta', 'standard_error', 't_stat', 'p_value']; computed_row_fields = {; field_name: per_y_list.map(lambda one_y: one_y[field_name][row_idx]); for field_name in computed_row_field_names; }; pass_through_rows = {field_name: block[field_name][row_idx] for field_name in row_field_names}. if not is_chained:; computed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant us",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26093,Availability,toler,tolerance,26093,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26237,Availability,toler,tolerance,26237,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27251,Availability,toler,tolerance,27251,"l[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present valu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27561,Availability,toler,tolerance,27561,"result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input va",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29671,Availability,error,error,29671," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32435,Availability,error,errors,32435,"g we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-com",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36690,Availability,toler,tolerance,36690,"l row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36837,Availability,toler,tolerance,36837,"l row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36863,Availability,toler,tolerance,36863,"`pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37150,Availability,toler,tolerance,37150,"lumn-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37160,Availability,toler,tolerance,37160,"lumn-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37177,Availability,toler,tolerance,37177,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37197,Availability,toler,tolerance,37197,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37222,Availability,toler,tolerance,37222,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38592,Availability,toler,tolerance,38592," no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38604,Availability,toler,tolerance,38604," no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39197,Availability,toler,tolerance,39197,"lectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:40988,Availability,toler,tolerance,40988,". b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). next_b = b + delta_b; next_mu = sigmoid(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = X.T @ (X * (next_mu * (1 - next_mu)).reshape(-1, 1)). return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(hl.abs(delta_b)); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(search, numerical_regression_fit_dtype, 1, b, mu, score, fisher). def wald_test(X, fit):; se = hl.sqrt(hl.nd.diagonal(hl.nd.inv(fit.fisher))); z = fit.b / se; p = z.map(lambda e: 2 * hl.pnorm(-hl.abs(e))); return hl.struct(; beta=fit.b[X.shape[1] - 1],; stand",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43413,Availability,toler,tolerance,43413,"it.select('n_iterations', 'converged', 'exploded'),; ). def logistic_score_test(X, y, null_fit):; m = X.shape[1]; m0 = null_fit.b.shape[0]; b = hl.nd.hstack([null_fit.b, hl.nd.zeros((hl.int32(m - m0)))]). X0 = X[:, 0:m0]; X1 = X[:, m0:]. mu = hl.expit(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44371,Availability,toler,tolerance,44371,"ricExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45453,Availability,toler,tolerance,45453,"oded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45572,Availability,toler,tolerance,45572,"lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; );",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45582,Availability,toler,tolerance,45582,"lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; );",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45819,Availability,toler,tolerance,45819," @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45829,Availability,toler,tolerance,45829," @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:46982,Availability,toler,tolerance,46982,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47130,Availability,toler,tolerance,47130,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50161,Availability,error,error,50161," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52891,Availability,error,errors,52891,"g we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-com",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57190,Availability,toler,tolerance,57190," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57210,Availability,toler,tolerance,57210," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57235,Availability,toler,tolerance,57235," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59569,Availability,toler,tolerance,59569,"ows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59579,Availability,toler,tolerance,59579,"ows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60066,Availability,toler,tolerance,60066,"d(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60076,Availability,toler,tolerance,60076,"d(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60172,Availability,toler,tolerance,60172,"ror(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60182,Availability,toler,tolerance,60182,"ror(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61042,Availability,toler,tolerance,61042,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61173,Availability,toler,tolerance,61173,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62201,Availability,toler,tolerance,62201,"with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fie",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62348,Availability,toler,tolerance,62348,"with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fie",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62374,Availability,toler,tolerance,62374,"regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selectin",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62588,Availability,toler,tolerance,62588,"ough` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62598,Availability,toler,tolerance,62598,"ough` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62615,Availability,toler,tolerance,62615,"e an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62635,Availability,toler,tolerance,62635,"e an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62660,Availability,toler,tolerance,62660,"e an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63848,Availability,toler,tolerance,63848,", x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63860,Availability,toler,tolerance,63860,", x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64138,Availability,toler,tolerance,64138,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64278,Availability,toler,tolerance,64278,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64347,Availability,toler,tolerance,64347,"ng an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64367,Availability,toler,tolerance,64367,"ng an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64392,Availability,toler,tolerance,64392,"ng an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:66009,Availability,toler,tolerance,66009,"x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67313,Availability,toler,tolerance,67313,"annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67875,Availability,toler,tolerance,67875,"vec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < toleranc",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68791,Availability,toler,tolerance,68791,"cExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True). exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(delta_b.map(lambda e: hl.abs(e))); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.select(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b, mu, score, fisher). def _poisson_score_test(null_fit, covmat, y, xvec):; dof = 1. X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = hl.exp(X @ b); score = hl.nd.hstack(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74770,Availability,avail,available,74770,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:76290,Availability,fault,fault,76290,"association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datas",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:76964,Availability,fault,fault,76964,"phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77433,Availability,fault,fault,77433,"--------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77818,Availability,error,errors,77818,"F[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78076,Availability,fault,fault,78076," | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expre",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79812,Availability,fault,fault,79812," We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alon",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:80415,Availability,fault,fault,80415,"Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:80443,Availability,fault,fault,80443,"-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.coun",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:86545,Availability,fault,fault,86545,"prox filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:86565,Availability,fault,fault,86565,"lues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:92010,Availability,fault,fault,92010,"n testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real dat",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:92686,Availability,fault,fault,92686,"otype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93155,Availability,fault,fault,93155,"------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93540,Availability,error,errors,93540,"F[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statisti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93800,Availability,fault,fault,93800," 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95669,Availability,error,errors,95669,"ters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype fr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95771,Availability,fault,fault,95771," This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alon",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:96374,Availability,fault,fault,96374,"s size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbool` True if the null model failed to converge due to numerical; ex",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:96402,Availability,fault,fault,96402," p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbool` True if the null model failed to converge due to numerical; explosion. """"""; mt = matrix_ta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:98435,Availability,toler,tolerance,98435," x); k = len(covariates); if k == 0:; raise ValueError('_logistic_skat: at least one covariate is required.'); _warn_if_no_intercept('_logistic_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); if mt.y.dtype != hl.tbool:; mt = mt.annotate_cols(; y=(; hl.case(); .when(hl.any(mt.y == 0, mt.y == 1), hl.bool(mt.y)); .or_error(; hl.format(; f'hl._logistic_skat: phenotypes must either be True, False, 0, or 1, found: %s of type {mt.y.dtype}',; mt.y,; ); ); ); ); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); null_fit = logreg_fit(mt.covmat, mt.yvec, None, max_iterations=null_max_iterations, tolerance=null_tolerance); mt = mt.annotate_globals(; null_fit=hl.case(); .when(null_fit.converged, null_fit); .or_error(hl.format('hl._logistic_skat: null model did not converge: %s', null_fit)); ); null_mu = mt.null_fit.mu; y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=null_mu * (1 - null_mu)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= max_size, ht.G_take)).T,; ); ht = ht.annotate(; # Q=ht.y_residual @ (ht.G * ht.weight) @ ht.G.T @ ht.y_residual.T; Q=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101146,Availability,fault,fault,101146,"ambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, th",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101166,Availability,fault,fault,101166," eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterat",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104187,Availability,fault,fault,104187,"icitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104331,Availability,fault,fault,104331,"h the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104702,Availability,fault,fault,104702,"icate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +-----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105298,Availability,fault,fault,105298," | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105554,Availability,fault,fault,105554,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105826,Availability,error,error,105826,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107035,Availability,toler,tolerance,107035,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107175,Availability,toler,tolerance,107175,"ory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107343,Availability,fault,fault,107343,": :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariate",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109064,Availability,toler,tolerance,109064,"ntry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109149,Availability,toler,tolerance,109149,"ntry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109269,Availability,toler,tolerance,109269,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109630,Availability,toler,tolerance,109630,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:112309,Availability,error,errors,112309,"`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113184,Availability,down,downcoding,113184,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113405,Availability,down,downcode,113405,"not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113587,Availability,down,downcode,113587,"then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114164,Availability,error,error,114164,"updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt._rvrow.annotate(; locus=mt[new_id]['loc",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117932,Availability,error,errors,117932,"]), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119009,Availability,down,downcoded,119009,"this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119593,Availability,down,downcode,119593,"eles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value `",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119765,Availability,down,downcoding,119765,"unc:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value ``[10, 2]`` is split, each split; site will contain the same array ``[10, 2]``. The provided allele index; field `a_index` can be used to select the value corresponding to the split; allele's position:. >>> split_ds = hl.split_multi_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122464,Availability,error,error,122464,"split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (; 'intergenic_consequences',; 'motif_fe",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:124421,Availability,down,downcode,124421,"ic_consequences',; 'motif_feature_consequences',; 'regulatory_feature_consequences',; 'transcript_consequences',; ); }). if isinstance(ds, Table):; return split.annotate(**update_rows_expression).drop('old_locus', 'old_alleles'). split = split.annotate_rows(**update_rows_expression); entry_fields = ds.entry. expected_field_types = {; 'GT': hl.tcall,; 'AD': hl.tarray(hl.tint),; 'DP': hl.tint,; 'GQ': hl.tint,; 'PL': hl.tarray(hl.tint),; 'PGT': hl.tcall,; 'PID': hl.tstr,; }. bad_fields = []; for field in entry_fields:; if field in expected_field_types and entry_fields[field].dtype != expected_field_types[field]:; bad_fields.append((field, entry_fields[field].dtype, expected_field_types[field])). if bad_fields:; msg = '\n '.join([f""'{x[0]}'\tfound: {x[1]}\texpected: {x[2]}"" for x in bad_fields]); raise TypeError(""'split_multi_hts': Found invalid types for the following fields:\n "" + msg). update_entries_expression = {}; if 'GT' in entry_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:;",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:124897,Availability,down,downcode,124897," bad_fields = []; for field in entry_fields:; if field in expected_field_types and entry_fields[field].dtype != expected_field_types[field]:; bad_fields.append((field, entry_fields[field].dtype, expected_field_types[field])). if bad_fields:; msg = '\n '.join([f""'{x[0]}'\tfound: {x[1]}\texpected: {x[2]}"" for x in bad_fields]); raise TypeError(""'split_multi_hts': Found invalid types for the following fields:\n "" + msg). update_entries_expression = {}; if 'GT' in entry_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:; update_entries_expression['PID'] = split.PID; return split.annotate_entries(**update_entries_expression).drop('old_locus', 'old_alleles'). [docs]@typecheck(call_expr=expr_call); def genetic_relatedness_matrix(call_expr) -> BlockMatrix:; r""""""Compute the genetic relatedness matrix (GRM). Examples; --------. >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; -----; The genetic relationship matrix (GRM) :math:`G` encodes genetic correlation; between each pair of sa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:125367,Availability,down,downcode,125367,"_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:; update_entries_expression['PID'] = split.PID; return split.annotate_entries(**update_entries_expression).drop('old_locus', 'old_alleles'). [docs]@typecheck(call_expr=expr_call); def genetic_relatedness_matrix(call_expr) -> BlockMatrix:; r""""""Compute the genetic relatedness matrix (GRM). Examples; --------. >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; -----; The genetic relationship matrix (GRM) :math:`G` encodes genetic correlation; between each pair of samples. It is defined by :math:`G = MM^T` where; :math:`M` is a standardized version of the genotype matrix, computed as; follows. Let :math:`C` be the :math:`n \times m` matrix of raw genotypes; in the variant dataset, with rows indexed by :math:`n` samples and columns; indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the; number of alternate alleles of variant :math:`j` carried by sample; :math:`i`, which can be 0, 1, 2, or missing. For each vari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:140007,Availability,error,error,140007,"ghborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:140326,Availability,error,error,140326,"k that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as `locus_expr`.; By default, the row value is given by the locus position.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Windowed correlation matrix between variants.; Row and column indices correspond to matrix table variant index.; """"""; starts_and_stops = hl.linalg.utils.locus_windows(locus_expr, radius, coord_expr, _localize=False); start",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159084,Availability,Down,Downcode,159084,"nfo). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159403,Availability,down,downcode,159403," rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, comb",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159637,Availability,down,downcoding,159637,"mputing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160011,Availability,down,downcode,160011,"gth is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160166,Availability,Down,Downcode,160166,"g the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most l",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160407,Availability,Down,Downcode,160407,"ces of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162495,Availability,down,downcode,162495," | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162583,Availability,down,downcodes,162583," based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]])",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:163661,Availability,down,downcode,163661,"mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]]),; hl.missing(tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=newPL,; ); # otherwise downcode; else:; mt = mt.annotate_rows(__old_to_new_no_na=mt.old_to_new.map(lambda x: hl.or_else(x, 0))); newPL = hl.if_else(; hl.is_defined(mt.PL),; (; hl.range(0, hl.triangle(hl.len(mt.alleles))).map(; lambda newi: hl.min(; hl.range(0, hl.triangle(hl.len(mt.old_alleles))); .filter(; lambda oldi: hl.bind(; lambda oldc: hl.call(mt.__old_to_new_no_na[oldc[0]], mt.__old_to_new_no_na[oldc[1]]); == hl.unphased_diploid_gt_index_call(newi),; hl.unphased_diploid_gt_index_call(oldi),; ); ); .map(lambda oldi: mt.PL[oldi]); ); ); ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.call(mt.__old_to_new_no_na[mt.GT[0]], mt.__old_to_new_no_na[mt.GT[1]]),; AD=hl.if_else(; hl.is_defined(mt.AD),; (; hl.range(0, hl.len(mt.alleles)).map(; lambda newi: hl.sum(; hl.range(0, hl.len(mt.old_alleles)); .filter(lambda oldi: mt.__old_to_new_no_na[oldi] == newi); .map(lambda oldi: mt.AD[oldi]); ); ); ),; hl.missing(tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=ne",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:168667,Availability,error,errors,168667,"nor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""; if block_size is None:; block_size = BlockMatrix.default_block_size(). if not 0.0 <= r2 <= 1:; raise ValueError(f'r2 must be in the ran",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:158,Deployability,Install,Installation,158,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:226,Deployability,Configurat,Configuration,226,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,Deployability,integrat,integration,77492,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,Deployability,integrat,integration,77668," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,Deployability,integrat,integration,93214,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,Deployability,integrat,integration,93390,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,Deployability,integrat,integration,106025,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113156,Deployability,update,updates,113156,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:146646,Deployability,continuous,continuous,146646,"omogeneous modern populations that have; each diverged from a single ancestral population (a `star phylogeny`). Each; sample is assigned a population by sampling from the categorical; distribution :math:`\pi`. Note that the actual size of each population is; random. Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum :math:`P_0`. The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding :math:`F_{ST}`; parameter :math:`F_k` (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots,; K`). For each variant and population, allele frequencies are drawn from a; `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__; whose parameters are determined by the ancestral allele frequency and; :math:`F_{ST}` parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by :math:`k_n`, ancestral allele frequencies by :math:`p_m`,; population allele frequencies by :math:`p_{k, m}`, and diploid, unphased; genotype calls by :math:`g_{n, m}` (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively). The generative model is then given by:. .. math::; \begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. The beta distribution by its mean and variance above; the usual parameters; are :math:`a = (1 - p) \frac{1 - F}{F}` and :math:`b = p \frac{1 - F}{F}` with; :math:`F = F_k` and :math:`p = p_m`. The resulting dataset has the following fields. Global fields:. - `bn.n_populations` (:py:data:`.tint32`) -- Number of populations.; - `bn.n_samples` (:py:data:`.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:153785,Deployability,Update,Update,153785,"([(1 - x) / x for x in fst]).map(; lambda x: hl.rand_beta(ancestral * x, (1 - ancestral) * x); ),; entries=hl.repeat(hl.struct(), n_samples),; ),; af_dist,; ); ),; ). bn = bn._unlocalize_entries('entries', 'cols', ['sample_idx']). # entry info; p = hl.sum(bn.pop * bn.af) if mixture else bn.af[bn.pop]; q = 1 - p. if phased:; mom = hl.rand_bool(p); dad = hl.rand_bool(p); return bn.select_entries(GT=hl.call(mom, dad, phased=True)). idx = hl.rand_cat([q**2, 2 * p * q, p**2]); return bn.select_entries(GT=hl.unphased_diploid_gt_index_call(idx)). [docs]@typecheck(mt=MatrixTable, f=anytype); def filter_alleles(mt: MatrixTable, f: Callable) -> MatrixTable:; """"""Filter alternate alleles. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Keep SNPs:. >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:. >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[allele_index - 1] > 0). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whet",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155564,Deployability,update,update,155564,"s the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155753,Deployability,update,updated,155753," filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155770,Deployability,update,update,155770,"nate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:157652,Deployability,update,update,157652,"hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), old_to_new_dict); mt = mt.annotate_rows(old_to_new=old_to_new, new_to_old=new_to_old); new_locus_alleles = hl.min_rep(mt.locus, mt.new_to_old.map(lambda i: mt.alleles[i])); mt = mt.annotate_rows(__new_locus=new_locus_alleles.locus, __new_alleles=new_locus_alleles.alleles); mt = mt.filter_rows(hl.len(mt.__new_alleles) > 1); left = mt.filter_rows((mt.locus == mt.__new_locus) & (mt.alleles == mt.__new_alleles)). right = mt.filter_rows((mt.locus != mt.__new_locus) | (mt.alleles != mt.__new_alleles)); right = right.key_rows_by(locus=right.__new_locus, alleles=right.__new_alleles); return left.union_rows(right, _check_cols=False).drop('__allele_inclusion', '__new_locus', '__new_alleles'). [docs]@typecheck(mt=MatrixTable, f=anytype, subset=bool); def filter_alleles_hts(mt: MatrixTable, f: Callable, subset: bool = False) -> MatrixTable:; """"""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, befo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:157881,Deployability,Update,Update,157881,"s, mt.new_to_old.map(lambda i: mt.alleles[i])); mt = mt.annotate_rows(__new_locus=new_locus_alleles.locus, __new_alleles=new_locus_alleles.alleles); mt = mt.filter_rows(hl.len(mt.__new_alleles) > 1); left = mt.filter_rows((mt.locus == mt.__new_locus) & (mt.alleles == mt.__new_alleles)). right = mt.filter_rows((mt.locus != mt.__new_locus) | (mt.alleles != mt.__new_alleles)); right = right.key_rows_by(locus=right.__new_locus, alleles=right.__new_alleles); return left.union_rows(right, _check_cols=False).drop('__allele_inclusion', '__new_locus', '__new_alleles'). [docs]@typecheck(mt=MatrixTable, f=anytype, subset=bool); def filter_alleles_hts(mt: MatrixTable, f: Callable, subset: bool = False) -> MatrixTable:; """"""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are f",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:161986,Deployability,update,update,161986,"iltered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162138,Deployability,update,updated,162138,"ype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162155,Deployability,update,update,162155," so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.P",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:172830,Deployability,update,updated,172830,"pruned_table.locus, bp_window_size). entries = r2_bm.sparsify_row_intervals(range(stops.size), stops, blocks_only=True).entries(keyed=False); entries = entries.filter((entries.entry >= r2) & (entries.i < entries.j)); entries = entries.select(i=hl.int32(entries.i), j=hl.int32(entries.j)). if keep_higher_maf:; fields = ['mean', 'locus']; else:; fields = ['locus']. info = locally_pruned_table.aggregate(; hl.agg.collect(locally_pruned_table.row.select('idx', *fields)), _localize=False; ); info = hl.sorted(info, key=lambda x: x.idx). entries = entries.annotate_globals(info=info). entries = entries.filter(; (entries.info[entries.i].locus.contig == entries.info[entries.j].locus.contig); & (entries.info[entries.j].locus.position - entries.info[entries.i].locus.position <= bp_window_size); ). if keep_higher_maf:; entries = entries.annotate(; i=hl.struct(; idx=entries.i, twice_maf=hl.min(entries.info[entries.i].mean, 2.0 - entries.info[entries.i].mean); ),; j=hl.struct(; idx=entries.j, twice_maf=hl.min(entries.info[entries.j].mean, 2.0 - entries.info[entries.j].mean); ),; ). def tie_breaker(left, right):; return hl.sign(right.twice_maf - left.twice_maf). else:; tie_breaker = None. variants_to_remove = hl.maximal_independent_set(; entries.i, entries.j, keep=False, tie_breaker=tie_breaker, keyed=False; ). locally_pruned_table = locally_pruned_table.annotate_globals(; variants_to_remove=variants_to_remove.aggregate(; hl.agg.collect_as_set(variants_to_remove.node.idx), _localize=False; ); ); return (; locally_pruned_table.filter(; locally_pruned_table.variants_to_remove.contains(hl.int32(locally_pruned_table.idx)), keep=False; ); .select(); .persist(); ). def _warn_if_no_intercept(caller, covariates):; if all([e._indices.axes for e in covariates]):; warning(; f'{caller}: model appears to have no intercept covariate.'; '\n To include an intercept, add 1.0 to the list of covariates.'; ); return True; return False. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33668,Energy Efficiency,reduce,reduces,33668,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54124,Energy Efficiency,reduce,reduces,54124,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:81671,Energy Efficiency,reduce,reduced,81671,"ot explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; covmat_Q, _ = hl.nd.qr(mt.covmat); mt = mt.annotate_globals(covmat_Q=covmat_Q); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= ma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83030,Energy Efficiency,reduce,reduced,83030,"); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= max_size, ht.G_take)).T,; ); ht = ht.annotate(Q=((ht.y_residual @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:89697,Energy Efficiency,reduce,reduced,89697,"etric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:112204,Energy Efficiency,efficient,efficient,112204," yourself using; one of the entry modification methods: :meth:`.MatrixTable.annotate_entries`,; :meth:`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117823,Energy Efficiency,efficient,efficient,117823," locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic varia",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:134975,Energy Efficiency,efficient,efficiently,134975,"of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / block_size)^2`` if all blocks are computed. Warning; -------; See all warnings on :meth:`.BlockMatrix.from_entry_expr`. In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; :meth:`.BlockMatrix.write_from_entry_expr`. The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; :meth:`ld_matrix`. To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index.; """"""; bm = BlockMatrix.from_entry_expr(entry_expr, mean_impute=True, center=True, normalize=True, block_size=block_size); return bm @ bm.T. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; block_size=nullable(int),; ); def ld_matrix(entry_expr, locus_expr, radius, coord_expr=None, block_size=None) -> BlockMatrix:; """"""Com",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:17244,Integrability,Wrap,Wrapping,17244,"d isinstance(weights, list):; raise ValueError(""When y is a single list, weights should be a single expression.""); elif not y_is_list and isinstance(weights, list):; raise ValueError(""When y is a single expression, weights should be a single expression.""). weights = wrap_to_list(weights) if weights is not None else None. for e in itertools.chain.from_iterable(y) if is_chained else y:; analyze('linear_regression_rows_nd/y', e, mt._col_indices). for e in covariates:; analyze('linear_regression_rows_nd/covariates', e, mt._col_indices). _warn_if_no_intercept('linear_regression_rows_nd', covariates). x_field_name = Env.get_uid(); if is_chained:; y_field_name_groups = [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_name_groups), itertools.chain.from_iterable(y))); if weights is not None and len(weights) != len(y):; raise ValueError(""Must specify same number of weights as groups of phenotypes""); else:; y_field_name_groups = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_name_groups, y)); # Wrapping in a list since the code is written for the more general chained case.; y_field_name_groups = [y_field_name_groups]; if weights is not None and len(weights) != 1:; raise ValueError(""Must specify same number of weights as groups of phenotypes""). cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); weight_field_names = list(f'__weight_for_group_{i}' for i in range(len(weights))) if weights is not None else None; weight_dict = dict(zip(weight_field_names, weights)) if weights is not None else {}. row_field_names = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows_nd'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **weight_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_field_names,; col_key=[],; entry_exprs={x_field_name: x},; ). entries_field_name = 'ent'",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29304,Integrability,depend,depends,29304,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31677,Integrability,depend,dependent,31677," =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44720,Integrability,depend,depending,44720,"type = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:49794,Integrability,depend,depends,49794,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52133,Integrability,depend,dependent,52133,"fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58794,Integrability,depend,dependent,58794,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,Integrability,integrat,integration,77492,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,Integrability,integrat,integration,77668," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79251,Integrability,depend,dependent,79251,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,Integrability,integrat,integration,93214,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,Integrability,integrat,integration,93390,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94956,Integrability,depend,dependent,94956,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,Integrability,integrat,integration,106025,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162549,Integrability,depend,depend,162549," based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]])",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:226,Modifiability,Config,Configuration,226,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8336,Modifiability,variab,variable,8336,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8376,Modifiability,variab,variables,8376,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8881,Modifiability,variab,variable,8881,"heck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8947,Modifiability,variab,variables,8947,"heck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:10512,Modifiability,variab,variable,10512,"of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math::. \mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). Boolean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; -",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:11957,Modifiability,variab,variable,11957,"ean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_ind",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14507,Modifiability,config,config,14507,"_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('linear_regression_rows', covariates). x_field_name = Env.get_uid(); if is_chained:; y_field_names = [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14762,Modifiability,config,config,14762,"= [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list). if is_chained and any(len(lst) == 0 for lst in y):; raise ValueError(""'linear_regression_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22005,Modifiability,variab,variables,22005,".annotate_globals(; weight_nds=hl.enumerate(ht.kept_samples).starmap(; lambda group_idx, group_sample_indices: hl.nd.array(; group_sample_indices.map(lambda group_sample_idx: ht.weight_arrays[group_sample_idx][group_idx]); ); ); ); ht = ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22029,Modifiability,variab,variables,22029,".annotate_globals(; weight_nds=hl.enumerate(ht.kept_samples).starmap(; lambda group_idx, group_sample_indices: hl.nd.array(; group_sample_indices.map(lambda group_sample_idx: ht.weight_arrays[group_sample_idx][group_idx]); ); ); ); ht = ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22241,Modifiability,variab,variables,22241," ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_rows_with_themselves(ht.Qtys[i].T); ),; ). ht = setup_globals(ht). def process_block(block):; rows_in_block = hl.len(block). # Processes one block group based on given idx. Returns a single struct.; def process_y_group(idx):; if weights ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22265,Modifiability,variab,variables,22265," ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_rows_with_themselves(ht.Qtys[i].T); ),; ). ht = setup_globals(ht). def process_block(block):; rows_in_block = hl.len(block). # Processes one block group based on given idx. Returns a single struct.; def process_y_group(idx):; if weights ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26317,Modifiability,variab,variable,26317,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26366,Modifiability,variab,variable,26366,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27753,Modifiability,variab,variables,27753," dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28043,Modifiability,variab,variable,28043,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28099,Modifiability,variab,variable,28099,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28162,Modifiability,variab,variable,28162,"stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28484,Modifiability,variab,variable,28484,"tes=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36375,Modifiability,variab,variable,36375,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38358,Modifiability,config,config,38358,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38665,Modifiability,config,config,38665," y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hsta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47210,Modifiability,variab,variable,47210,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47259,Modifiability,variab,variable,47259,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48243,Modifiability,variab,variables,48243,"or association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48533,Modifiability,variab,variable,48533,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48589,Modifiability,variab,variable,48589,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48652,Modifiability,variab,variable,48652,"nary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48974,Modifiability,variab,variable,48974,"... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:56820,Modifiability,variab,variable,56820,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58804,Modifiability,variab,variable,58804,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61253,Modifiability,variab,variable,61253,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61301,Modifiability,variab,variable,61301,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61950,Modifiability,variab,variable,61950,"t64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63611,Modifiability,config,config,63611,"se ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowere",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63919,Modifiability,config,config,63919,"_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=d",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71061,Modifiability,variab,variable,71061,"vmat.T) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). fisher_div_score = hl.nd.solve(fisher, score, no_crash=True); chi_sq = hl.or_missing(~fisher_div_score.failed, score @ fisher_div_score.solution); p = hl.pchisqtail(chi_sq, dof); return chi_sq, p. [docs]def linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True):; r""""""Initialize a linear mixed model from a matrix table. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). [docs]@typecheck(; entry_expr=expr_float64,; model=LinearMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73297,Modifiability,variab,variables,73297,"nder the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; r &= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73602,Modifiability,rewrite,rewrite,73602,"&= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73687,Modifiability,variab,variables,73687,"&= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74340,Modifiability,variab,variables,74340,"he variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; se",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74404,Modifiability,variab,variables,74404," math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74490,Modifiability,variab,variables,74490," r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74690,Modifiability,variab,variables,74690,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:75170,Modifiability,variab,variables,75170,"rm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79261,Modifiability,variab,variable,79261,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79365,Modifiability,variab,variable,79365,"----+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the nul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83827,Modifiability,variab,variables,83827,"al @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenval",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84346,Modifiability,variab,variables,84346," # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84631,Modifiability,variab,variables,84631,"re symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84689,Modifiability,variab,variables,84689,"erted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84759,Modifiability,variab,variable,84759,"t will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weight",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94966,Modifiability,variab,variable,94966,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95070,Modifiability,variab,variable,95070,"----+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. -",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106727,Modifiability,variab,variable,106727,"-------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logist",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109291,Modifiability,config,config,109291,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109689,Modifiability,config,config,109689,"= '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(p_value, approximate=True):; chisq = hl.qchisqtail(p_value, 1); if approximate:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.agg.approx_quantiles(chisq, 0.5)); else:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122546,Modifiability,variab,variable-length,122546,"New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (; 'intergenic_consequences',; 'motif_feature_consequences',; 'regulatory_feature_consequences',; 'transcript_consequences',; ); }). if isinstance(ds, Table):; return split.annotate(**update_rows_expression).drop('",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12126,Performance,perform,perform,12126,"ssion model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chain",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12210,Performance,perform,performance,12210,"ing, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list); if is_chained and any(len(lst) ==",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27985,Performance,perform,performs,27985,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48475,Performance,perform,performs,48475,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78681,Performance,perform,perform,78681,"d a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will ha",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94405,Performance,perform,perform,94405,"a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103176,Performance,scalab,scalable,103176,"ero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:108486,Performance,optimiz,optimized,108486,"; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117351,Performance,throughput,throughput,117351,"); .or_error(""Found non-left-aligned variant in split_multi""); ). return hl.bind(error_on_moved, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). return split_rows(hl.sorted(kept_alleles.map(make_struct)), permit_shuffle); else:. def make_struct(i, cond):; def struct_or_empty(v):; return hl.case().when(cond(v.locus), hl.array([new_struct(v, i)])).or_missing(). return hl.bind(struct_or_empty, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). def make_array(cond):; return hl.sorted(kept_alleles.flatmap(lambda i: make_struct(i, cond))). left = split_rows(make_array(lambda locus: locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split tho",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:165589,Performance,queue,queue,165589,"tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=newPL,; ).drop('__old_to_new_no_na'). @typecheck(mt=MatrixTable, call_field=str, r2=numeric, bp_window_size=int, memory_per_core=int); def _local_ld_prune(mt, call_field, r2=0.2, bp_window_size=1000000, memory_per_core=256):; bytes_per_core = memory_per_core * 1024 * 1024; fraction_memory_to_use = 0.25; variant_byte_overhead = 50; genotypes_per_pack = 32; n_samples = mt.count_cols(); min_bytes_per_core = math.ceil((1 / fraction_memory_to_use) * 8 * n_samples + variant_byte_overhead); if bytes_per_core < min_bytes_per_core:; raise ValueError(""memory_per_core must be greater than {} MB"".format(min_bytes_per_core // (1024 * 1024))); bytes_per_variant = math.ceil(8 * n_samples / genotypes_per_pack) + variant_byte_overhead; bytes_available_per_core = bytes_per_core * fraction_memory_to_use; max_queue_size = int(max(1.0, math.ceil(bytes_available_per_core / bytes_per_variant))). info(f'ld_prune: running local pruning stage with max queue size of {max_queue_size} variants'). return Table(; ir.MatrixToTableApply(; mt._mir,; {; 'name': 'LocalLDPrune',; 'callField': call_field,; 'r2Threshold': float(r2),; 'windowSize': bp_window_size,; 'maxQueueSize': max_queue_size,; },; ); ).persist(). [docs]@typecheck(; call_expr=expr_call,; r2=numeric,; bp_window_size=int,; memory_per_core=int,; keep_higher_maf=bool,; block_size=nullable(int),; ); def ld_prune(call_expr, r2=0.2, bp_window_size=1000000, memory_per_core=256, keep_higher_maf=True, block_size=None):; """"""Returns a maximal subset of variants that are nearly uncorrelated within each window. .. include:: ../_templates/req_diploid_gt.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Prune variants in linkage disequilibrium by filtering a dataset to those variants returned; by :func:`.ld_prune`. If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split befo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167518,Performance,queue,queue,167518,"contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to :func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167581,Performance,queue,queue,167581,"func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:169100,Performance,queue,queue,169100,"y_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""; if block_size is None:; block_size = BlockMatrix.default_block_size(). if not 0.0 <= r2 <= 1:; raise ValueError(f'r2 must be in the range [0.0, 1.0], found {r2}'). if bp_window_size < 0:; raise ValueError(f'bp_window_size must be non-negative, found {bp_window_size}'). raise_unless_entry_indexed('ld_prune/call_expr', call_expr); mt = matrix_table_source('ld_prune/call_expr', call_expr). require_row_key_variant(mt, 'ld_prune'). # FIXME: remove once select_entries on a field is free; if call_expr in mt._fields_inverse:; field = mt._fields_inverse[call_expr]; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28055,Safety,predict,predicting,28055,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48545,Safety,predict,predicting,48545,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:81642,Safety,predict,predicted,81642,"ot explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; covmat_Q, _ = hl.nd.qr(mt.covmat); mt = mt.annotate_globals(covmat_Q=covmat_Q); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= ma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114112,Safety,avoid,avoids,114112,"iants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122411,Safety,avoid,avoids,122411,"ows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:133308,Safety,avoid,avoid,133308,"G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:. >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:. >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; -------; Rows with a constant value (i.e., zero variance) will result `nan`; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of :func:`.aggregators.stats`). Notes; -----; In this method, each row of entries is regarded as a vector with elements; defined by `entry_expr` and missing values mean-imputed per row.; The ``(i, j)`` element of the resulting block matrix is the correlation; between rows ``i`` and ``j`` (as 0-indexed by order in the matrix table;; see :meth:`~hail.MatrixTable.add_row_index`). The correlation of two vectors is defined as the; `Pearson correlation coeffecient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / bl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:139327,Safety,avoid,avoid,139327,"ly compute linkage disequilibrium between nearby; variants. Use :meth:`row_correlation` directly to calculate correlation; without windowing. More precisely, variants are 0-indexed by their order in the matrix table; (see :meth:`~hail.MatrixTable.add_row_index`). Each variant is regarded as a vector of; elements defined by `entry_expr`, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant. The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment ma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:380,Testability,Log,Log,380,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:7271,Testability,assert,assert,7271,"parse_locus_interval(x_contig, rg), rg.x_contigs), keep=True; ); if not include_par:; interval_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); mt = hl.filter_intervals(mt, hl.literal(rg.par, interval_type), keep=False). mt = mt.filter_rows((mt[aaf] > aaf_threshold) & (mt[aaf] < (1 - aaf_threshold))); mt = mt.annotate_cols(ib=agg.inbreeding(mt.call, mt[aaf])); kt = mt.select_cols(; is_female=hl.if_else(; mt.ib.f_stat < female_threshold, True, hl.if_else(mt.ib.f_stat > male_threshold, False, hl.missing(tbool)); ),; **mt.ib,; ).cols(). return kt. def _get_regression_row_fields(mt, pass_through, method) -> Dict[str, str]:; row_fields = dict(zip(mt.row_key.keys(), mt.row_key.keys())); for f in pass_through:; if isinstance(f, str):; if f not in mt.row:; raise ValueError(f""'{method}/pass_through': MatrixTable has no row field {f!r}""); if f in row_fields:; # allow silent pass through of key fields; if f in mt.row_key:; pass; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8322,Testability,test,test,8322,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:10629,Testability,test,tested,10629,"1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math::. \mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). Boolean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rs",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:25855,Testability,test,test,25855,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26155,Testability,test,test,26155,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26303,Testability,test,test,26303,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26381,Testability,log,logistic,26381,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26430,Testability,log,logistic,26430,". res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26455,Testability,test,test,26455,". res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26614,Testability,test,test,26614,"; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regressi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26766,Testability,log,logistic,26766," [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mea",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26791,Testability,test,test,26791," [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mea",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26964,Testability,test,test,26964,",; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27321,Testability,test,test,27321,"r association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28024,Testability,test,test,28024,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28122,Testability,log,logistic,28122,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28334,Testability,test,test,28334,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28366,Testability,test,test,28366,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28391,Testability,test,test,28391,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28417,Testability,test,test,28417,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29319,Testability,test,test,29319,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29449,Testability,Test,Test,29449," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29849,Testability,test,testing,29849," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30052,Testability,test,testing,30052," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30165,Testability,test,testing,30165," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30312,Testability,test,tests,30312,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iter",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30333,Testability,log,logistic,30333,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iter",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30497,Testability,test,test,30497,"========================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30738,Testability,Test,Test,30738,"ual to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31417,Testability,test,testing,31417,"ields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence p",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31959,Testability,test,testing,31959,"for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression model",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32079,Testability,log,logistic,32079,"teration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32228,Testability,test,testing,32228,"ta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32527,Testability,log,logistic,32527,"to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32583,Testability,test,tests,32583,"to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32918,Testability,log,logistic,32918,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32934,Testability,log,logistic,32934,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33040,Testability,log,logistf,33040,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33062,Testability,log,logistf,33062,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33188,Testability,log,logfit,33188,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33241,Testability,log,logistf,33241,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33663,Testability,test,test,33663,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33872,Testability,test,test,33872,"nd linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Cour",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34157,Testability,test,testing,34157,"10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34260,Testability,log,logreg,34260,"- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34426,Testability,test,testing,34426," to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34557,Testability,log,logistic,34557," from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34595,Testability,test,tests,34595," from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34672,Testability,test,tests,34672,"rom small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Si",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35107,Testability,log,logistic,35107," for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35267,Testability,log,logistic,35267,"the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35287,Testability,test,tests,35287,"the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35351,Testability,test,tests,35351,"ended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed exp",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35944,Testability,test,test,35944,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35998,Testability,test,test,35998,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36973,Testability,test,test,36973,"', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37081,Testability,test,test,37081,"lumn-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37215,Testability,assert,assert,37215,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37283,Testability,log,logistic,37283,"is property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_ex",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38379,Testability,Log,LogisticRegression,38379,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38402,Testability,test,test,38402,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38409,Testability,test,test,38409,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38793,Testability,log,logreg,38793,"logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39311,Testability,log,logit,39311,"lectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39409,Testability,assert,assert,39409,"': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39437,Testability,assert,assert,39437,"': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39457,Testability,assert,assert,39457,"ame,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39631,Testability,log,log,39631,"TableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:40603,Testability,log,log,40603,"t_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). next_b = b + delta_b; next_mu = sigmoid(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = X.T @ (X * (next_mu * (1 - next_mu)).reshape(-1, 1)). return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(hl.abs(delta_b)); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43552,Testability,log,logit,43552,"it.select('n_iterations', 'converged', 'exploded'),; ). def logistic_score_test(X, y, null_fit):; m = X.shape[1]; m0 = null_fit.b.shape[0]; b = hl.nd.hstack([null_fit.b, hl.nd.zeros((hl.int32(m - m0)))]). X0 = X[:, 0:m0]; X1 = X[:, m0:]. mu = hl.expit(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43650,Testability,assert,assert,43650,"(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b));",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43678,Testability,assert,assert,43678,"(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b));",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43698,Testability,assert,assert,43698,"e; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariate",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43718,Testability,assert,assert,43718,"(y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44006,Testability,log,log,44006,"0, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44080,Testability,log,log,44080,"True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:46744,Testability,test,test,46744,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47048,Testability,test,test,47048,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47196,Testability,test,test,47196,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47274,Testability,log,logistic,47274,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47323,Testability,log,logistic,47323,"i_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47348,Testability,test,test,47348,"i_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47507,Testability,test,test,47507,"irth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47659,Testability,log,logistic,47659,"). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Bo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47684,Testability,test,test,47684,"). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Bo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47857,Testability,test,test,47857,"ovariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('l",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48514,Testability,test,test,48514,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48612,Testability,log,logistic,48612,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48824,Testability,test,test,48824,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48856,Testability,test,test,48856,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48881,Testability,test,test,48881,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48907,Testability,test,test,48907,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:49809,Testability,test,test,49809,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:49939,Testability,Test,Test,49939," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50339,Testability,test,testing,50339," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50542,Testability,test,testing,50542," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50655,Testability,test,testing,50655," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50802,Testability,test,tests,50802,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50823,Testability,log,logistic,50823,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50987,Testability,test,test,50987,"========================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level nu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:51228,Testability,Test,Test,51228,"ual to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:51873,Testability,test,testing,51873,"on and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produce",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52415,Testability,test,testing,52415,"reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression model",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52535,Testability,log,logistic,52535,"ed` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52684,Testability,test,testing,52684,"rdinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52983,Testability,log,logistic,52983,"to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53039,Testability,test,tests,53039,"to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53374,Testability,log,logistic,53374,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53390,Testability,log,logistic,53390,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53496,Testability,log,logistf,53496,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53518,Testability,log,logistf,53518,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53644,Testability,log,logfit,53644,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly alwa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53697,Testability,log,logistf,53697,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly alwa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54119,Testability,test,test,54119,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54329,Testability,test,test,54329,"nd linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54602,Testability,test,testing,54602,"00), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54705,Testability,log,logreg,54705," firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54871,Testability,test,testing,54871,".991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55002,Testability,log,logistic,55002,"he p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55040,Testability,test,tests,55040,"he p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55117,Testability,test,tests,55117,"uces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Si",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55552,Testability,log,logistic,55552," for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
